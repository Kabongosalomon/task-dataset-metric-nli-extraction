<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging the Feature Distribution in Transfer-based Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IMT Atlantique Orange Labs IMT Atlantique Orange Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IMT Atlantique Orange Labs IMT Atlantique Orange Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Pateux</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IMT Atlantique Orange Labs IMT Atlantique Orange Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging the Feature Distribution in Transfer-based Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification is a challenging problem due to the uncertainty caused by using few labelled samples. In the past few years, many methods have been proposed to solve few-shot classification, among which transferbased methods have proved to achieve the best performance. Following this vein, in this paper we propose a novel transfer-based method that builds on two successive steps: 1) preprocessing the feature vectors so that they become closer to Gaussian-like distributions, and 2) leveraging this preprocessing using an optimal-transport inspired algorithm (in the case of transductive settings). Using standardized vision benchmarks, we prove the ability of the proposed methodology to achieve state-of-the-art accuracy with various datasets, backbone architectures and few-shot settings. The code can be found at https://github.com/yhu01/PT-MAP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thanks to their outstanding performance, Deep Learning methods are widely considered for vision tasks such as object classification or detection. To reach top performance, these systems are typically trained using very large labelled datasets that are representative enough of the inputs to be processed afterwards.</p><p>However, in many applications, it is costly to acquire or to annotate data, resulting in the impossibility to create such large labelled datasets. In this context, it is challenging to optimize Deep Learning architectures considering the fact they typically are made of way more parameters than the dataset contains. This is why in the past few years, few-shot learning (i.e. the problem of learning with few labelled examples) has become a trending research subject in the field. In more details, there are two settings that authors often consider: a) "inductive few-shot", where only a few labelled samples are available during training and prediction is performed on each test input independently, and b) "transductive few-shot", where prediction is performed on a batch of (non-labelled) test inputs, allowing to take into account their joint distribution.</p><p>Many works in the domain are built based on a "learning to learn" guidance, where the pipeline is to train an optimizer <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref> with different tasks of limited data so that the model is able to learn generic experience for novel tasks. Namely, the model learns a set of initialization parameters that are in an advantageous position for the model to adapt to a new (small) dataset. Recently, the trend evolved towards using well-thoughtout transfer architectures (called backbones) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b5">6]</ref> trained one time on the same training data, but seen as a unique large dataset.</p><p>A main problem of using feature vectors extracted using a backbone architecture is that their distribution is likely to be complex, as the problem the backbone has been optimized for most of the time differs from the considered task. As such, methods that rely on strong assumptions about the data distributions are likely to fail in leveraging the quality of features. In this paper, we tackle the problem of transfer-based fewshot learning with a twofold strategy: 1) preprocessing the data extracted from the backbone so that it fits a particular distribution (i.e. Gaussian-like) and 2) leveraging this specific distribution thanks to a well-thought proposed algorithm based on maximum a posteriori and optimal transport (only in the case of transductive few-shot). Using standardized benchmarks in the field, we demonstrate the ability of the proposed method to obtain state-of-the-art accuracy, for various problems and backbone architectures in some inductive settings and most transductive ones. <ref type="figure">Figure 1</ref>: Illustration of the proposed method. First we extract feature vectors of all the inputs in D novel and preprocess them to obtain f S ∪ f Q . Note that the Power transform (PT) has the effect of mapping a skewed feature distribution into a gaussian-like distribution (h j (k) denotes the histogram of feature k in class j). In MAP, we perform Sinkhorn mapping with class center c j initialized on f S to obtain the class allocation matrix M * for f Q , and we update the class centers for the next iteration. After n steps we evaluate the accuracy on f Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A large volume of works in few-shot classification is based on meta learning <ref type="bibr" target="#b29">[30]</ref> methods, where the training data is transformed into few-shot learning episodes to better fit in the context of few examples. In this branch, optimization based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23</ref>] train a well-initialized optimizer so that it quickly adapts to unseen classes with a few epochs of training. Other works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4]</ref> utilize data augmentation techniques to artificially increase the size of the training datasets.</p><p>In the past few years, there have been a growing interest in transfer-based methods. The main idea consists in training feature extractors able to efficiently segregate novel classes it never saw before. For example, in <ref type="bibr" target="#b2">[3]</ref> the authors train the backbone with a distance-based classifier <ref type="bibr" target="#b21">[22]</ref> that takes into account the inter-class distance. In <ref type="bibr" target="#b20">[21]</ref>, the authors utilize self-supervised learning techniques <ref type="bibr" target="#b1">[2]</ref> to co-train an extra rotation classifier for the output features, improving the accuracy in few-shot settings. Many approaches are built on top of a feature extractor. For instance, in <ref type="bibr" target="#b37">[38]</ref> the authors implement a nearest class mean classifier to associate an input with a class whose centroid is the closest in terms of the 2 distance. In <ref type="bibr" target="#b17">[18]</ref> an iterative approach is used to adjust the class centers. In <ref type="bibr" target="#b12">[13]</ref> the authors build a graph neural neural network to gather the feature information from similar samples. Transferbased techniques typically reach the best performance on standardized benchmarks. Although many works involve feature extraction, few have explored the features in terms of their distribution <ref type="bibr" target="#b10">[11]</ref>. Often, assumptions are made that the features in a class align to a certain distribution, even though these assumptions are rarely experimentally discussed. In our work, we analyze the impact of the features distributions and how they can be transformed for better processing and accuracy. We also introduce a new algorithm to improve the quality of the association between input features and corresponding classes in typical few-shot settings.</p><p>Contributions. Let us highlight the main contributions of this work. (1) We propose to preprocess the raw extracted features in order to make them more aligned with Gaussian assumptions. Namely we introduce transforms of the features so that they become less skewed. (2) We use a wasserstein-based method to better align the distribution of features with that of the considered classes. (3) We show that the proposed method can bring large increase in accuracy with a variety of feature extractors and datasets, leading to state-of-the-art results in the considered benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section we introduce the problem settings. We discuss the training of the feature extractors, the preprocessing steps that we apply on the trained features and the final classification algorithm. A summary of our proposed method is depicted in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem statement</head><p>We consider a typical few-shot learning problem. We are given a base dataset D base and a novel dataset D novel such that D base ∩ D novel = ∅. D base contains a large number of labelled examples from K different classes. D novel , also referred to as a task in other works, contains a small number of labelled examples (support set S), along with some unlabelled ones (query set Q), all from w new classes. Our goal is to predict the class of the unlabelled examples in the query set. The following parameters are of particular importance to define such a few-shot problem: the number of classes in the novel dataset w (called w-way), the number of labelled samples per class s (called s-shot) and the number of unlabelled samples per class q. So the novel dataset contains a total of w(s + q) samples, ws of them being labelled, and wq of them being those to classify. In the case of inductive few-shot, the prediction is performed independently on each one of the wq samples. In the case of transductive few-shot <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>, the prediction is performed considering all wq samples together. In the latter case, most works exploit the information that there are exactly q samples in each class. We discuss this point in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature extraction</head><p>The first step is to train a neural network backbone model using only the base dataset. In this work we consider multiple backbones, with various training procedures. Once the considered backbone is trained, we obtain robust embeddings that should generalize well to novel classes. We denote by f ϕ the backbone function, obtained by extracting the output of the penultimate layer from the considered architecture, with ϕ being the trained architecture parameters. Note that importantly, in all backbone architectures used in the experiments of this work, the penultimate layers are obtained by applying a ReLU function, so that all feature components coming out of f ϕ are nonnegative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature preprocessing</head><p>As mentioned in Section 2, many works hypothesize, explicitly or not, that the features from the same class are aligned with a specific distribution (often Gaussianlike). But this aspect is rarely experimentally verified. In fact, it is very likely that features obtained using the backbone architecture are not Gaussian. Indeed, usually the features are obtained after applying a relu function, and exhibit a positive distribution mostly concentrated around 0 (see details in the next section).</p><p>Multiple works in the domain <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b17">18]</ref> discuss the different statistical methods (e.g. normalization) to better fit the features into a model. Although these methods may have provable assets for some distributions, they could worsen the process if applied to an unexpected input distribution. This is why we propose to preprocess the obtained feature vectors so that they better align with typical distribution assumptions in the field. Namely, we use a power transform as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Power transform (PT).</head><p>Denote</p><formula xml:id="formula_0">v = f ϕ (x) ∈ (R + ) d , x ∈ D novel as the obtained features on D novel .</formula><p>We hereby perform a power transformation method, which is similar to Tukey's Transformation Ladder <ref type="bibr" target="#b31">[32]</ref>, on the features. We then follow a unit variance projection, the formula is given by:</p><formula xml:id="formula_1">f (v) = (v+ ) β (v+ ) β 2 if β = 0 log (v+ ) log (v+ ) 2 if β = 0 ,<label>(1)</label></formula><p>where = 1e−6 is used to make sure that v+ is strictly positive and β is a hyper-parameter. The rationales of the preprocessing above are: (1) Power transforms have the functionality of reducing the skew of a distribution, adjusted by β, (2) Unit variance projection scales the features to the same area so that large variance features do not predominate the others. This preprocessing step is often able to map data from any distribution to a close-to-Gaussian distribution. We will analyse this ability and the effect of power transform in more details in Section 4.</p><p>Note that β = 1 leads to almost no effect. More generally, the skew of the obtained distribution changes when β varies. For instance, if a raw distribution is rightskewed, decreasing β phases out the right skew, and phases into a left-skewed distribution when β becomes negative. After experiments, we found that β = 0.5 gives the most consistent results for our considered experiments. More details based on our considered experiments are available in Section 4.</p><p>This first step of feature preprocessing can be performed in both inductive and transductive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MAP</head><p>Let us assume that the preprocessed feature distribution for each class is Gaussian or Gaussian-like. As such, a well-positioned class center is crucial to a good prediction. In this section we discuss how to best estimate the class centers when the number of samples is very limited and classes are only partially labelled. In more details, we propose an Expectation-Maximization [7]like algorithm that will iteratively find the Maximum A Posteriori (MAP) estimates of the class centers.</p><p>We firstly show that estimating these centers through MAP is similar to the minimization of Wasserstein distance. Then, an iterative procedure based on a Wasserstein distance estimation, using the sinkhorn algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>, is designed to estimate the optimal transport from the initial distribution of the feature vectors to one that would correspond to the draw of samples from Gaussian distributions.</p><p>Note that in this step we consider what is called the "transductive" setting in many other few shot learning works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39]</ref>, where we exploit unlabelled samples during the procedure as well as priors about their relative proportions.</p><p>In the following, we denote by f S the set of feature vectors corresponding to labelled inputs and by f Q the set of feature vectors corresponding to unlabelled inputs. For a feature vector f ∈ f S ∪ f Q , we denote by (f ) the corresponding label. We use 0 &lt; i ≤ wq to denote the index of an unlabelled sample, so that f Q = (f i ) i , and we denote c j , 0 &lt; j ≤ w the estimated center for feature vectors corresponding to class j.</p><p>Our algorithm consists in several steps in which we estimate class centers from a soft allocation matrix M * , then we update the allocation matrix based on the newly found class centers and iterate the process. In the following paragraphs, we detail these steps.</p><p>Sinkhorn mapping. Considering using MAP estimation for the class centers, and assuming a Gaussian distribution for each class, we typically aim at solving:</p><formula xml:id="formula_2">{l(f i )}, {ĉ j } = arg max { (fi)}∈C,{cj } i P (f i |j = (f i )) = arg min { (fi)}∈C,{cj } i (f i − c (fi) ) 2 ,<label>(2)</label></formula><p>where C represents the set of admissible labelling sets. Let us point out that the last term corresponds exactly to the Wasserstein distance used in the Optimal Transport problem formulation <ref type="bibr" target="#b4">[5]</ref>.</p><p>Therefore, in this step we find the class mapping matrix that minimizes the Wasserstein distance. Inspired by the Sinkhorn algorithm <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b4">5]</ref>, we define the mapping matrix M * as follows:</p><formula xml:id="formula_3">M * = Sinkhorn(L, p, q, λ) = arg min M∈U(p,q) ij M ij L ij + λH(M),<label>(3)</label></formula><p>where U(p, q) ∈ R wq×w + is a set of positive matrices for which the rows sum to p and the columns sum to q. Formally, U(p, q) can be written as:</p><formula xml:id="formula_4">U(p, q) = {M ∈ R wq×w + |M1 w = p, M T 1 wq = q}, (4)</formula><p>p denotes the distribution of the amount that each unlabelled example uses for class allocation, and q denotes the distribution of the amount of unlabelled examples allocated to each class. Therefore, U(p, q) contains all the possible ways of allocating examples to classes. The cost function L ∈ R wq×w in Equation <ref type="formula" target="#formula_3">(3)</ref> consists of the euclidean distances between unlabelled examples and class centers, hence L ij denotes the euclidean distance between example i and class center j.</p><p>Here we assume a soft class mapping, meaning that each example can be "sliced" into different classes.</p><p>The second term on the right of Equation <ref type="formula" target="#formula_3">(3)</ref>  force the entropy to become smaller, so that the mapping is less homogeneous. This term also makes the objective function strictly convex <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> and thus a practical and effective computation. From lemma 2 in <ref type="bibr" target="#b4">[5]</ref>, the result of this Sinkhorn mapping has the typical form M * = diag(u) · exp(−L/λ) · diag(v).</p><p>Iterative center estimation. In this step, our aim is to estimate class centers. As shown in Algorithm 1, we initialize c j as the average of labelled samples belonging to class j. Then c j is iteratively re-estimated. At each iteration, we compute a mapping matrix M * on the unlabelled examples using the sinkhorn mapping. Along with labelled examples, we re-estimate c j (temporarily denoted µ j ) by weighted-averaging the features with their allocated portions for class j:</p><formula xml:id="formula_5">µ j = g(M * , j) = wq i=1 M * ij f i + f ∈f S , (f )=j f s + wq i=1 M * ij .<label>(5)</label></formula><p>This formula corresponds to the minimization of Equation <ref type="bibr" target="#b2">(3)</ref>. Note that labelled examples do not participate in the mapping process. Since their labels are known, we instead set allocations for their belonging classes to be 1 and to the others to be 0. Therefore, labelled examples have the largest possible weight when re-estimating the class centers.</p><p>Proportioned center update. In order to avoid taking risky harsh decisions in early iterations of the algorithm, we propose to proportionate the update of class centers using an inertia parameter. In more details, we update the center with a learning rate 0 &lt; α ≤ 1. When α is close to 0, the update becomes very slow, whereas α = 1 corresponds to directly allocating the newly found class centers:</p><formula xml:id="formula_6">c j ← c j + α(µ j − c j ).<label>(6)</label></formula><p>Final decision. After a fixed number of steps n steps , the rows of M * are interpreted as probabilities to belong to each class. The maximal value corresponds to the decision of the algorithm.</p><p>A summary of our proposed algorithm is presented in Algorithm 1. In <ref type="table" target="#tab_0">Table 1</ref> we summarize the main parameters and hyperparameters of the considered problem and proposed solution. The code is available at XXX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the performance of the proposed method using standardized few-shot classification datasets: miniImageNet <ref type="bibr" target="#b35">[36]</ref>, tieredImageNet <ref type="bibr" target="#b23">[24]</ref>, CUB <ref type="bibr" target="#b36">[37]</ref> and Algorithm 1: Proposed algorithm Parameters : w, s, q, λ, α, n steps Initialization : c j = 1 s · f ∈f S , (f )=j f repeat n steps times:</p><formula xml:id="formula_7">L ij = f i − c j 2 , ∀i, j M * = Sinkhorn(L, p = 1 wq , q = q1 w , λ) µ j = g(M * , j) c j ← c j + α(µ j − c j ) end returnˆ (f i ) = arg max j (M * [i, j])</formula><p>CIFAR-FS <ref type="bibr" target="#b0">[1]</ref>. The miniImageNet dataset contains 100 classes randomly chosen from ILSVRC-2012 <ref type="bibr" target="#b24">[25]</ref> and 600 images of size 84×84 pixels per class. It is split into 64 base classes, 16 validation classes and 20 novel classes. The tieredImageNet dataset is another subset of ImageNet, it consists of 34 high-level categories with 608 classes in total. These categories are split into 20 meta-training superclasses, 6 meta-validation superclasses and 8 meta-test superclasses, which corresponds to 351 base classes, 97 validation classes and 160 novel classes respectively. The CUB dataset contains 200 classes and has 11,788 images of size 84 × 84 pixels in total. Following <ref type="bibr" target="#b12">[13]</ref>, it is split into 100 base classes, 50 validation classes and 50 novel classes. The CIFAR-FS dataset has 100 classes, each class contains 600 images of size 32 × 32 pixels. The splits of this dataset are the same as those in miniImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>In order to stress the genericity of our proposed method with regards to the chosen backbone architecture and training strategy, we perform experiments using WRN <ref type="bibr" target="#b39">[40]</ref>, ResNet18 and ResNet12 <ref type="bibr" target="#b11">[12]</ref>, along with some other pretrained backbones (e.g. DenseNet <ref type="bibr" target="#b14">[15]</ref>). For each dataset we train the feature extractor with base classes, tune the hyperparameters with validation classes and test the performance using novel classes. Therefore, for each test run, w classes are drawn uniformly at random among novel classes. Among these w classes, s labelled examples and q unlabelled examples per class are uniformly drawn at random to form D novel . The WRN and ResNet are trained following <ref type="bibr" target="#b20">[21]</ref>. In the inductive setting, we use our proposed Power Transform followed by a basic Nearest Class Mean (NCM) classifier. In the transductive setting, the MAP or an alternative is applied after PT. In order to better segregate between feature vectors of corresponding classes for each task, we implement the "trans-mean-sub" <ref type="bibr" target="#b17">[18]</ref> before MAP where we separately subtract inputs by the means of labelled and unlabelled examples, followed by a unit hypersphere projection. All our experiments are performed using w = 5, q = 15, s = 1 or 5. We run 10,000 random draws to obtain mean accuracy score and indicate confidence scores (95%) when relevant. The tuned hyperparameters for miniImageNet are β = 0.5, λ = 10, α = 0.4 and n steps = 30 for s = 1; β = 0.5, λ = 10, α = 0.2 and n steps = 20 for s = 5. Hyperparameters for other datasets are detailed in the experiments below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art methods</head><p>In the first experiment, we conduct our proposed method on different benchmarks and compare the performance with other state-of-the-art solutions. The results are presented in <ref type="table" target="#tab_1">Table 2</ref>, we observe that our method with WRN as backbone reaches the state-ofthe-art performance for most cases in both inductive and transductive settings on all the benchmarks. In <ref type="table" target="#tab_2">Table 3</ref> we also implement our proposed method on tieredImageNet based on a pre-trained DenseNet121 backbone following the procedure described in <ref type="bibr" target="#b37">[38]</ref>.</p><p>From these experiments we conclude that the proposed method can bring an increase of accuracy with a variety of backbones and datasets, leading to competitive performance. In terms of execution time, we measured an average of 0.002s per run.</p><p>Performance on cross-domain settings. We also test our method in a cross-domain setting, where the backbone is trained with the base classes in miniIma-geNet but tested with the novel classes in CUB dataset. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the proposed method gives the best accuracy both in the case of 1-shot and 5-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Other experiments</head><p>Ablation study. To prove the interest of the ingredients on the proposed method in order to reach top performance, we report in <ref type="table" target="#tab_5">Tables 5 and 6</ref> the results of ablation studies. In <ref type="table" target="#tab_5">Table 5</ref>, we first investigate the impact of changing the backbone architecture. Together with previous experiments, we observe that the proposed method consistently achieves the best results for any fixed backbone architecture. We also report performance in the case of inductive few-shot using a simple Nearest-Class Mean (NCM) classifier instead of the iterative MAP procedure described in Section 3. We perform another experiment where we replace the MAP algorithm with a standard K-Means where centroids are initialized with the available labelled samples for each class. We can observe significant drops in accuracy, emphasizing the interest of the proposed MAP procedure to better estimate the class centers.</p><p>In <ref type="table" target="#tab_6">Table 6</ref> we show the impact of PT in the transductive setting, where we can see about 6% gain for 1-shot and Effect of Power Transform. To visualize the effect of PT on the feature distributions, we depict in <ref type="figure" target="#fig_1">Figure 2</ref> the distributions of an arbitrarily selected feature for 5 randomly selected novel classes of miniImageNet when using WRN, before and after applying PT. We observe quite clearly how PT is able to reshape the feature distributions to close-to-gaussian distributions. We observed similar behaviors with other datasets as well. Influence of the number of unlabelled samples. Small values of q lead to settings that are closer to the inductive case. In order to better understand the gain in accuracy due to having access to more unlabelled samples, we depict in <ref type="figure" target="#fig_5">Figure 4</ref> the evolution of accuracy as a function of q, when w = 5 is fixed. Interestingly, the accuracy quickly reaches a close-to-asymptotical plateau, emphasizing the ability of the method to soon exploit available information in the task.</p><p>Impact of class imbalance. In all previous transductive experiments, we assumed a balanced number of unlabelled samples per class. We now consider the case of 2 classes, where we vary the number of unlabelled examples q1 of class 1 with respect to that of class 2 (100 − q1). In <ref type="figure" target="#fig_3">Figure 3</ref> we depict: 1) the performance of the inductive version of our method (PT-NCM), which is independent of q1, 2) the performance of the proposed transductive method when the vector q is appropriately defined (knowing the proportion of elements in class 1 vs. class 2), and 3) a mixed case where we expect at least 30 elements in both classes but do not know exactly how many (q = <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b29">30]</ref>). Interestingly, we observe that the transductive setting still outperforms the inductive ones even when the proportion of elements in both classes is only approximately known.  When q 1 = 1, we obtain the most imbalanced case, whereas q 1 = 50 corresponds to a balanced case.</p><p>Hyperparameter tuning. In the next experiment we tune β, λ and α on the validation classes of each dataset, and then apply them to test our model on  novel classes. We vary each hyperparamter in a certain range and observe the evolution of accuracy to choose the peak that corresponds to the highest prediction. For example, the evolving curve for β, λ and α with miniImageNet are presented in <ref type="figure" target="#fig_5">Figure 4</ref> (2) to <ref type="bibr" target="#b3">(4)</ref>. For comparison purposes, we also trace the corresponding curves on novel classes. We draw a dash line on the hyperparameter values where the accuracy on the vali-  <ref type="bibr" target="#b33">[34]</ref> 46.21 ± 0.77% 66.03 ± 0.71% S2M2 R <ref type="bibr" target="#b20">[21]</ref> 48. <ref type="bibr" target="#b23">24</ref>  dation classes peaks, meaning that this is the chosen value resulting in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The following observations can be drawn from this experiment: 1) The evolving curves on validation classes (red) and novel classes (blue) have generally similar trend for each hyperparameter. In particular, two curves peak at the same β (β = 0.5) and λ (λ = 10),  meaning that validation classes and novel classes share the same β and λ that reach the highest accuracy. 2) A small λ tends to lead to a homogeneous class partition for M * , where each sample are uniformly allocated to w classes. Hence the sharp drop on the accuracy when λ &lt; 5. 3) A too small α results in an insufficient class center update. On the contrary, the impact on a large α is relatively mild. Overall, it is interesting to point out the little sensitivity of the proposed method accuracy with regards to hyperparameter tuning.</p><p>We followed this procedure to find the tuned hyperparameters for each dataset. Therefore, we obtained that working with CUB leads to the the same hyperparameters as miniImageNet. For tieredImageNet and CIFAR-FS, the best accuracy are obtained on validation classes when β = 0.5, λ = 10, α = 0.3 for s = 1; β = 0.5, λ = 10, α = 0.2 for s = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we introduced a new pipeline to solve the few-shot classification problem. Namely, we proposed to firstly preprocess the raw feature vectors to better align to a Gaussian distribution and then we designed an optimal-transport inspired iterative algorithm to estimate the class centers. Our experimental results on standard vision benchmarks reach state-of-the-art accuracy, with important gains in both 1-shot and 5shot classification. Moreover, the proposed method can bring gains with a variety of feature extractors, with few hyperparameters. Thus we believe that the proposed method is applicable to many practical problems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials 6 ADDITIONAL EXPERIMENTS</head><p>In this section, we provide additional experiments and results on our proposed method, including a combination of multi-backbones in terms of features. We demonstrate that with PT-MAP, a direct concatenation of different backbone features can increase the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effect of PT-MAP on pre-trained backones</head><p>In the paper we trained the different backbones following <ref type="bibr" target="#b20">[21]</ref>. To evaluate the generosity of our proposed method, here we tested the performance of PT-MAP based on a set of pre-trained backbones <ref type="bibr" target="#b37">[38]</ref> that follow a different training procedure. As in <ref type="table" target="#tab_7">Table 7</ref>, we can see that our method is still able to bring a large accuracy increase on all backbones, no matter what their training procedure is. Therefore, this proves the generosity of PT-MAP, which can be applied in various applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of PT-MAP on multi-backbones</head><p>To further investigate the effect of our proposed method on the features, we perform a direct concatenation of raw feature vectors extracted from multiple backbones before PT-MAP. In <ref type="table" target="#tab_8">Table 8</ref> we chose the feature vectors from three backbones (WRN, ResNet18 and ResNet12) and evaluated the performance with different combinations. We observe that a direct concatenation, depending on the backbones, can bring about 1% gain in both 1-shot and 5-shot settings. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>denotes the entropy of M: H(M) = − ij M ij log M ij , regularized by a hyper-parameter λ. Increasing λ would</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Distributions of an arbitrarily chosen feature for 5 novel classes before (a) and after (b) PT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>= [q1, 100 − q1])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy of 2-ways classification on miniImagenet (1-shot) with unevenly distributed query data for each class in different settings, where the total number of query inputs remains constant (total: 100 elements).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(1) represents 5-way 1-shot accuracy on miniImagenet, CUB and CIFAR-FS (backbone: WRN) as a function of q. (2), (3) and (4) represent 1-shot accuracy on miniImageNet (backbone: WRN) as a function of β, λ and α respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Important parameters and hyperparameters.</figDesc><table><row><cell></cell><cell cols="2">Novel dataset parameters</cell></row><row><cell>Notation</cell><cell>Value</cell><cell>Description</cell></row><row><cell>w</cell><cell>typically 5</cell><cell>number of classes</cell></row><row><cell>s</cell><cell>typically 1 or 5</cell><cell>number of labelled inputs per class</cell></row><row><cell>q</cell><cell>typically 15</cell><cell>number of unlabelled inputs per class</cell></row><row><cell></cell><cell cols="2">Proposed method hyperparameters</cell></row><row><cell>Notation</cell><cell>Range</cell><cell>Description</cell></row><row><cell>β</cell><cell>{−2, −1, −0.5, 0, 0.5, 1, 2}</cell><cell>coefficient to adjust distribution skew</cell></row><row><cell>λ</cell><cell>λ ∈ R +</cell><cell>regularization coefficient for sinkhorn mapping</cell></row><row><cell>α</cell><cell>0 &lt; α ≤ 1</cell><cell>learning rate for class center updates</cell></row><row><cell cols="2">4% gain for 5-shot in terms of accuracy.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>1-shot and 5-shot accuracy of state-of-the-art methods in the literature, compared with the proposed solution. We present results using WRN as backbones for our proposed solutions.</figDesc><table><row><cell>miniImageNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: 1-shot and 5-shot accuracy of state-of-the-art</cell></row><row><cell cols="3">methods on tieredImageNet.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">tieredImageNet</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>ProtoNet [28]</cell><cell>ConvNet4</cell><cell>53.31 ± 0.89%</cell><cell>72.69 ± 0.74%</cell></row><row><cell>LEO [26]</cell><cell>WRN</cell><cell>66.33 ± 0.05%</cell><cell>81.44 ± 0.09%</cell></row><row><cell>SimpleShot [38]</cell><cell cols="3">DenseNet121 71.32 ± 0.22% 86.66 ± 0.15%</cell></row><row><cell>PT+NCM(ours)</cell><cell cols="2">DenseNet121 69.96 ± 0.22%</cell><cell>86.45 ± 0.15%</cell></row><row><cell>DFMN-MCT [17]</cell><cell>ResNet12</cell><cell>80.89 ± 0.84%</cell><cell>87.30 ± 0.49%</cell></row><row><cell>TAFSSL [18]</cell><cell cols="2">DenseNet121 84.29 ± 0.25%</cell><cell>89.31 ± 0.15%</cell></row><row><cell>PT+MAP(ours)</cell><cell cols="3">DenseNet121 85.67 ± 0.26% 90.45 ± 0.14%</cell></row><row><cell>: Inductive setting.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">: Transductive setting.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: 1-shot and 5-shot accuracy of state-of-the-art</cell></row><row><cell cols="3">methods when performing cross-domain classification</cell></row><row><cell>(backbone: WRN).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Baseline++ [3]</cell><cell>40.44 ± 0.75%</cell><cell>56.64 ± 0.72%</cell></row><row><cell>Manifold Mixup</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of the proposed method in inductive and transductive settings, with different backbones, and comparison with K-Means and NCM baselines. 65.35 ± 0.20% (78.33) 83.87 ± 0.13% 76.67 ± 0.22% 86.73 ± 0.13% 82.92 ± 0.26% 88.82 ± 0.13% 80.57 ± 0.20% (88.33) 91.15 ± 0.10% 88.28 ± 0.19% 92.37 ± 0.10% 91.55 ± 0.19% 93.99 ± 0.10% 74.64 ± 0.21% (86.81) 87.64 ± 0.15% 83.69 ± 0.22% 89.19 ± 0.15% 87.69 ± 0.23% 90.68 ± 0.15%</figDesc><table><row><cell cols="2">Setting</cell><cell cols="2">Inductive</cell><cell></cell><cell cols="2">Transductive</cell></row><row><cell></cell><cell></cell><cell cols="2">(NCM baseline) Proposed PT+NCM</cell><cell cols="2">PT+K-Means</cell><cell cols="2">Proposed PT+MAP</cell></row><row><cell>Dataset</cell><cell cols="2">Backbone 1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell cols="2">ResNet12 (49.08) 62.68 ± 0.20%</cell><cell>(70.85) 81.99 ± 0.14%</cell><cell>72.73 ± 0.23%</cell><cell>84.05 ± 0.14%</cell><cell>78.47 ± 0.28%</cell><cell>85.84 ± 0.15%</cell></row><row><cell>miniImageNet</cell><cell cols="2">ResNet18 (47.63) 62.50 ± 0.20%</cell><cell>(72.89) 82.17 ± 0.14%</cell><cell>73.08 ± 0.22%</cell><cell>84.67 ± 0.14%</cell><cell>80.00 ± 0.27%</cell><cell>86.96 ± 0.14%</cell></row><row><cell cols="3">WRN ResNet12 (61.30) 78.40 ± 0.20% (55.31) CUB ResNet18 (58.92) 76.98 ± 0.20%</cell><cell>(82.83) 91.12 ± 0.10% (82.69) 90.56 ± 0.10%</cell><cell>87.35 ± 0.19% 87.16 ± 0.19%</cell><cell>92.31 ± 0.10% 91.97 ± 0.09%</cell><cell>90.96 ± 0.20% 91.10 ± 0.20%</cell><cell>93.77 ± 0.09% 93.78 ± 0.09%</cell></row><row><cell cols="3">WRN ResNet12 (52.50) 71.02 ± 0.22% (69.21) CIFAR-FS ResNet18 (56.40) 71.41 ± 0.22%</cell><cell>(74.16) 84.68 ± 0.16% (78.30) 85.50 ± 0.15%</cell><cell>78.39 ± 0.24% 79.95 ± 0.23%</cell><cell>85.73 ± 0.16% 86.74 ± 0.16%</cell><cell>82.45 ± 0.27% 84.80 ± 0.25%</cell><cell>87.33 ± 0.17% 88.55 ± 0.16%</cell></row><row><cell></cell><cell>WRN</cell><cell>(68.93)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of Power Transform in the transductive setting with different backbones on miniImageNet. ± 0.26% 88.82 ± 0.13% 80.00 ± 0.27% 86.96 ± 0.14% 78.47 ± 0.28% 85.84 ± 0.15%</figDesc><table><row><cell></cell><cell cols="2">WRN</cell><cell cols="2">ResNet18</cell><cell cols="2">ResNet12</cell></row><row><cell>PT MAP</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell>75.60 ± 0.29%</cell><cell>84.13 ± 0.16%</cell><cell>74.48 ± 0.29%</cell><cell>82.88 ± 0.17%</cell><cell>72.04 ± 0.30%</cell><cell>80.98 ± 0.18%</cell></row><row><cell></cell><cell>82.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>1-shot and 5-shot accuracy (dataset: miniImagenet) on baseline and our proposed PT-MAP. ± 0.17% 63.25 ± 0.17% 58.18 ± 0.28% 70.79 ± 0.18% Mobilenet 55.70 ± 0.20% 77.46 ± 0.15% 73.58 ± 0.29% 82.81 ± 0.15% ResNet10 54.45 ± 0.21% 76.98 ± 0.15% 74.91 ± 0.29% 83.73 ± 0.15% ResNet18 56.06 ± 0.20% 78.63 ± 0.15% 77.28 ± 0.28% 85.13 ± 0.14% WRN 57.26 ± 0.21% 78.99 ± 0.14% 78.86 ± 0.28% 86.17 ± 0.14% DenseNet121 57.81 ± 0.21% 80.43 ± 0.15% 79.98 ± 0.28% 87.19 ± 0.13%</figDesc><table><row><cell></cell><cell></cell><cell>Baseline</cell><cell></cell><cell>PT-MAP</cell></row><row><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Conv4</cell><cell>33.17</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>1-shot and 5-shot accuracy (datasets: miniImageNet, CUB and CIFAR-FS) on our proposed PT-MAP with multi-backbones ('+' denotes a concatenation of backbone features).</figDesc><table><row><cell></cell><cell cols="2">miniImageNet</cell><cell cols="2">CUB</cell><cell cols="2">CIFAR-FS</cell></row><row><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>WRN</cell><cell>82.92%</cell><cell>88.82%</cell><cell>91.55%</cell><cell cols="2">93.99% 87.69%</cell><cell>90.68%</cell></row><row><cell>RN18</cell><cell>80.00%</cell><cell>86.96%</cell><cell>91.10%</cell><cell cols="2">93.78% 84.80%</cell><cell>88.55%</cell></row><row><cell>RN12</cell><cell>78.47%</cell><cell>85.84%</cell><cell>90.96%</cell><cell cols="2">93.77% 82.45%</cell><cell>87.33%</cell></row><row><cell>RN18+RN12</cell><cell>81.27%</cell><cell>87.89%</cell><cell>93.05%</cell><cell cols="2">95.15% 86.10%</cell><cell>89.67%</cell></row><row><cell>WRN+RN18</cell><cell cols="3">83.87% 89.64% 93.28%</cell><cell cols="2">95.27% 88.05%</cell><cell>91.18%</cell></row><row><cell>WRN+RN12</cell><cell>83.63%</cell><cell>89.47%</cell><cell>93.37%</cell><cell cols="2">95.35% 87.72%</cell><cell>90.98%</cell></row><row><cell cols="2">WRN+RN18+RN12 83.79%</cell><cell>89.63%</cell><cell cols="4">94.04% 95.76% 88.15% 91.25%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<title level="m">Meta-learning with differentiable closedform solvers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semisupervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for oneshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A two-stage approach to fewshot learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving accuracy of nonparametric transfer learning via vector segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Hacene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vermet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2966" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploiting unsupervised inputs for accurate few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08605</idno>
		<title level="m">Are few-shot learning benchmarks too simple</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transductive few-shot learning with meta-learned confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12017</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06670</idno>
		<title level="m">Tafssl: Task-adaptive feature sub-space learning for few-shot classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="488" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4136" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional wasserstein distances: Efficient optimal transportation on geometric domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Goes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Calculation of the wasserstein distance between probability distributions on the line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vallender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="784" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236</idno>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2770" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
