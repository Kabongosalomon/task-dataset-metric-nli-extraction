<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVING RNN TRANSDUCER BASED ASR WITH AUXILIARY TASKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
							<email>chunxiliu@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
							<email>frankz@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
							<email>suyounkim@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
							<email>ysaraf@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
							<email>gzweig@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVING RNN TRANSDUCER BASED ASR WITH AUXILIARY TASKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-recurrent neural network transducer</term>
					<term>speech recognition</term>
					<term>auxiliary learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end automatic speech recognition (ASR) models with a single neural network have recently demonstrated state-of-the-art results compared to conventional hybrid speech recognizers. Specifically, recurrent neural network transducer (RNN-T) has shown competitive ASR performance on various benchmarks. In this work, we examine ways in which RNN-T can achieve better ASR accuracy via performing auxiliary tasks. We propose (i) using the same auxiliary task as primary RNN-T ASR task, and (ii) performing context-dependent graphemic state prediction as in conventional hybrid modeling. In transcribing social media videos with varying training data size, we first evaluate the streaming ASR performance on three languages: Romanian, Turkish and German. We find that both proposed methods provide consistent improvements. Next, we observe that both auxiliary tasks demonstrate efficacy in learning deep transformer encoders for RNN-T criterion, thus achieving competitive results -2.0%/4.2% WER on LibriSpeech test-clean/other -as compared to prior top performing models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Building conventional hidden Markov model (HMM) based hybrid automatic speech recognition (ASR) systems include multiple engineered steps like bootstrapping, decision tree clustering of context-dependent phonetic/graphemic states <ref type="bibr" target="#b0">[1]</ref>, acoustic and language model training, etc. End-to-end ASR models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> use neural networks to transduce audio into word sequences, and can be learned in a single step from scratch. Specifically, recurrent neural network transducer (RNN-T) originally presented in <ref type="bibr" target="#b1">[2]</ref> also referred to as sequence transducer -has been shown preferable on numerous applications. For example, the model size of RNN-T is much more compact than conventional hybrid models, being favorable as an on-device recognizer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. It also has been demonstrated as a high-performing streaming model in extensive benchmarks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Such recent success has motivated the efforts to improve RNN-T from various aspects, e.g. model pretraining <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, generalization ability on long-form audios <ref type="bibr" target="#b14">[15]</ref>, training algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, speech enhancement <ref type="bibr" target="#b16">[17]</ref>, etc.</p><p>In this work, we make an attempt on improving RNN-T via auxiliary learning, which aims to improve the generalization ability of a primary task by training on additional auxiliary tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. While multitask learning <ref type="bibr" target="#b19">[20]</ref> may aim to improve the performance of multiple tasks simultaneously, auxiliary learning selectively serves to assist the primary task and only the primary task performance is in focus. Auxiliary learning has been studied extensively in reinforcement learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>, where pseudo-reward functions are designed to enable the main policy to be learned more efficiently. In the context of attention-based sequence-to-sequence (seq2seq) ASR models, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> show that learning encoders with auxiliary tasks of predicting phonemes or context-dependent phonetic HMM states (i.e. senones <ref type="bibr" target="#b23">[24]</ref>) can improve the primary ASR word error rate (WER). <ref type="bibr" target="#b24">[25]</ref> shows that using auxiliary syntactic and semantic tasks can improve the main low-resource machine translation task.</p><p>In this paper, we consider the application of auxiliary tasks to RNN-T based ASR. First, we design an auxiliary task to be the same ASR task, where the transducer encoder forks from an intermediate encoder layer, and both the primary branch and auxiliary branch perform ASR tasks. Note that in this way, both primary and auxiliary branches can provide posterior distributions over output labels -characters or wordpieces. Inspired by the prior works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, we exploit a symmetric Kullback-Leibler (KL) divergence loss between the output posterior distributions of primary and auxiliary branches, along with the standard RNN-T loss. Such mutual KL divergence loss is expected to implicitly penalize the inconsistent gradients from the primary and auxiliary losses with respect to their shared parameters, and relieve the optimization inconsistency across tasks <ref type="bibr" target="#b27">[28]</ref>. Overall, the knowledge distilled from auxiliary tasks help a model learn better representations shared between primary and auxiliary branches, by enabling the model to find a more robust (flatter) minima and to better generalize to test data <ref type="bibr" target="#b25">[26]</ref>.</p><p>Secondly, we propose an alternative auxiliary task of predicting context-dependent graphemic states, also referred to as chenones <ref type="bibr" target="#b28">[29]</ref>, as in standard HMM-based hybrid modeling. Similar to the auxiliary senone classification for improving attention-based seq2seq model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, we exploit chenone prediction for improving RNN-T without relying on language-specific phonemic lexicon. HMM-based graphemic hybrid ASR systems have been shown to achieve comparable performance to phonetic lexicon based approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref>, and still demonstrate state-of-the-art results on common benchmarks when compared to end-to-end models <ref type="bibr" target="#b31">[32]</ref>. In this paper, we examine if the context-dependent graphemic knowledge -from a decision tree clustering of tri-grapheme HMM states -can be complementary to the character or wordpiece (i.e. subword unit) modeling used in end-to-end ASR <ref type="bibr" target="#b12">[13]</ref>, and if the auxiliary chenone prediction task provides an avenue of distilling such context-dependent graphemic knowledge into RNN-T training by providing additional discriminative information.</p><p>To evaluate our proposed methods, we first use streamable ASR models on a challenging task of transcribing social media videos, in both low-resource (training data size 160 hours) and mediumresource ( 3K hours) conditions. Next, on LibriSpeech, we consider the application of auxiliary tasks to the sequence transducers built with deep transformer encoders. arXiv:2011.03109v2 [cs.CL] 9 Nov 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODELING APPROACHES</head><p>In this section we begin with a review of RNN-T based ASR, as originally presented in <ref type="bibr" target="#b1">[2]</ref>. Then we present our proposed auxiliary RNN-T task. Lastly, we describe the auxiliary context-dependent graphemic state prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RNN-T</head><p>ASR can be formulated as a sequence-to-sequence problem. Each speech utterance is parameterized as an input acoustic feature vector sequence x = {x1 . . . xT } = x1:T , where xt ∈ R d and T is the number of frames. Denote a grapheme set or a wordpiece inventory as Y, and the corresponding output sequence of length U as y = {y1 . . . yU } = y 1:U , where yu ∈ Y.</p><p>We defineȲ as Y ∪{∅}, where ∅ is the blank label. DenoteȲ * as the set of all sequences over output spaceȲ, and the element a ∈Ȳ * as an alignment sequence. Then we have the posterior probability as:</p><formula xml:id="formula_0">P (y|x) = a∈B −1 (y) P (a|x)<label>(1)</label></formula><p>where B :Ȳ * → Y * is a function that removes blank symbols from an alignment a. RNN-T model parameterizes the alignment probability P (a|x) and computes it with an encoder network (i.e. transcription network in <ref type="bibr" target="#b1">[2]</ref>), a prediction network and a joint network. The encoder performs a mapping operation, denoted as f enc , which converts x into another sequence of representations h enc 1:</p><formula xml:id="formula_1">T = {h enc 1 . . . h enc T }: h enc 1:T = f enc (x)<label>(2)</label></formula><p>A prediction network f pred , based on RNN or its variants, takes both its state vector and the previous non-blank output label yu−1 predicted by the model, to produce the new representation h pred u :</p><formula xml:id="formula_2">h pred 1:u = f pred (y 0:(u−1) )<label>(3)</label></formula><p>where u is output label index and y0 = ∅. The joint network f join is a feed-forward network that combines encoder output h enc t and prediction network output h pred u to compute logits zt,u:</p><formula xml:id="formula_3">zt,u = f join (h enc t , h pred u )<label>(4)</label></formula><p>P (yu|x1:t, y 1:(u−1) ) = Softmax(zt,u) <ref type="bibr" target="#b4">(5)</ref> such that the logits go through a softmax function and produce a posterior 1 distribution of the next output label overȲ. Finally, the RNN-T loss function is then the negative log posterior as in Eq. 1:</p><formula xml:id="formula_4">L RNN-T (θ) = − log P (y|x)<label>(6)</label></formula><p>where θ denotes the model parameters. Note that the encoder is analogous to an acoustic model, and the combination of prediction network and joint network can be seen as a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Auxiliary sequence transducer modeling</head><p>The RNN-T decoder can be viewed as a RNN language model. The RNN takes both its state vector and yu−1 to predict yu, so implicitly predicting yu is conditioned on the whole label history y1 . . . yu−1 as in Eq. 5. Since the label history can be very informative in predicting the next output label, we conjecture that the posterior entropy overȲ computed by Eq. 5 may be excessively reduced, resulting in encoder undertraining. In other words, if the decoder has played a major role in predicting each yu by such teacher forcing procedure, which can still result in a reasonable training loss, the encoder may underfit the input x, and the resulting generalization can be worse than a model with an adequately trained encoder. Additionally, gradient flow <ref type="bibr" target="#b33">[34]</ref> through a deep neural network architecture is difficult in general, due to the gradient vanishing/exploding problem at lower layers. Although we could add shortcut connections <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> across encoder layers that would help gradient flow through the encoder, it does not address the encoder undertraining problem -if the posterior of Eq. 5 has been peaked at the true label due to the strong cue from previous label history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Auxiliary RNN-T criterion</head><p>An alternative proposal to increase the gradient signal is based on connecting auxiliary classifiers to intermediate layers directly <ref type="bibr" target="#b36">[37]</ref>. In this work, to address encoder underfitting and provide the encoder with more backward gradients, we take the approach of connecting an auxiliary branch to an intermediate encoder layer and applying the same RNN-T loss function.</p><p>As in <ref type="figure" target="#fig_0">Figure 1</ref>, given an L-layer encoder network, denote h enc,l as the hidden activations of an intermediate layer l, where 1 l &lt; L. h enc,l goes through a one-hidden-layer multi-layer perceptron (MLP), parameterized by φ l , and use the same decoder to compute the logits of auxiliary branch:</p><formula xml:id="formula_5">z aux,l t,u = f join (MLP(h enc,l t ), h pred u )<label>(7)</label></formula><p>P aux,l (yu|x1:t, y 1:(u−1) ) = Softmax(z aux,l t,u )</p><p>such that we can apply another RNN-T objective function to this auxiliary branch, and the overall objective function becomes:</p><formula xml:id="formula_7">L(θ, φ) = L RNN-T (θ) + λauxL RNN-T (θ enc shared , φ) = − log P (y|x) − λaux log P aux,l (y|x)<label>(9)</label></formula><p>where θ denotes the parameters of primary branch including the whole encoder and decoder, and θ enc shared denotes the encoder layers 1 -l shared by primary and auxiliary branches, and λaux is a weighting parameter. Note that the auxiliary branch requires a decoder to compute h pred u and then z aux,l t,u . Instead of adding another decoder specifically for auxiliary branch, we propose to share the primary decoder during the forward pass; however, we do not update the decoder parameters if the gradients are back propagated from the auxiliary RNN-T loss. Because the auxiliary loss is to address the encoder underfitting issue, decoder is not explicitly learned to fit the auxiliary objective function.</p><p>Note that for the auxiliary model, we connect a nonlinear MLP (Eq. 7) -rather than a single linear layer -to the intermediate encoder layer l. Since the lower encoder layers are focused on feature extraction rather than the meaningful final label prediction, directly encouraging discrimination in the low-level representations is suboptimal. This is similar to the primary branch, where additional encoder layers of l + 1 to L are added on top of layer l; thus, adding the MLP allows for a similar coarse-to-fine architecture, and the shared encoder layers play a more consistent role for both branches.</p><p>Finally, when an encoder has a large network depth, we can apply such criterion to multiple encoder layers. Denote Φ as a set of encoder layer indices that are connected with each auxiliary criterion, and Φ ⊆ {1 . . . L − 1}. Denote I as a binary indicator function as</p><formula xml:id="formula_8">I(l) = 1, l ∈ Φ 0, l / ∈ Φ<label>(10)</label></formula><p>where 1 l &lt; L. Then Eq. 9 becomes</p><formula xml:id="formula_9">L(θ, φ Φ ) = L RNN-T (θ) + λauxL RNN-T (θ enc shared , φ Φ ) = − log P (y|x) − λaux L−1 l=1 I(l) log P aux,l (y|x)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Auxiliary symmetric KL divergence criterion</head><p>Further, prior works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> show that aligning the pairwise posterior distributions of multiple (sub)networks in a mutual learning strategy achieves better performance than learning independently. Thus, other than the supervised learning objective function, i.e. RNN-T criterion, we also explore an additional symmetric KL divergence criterion between the output posterior distributions of both branches:</p><formula xml:id="formula_10">L KL (θ, φ Φ ) = 1 T T t=1 L−1 l=1 1 U U u=1 I(l) DKL(P (yu|x1:t, y 1:(u−1) ) P aux,l (yu|x1:t, y 1:(u−1) ) + DKL(P aux,l (yu|x1:t, y 1:(u−1) ) P (yu|x1:t, y 1:(u−1) )<label>(12)</label></formula><p>where the posteriors are given by Eq. 5 and 8. Such KL divergence criterion can also guide the auxiliary branch with the supervision signals from primary branch, as a knowledge distillation procedure. As analyzed in <ref type="bibr" target="#b27">[28]</ref>, the gradients of multiple loss functions can be counteractive, and such KL loss penalizes the inconsistent gradients with respect to their shared parameters. Thus, the training objective can be:</p><formula xml:id="formula_11">L(θ, φ Φ ) = L RNN-T (θ) + λauxL KL (θ, φ Φ )<label>(13)</label></formula><p>However, the direct application of RNN-T criterion to the auxiliary model can still be useful, since the auxiliary branch thus always contributes meaningful gradients before the primary model outputs are informative. Therefore, the overall training objective becomes: Finally, after training, we discard the auxiliary branch and there is no additional computation overhead for ASR decoding.</p><formula xml:id="formula_12">L(θ, φ Φ ) = L RNN-T (θ) + λaux(L RNN-T (θ enc shared , φ Φ ) + L KL (θ, φ Φ ))<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Auxiliary context-dependent graphemic state prediction</head><p>In an HMM-based phonetic hybrid ASR system, the triphone HMM states are tied via traditional decision tree clustering <ref type="bibr" target="#b0">[1]</ref>. Such a set of tied triphone HMM states -also referred to as context-dependent phonetic states or senones <ref type="bibr" target="#b23">[24]</ref> -are used as the output units for the neural network based acoustic model. To further remove the need of a pronunciation lexicon, context-dependent graphemic hybrid models have been developed, and the tri-grapheme HMM states are tied instead. Accordingly, the neural network output units become tied tri-grapheme states, i.e. chenones <ref type="bibr" target="#b28">[29]</ref>, and the training criterion is cross entropy (CE) loss in conventional hybrid CE models. While RNN-T uses context-independent graphemes or wordpieces as output units, adding the chenone prediction supervision to encoder layers can transfer complementary tri-grapheme knowledge, encouraging diverse and discriminative encoder representations. Then we can apply such CE criterion to multiple encoder layers. Similarly, given an L-layer encoder, denote Φ as a set of encoder layer indices that are connected to chenone prediction, and Φ ⊆ {1 . . . L}. Denote I as a binary indicator function, and 1 l L. As in <ref type="figure" target="#fig_1">Figure  2</ref>, if I(l) = 1, h enc,l goes through a one-hidden-layer multi-layer perceptron (MLP) 2 , parameterized by φ l , and then a softmax function to provide a posterior distribution over chenone label set S:</p><formula xml:id="formula_13">P (st|h enc,l t ) = Softmax(MLP(h enc,l t ))<label>(15)</label></formula><p>where st ∈ S, and the auxiliary CE loss is</p><formula xml:id="formula_14">L CE (θ enc shared , φ Φ ) = − 1 T T t=1 L l=1 I(l) log P (st|h enc,l t )<label>(16)</label></formula><p>The overall training objective is:</p><formula xml:id="formula_15">L(θ, φ Φ ) = L RNN-T (θ) + λceL CE (θ enc shared , φ Φ )<label>(17)</label></formula><p>where λce is a tunable weighting parameter. We first evaluate our proposed approaches on our in-house Romanian, Turkish and German video datasets, which are sampled from public social media videos and de-identified before transcription. These videos contain a diverse range of acoustic conditions, speakers, accents and topics. The test sets for each language are composed of clean and noisy categories, with noisy category being more acoustically challenging than clean. The dataset sizes are shown in <ref type="table" target="#tab_0">Table  1</ref>. Moreover, we also perform evaluations on the public LibriSpeech dataset <ref type="bibr" target="#b37">[38]</ref>. Input acoustic features are 80-dimensional log-mel filterbank coefficients with 25 ms window size, and we apply mean and variance normalization. We apply the frequency and time masking as in the policy LD from SpecAugment <ref type="bibr" target="#b38">[39]</ref>, with p = 0.2 on video datasets and p = 1.0 on LibriSpeech. We perform speed perturbation <ref type="bibr" target="#b39">[40]</ref> of the training data, and produce three versions of each audio with speed factors 0.9, 1.0 and 1.1. The training data size is thus tripled. For the low-resource Romanian, we further apply another 2-fold data augmentation based on additive noise as in <ref type="bibr" target="#b40">[41]</ref>, and the training data size is thus 6 times the size of original train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">System implementation details</head><p>For each video language, RNN-T output labels consist of a blank label and 255 wordpieces generated by the unigram language model algorithm from SentencePiece toolkit <ref type="bibr" target="#b41">[42]</ref>. To provide chenone labels (Section 2.3), forced alignments are generated via a graphemic hybrid model <ref type="bibr" target="#b28">[29]</ref> for each language, and the number of unique chenone labels range from 7104 to 9272.</p><p>For video datasets, we build each RNN-T encoder based on latency-controlled bidirectional long short-term memory (LC-BLSTM) network <ref type="bibr" target="#b42">[43]</ref>. Each encoder is a 5-layer LC-BLSTM network with 800 hidden units in each layer and direction, and dropout 0.3. Two subsampling layers with stride 2 are applied after first and second LC-BLSTM layer. The prediction network is a 2-layer LSTM of 160 hidden units for Romanian, and 512 units for Turkish and German, with dropout 0.3. Each joint network has 1024 hidden units, and a softmax layer of 256 units for blank and wordpieces. For all neural network implementation, we use an in-house extension of PyTorch-based fairseq <ref type="bibr" target="#b43">[44]</ref> toolkit. All experiments use multi-GPU and mixed precision training supported in fairseq, Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, and tri-stage <ref type="bibr" target="#b38">[39]</ref> learning rate schedule with peak learning rate 4e −4 .</p><p>For LibriSpeech, we experiment with two VGG transformer encoders of 24 and 36 layers as in <ref type="bibr" target="#b31">[32]</ref>, except that we use three VGG blocks with stride 2 in the first two blocks and 1 in the third block. Each transformer layer has an embedding dimension 512 and attention heads 8; feed-forward network (FFN) size is 2048 for 24-layer transformer, and 3072 for the 36-layer. Wordpiece size is 1000 for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Auxiliary RNN-T modeling results on video datasets</head><p>We first perform experimental evaluations on the low-resource language Romanian, and obtain the optimal λaux in Eq. 9, 13 and 14. ASR word error rate (WER) results are shown in <ref type="table">Table 2</ref>. For both clean and noisy test sets, we first compute the relative WER reduction (WERR) over respective baseline as a percentage, and then take the unweighted average of two percentages, which we refer to as an average WERR.</p><p>As shown in <ref type="table">Table 2</ref>, for auxiliary RNN-T loss (Eq. 9), we vary λaux over {0.1, 0.3, 0.6}, and observe 0.3 gives the lowest WER on valid set. So we proceed with λaux = 0.3 to decode the clean and noisy test sets, and see an average WERR 4.5%. Similarly for the symmetric KL divergence loss (Eq. 13), we vary λaux over {0.3, 0.6, 0.9}; we find λaux = 0.6 works best and provides an average WERR 6.1%. When combining the two objectives with λaux = 0.3 (Eq. 14), we find it also gives an average WERR 6.1%, which is better than using auxiliary RNN-T loss on its own.</p><p>For the low-resource scenario, one approach to address the lack of resources are to make use of data from high-resource languages. We thus perform crosslingual pretraining experiments with a Spanish RNN-T model trained on 7K hours. We use the Spanish encoder as the pretrained encoder for Romanian, and proceed with RNN-T training as before, which provides substantial improvements as in <ref type="table">Table 2</ref>. While on top of crosslingual pretraining, adding auxiliary RNN-T and KL divergence loss provides moderate gain.</p><p>We use the optimal λaux found in each condition and evaluate the performance on Turkish and German. As shown in <ref type="table" target="#tab_2">Table 4</ref>, the proposed combination of auxiliary RNN-T and KL divergence loss provides consistent improvements, which is also better than using each individually. We use the same Spanish encoder for crosslingual pretraining, and the improvements are much less due to the increased training data size. Along with the proposed auxiliary RNN-T modeling, they combine to produce noticeable gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Auxiliary chenone prediction results on video datasets</head><p>Since we build graphemic hybrid systems to provide chenone labels, we can additionally use the hybrid model as pretrained encoder for RNN-T. As shown in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, pretraining RNN-T encoder with connectionist temporal classification (CTC) or hybrid CE criterion can improve performance, and we also find CE pretraining produces an average 5.4% WERR on the low-resource Romanian as in <ref type="table">Table 3</ref>.</p><p>For the medium-resource Turkish and German (i.e. training data size of 3K hours), we initially find pretraining with hybrid CE model can provide 2 -4% improvements with a relatively small training mini-batch size. However, after optimizing the memory cost by mixed precision training and function merging <ref type="bibr" target="#b6">[7]</ref>, RNN-T training can enable larger mini-batch size, and we only observe minor improvements 0.9 -1.2% in <ref type="table" target="#tab_2">Table 4</ref>.</p><p>Then we experiment with λce (Eq. 17) on Romanian. Given each 5-layer LC-BLSTM encoder, we also examine connecting chenone prediction to 3rd (middle) layer or 5th (topmost) layer. As in <ref type="table">Table 3</ref>, λce = 0.6 works best in each case. While attaching chenone prediction to middle layer performs better than top layer, they combine to provide further improvements on top of CE pretraining.</p><p>We continue to evaluate the Turkish and German performance with λce = 0.6. As in <ref type="table" target="#tab_2">Table 4</ref>, training on both middle and top layers for auxiliary chenone prediction outperforms training on each alone, and produces noticeable improvements of 4.2 -4.4% when combined with CE pretraining. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results on LibriSpeech with transformer encoders</head><p>While we use streamable 5-layer LC-BLSTM encoders on video datasets above, we experiment with 24/36-layer transformer encoders instead on LibriSpeech. Given the much larger encoder depth, when evaluating the auxiliary RNN-T and KL divergence, we find it more effective to apply the loss at multiple layers. Thus for the 24-layer transformer, we apply it to the 6th, 12th and 18th encoder layers. As in <ref type="table" target="#tab_3">Table 5</ref>, it provides about 11% and 15% WERR on each test set. When evaluating the auxiliary CE loss, we apply it at the middle (12th) and top (24th) layer again, which also produces substantial relative gains about 13%. Additionally, we also attempt to apply both auxiliary tasks simultaneously, i.e., auxiliary RNN-T and KL divergence loss at 6th and 18th layers, and CE loss at 12th and 24th layers. In all cases, we use λaux = 0.3 and λce = 0.6 found above (Section 3.2 and 3.3). As in <ref type="table" target="#tab_3">Table 5</ref>, both auxiliary tasks combine to produce significant and complementary improvements. These performance gains are much larger than those on the video datasets with 5-layer LC-BLSTM encoder. We conjecture that transformer networks of increased depth suffer more from the encoder undertraining and gradient vanishing problem at lower layers (as discussed in Section 2.2), and auxiliary tasks play more effective roles in addressing it.</p><p>We proceed to increase transformer encoder depth from 24 to 36 layers, FFN size from 2048 to 3072, and wordpiece size from 1000 to 2048. We observe that without the auxiliary tasks, neither 24-layer transformer of FFN size 3072 nor 36-layer transformer of FFN 2048 is able to converge. Instead both can converge while using either of the two auxiliary tasks. Finally, the 36-layer transformer of FFN 3072 -which uses auxiliary RNN-T and KL divergence loss at 9th and 27th layers, and CE loss at 18th and 36th layers -produces our best results in <ref type="table">Table 6</ref>. Auxiliary tasks thus provide an opening for learning deep encoder network, and the increased depth is central to accuracy gains.</p><p>We further perform first-pass shallow fusion <ref type="bibr" target="#b48">[49]</ref> with an external language model (LM). We use a 4-layer LSTM LM with 4096 hidden units, and LM training data consists of LibriSpeech transcripts and text-only corpus (800M word tokens), tokenized with the 2048 wordpiece model. As in <ref type="table">Table 6</ref>, we thus achieve competitive results compared to the prior top-performing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>Attaching auxiliary objective functions to intermediate layers has been explored in various prior works. For improving image recognition, multiple auxiliary classifiers with squared hinge losses were used in <ref type="bibr" target="#b49">[50]</ref>, and CE objective functions used in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51]</ref>, while later <ref type="bibr" target="#b50">[51]</ref> only reported limited performance gain. <ref type="bibr" target="#b27">[28]</ref> made further progress by showing that, the gradients of multiple loss functions with respect to their shared parameters can counteract each other, and minimizing the symmetric KL divergence between the multiple classifier outputs can penalize such inconsistent gradients and provide more performance gains.</p><p>Similarly, for improving hybrid ASR models trained with CE criterion, <ref type="bibr" target="#b26">[27]</ref> connected an intermediate layer directly with a linear projection layer to compute the logits over senones, and used an asymmetric KL divergence loss between the primary model output (i.e. senone posterior) and the auxiliary classifier output. While in our work, we found connecting a nonlinear MLP -rather than a single linear layer -to the intermediate layer is more effective, which disentangled low-level feature extraction from final wordpiece prediction. Also for improving ASR, <ref type="bibr" target="#b51">[52]</ref> applied CTC or CE objective functions to multiple encoder layers, although without the cross-layer KL divergence loss. While CTC or hybrid senone/chenone models can directly produce posteriors over output labels, RNN-T requires a decoder to compute the output (wordpiece) posterior. Thus, in applying the auxiliary RNN-T or KL divergence loss, we specifically share the RNN-T decoder during the forward pass while keeping it intact from the backward pass (as discussed in Section 2.2.1).</p><p>Note that compared to CTC or hybrid models, attention-based seq2seq model is more similar to RNN-T, since both have a neural encoder and decoder. And for attention-based seq2seq model, using auxiliary senone labels has shown improved WERs in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, while recent work <ref type="bibr" target="#b52">[53]</ref> showed contrary observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this work, we propose the use of auxiliary tasks in improving RNN-T based ASR. We first benchmark the streamable LC-BLSTM encoder based performance on video datasets. Applying either auxiliary RNN-T or symmetric KL divergence objective function to intermediate encoder layers has been shown to improve ASR performance, and combining both is more effective than each on its own. Performing auxiliary chenone prediction also provides noticeable complementary gains on top of hybrid CE pretraining.</p><p>Next, we demonstrate the efficacy of both auxiliary tasks in improving the transformer encoder based sequence transducer results on LibriSpeech. Both auxiliary tasks provide substantial and complementary gains, and we find that, critical to the convergence of learning deep transformer encoders is the application of auxiliary objective functions to multiple encoder layers. Lastly, to participate in the LibriSpeech benchmark challenge, we develop a 36-layer transformer encoder via both auxiliary tasks, which achieves a WER of 2.0% on test-clean, 4.2% on test-other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the proposed auxiliary RNN-T and KL divergence criteria. For the auxiliary criteria, decoder is shown in a dashed box when it is used by the auxiliary branch to compute the logits (Eq. 7) in the forward pass, while the decoder is not updated in the backward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the proposed auxiliary context-dependent graphemic state prediction task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The amounts of audio data in hours.</figDesc><table><row><cell>Language</cell><cell cols="2">Train Valid</cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">clean noisy</cell></row><row><cell>Romanian</cell><cell>161</cell><cell>5.2</cell><cell>5.1</cell><cell>10.2</cell></row><row><cell>Turkish</cell><cell>3.1K</cell><cell>13.6</cell><cell>21.2</cell><cell>23.4</cell></row><row><cell>German</cell><cell>3.2K</cell><cell>13.8</cell><cell>24.5</cell><cell>24.0</cell></row><row><cell></cell><cell cols="3">3. EXPERIMENTS</cell><cell></cell></row><row><cell cols="2">3.1. Experimental setup</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.1.1. Data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>WER results on Romanian dataset. λaux is used in Eq. 9, 13 and 14. "aux" and "kl" loss denote the auxiliary RNN-T (Section 2.2.1) and KL divergence criterion (Section 2.2.2) respectively. "crosslingual pretrain" denotes the encoder pretrained from a highresource Spanish RNN-T. WERR (%) is the unweighted average of the respective relative WER reductions on clean and noisy test sets. WER results on Romanian. λce is used in Eq. 17. ce pretrain, ce loss, mid, top 0.6 21.2 17.8 19.5 12.3% the 24-layer, and 2048 for the 36-layer, resulting in total model parameters of 83.3M and 160.3M respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">λaux valid clean noisy WERR</cell></row><row><cell>baseline</cell><cell>-</cell><cell>24.0 20.5 22.0</cell><cell>-</cell></row><row><cell></cell><cell cols="2">0.1 23.2</cell><cell></cell></row><row><cell>+ aux loss</cell><cell cols="3">0.3 22.8 19.6 21.0 4.5%</cell></row><row><cell></cell><cell cols="2">0.6 23.1</cell><cell></cell></row><row><cell></cell><cell cols="2">0.3 22.9</cell><cell></cell></row><row><cell>+ kl loss</cell><cell cols="3">0.6 22.6 19.3 20.6 6.1%</cell></row><row><cell></cell><cell cols="2">0.9 22.7</cell><cell></cell></row><row><cell>+ aux + kl loss</cell><cell cols="3">0.3 22.5 19.1 20.8 6.1%</cell></row><row><cell>+ crosslingual pretrain</cell><cell>-</cell><cell cols="2">19.4 15.9 17.6 21.2%</cell></row><row><cell>+ aux + kl loss</cell><cell cols="3">0.3 18.9 15.7 17.2 22.6%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>"ce</cell></row><row><cell cols="4">pretrain" denotes encoder pretraining from graphemic hybrid CE</cell></row><row><cell cols="4">model. "ce loss" denotes auxiliary chenone prediction objective</cell></row><row><cell cols="4">function (Section 2.3). "mid" denotes connecting CE loss to the 3rd</cell></row><row><cell cols="4">(middle) encoder layer, and "top" denotes connecting CE loss to the</cell></row><row><cell>5th (topmost) encoder layer.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">λce valid clean noisy WERR</cell></row><row><cell>baseline</cell><cell></cell><cell>-24.0 20.5 22.0</cell><cell>-</cell></row><row><cell>+ ce pretrain</cell><cell></cell><cell cols="2">-22.8 19.3 20.9 5.4%</cell></row><row><cell></cell><cell></cell><cell>0.3 23.2</cell><cell></cell></row><row><cell>+ ce loss, top</cell><cell></cell><cell cols="2">0.6 22.9 19.8 21.2 3.5%</cell></row><row><cell></cell><cell></cell><cell>0.9 23.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.3 22.3</cell><cell></cell></row><row><cell>+ ce loss, mid</cell><cell></cell><cell cols="2">0.6 22.0 18.5 20.3 8.7%</cell></row><row><cell></cell><cell></cell><cell>0.9 22.0</cell><cell></cell></row><row><cell cols="2">+ ce pretrain, ce loss, mid</cell><cell cols="2">0.6 21.4 17.9 19.6 11.8%</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>WER results on Turkish and German, with λaux = 0.3 and λce = 0.6.</figDesc><table><row><cell></cell><cell>Turkish</cell><cell></cell><cell>German</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">clean noisy WERR clean noisy WERR</cell></row><row><cell>baseline</cell><cell>17.1 18.9</cell><cell>-</cell><cell>11.6 13.0</cell><cell>-</cell></row><row><cell>+ aux loss</cell><cell cols="4">16.8 18.8 1.1% 11.3 12.6 2.8%</cell></row><row><cell>+ kl loss</cell><cell cols="4">16.7 18.8 1.4% 11.5 12.8 1.2%</cell></row><row><cell>+ aux + kl loss</cell><cell cols="4">16.4 18.5 3.1% 11.3 12.6 2.8%</cell></row><row><cell cols="5">+ crosslingual pretrain 16.6 18.6 2.3% 11.4 12.8 1.6%</cell></row><row><cell>+ aux + kl loss</cell><cell cols="4">16.1 18.1 5.0% 11.3 12.4 3.6%</cell></row><row><cell>+ ce pretrain</cell><cell cols="4">16.8 18.9 0.9% 11.5 12.8 1.2%</cell></row><row><cell>+ ce loss, mid</cell><cell cols="4">16.5 18.4 3.1% 11.3 12.5 3.2%</cell></row><row><cell cols="5">+ ce loss, mid, top 16.3 18.2 4.2% 11.2 12.3 4.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>WER results on LibriSpeech, with 24-layer transformer encoder and 83M total model parameters.</figDesc><table><row><cell>Model</cell><cell cols="4">test-clean WERR test-other WERR</cell></row><row><cell>baseline</cell><cell>2.77</cell><cell>-</cell><cell>6.60</cell><cell>-</cell></row><row><cell>+ aux + kl loss</cell><cell>2.48</cell><cell>10.6%</cell><cell>5.62</cell><cell>14.8%</cell></row><row><cell>+ ce loss</cell><cell>2.42</cell><cell>12.6%</cell><cell>5.75</cell><cell>12.9%</cell></row><row><cell>+ aux + kl + ce loss</cell><cell>2.31</cell><cell>16.5%</cell><cell>5.26</cell><cell>20.3%</cell></row><row><cell cols="5">Table 6. Comparison of our models (with 36-layer transformer</cell></row><row><cell cols="5">encoder and 160M total model parameters) with recently published</cell></row><row><cell cols="2">best results on LibriSpeech.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">w/o LM</cell><cell cols="2">w/ LM</cell></row><row><cell></cell><cell cols="4">test-clean test-other test-clean test-other</cell></row><row><cell>LAS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM [46]</cell><cell>2.6</cell><cell>6.0</cell><cell>2.2</cell><cell>5.2</cell></row><row><cell>Hybrid</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer [32]</cell><cell>2.6</cell><cell>5.6</cell><cell>2.3</cell><cell>4.9</cell></row><row><cell>CTC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer [47]</cell><cell>2.3</cell><cell>4.8</cell><cell>2.1</cell><cell>4.2</cell></row><row><cell>Sequence Transducer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer [33]</cell><cell>2.4</cell><cell>5.6</cell><cell>2.0</cell><cell>4.6</cell></row><row><cell>Conformer [48]</cell><cell>2.1</cell><cell>4.3</cell><cell>1.9</cell><cell>3.9</cell></row><row><cell>Transformer (Ours)</cell><cell>2.2</cell><cell>4.7</cell><cell>2.0</cell><cell>4.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that, the posterior distribution in Eq. 5 can also be written as P (yu|x 1:T , y 1:(u−1) ), if the encoder uses global/infinite context, like a BLSTM or non-streaming transformer network<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we use a linear layer rather than a MLP for the topmost/Lth layer, since the top encoder layer has been designed for final label prediction.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Treebased state tying for high accuracy acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip C</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="307" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advances in all-neural speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural aligner: An encoder-decoder neural network model for sequence to sequence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving RNN transducer modeling for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A streaming on-device end-toend model surpassing server-side conventional model quality and latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhang</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuo-Yiin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raziel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh Gaur Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparison of end-to-end models for long-form speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Kishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the comparison of popular end-to-end models for large scale speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14327</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking LF-MMI, CTC and RNN-T criteria for streaming ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjell</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradyot</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Feng</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haşim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring pre-training with alignments for rnn transducer based end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing long-form speech using streaming end-to-end models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Minimum Bayes risk training of RNN-transducer for endto-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12487</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual application of speech enhancement for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selfsupervised generalisation with meta auxiliary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adapting auxiliary losses using gradient similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02224</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multitask learning with low-level auxiliary tasks for encoderdecoder based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning with augmentation strategy for acoustic-to-word attention-based encoder-decoder speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takafumi</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sei</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikazu</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for largevocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptively scheduled multitask learning: The case of low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poorya</forename><surname>Zaremoodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-teaching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic hierarchical mimicking towards consistent optimization objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From senones to chenones: tied context-dependent graphemes for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fügen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unicodebased graphemic systems for limited resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ragni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformerbased acoustic modeling for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-T loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshuman</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LibriSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multilingual graphemic hybrid ASR with massive data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaochu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kritika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</title>
		<meeting>the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Highway long short-term memory RNNs for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SpecAugment on large scale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deja-vu: Double feature presentation and iterated loss in deep transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Minimum latency training strategies for streaming sequence-to-sequence ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
