<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BARThez: a Skilled Pretrained French Sequence-to-Sequence Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-09">9 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moussa</forename><surname>Kamal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michalis Vazirgianniś Ecole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddiné</forename><surname>Ecole</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michalis Vazirgianniś Ecole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polytechnique</forename><surname>Antoine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michalis Vazirgianniś Ecole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tixieŕ</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michalis Vazirgianniś Ecole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecole</forename><surname>Polytechnique</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michalis Vazirgianniś Ecole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BARThez: a Skilled Pretrained French Sequence-to-Sequence Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-09">9 Feb 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale pretrained seq2seq model for French. Being based on BART, BARThez is particularly well-suited for generative tasks. We evaluate BARThez on five discriminative tasks from the FLUE benchmark and two generative tasks from a novel summarization dataset, Orange-Sum, that we created for this research. We show BARThez to be very competitive with state-of-the-art BERT-based French language models such as CamemBERT and FlauBERT. We also continue the pretraining of a multilingual BART on BARThez' corpus, and show our resulting model, mBARThez, to significantly boost BARThez' generative performance. Code, data and models are publicly available.</p><p>2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inductive transfer learning, that is, solving tasks with models that have been pretrained on very large amounts of data, was a game changer in computer vision <ref type="bibr">(Krizhevsky et al., 2012)</ref>. In NLP, while annotated data are scarce, raw text is virtually unlimited and readily available. It thus emerged that the ability to learn good representations from plain text could greatly improve general natural language understanding.</p><p>Trained on gigantic amounts of raw data and with hundreds of GPUs, models based on the Transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref>, such as <ref type="bibr">GPT (Radford et al., 2018)</ref> and <ref type="bibr">BERT (Devlin et al., 2018)</ref>, have set new state-of-the-art performance in every NLU task. Moreover, users around the world can easily benefit from these improvements, by finetuning the publicly available pretrained models to their specific applications. This also saves considerable amounts of time, resources and energy, compared with training models from scratch.</p><p>BART <ref type="bibr">(Lewis et al., 2019)</ref> combined a BERTliked bidirectional encoder with a GPT-like forward decoder, and pretrained this seq2seq architecture as a denoising autoencoder with a more general formulation of the masked language modeling objectives of BERT. Since not only BART's encoder but also its decoder is pretrained, BART excels on tasks involving text generation.</p><p>While the aforementioned efforts have made great strides, most of the research and resources were dedicated to the English language, despite a few notable exceptions. In this paper, we partly address this limitation by contributing BARThez 1 , the first pretrained seq2seq model for French.</p><p>BARThez, based on BART, was pretrained on a very large monolingual French corpus from past research that we adapted to suit BART's specific perturbation schemes. Unlike already existing BERT-based French language models such as <ref type="bibr">CamemBERT (Martin et al., 2019)</ref> and <ref type="bibr">FlauBERT (Le et al., 2019)</ref>, BARThez is particularly wellsuited for generative tasks. We evaluate BARThez on five sentiment analysis, paraphrase identification, and natural language inference tasks from the recent FLUE benchmark, and two generative tasks from a novel French summarization dataset, OrangeSum, that we created for this research. We show that BARThez is very competitive with CamemBERT, FlauBERT, and mBART. We also continue the pretraining of an already pretrained multilingual BART on BARThez's corpus. Our resulting model, mBARThez, significantly boosts BARThez' performance on generative tasks.</p><p>Our contributions are as follows: • We publicly release the first large-scale pretrained seq2seq model dedicated to the French language, BARThez, featuring 165M parameters, and trained on 101 GB of text for 60 hours with 128 GPUs. We evaluate BARThez on five discriminative tasks and two generative tasks, with automated and human evaluation, and show that BARThez is very competitive with the state of the art.</p><p>• To address the lack of generative tasks in the existing FLUE benchmark, we put together a novel dataset for summarization in French, OrangeSum, that we publicly release 2 and analyze in this paper. OrangeSum is more abstractive than traditional summarization datasets, and can be considered the French equivalent of XSum <ref type="bibr">(Narayan et al., 2018)</ref>.</p><p>• We continue the pretraining of a multilingual BART on BARThez' corpus, and show that our resulting model, named mBARThez, offers a significant boost over BARThez on generative tasks. • We publicly release our code and models 3 . Our models were also integrated into the highlypopular Hugging Face Transformers library 4 . As such, they can easily be distributed and deployed for research or production within a standard, industrial-strength framework. They also have their own APIs and can be interactively tested online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Learning without labels is enabled via selfsupervised learning 5 , a setting in which a system learns to predict part of its input from other parts of its input. In practice, one or more supervised tasks are created from the unlabeled data, and the model learns to solve these tasks with custom objectives.</p><p>Some of the earliest and most famous selfsupervised representation learning approaches in NLP are word2vec <ref type="bibr">(Mikolov et al., 2013)</ref>, <ref type="bibr">GloVe (Pennington et al., 2014)</ref> and FastText <ref type="bibr">(Bojanowski et al., 2017)</ref>. While these methods were significant advancements, they produce static representations, which is a major limitation, as words have different meanings depending on the unique contexts in which they are used.</p><p>Deep pretrained language models.</p><p>ELMo <ref type="bibr">(Peters et al., 2018)</ref> provided the first contextualized embeddings, by extracting and combining the internal states of a pretrained deep bi-LSTM language model. Except for the word embeddings and the softmax layer, the forwards and backwards RNNs have different parameters. The authors of ELMo showed that the learned representations could be transferred with great benefits to downstream architectures, to solve a variety of supervised NLU tasks.</p><p>Beyond simply combining internal states, <ref type="bibr">Howard and Ruder (2018)</ref> proposed ULMFiT, a universal transfer learning method for text classification where the language model is pretrained on a large, general dataset, finetuned on a specific dataset, and finally augmented with classification layers trained from scratch on downstream tasks.</p><p>With the OpenAI GPT, Radford et al. (2018) capitalized on the Transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref>, superior and conceptually simpler than recurrent neural networks. More precisely, they pretrained a left-to-right Transformer decoder as a general language model, and finetuned it on 12 language understanding tasks by applying different transformations to the input.</p><p>By combining ideas from all the aforementioned models, and introducing bidirectional pretraining, <ref type="bibr">BERT (Devlin et al., 2018)</ref> disrupted the NLP field by setting new state-of-the-art performance on 11 NLU tasks, with very wide margins. More precisely, BERT uses a bidirectional Transformer encoder with a masked language model objective, making the learned representations capture both the left and the right contexts, instead of just the left context. The sheer size of BERT, with up to 24 Transformer blocks, plays a role in performance too.</p><p>With GPT-2, a version of GPT with over an order of magnitude more parameters than <ref type="bibr">GPT, Radford et al. (2019)</ref> showed that as long as they have very large capacities, general language models can reach reasonable performance on many specific NLU tasks out-of-the-box, without any finetuning, i.e., accomplish zero-shot transfer. This demonstrates the fundamental nature and importance of the language modeling objective for inductive transfer learning.</p><p>In <ref type="bibr">RoBERTa, Liu et al. (2019)</ref> showed that the performance of BERT could be improved by optimizing its hyperparameters and training proce-dure. The study of why and how BERT works so well has now its own dedicated research field, known as BERTology <ref type="bibr">(Rogers et al., 2020)</ref>. Languages. Following the success of BERT for the English language, some BERT models were pretrained and evaluated in other languages.</p><p>Some examples include Arabic <ref type="bibr">(Antoun et al.)</ref>, <ref type="bibr">Dutch (de Vries et al., 2019;</ref><ref type="bibr">Delobelle et al., 2020</ref><ref type="bibr">), French (Martin et al., 2019</ref><ref type="bibr">Le et al., 2019)</ref>, <ref type="bibr">Italian (Polignano et al., 2019)</ref>, <ref type="bibr">Portuguese (Souza et al., 2019)</ref>, <ref type="bibr">Russian (Kuratov and</ref><ref type="bibr">Arkhipov, 2019), and</ref><ref type="bibr">Spanish (Cañete et al., 2020)</ref>.</p><p>In addition to the aforelisted monolingual models, multilingual models were also proposed, notably mBERT <ref type="bibr">(Devlin et al., 2018), XLM (Lample and</ref><ref type="bibr">Conneau, 2019)</ref> and <ref type="bibr">XLM-R (Conneau et al., 2019)</ref>. Abstractive summarization. Abstractive summarization is an important and challenging task, requiring diverse and complex natural language understanding and generation capabilities. A good summarization model needs to read, comprehend, and write well.</p><p>GPT-2 can be used for summarization, by sampling a certain numbers of tokens from a given start seed. However, while the generated text is grammatical and fluent, summarization performance is only slightly superior to that of a random extractive baseline.</p><p>Being a bidirectional encoder, BERT cannot be used out-of-the-box for language generation, unlike GPT-2. Furthermore, BERT produces singlesentence representations, whereas for summarization, reasoning over multiple sentence and paragraph representations is necessary. Liu and Lapata (2019) proposed a way to overcome these challenges. At the input level, they introduced special tokens to encode individual sentences, interval segment embeddings, and used more position embeddings than in BERT. Then, they combined a pretrained BERT encoder with a Transformerbased decoder initialized at random and jointly trained the two models with different optimizers and learning rates. <ref type="bibr">BART and mBART. BART (Lewis et al., 2019)</ref> is a denoising auto-encoder that jointly pretrains a bidirectional encoder (like in BERT) and a forward decoder (like in GPT) by learning to reconstruct a corrupted input sequence. Both the encoder and the decoder are Transformers. Since not only the encoder but also the decoder is pretrained, BART is particularly effective when applied to text generation tasks. <ref type="bibr">Liu et al. (2020)</ref> pretrained a multilingual BART (mBART) on 25 different languages. They showed that this multilingual pretraining brings significant performance gains on a variety of machine translation tasks. <ref type="bibr">MASS (Song et al., 2019)</ref> is another multilingual pretrained sequence to sequence model, that learns to predict a masked span in the input sequence. The main difference between MASS and BART, is that the former only predicts the masked fragment of the sentence, while the latter learns to reconstruct the entire corrupted sentence. This difference makes MASS less effective in discriminative tasks, given that only the masked span is fed to the decoder <ref type="bibr">(Lewis et al., 2019)</ref>. <ref type="bibr">ProphetNet (Yan et al., 2020)</ref> which also adopts the encoder-decoder structure, introduces a new learning objective called future n-gram prediction. This objective reduces overfitting on local correlations by learning to predict the next n-grams (instead of unigrams) at each time step given the previous context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BARThez</head><p>Our model is based on BART (Lewis et al., 2019), a denoising auto-encoder. It consists of a bidirectional encoder and a left-to-right auto-regressive decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>We use the BASE architecture, with 6 encoder and 6 decoder layers. We did not opt for a LARGE architecture due to resource limitations. Our BASE architecture uses 768 hidden dimensions and 12 attention heads in both the encoder and the decoder. In total, our model has roughly 165M parameters. The architecture has two differences compared with the vanilla seq2seq Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>. The first one is the use of GeLUs activation layers instead of ReLUs, and the second is the presence of a normalization layer on top of the encoder and the decoder, following <ref type="bibr">Liu et al. (2020)</ref>. These additional layers help stabilizing the training when using FP16 precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vocabulary</head><p>To generate our vocabulary, we use Sentence-Piece (Kudo and Richardson, 2018) that implements byte-pair-encoding (BPE) <ref type="bibr">(Sennrich et al., 2015)</ref>. We do not perform any type of pretokenization and we fix the size of the vocabulary to 50K sub-words. The SentencePiece model is trained on a 10GB random sample of the pretraining corpus. We fix the character coverage to 99.95%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-supervised learning</head><p>We use the same pretraining as in BART. That is, BARThez learns to reconstruct a corrupted input. More precisely, the input text is perturbed with a noise function, and the model has to predict it by minimizing the cross-entropy between the predicted and the original text. Formally, having a set of documents {X 1 , X 2 , ..., X n } and a noising function n, we aim at finding the parameters θ that minimize:</p><formula xml:id="formula_0">L θ = − i log P (X i |n(X i ); θ)</formula><p>Two different types of noise are applied in n. First, we use the text infilling scheme, where a number of text spans are sampled and replaced with one [MASK] special token. The length of the spans is sampled from a Poisson distribution with (λ = 3.5) and 30% of the text is masked. The second perturbation scheme is sentence permutation, where the input document, seen as a list of sentences, is shuffled.</p><p>Note that here, we follow <ref type="bibr">Lewis et al. (2019)</ref>, who showed that both text infilling and sentence shuffling were necessary to obtain best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pretraining corpus</head><p>We created a version of FlauBERT's corpus <ref type="bibr">(Le et al., 2019)</ref> suitable for the two perturbation schemes described in subsection 3.3. Indeed, in the original FlauBERT corpus, each sentence is seen as an independent instance, while in our case, we need instances to correspond to complete documents.</p><p>Other than that, BARThez' corpus is similar to FlauBERT's. It primarily consists in the French part of CommonCrawl, NewsCrawl, Wikipedia and other smaller corpora that are listed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training details</head><p>We pretrained BARThez on 128 NVidia V100 GPUs. We fixed the batch size to 6000 tokens per GPU and the update frequency to 2, which gave a total number of roughly 22k documents per update. We used the Adam optimizer (Kingma and Ba, 2014) with ǫ = 10 −6 , β 1 = 0.9, and β 2 = 0.999, with a learning rate starting from 6.10 −4 and decreasing linearly as a function of the training step. We used a warm up of 6% of the total number of training steps. Pretraining lasted for approximately 60 hours, allowing for 20 passes over the whole corpus. In the first 12 epochs, we fixed the dropout to 0.1, for epochs 12 to 16 we decreased it to 0.05, and finally we set it to zero for epochs 16 to 20. All experiments were carried out using the Fairseq library (Ott et al., 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">mBARThez</head><p>mBART (Liu et al., 2020) is a multilingual BART. It follows a LARGE architecture, with 12 layers in both the encoder and the decoder, hidden vectors of size 1024, and 16 attention heads. It was trained on a multilingual corpus containing 1369 GB of raw text, for over 2.5 weeks on 256 Nvidia V100 GPUs. The multilingual corpus covers 25 different languages, including 56 GB of French text. In the original paper, the authors evaluated mBART on machine translation. However, mBART can also be used to perform monolingual tasks. We continued the pretraining of the pretrained mBART on BARThez' corpus (see subsection 3.4) for about 30 hours on 128 Nvidia V100 GPUs, which allowed for 4 passes over BARThez' corpus. This can be seen as an instance of language-adaptive pretraining, which goes a step further than domain-adaptive pretraining <ref type="bibr">(Gururangan et al., 2020)</ref>. The initial   learning rate was set to 0.0001 and linearly decreased towards zero. We call the resulting model mBARThez.</p><p>Note that being multilingual, mBART uses a vocabulary containing tokens with non-latin characters. We eliminated these tokens from all embedding layers of mBARThez, reducing its number of parameters from 610M to 458M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OrangeSum</head><p>BART-based models are particularly well-suited to generative tasks, but unfortunately, FLUE <ref type="bibr">(Le et al., 2019)</ref>, the French equivalent of GLUE, only contains discriminative tasks 7 <ref type="bibr">(Wang et al., 2018)</ref>.</p><p>We therefore decided to create one such task. We opted for single-document abstractive summarization, as it is a generative task that also requires the model to encode its input very well. In other words, for a model to summarize well, it needs to both read, comprehend, and write well, making abstractive summarization one of the most central and challenging evaluation tasks in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation.</head><p>Our strategy here was to create a French equivalent of the recently introduced XSum dataset <ref type="bibr">(Narayan et al., 2018)</ref>. Unlike the historical summarization datasets, CNN, DailyMail, and NY Times, introduced by Hermann et al. <ref type="bibr">(2015)</ref>, which favor extractive strategies, XSum requires the models to display a high degree of abstractivity to perform well. XSum was created by scraping articles and their one-sentence summaries from the BBC website, where the one-sentence summaries are not catchy headlines, but rather capture the gist of the articles. Data collection. We adopted an analogous strategy, and scraped the "Orange Actu" website 8 . Orange S.A. is a large French multinational telecommunications corporation, with 266M customers worldwide. Our scraped pages cover almost a decade from Feb 2011 to Sep 2020. They belong to five main categories: France, world, politics, automotive, and society 9 . The society category is itself divided into 8 subcategories: health, environment, people, culture, media, high-tech, unsual ("insolite" in French), and miscellaneous.</p><p>Each article featured a single-sentence title as well as a very brief abstract, both professionally written by the author of the article. We extracted these two fields from each page, thus creating two summarization tasks: OrangeSum Title and Or-angeSum Abstract. Gold summaries are respectively 11.42 and 32.12 words in length on average, for these two tasks (see <ref type="table" target="#tab_2">Table 2</ref>). Note that like in XSum, titles in OrangeSum tend not to be catchy Le 18 octobre dernier, Jacline Mouraud se faisait connaître en publiant sur Facebook une vidéo dans laquelle elle poussait un "coup de gueule" contre le gouvernement. Aujourd'hui, la Bretonne a pris ses distances par rapport au mouvement, notamment faceà d'autres figures plus radicales commeÉric Drouet. Jacline Mouraud réfléchit désormaisà créer son propre parti, "la seule chose envisageable", comme elle l'explique au JDD. Nicolas Sarkozy, "le seul qui a des couilles". Cette figure des "gilets jaunes", accusée de faire le jeu de LREM estime que "le problème" d'Emmanuel Macron "c'est qu'il est jeune". "Il devrait y avoir unâge minimum pourêtre président : 50 ans", souligne Jacline Mouraud. Dans le JDD, elle raconte d'ailleurs avoir voté blanc lors de la dernière présidentielle. En 2007 et 2012, c'est Nicolas Sarkozy, "le seul qui a des couilles", que la figure des "gilets jaunes" avait soutenu. En attendant de se lancer, pas question pour elle en tous les cas d'être candidate aux européennes sur une liste de La République en marche.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>L'une des figures du mouvement ne sera toutefois pas candidate aux prochainesélections européennes.</p><p>mBART Jacline Mouraud, figure des "gilets jaunes", estime que le président d'Emmanuel Macron est trop jeune pourêtre président.</p><p>mBARThez Dans un entretien au JDD, la figure des "gilets jaunes" Jacline Mouraud révèle qu'elle réfléchit a créer son propre parti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>Dans les colonnes du JDD, la figure des "gilets jaunes" explique qu'elle envisage de se présenter aux européennes sur une liste La République en marche.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Retirée de la vie politique depuis plusieurs mois, Bretone Mouraud envisage de se lancer en politique. Et elle réfléchità quelque chose de plus, rapporte le JDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TITLE</head><p>Gold "Gilets jaunes" : Jacline Mouraud réfléchità créer son parti mBART "Gilets jaunes" : Jacline Mouraud lance son propre parti mBARThez "Gilets jaunes" : Jacline Mouraud prend ses distances BARThez La figure des "gilets jaunes" Jacline Mouraud va créer son propre parti C2C "Gilets jaunes" : Jacline Mouraud réfléchità sa propre candidature headlines but rather convey the essence of the article. The same can be said about the abstracts. Post-processing. As a post-processing step, we removed all empty articles, and articles whose titles were shorter than 5 words. For Orange-Sum Abstract, we removed the top 10% articles in terms of proportion of novel unigrams in the abstracts, as we observed that such abstracts tended to be introductions rather than real abstracts. This corresponded to a threshold of 57% novel unigrams. For both OrangeSum Title and OrangeSum Abstract, we set aside 1500 pairs for testing, 1500 for validation, and used all the remaining ones for training. We make the dataset publicly available 10 .</p><p>An example document with its summaries is provided in <ref type="table" target="#tab_4">Table 4</ref>. More examples are available in appendix. Analysis. <ref type="table" target="#tab_2">Table 2</ref> compares OrangeSum with XSum and the well-known CNN, DailyMail, and NY Times datasets. We can see that the two Or-angeSum datasets are very similar to XSum in terms of statistics, but is one order of magnitude smaller than XSum. However, the size of Orange-Sum still allows for effective finetuning, as we later demonstrate in our experiments. <ref type="table" target="#tab_3">Table 3</ref> provides empirical evidence showing that like XSum, OrangeSum is less biased towards extractive systems compared with the traditional datasets used for abstractive summarization. There are 30% novel unigrams in the OrangeSum Abstract reference summaries and 26.5% in Or-angeSum Title, compared with 35.7% in Xsum, 17% in CNN, 17% in DailyMail, and 23% in NY Times. This indicates that XSum and OrangeSum summaries are more abstractive. These observations are also confirmed by the fact that the two extractive baselines LEAD and EXT-ORACLE perform much more poorly on XSum and Orange-Sum than on the other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We compare BARThez and mBARThez with the following models, summarized in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>• mBART. The multilingual BART LARGE described in section 4. • CamemBERT2CamemBERT (C2C). To apply CamemBERT to our generative task, we used the BERT2BERT approach proposed by Rothe et al. <ref type="bibr">(2020)</ref>. More precisely, we fine-tuned a sequenceto-sequence model whose both encoder and decoder parameters were initialized with Camem-BERT LARGE weights. The only weights that  were initialized randomly are the encoder-decoder attention weights.</p><p>• BART-random. As an additional baseline, we train a model with the same architecture and vocabulary as BARThez from scratch on the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Summarization</head><p>All pretrained models were finetuned for 30 epochs and we used a learning rate that warmed up to 0.0001 (6% of the training steps) and then decreased linearly to 0. BART-random was trained for 60 epochs. We selected the checkpoint associated with the best validation score to generate the test set summaries, using beam-search with a beam size of 4. We classically report ROUGE-1, ROUGE-2 and ROUGE-L scores <ref type="bibr">(Lin, 2004)</ref> in <ref type="table" target="#tab_8">Table 6</ref>. However, since ROUGE is limited to capturing n-gram overlap, which is poorly suited to the abstractive summarization setting, we also report BERTScore scores. BERTScore (Zhang et al., 2019) is a recently introduced metric that leverages the contextual representations of the candidate and gold sentences.</p><p>Following Narayan et al. <ref type="formula">(2018)</ref>, we included two extractive baselines in our evaluation, LEAD and EXT-ORACLE. LEAD creates a summary by extracting the first n sentences from the document. In our case, we set n = 1. The second baseline, EXT-ORACLE, extracts from the document the set of sentences that maximizes a specific score. In our case, we extracted the one sentence maximizing ROUGE-L. Quantitative results. <ref type="table" target="#tab_8">Table 6</ref> compares the performance of the models finetuned on the summarization task. While having four times less parameters, BARThez is on par with mBART, both in terms of ROUGE and BERTScore. mBARThez provides a significant boost over BARThez and mBART and reaches best performance everywhere. This highlights the importance of adapting a multilingual pretrained model to a specific lan-guage before finetuning (language-adaptive pretraining). This also suggests that, when proper adaptation is conducted, it can be advantageous to capitalize on a multilingual model to perform monolingual downstream tasks, probably because there are some translingual features and patterns to be learned. Finally, all BART-based models outperform CamemBERT2CamemBERT by a significant margin. Human evaluation. To validate our positive quantitative results, we conducted a human evaluation study with 11 French native speakers. Following Narayan et al. <ref type="formula">(2018)</ref>, we used Best-Worst Scaling <ref type="bibr">(Louviere et al., 2015)</ref>. In this approach, two summaries from two different systems, along with their input document, are presented to a human annotator who has to decide which one is better. We asked evaluators to base their judgments on three criteria: accuracy (does the summary contain accurate facts?), informativeness (does the summary capture the important information in the document?) and fluency (is the summary written in well-formed French?).</p><p>We included the BARThez, mBARThez, mBART and C2C models in our analysis, along with the ground-truth summaries. We randomly sampled 14 documents from the test set of OrangeSum Abstract, and generated all possible summary pairs for each document, resulting in 140 pairs. Each pair was randomly assigned to three different annotators, resulting in 420 evaluation tasks in total. The final score of a model was given as the percentage of time its summary was chosen as best minus the percentage of time it was chosen as worst. Scores are reported in <ref type="table" target="#tab_11">Table  9</ref>. mBARThez reaches first place, like for the quantitative results, but with an even wider margin. It is also interesting to note that BARThez, which was on par with mBART quantitatively, significantly outperforms it this time around, in terms of human evaluations. Note that the negative score of CamemBERT2CamemBERT should be analyzed in comparison with the other     models. That is, C2C's summaries were judged to be worse more often than not. Suprisingly, BARThez and mBARThez' summaries were often judged better than the ground truth ones. We hypothesize that since the GT summaries are short abstracts written by the authors of the articles, they may be well-written but contain information that is missing from the documents, such as dates. In such situations, the annotators may consider such information as inaccurate (e.g., due to model hallucinations) and favor the other model. Qualitative results.</p><p>As shown in <ref type="table" target="#tab_9">Table 7</ref>, mBARThez is more abstractive than BARThez and mBART, as measured by the proportion of novel n-grams in the generated summaries. E.g., mBARThez introduces on average 15.48% of novel unigrams in its summaries for the Abstract task, compared with 10.93 and 13.40 for BARThez and mBART, respectively. It is interesting to note that despite this superior abstractivity, mBARThez still reaches first place everywhere in terms of the ROUGE metric, which measures n-gram overlap.</p><p>We hypothesize that BARThez is less abstractive than mBART and mBARThez due to the fact that it is based on a BASE architecture instead of a LARGE one, and has thus four times less parameters.</p><p>Finally, it is also to be noted that Camem-BERT2CamemBERT (C2C) introduces many new words, which could be considered a good thing at first. However, it also repeats itself a lot (see <ref type="table" target="#tab_10">Table  8</ref>) and has low ROUGE, BERTSum, and human evaluation scores. A manual observation revealed that actually, many of the new words introduced by C2C are irrelevant (see appendix for summary examples).</p><p>Also, like in Rothe et al. (2020), we computed the length of the summaries, and the percentage of summaries with at least one non-stopword repetition. We used as stopwords the 500 most frequent words from the system and gold summaries, across all documents. As can be seen in <ref type="table" target="#tab_10">Table 8</ref>, for both the Abstract and Title tasks, all models generated summaries of sizes very close to that of  the Gold summaries.</p><p>In terms of repetitions, the less redundant models, closest to the ground truth, are mBART and mBARThez. This is especially apparent on the Abstract task, where potential for repetition is greater. On this task, mBART and mBARThez show less than 9% repetitions, compared with 14.5 and 23 for BARThez and C2C (resp.), and 11.5 in the references. C2C is also way more redundant than the other models and far from the reference on the Title task, with 19.5% repetitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discriminative tasks</head><p>In addition to generative tasks, BART-like models can perform discriminative tasks <ref type="bibr">(Lewis et al., 2019)</ref>. In the case of sequence classification, the input sequence is fed to both the encoder and the decoder, and the representation of the last token in the sequence is used by adding a classification head on top of it. When the input consists of several sentences, these sentences are separated with a special token and pasted together. We evaluate the different models on five discriminative tasks from the FLUE benchmark <ref type="bibr">11</ref>   In all experiments, we finetune the model for 10 epochs with a learning rate chosen from {10 −4 , 5.10 −5 , 10 −5 } based on the best validation score. We repeat each experiment 3 times with different seeds and report the mean and standard deviation.</p><p>Results. <ref type="table" target="#tab_0">Table 10</ref> reports the test set accuracies. For comparison purposes, we also copy that of other relevant BERT-based models as reported in Among the models having a BASE architecture, BARThez is best in the three sentiment analysis tasks, while being very close to CamemBERT and FlauBERT in the paraphrasing and inference tasks.</p><p>Among the LARGE models, mBARThez outperforms mBART in all tasks, showing again the importance of language-adaptive pretraining. On the other hand, CamemBERT and FlauBERT outperform mBARThez in most of the tasks, which could be attributed to the fact that CamemBERT and FlauBERT were trained for approximately 10 times more GPU hours on a monolingual French corpus. Nevertheless, given that huge difference in monolingual training time, it is remarkable that mBARThez is so close, and sometimes outperforms, FlauBERT, with e.g., a comfortable 1.56 margin on PAWSX.</p><p>We can conclude that the ability of BARThez and mBARThez to perform well on generative tasks does not appear to come at the expense of a decrease in performance on discriminative tasks, which is in line with the results presented in the BART paper <ref type="bibr">(Lewis et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We released BARThez and mBARThez, the first large-scale pretrained seq2seq models for the French language, as well as a novel summarization dataset for French, inspired by the XSum dataset. By evaluating our models on the summarization dataset we showed that: (1) BARThez is on par with mBART while having four times less parameters, and that (2) mBARThez provides a significant boost over mBART by simply adding a relatively affordable language-adaptive phase to the pretraining. In addition, we evaluated BARThez and mBARThez on 5 sentiment analysis, paraphrasing, and natural language inference tasks against cutting edge BERT-based French language models (FlauBERT and CamemBERT), and obtained very competitive results. An interesting area for future work is to further explore the language-adaptive pretraining approach. mBARThez Une explosion a fait cinq morts mercredi dans une usine de pesticides du centre de l'Inde, après que le gaz a fui dans les réservoirs après une réaction chimique, ont indiqué les autorités.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>Une dizaine de personnes ont péri et des centaines d'autres ontété blessées mercredi dans une usine de pesticides près de Visakhapatnam, dans le sud de l'Inde, a annoncé la police.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Au moins vingt personnes sont mortes, dont cinq sont mortes et cinq sont portées disparues, selon un bilan officiel lundi après-midi en Inde, faisant craindre une fuite de gaz meurtrière dans le pays, selon une source gouvernementaleà l'AFP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TITLE Gold</head><p>Fuite de gaz dans une usine en Inde: 5 morts, au moins 1.000 personnes hospitalisées mBART Inde: cinq morts dans un accident de la usine de pesticides mBARThez Inde: au moins cinq morts dans le crash d'une usine de pesticides BARThez Inde: cinq morts dans un glissement de terrainà Visakhapatnam C2C Inde: cinq morts dans un gaz mortel dans un usine de recyclage </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>De nombreux scientifiques occidentaux ont fait part de leurs doutes quantà la rapidité avec laquelle ce vaccin auraitété mis au point.Le ministre américain de la Santé Alex Azar s'est fait l'écho mercredi de leurs points de vue,à l'issue d'une visite de trois joursà Taïwan."Il est important que nous fournissions des vaccins sans danger et efficaces et que les données soient transparentes... Ce n'est pas une course pourêtre le premier", a-t-il déclaréà la presse lors d'une conférence téléphonique."Je dois souligner que deux des six vaccins américains dans lesquels nous avons investi sont entrés dans la phase des essais cliniques il y a trois semaines, alors que le vaccin russe ne fait que commencer", a-t-il ajouté."Les données des premiers essais en Russie n'ont pasété divulguées, ce n'est pas transparent", a estimé le ministre américain.Mardi, le président russe Vladimir Poutine a annoncé le développement par son pays du "premier" vaccin sans danger contre le Covid-19, affirmant que l'une de ses filles se l'est fait inoculer.Ce vaccin á eté baptisé "Spoutnik V" (V comme vaccin, ndlr), en référenceà la victoire politico-scientifique russe qu'était la mise en orbite en 1957 du satellite du même nom en pleine Guerre froide.Peu après la déclaration du Kremlin, l'Organisation mondiale de la santé (OMS) a réagi en appelant a la prudence, rappelant que la "pré-qualification" et l'homologation d'un vaccin passaient par des procédures "rigoureuses".De nombreux pays se sont lancés dans une véritable* course contre la montre pour trouver un vaccin efficace contre le coronavirus qui a tué plus de 740.000 personnesà travers la planète depuis son apparition l'an dernier en Chine.Les Etats-Unis sont le pays le touché avec 5,1 millions de cas de Covid-19 qui a fait plus de 164.000 morts. M. Azar s'est dit confiant sur la capacité des Américainsà mettre au point un vaccin."Nous pensons qu'il est très crédible que nous ayons des dizaines de millions de doses de vaccin de référence, sûres et efficaces d'ici la fin de cette année, et plusieurs centaines de millions de doses au début de l'année prochaine", a-t-il affirmé.Le président américain a lancé l'opération "Warp Speed" qui vise explicitementà obtenir de quoi vacciner tous les Américains dès janvier 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Le ministre américain de la Santé a fait part mercredi de son scepticisme après l'annonce du développement par la Russie du "premier" vaccin contre le coronavirus assurant une "immunité durable".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>Une première depuis la Seconde guerre mondiale, la consommation d'alcool ne baisse plus en France. L'Académie nationale de médecine a appelé lundi 29 avril les pouvoirs publicsà "prendre des mesures plus fortes" pour lutter contre les problèmes de santé publique causés par la consommation d'alcool. "Pour la première fois depuis la Seconde guerre mondiale, la consommation d'alcool ne baisse plus en France. C'est une défaite majeure pour la santé publique, car l'alcool en est un déterminant fondamental", estime l'Académie dans un communiqué diffusé lundi 29 avril. L'organisme déplore en particulier "l'affaiblissement continu de la loi Evin sous la pression du lobby alcoolier, jusqu'à autoriser la publicité sur l'internet, support médiatique particulièrement affectionné des jeunes". L'alcool serait la première causeévitable* de mortalité des 15-30 ans, selon l'Académie de Médecine. Elle invite donc le gouvernementà revenir aux "principes initi- BARThez L'Académie de médecine réclame notamment "l'affaiblissement continu de la loi Evin sous la pression du lobby alcoolier", jusqu'à autoriser la publicité sur l'internet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2CÀ</head><p>l'inverse de ce qui se fait en France, la mesure doit inciterà la consommation d'alcool dès l'âge de 18 ans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TITLE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Stagnation de la consommation d'alcool en France : "une défaite majeure pour la santé publique" mBART Santé : l'Académie de médecine demande des mesures plus fortes mBARThez alcool : l'Académie de médecine appelle le gouvernementà des mesures plus fortes BARThez Alcool : une " défaite majeure" pour la santé publique, selon l'Académie de médecine C2C La consommation d'alcool en forte hausse : l'Académie de médecine appellè a plus de fermeté </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>De petites dimensions (20 cm de largeur et 30 de hauteur), ces ouverturesà hauteur d'homme percéesà côté du porche des somptueux palais appartenant aux grandes familles florentines servaientàécouler le vin directement du producteur au consommateur.Au fil des siècles, ce détail architectural et sa fonction sont tombés dans les oubliettes de l'Histoire jusqu'à ce que Massimo Casprini, unérudit florentin, parteà leur redécouverte et y consacre un livre, "I finestrini del vino" ("Les fenêtresà vin"), publié en 2005.Ces fenêtres "ontété crééesà partir de 1532 après la chute de la République, quand les Médicis sont revenus au pouvoir et ont voulu favoriser l'agriculture, incitant les grands propriétaires florentins a investir dans les oliveraies et les vignes (...) tout en leur donnant des avantages fiscaux pour revendre directement leur production en ville", expliqueà l'AFP M. Casprini lors d'une promenadeà travers les rues de Florence dans la touffeur estivale.Unique restriction: "Ils pouvaient y vendre seulement le vin de leur propre production et sous un format particulier d'environ 1,4 litre"."L'autre fonction de ces petites fenêtresétait sociale, en permettant aux gens du peuple d'acquérir du vinà prix plus raisonnable que chez les commerçants, sans intermédiaire", ajoute-t-il, précisant dans un sourire qu'"à l'époque la consommation de vinétaiténorme".-Episodes de peste -A l'heure du coronavirus et de la distanciation sociale, Massimo Casprini rappelle que "grâceà ce système onévitait les contacts", alors qu'"épidémies etépisodes de pesté etaient très fréquents au XVIe siècle"."En effet, la fenêtreà vinétait fermée par un panneau de bois, le client se présentait et frappait avec le heurtoir,à l'intérieur il y avait un caviste qui prenait la bouteille vide et la remplissait. Il n'y avait donc pas de contact direct!" s'extasie le fringuant septuagénaire,également amateur de motos anciennes et auteur de quelque 70 ouvrages centrés sur la capitale toscane.Jusqu'ici, 267 de ces fenêtresà vin ontété répertoriées en Toscane, dont 149 dans le centre de Florence. "Il y en avait beaucoup plus!" estime M. Casprini, "presque tous les propriétaires terriens avaient une fenêtreà vin, mais nombre d'entre elles ont disparu, notamment lors des bombardements de la Seconde Guerre mondiale".Certaines ont aussiété murées, mais grâceà l'oeil de lynx de notre expert on réussit encoreà reconnaître les contours de leur encadrement en pietra serena (grès gris) ou pierre des carrières de Fiesole, près de Florence.Dans le fil du livre du professeur Casprini aété fondée une association, baptisée "Le buchette del vino", qui recense et appose une plaque sur chaque fenêtre. Son site internet (https://buchettedelvino.org/) propose même une carte interactive permettant de partir a leur découverte, ainsi qu'une galerie de photos et une présentation historique de ces petits trésors architecturaux.On y cite par exemple un guide en français de Florence datant de 1892 qui mentionne la fenêtre d'un palais: "cette cave assez renommée pour ses vins millésimés ne livre aux consommateurs que ceux provenant des propriétés de la marquise Leonia degli Albizi Frescobaldi".Tombées en désuétude, les "finestrini del vino" font aujourd'hui l'objet d'un regain d'intérêt et d'une forme de recyclage: présentoir de magasin, passe-plat dans un café, ou encore petit autel dédiéà la Vierge.Même si elles sont protégées par la loi, M. Casprini déplore que "trois fenêtres ont déjà disparu" depuis son premier recensement en 2005.</p><p>ABSTRACT Gold Florence, joyau de la Renaissance, peut s'enorgueillir d'un patrimoine mondialement célèbre, mais dont certains détails restent encore aujourd'hui méconnus: c'est le cas des discrètes "fenêtresà vin" ornant la façade de certains palais, qui permettaient la vente de vin "sans contact", un concept redevenu d'actualité en ces temps de coronavirus.</p><p>mBART "A l'heure du coronavirus et de la distanciation sociale, il n'y avait pas de contact direct !" A Florence, des fenêtresà vin, remplacées par des pierres, auraientété oubliées dans lesoubliettes de l'Histoire.</p><p>mBARThez Massimo Casprini, spécialiste des fenêtresà vins, est revenu mercredi sur la révolution de Florence (Italie) où il a redécouvert l'existence de ces ouvertures en plein air età ciel ouvert qui permettaient, autrefois,à des propriétaires de vins de revendre leur productionà la ville.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>De 1532à nos jours, les fenêtresà vin des palais anciens de Florence sont les plus souvent murées, un détail qui a sans doute survécuà l'épidémie de nouveau coronavirus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Au lieu de la pandémie de coronavirus, un jardin italien a retrouvé des crus du monde entier: ilsétaient des cavesà vin français,à quelques dizaines de mètres du sol, pour ne pasêtre contaminés par le Covid-19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TITLE</head><p>Gold Virus: comment la Florence des Médicis inventa la vente de vin "sans contact" mBART Les fenêtresà vin sont tombées dans les oubliettes de l'Histoire mBARThez Florence: les fenêtresà vin cachées dans un livre BARThez Florence: des fenêtresà vin traditionnellesà l'heure des peste C2C Les fenêtres de la Florence en "huile de vin" : actualité automobile, infos, scoop </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>L'ancien chef de l'Étatétait entendu depuis mardi matin, avec une interruption dans la nuit, dans les locaux de l'office anticorruption (OCLCIFF) situésà Nanterre (Hauts-de-Seine). L'ancien président de l'UMP a regagné son domicile parisien du XVIe arrondissement après la fin de sa gardeà vue.Également entendu, mais sous le statut de "suspect libre", Brice Hortefeux, un proche de l'ex-président qui occupa plusieurs postes ministériels pendant le quinquennat Sarkozy (2007-2012), a de son côté quitté les locaux de l'office anticorruption mardi soir, assurant sur Twitter avoir apporté des précisions pour "permettre de clore une succession d'erreurs et de mensonges".Depuis la publication, en mai 2012, par le site d'informations Mediapart d'un document libyen -attribuéà l'ex-chef des renseignements Moussa Koussa -accréditant un financement d'environ 50 millions d'euros, les investigations des juges ont considérablement avancé. Plusieurs protagonistes du dossier, dont plusieurs ex-responsables libyens, ont accrédité la thèse de versements illicites. Ziad Takieddine persiste et signeLe sulfureux homme d'affaires Ziad Takieddine a lui-même assuré avoir remis entre fin 2006 et début 2007 trois valises contenant 5 millions d'euros en provenance du régime de Kadhafià Nicolas Sarkozy, alors ministre de l'Intérieur, età son directeur de cabinet Claude Guéant. Sur BFMTV, il a réédité ses accusations mais répété que cet argent "n'était pas liéà la campagne présidentielle" de 2007."Cet argent faisait partie des accords entre les deux pays sur le contrôle des frontières maritimes, avecéchanges d'informations", a précisé l'homme d'affaires, mis en examen autour de ce dossier pour complicité de corruption et complicité de diffamation. "Il y avait un devoir de former en France deséquipes libyennes avant la livraison du matériel. Dans ce cadre-là, il y avait des formationsà destination de quelques centaines de Libyens. Ils ontétabli en France que ça allait coûter dans les cinq millions d'euros", a-t-il ajouté.Nicolas Sarkozy "est un vrai menteur et vous allez voir, il va passer son temps avec les juges d'instructionà dire 'non, non, non c'est pas vrai'. Tout ça pour gagner du temps, c'est sa méthode habituelle", aégalement lancé Ziad Takieddine sur BFMTV mercredi, assurant "dire la vérité". L'ancien chef de l'État a toujours rejeté ces mises en cause. D'autres dignitaires libyens ont démenti tout financement de la Libye de Mouammar Kadhafi, que Nicolas Sarkozy avait reçu en grande pompeà l'Élysée en 2007.De nouveauxéléments compromettantsOuverte notamment pour "détournements de fonds publics" et "corruption active et passive", l'enquête aétéélargie en janvierà des soupçons de "financement illégal de campagneélectorale", suiteà un rapport de l'office anticorruption qui pointe une circulation importante d'argent liquide dans l'entourage de ABSTRACT Gold Après une vingtaine d'heures, la gardeà vue de Nicolas Sarkozy s'est achevée mercredi soir. L'ancien président aété mis en examen pour "corruption passive", "financement illégal de campagneélectorale" et "recel de fonds publics libyens" et placé sous contrôle judiciaire dans le cadre de l'enquête sur des soupçons de financement de sa campagne présidentielle de 2007 par la Libye de Mouammar Kadhafi.</p><p>mBART Ziad Takieddine, mis en examen autour de l'affaire des soupçons de financement libyen de la campagne présidentielle de 2007 de Nicolas Sarkozy, a de nouveau quitté les locaux de sonétablissement,à Nanterre, dans la nuit de mardià mercredi.</p><p>mBARThez Cinq jours après la révélation d'un document par Mediapart, l'ancien président de l'UMP et principal suspect dans l'affaire des soupçons de financement libyen de sa campagne présidentielle de 2007 a quitté mardi soir les locaux où ilétait auditionné. Brice Hortefeux et Claude Guéant ont apporté des précisions.</p><p>BARThez L'ancien président de la République Nicolas Sarkozy a quitté mardi matin les locaux de l'office anticorruption où ilétait entendu. Les soupçons de financement libyen de sa campagne présidentielle de 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Nicolas Sarkozy est mis en examen dans le cadre de l'enquête sur les soupçons de financement libyen de sa campagne présidentielle de 2007. Selon plusieurs médias, l'ancien chargé de mission a dit mercredi n'être "pas au courant" de ce que l'ex-TITLE Gold Soupçons de financement libyen : Nicolas Sarkozy mis en examen mBART Affaire libyenne : Nicolas Sarkozy en gardeà vue mBARThez Affaire libyenne : Nicolas Sarkozy entendu par les juges BARThez Nicolas Sarkozy en gardeà vue, la piste d'un financement libyen s'éloigne C2C Nicolas Sarkozy est "un vrai traître" selon l'entourage de Nicolas Sarkozy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT Gold</head><p>Jean-Paul Dufrègne,élu de l'Allier, a fait sensation sur les réseaux sociaux aprèsêtre apparu dans un reportage de TF1. On le voyait roulerà plus de 120 km/heure sur une route secondaire.</p><p>mBART L'élu communiste de l'Allier aété filmé le 4 avril au journal de 20 heures de TF1, comme le relève LCI. Son compteur affichait 124 km/heure au-dessus de la limite autorisée. mBARThez Son compteur affichait 124 km/heure au-dessus de la limite autorisée.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>Unélu communiste de l'Allier aété filmé le temps d'un reportage par les caméras de TF1, dans la soirée du 4 avril, alors qu'il roulait 90 km/heure audessus de la limite autorisée.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Un député communiste du Puy-de-Dôme n'a pas manqué de se faire remarquer en pleine séance de questions au gouvernement. Ilétait l'invité de Jean-Marc Remontini, l'élu apparenté du PCF, qui a fait l'éloge de TITLE Gold Unéluépingléà 124 km/heure sur une route limitéeà 90 mBART Unélu communiste flashéà 124 km/heure sur TF1 mBARThez Unélu communiste flashéà 124 km/heure sur TF1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>Unélu communiste passe un sale quart d'heureà cause de son compteur de vitesse C2C Ce député du Doubs qui n'a plus le temps de répondre aux radars </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>Mais où est donc passé Gérald Thomassin ? L'acteur français qui avait obtenu un César en 1991 est introuvable depuis le 28 août dernier, rapporte RTL. Le comédienâgé de 45 ans devait se rendreà un rendez-vous judiciaire dans une affaire de meurtre. Mais il ne s'y est jamais rendu. Et depuis, c'est toute sa famille qui s'inquiète. Interrogé par RTL, le frère de l'acteur, Jérôme Thomassin, a montré toute son inquiétude avant d'apporter des détails sur la journée du 28 août.Selon lui, Gérald Thomassin a bien "pris le train Rochefort-Lyon pour se rendreà la confrontation avec deux autres mis en examen". Parmi ces hommes, précise RTL, le principal suspect dans cette affaire de meurtre dans un bureau de poste. Les avocats du comédien qui appartiennent au cabinet d'Éric Dupond-Moretti ont signalé "une disparition inquiétante" au commissariat de Rochefort (Charente-Maritime) où l'acteur vivait. En toutétat de cause, son frèreétait "très heureux de pouvoir se rendreà ce rendez-vous judiciaire." "L'affaire Burgod" L'affaire remonteà 2013, lorsque Gérald Thomassin est interpellé et mis en examen pour "vol avec arme et homicide sur une personne chargée d'une mission de service public". Une employée de La Poste, Catherine Burgod, enceinte, avaitété tuée de 28 coups de couteau. Tenu responsable, l'acteur avaitété incarcéré en 2013 avant d'être remis en liberté, mais placé sous contrôle judiciaire en octobre 2015. Sauf qu'il décide de briser son braceletélectronique et retourne en prison. Gérald Thomassin sort finalement en 2016, après trois ans de détention provisoire, la limite. L'affaire prend une autre tournure en 2017 et 2018 avec l'arrestation d'un suspect et la mise en examen d'un autre, mais la justice ne parvient toujours pasà trancher. La reconstitution du jeudi 29 août aurait dû permettre une confrontation entre les trois protagonistes, mais Gérald Thomassin ne s'est jamais présenté, au grand dam de l'avocate des parties civiles. Aujourd'hui, la famille de l'acteur demande de vraies recherches. Ses appels sonnent dans le vide. Ses proches craignent qu'il ait pu faire une mauvaise rencontre en cours de route. Le comédien avait reçu le César du Meilleur jeune espoir en 1991 pour son rôle dans "Le Petit Criminel" de Jacques Doillon, avant de connaitre une carrière ponctuée de plus ou moins grands rôles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Gérald Thomassin a disparu fin août selon ses proches qui s'inquiètent, révèle RTL. Il devait se rendreà un rendez-vous judiciaire pouréclairer une affaire d'homicide.</p><p>mBART</p><p>Gérald Thomassin a disparu fin août. Il devait se rendreà un rendez-vous judiciaire pouréclairer une affaire d'homicide, révèle RTL.</p><p>mBARThez Gérald Thomassin a disparu fin août. Il devait se rendre, selon RTL,à un rendez-vous judiciaire pouréclairer une affaire de meurtre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>Le comédien de 45 ans devait se rendreà un rendez-vous judiciaire dans une affaire de meurtre, mais il ne s'y est jamais rendu, rapporte RTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Gérald Thomassin a disparu fin août. Il se rendaità un rendez-vous judiciaire pouréclairer une affaire d'homicide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TITLE</head><p>Gold Mystérieuse disparition d'un acteur césarisé soupçonné de meurtre mBART Disparition de l'acteur Gérald Thomassin : sa famille s'inquiète mBARThez L'acteur Gérald Thomassin porté disparu depuis le 28 août BARThez L'acteur Gérald Thomassin porté disparu depuis le 28 août C2C Disparition de l'acteur Gérald Thomassin : la famille n'est plus introuvable BARThez Harcèlement de rue : vers une contravention de 4e classe ? C2C Harcèlement de rue : un rapport préconise une amende de 5à 5 euros </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>Le 18 octobre dernier, Jacline Mouraud se faisait connaître en publiant sur Facebook une vidéo dans laquelle elle poussait un "coup de gueule" contre le gouvernement. Aujourd'hui, la Bretonne a pris ses distances par rapport au mouvement, notamment faceà d'autres figures plus radicales commeÉric Drouet.Jacline Mouraud réfléchit désormaisà créer son propre parti, "la seule chose envisageable", comme elle l'explique au JDD.Nicolas Sarkozy, "le seul qui a des couilles"Cette figure des "gilets jaunes", accusée de faire le jeu de LREM estime que "le problème" d'Emmanuel Macron "c'est qu'il est jeune". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARThez</head><p>Dans les colonnes du JDD, la figure des "gilets jaunes" explique qu'elle envisage de se présenter aux européennes sur une liste La République en marche.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C2C</head><p>Retirée de la vie politique depuis plusieurs mois, Bretone Mouraud envisage de se lancer en politique. Et elle réfléchità quelque chose de plus, rapporte le JDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TITLE</head><p>Gold "Gilets jaunes" : Jacline Mouraud réfléchità créer son parti mBART "Gilets jaunes" : Jacline Mouraud lance son propre parti mBARThez "Gilets jaunes" : Jacline Mouraud prend ses distances BARThez La figure des "gilets jaunes" Jacline Mouraud va créer son propre parti C2C "Gilets jaunes" : Jacline Mouraud réfléchità sa propre candidature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>Invité du "Grand rendez-vous Europe 1/CNews/LesÉchos dimanche 8 avril, Jean-Luc Mélenchon a appelé "à faire baisser la température dans ce pays". En cause : les menaces de mort dont il ferait l'objet, ainsi que d'autresélus LFI. Le député des Bouches-du-Rhône a confirmé avoir récemment demandé que le ministre de l'Intérieur Gérard Collomb soit entendu dans l'enquête sur un projet d'attentat d'ultra-droite où il aété cité comme cible potentielle. "Je me suis porté partie civile dans cette affaire. J'ai appris en octobre dernier qu'un groupe de gens avait l'intention de me tuer, ainsi que (le secrétaire d'État) M. Castaner". Or pendant la campagne législative de juin 2017, "j'ai demandéàêtre protégé" car "j'avais reçuà Marseille des menaces de mort. On me l'a refusé, et puis après je découvre que le 28 mai, ils ont arrêté ce personnage (...) Quatre mois plus tard ils en arrêtent neuf autres quiétaient toujours en action pendant ces quatre mois". "LA RECRUDESCENCE D'UN EXTRÉMISME D'EXTRÊME DROITE EXTRÊMEMENT VIOLENT"Un ancien militant du groupuscule royaliste Action française en Provence, Alexandre Nisin, aété mis en examen début juillet pour association de malfaiteurs terroriste criminelle. Huit autres suspects ontété mis en examen, soupçonnés d'appartenirà son réseau. "Ni moi, ni Castaner n'avonsété prévenus de rien", a déploré l'ancien candidatà la présidentielle. "Sur 17 que nous sommes au groupe La France insoumise (à l'Assemblée, ndlr), il y en a cinq qui font l'objet de menaces de mort"(/BOLD], a-t-il par ailleurs révélé. Jean-Luc Mélenchon a dénoncé "la recrudescence d'un extrémisme d'extrême droite extrêmement violent, dans toutes sortes de villes, qui va jusqu'à des tentatives d'assassinat". "L'extrême droite doitêtre prise au sérieux comme danger de violence et de meurtre. C'est eux qui attaquentà Montpellier un amphi d'étudiants, c'est eux qui attaquentà Tolbiac, c'est eux qui me menacent de mort. C'est eux qui font des contrôles d'identité dans la rue dans au moins deux villes. Ç a suffit. Maintenant le ministre de l'Intérieur doit prendre au sérieux la menace que représentent les groupuscules radicalisés de l'extrême droite", a-t-il poursuivi."Il y a des groupes d'extrême droite qui prolifèrent dans le pays. Qui souvent ont commencé leurs premiers pas avec le Front national et qui maintenant vont au bout de cette logique", a-t-il ajouté.</p><p>ABSTRACT Gold Le leader de La France insoumise (LFI) dénonce la "recrudescence" d'une "extrême droite extrêmement violent(e)" en France, qui doitêtre "prise au sérieux" par le gouvernement.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(Le et al., 2019), the French equivalent ofGLUE (Wang et al., 2018).• CLS. The Cross-lingual Sentiment analysis dataset (Prettenhofer and Stein, 2010) is made of Amazon reviews to be classified as positive or negative. It contains 3 product categories: books, DVD and music. The train and test sets are balanced and contain 2000 examples (each) per product category. Following Le et al. (2019), we used 20% of the train set as validation set. • PAWSX. The Cross-lingual Adversarial Dataset for Paraphrase Identification (Yang et al., 2019) contains pairs of sentences, and the task is to predict whether they are semantically equivalent. There are 49401 examples for training, 1992 for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Le et al. (2019). These models are mBERT (Devlin et al., 2018), CamemBERT (Martin et al., 2019) and FlauBERT (Le et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>aux" de la loi. Pour un pictogramme plus visible pour les femmes enceintesÀ l'instar d'autres institutions et associations, l'Académie recommande d'interdire la publicité pour l'alcool et de faire figurer sur les boissons alcoolisées la mention "l'alcool est dangereux pour la santé" (et non le seul excès). L'Académie de médecine veutégalement voir taxées les boissons au gramme d'alcool et demande la mise en place d'un prix minimum de vente par gramme d'alcool, comme c'est le cas en Ecosse depuis un an. Elle réclameégalement un pictogramme plus grand et plus lisible sur les bouteilles pour "dissuader de toute consommation la femme enceinte ou qui désire l'être".L'académie de médecine pointe clairement la responsabilité du lobby alcoolier. "Malgré l'enjeu de prévenir la première cause de retard mentalévitable* du nouveau-né et de l'enfant, les discussions pour l'agrandir et le contraster s'enlisent depuis des années faceà l'opposition farouche du lobby alcoolier". L'alcool serait la première cause de retard mental de l'enfant et de démence précoce souligne l'organisme. Un quart des Français boit trop Dans des chiffres publiés mi-février, Santé publique France avait indiqué que la consommation des Français n'avait quasiment pas reculé depuis 10 ans, passant de 27 gà 26 g d'alcool pur par jour entre 2009 et 2015. "C'est en février 2019 que Santé Publique France annonce que la consommation française d'alcool est la même en 2017 qu'en 2013", note l'académie dans son communiqué. Près d'un quart des Français, soit environ 10,5 millions d'adultes, boivent trop d'alcool, avaitégalement estimé fin mars Santé publique France. L'agence sanitaire a diffusé de nouveaux repères de consommation, résumés par le message "pour votre santé, c'est maximum deux verres par jour, et pas tous les jours".L'alcool constitue la deuxième cause de mortalitéévitable* après le tabac, avec 41.000 décès qui lui sont attribuables chaque année en France, 30.000 hommes et 11.000 femmes. L'alcool "est impliqué dans 40% des violences faites aux femmes et aux enfants et un tiers des décès par accidents de la route", ajoute l'Académie dans son communiqué.ABSTRACTGoldElle demande des "mesures plus fortes" pour lutter contre les problèmes de santé causés en France par une consommation d'alcool qui ne diminue plus.mBART En février 2019, Santé publique France avait indiqué que la consommation des Français n'avait quasiment pas reculé depuis 10 ans. mBARThez Près d'un quart des Français boivent trop d'alcool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>"Il devrait y avoir unâge minimum pourêtre président : 50 ans", souligne Jacline Mouraud.Dans le JDD, elle raconte d'ailleurs avoir voté blanc lors de la dernière présidentielle. En 2007 et 2012, c'est Nicolas Sarkozy, "le seul qui a des couilles", que la figure des "gilets jaunes" avait soutenu. En attendant de se lancer, pas question pour elle en tous les cas d'être candidate aux européennes sur une liste de La République en marche. ABSTRACT Gold L'une des figures du mouvement ne sera toutefois pas candidate aux prochaineś elections européennes. mBART Jacline Mouraud, figure des "gilets jaunes", estime que le président d'Emmanuel Macron est trop jeune pourêtre président. mBARThez Dans un entretien au JDD, la figure des "gilets jaunes" Jacline Mouraud révèle qu'elle réfléchità créer son propre parti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>mBART S'il dénonce la recrudescence d'un extrémisme d'extrême droite "extrêmement violent" dans certaines villes, le chef de file de La France insoumise (LFI), Jean-Luc Mélenchon, s'est attaqué au ministre de l'Intérieur, Gérard Collomb.mBARThez Au micro d'Europe 1 dimanche 8 avril, Jean-Luc Mélenchon a réagi aux menaces de mort dont il fait l'objet et dénoncé "la recrudescence d'un extrémisme d'extrême droite extrêmement violent". BARThez -Le chef de file de La France insoumise et ancien candidatà la présidentielle est vent debout contre le projet d'attentat déjouéà Marseille. Il estime que le ministre de l'Intérieur, Gérard Collomb, est menacé de mort par un groupe d'extrême droite. C2C Selon le leader de La France insoumise (LFI), le député des Bouches-du-Rhône, Jean-Luc Mélenchon, "rappelleà tous ceux suspectés d'avoir menacé d'assassiner le ministre de l'Intérieur, ce que conteste le parti et TITLE Gold VIDÉO. Cinq députés de La France insoumise font l'objet de menaces de mort, selon Jean-Luc Mélenchon mBART Menaces de mort : Jean-Luc Mélenchon s'en prendà Castaner mBARThez Jean-Luc Mélenchon dénonce les "menaces de mort" de Gérard Collomb BARThez VIDÉO. Jean-Luc Mélenchon dénonce les menaces de mort dont Gérard Collomb est victime C2C Projet VIDÉO. Menace de mortà Marseille : Jean-Luc Mélenchon menace de démissionner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>. To clean the corpus from noisy examples, we</cell></row><row><cell>used the script 6 provided by Le et al. (2019). Note</cell></row><row><cell>that we disabled the Moses tokenizer, as we used</cell></row><row><cell>SentencePiece which does not require any pre-</cell></row><row><cell>6 https://github.com/getalp/Flaubert</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Sizes (column 2) are given in thousands of documents. Document and summary lengths are in words. Vocab sizes are in thousands of tokens.</figDesc><table><row><cell>Dataset</cell><cell cols="4">% of novel n-grams in gold summary unigrams bigrams trigrams 4-grams</cell><cell>R-1</cell><cell>LEAD R-2</cell><cell>R-L</cell><cell cols="3">EXT-ORACLE R-1 R-2 R-L</cell></row><row><cell>CNN</cell><cell>16.75</cell><cell>54.33</cell><cell>72.42</cell><cell>80.37</cell><cell cols="6">29.15 11.13 25.95 50.38 28.55 46.58</cell></row><row><cell>DailyMail</cell><cell>17.03</cell><cell>53.78</cell><cell>72.14</cell><cell>80.28</cell><cell cols="6">40.68 18.36 37.25 55.12 30.55 51.24</cell></row><row><cell>NY Times</cell><cell>22.64</cell><cell>55.59</cell><cell>71.93</cell><cell>80.16</cell><cell cols="6">31.85 15.86 23.75 52.08 31.59 46.72</cell></row><row><cell>XSum</cell><cell>35.76</cell><cell>83.45</cell><cell>95.50</cell><cell>98.49</cell><cell>16.30</cell><cell>1.61</cell><cell cols="2">11.95 29.79</cell><cell>8.81</cell><cell>22.65</cell></row><row><cell>OrangeSum Title</cell><cell>26.54</cell><cell>66.70</cell><cell>84.18</cell><cell>91.12</cell><cell cols="6">19.84 08.11 16.13 31.62 17.06 28.26</cell></row><row><cell>OrangeSum Abstract</cell><cell>30.03</cell><cell>67.15</cell><cell>81.94</cell><cell>88.3</cell><cell cols="6">22.21 07.00 15.48 38.36 20.87 31.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Degree of abstractivity of OrangeSum compared with that of other datasets, as reported in Narayan et al.(2018). It can be observed that XSum and OrangeSum are more abstractive than traditional summarization datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Doc 19233 from OrangeSum's test set, and associated summaries. Incorrect information in orange. C2C stands for CamemBERT2CamemBERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Layers Params Vocab. size Pretraining hours Pretraining GPUs Corpus size</figDesc><table><row><cell>BASE</cell><cell>BART-random BARThez (ours)</cell><cell>12 12</cell><cell>165 165</cell><cell>50 50</cell><cell>0 60</cell><cell>NA 128</cell><cell>NA 66</cell></row><row><cell>LARGE</cell><cell>C2C mBART mBARThez (ours)</cell><cell>24 24 24</cell><cell>274 610 458</cell><cell>32 250 101</cell><cell>24 432 30</cell><cell>256 256 256 + 128</cell><cell>138 1369 1369 + 66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Summary of the models used in our experiments. Parameters are given in millions, vocab sizes in thousands, and corpus sizes in GB. C2C stands for CamemBERT2CamemBERT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>LEAD 22.21 07.00 15.48 14.66/68.02 19.84 08.11 16.13 15.75/68.43 EXT-ORACLE 38.36 20.87 31.08 28.99/73.39 31.62 17.06 28.26 25.15/71.95 LARGE BART-random 27.67 08.23 18.50 22.53/70.97 28.76 13.15 25.20 29.67/73.65 BARThez (ours) 31.44 12.77 22.23 27.51/72.84 40.86 23.68 36.03 40.61/77.74 CamemBERT2CamemBERT 29.23 09.79 19.95 25.53/72.09 34.92 18.04 30.83 36.40/76.17 mBART 31.85 13.10 22.35 27.80/72.94 40.74 23.70 36.04 40.42/77.67 mBARThez (ours) 32.67 13.73 23.18 28.80/73.32 41.08 24.11 36.41 41.42/78.05</figDesc><table><row><cell></cell><cell></cell><cell>Abstract</cell><cell></cell><cell></cell><cell></cell><cell>Title</cell><cell></cell></row><row><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BertScore</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BertScore</cell></row></table><note>BASE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on OrangeSum. The two BertScore scores are with/without rescaling(Zhang et al., 2019).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">OrangeSum Abstract</cell><cell></cell><cell></cell><cell cols="2">OrangeSum Title</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">1-grams 2-grams 3-grams 4-grams 1-grams 2-grams 3-grams 4-grams</cell></row><row><cell></cell><cell>Gold</cell><cell>30.03</cell><cell>67.15</cell><cell>81.94</cell><cell>88.30</cell><cell>26.54</cell><cell>66.70</cell><cell>84.18</cell><cell>91.12</cell></row><row><cell>BASE</cell><cell>BARThez (ours)</cell><cell>10.93</cell><cell>34.03</cell><cell>47.97</cell><cell>56.80</cell><cell>16.69</cell><cell>51.70</cell><cell>72.05</cell><cell>82.49</cell></row><row><cell>LARGE</cell><cell>C2C mBART mBARThez (ours)</cell><cell>39.43 13.40 15.48</cell><cell>79.12 38.94 43.34</cell><cell>92.04 53.69 58.53</cell><cell>96.28 62.61 67.30</cell><cell>33.82 16.89 17.79</cell><cell>75.74 52.28 53.41</cell><cell>91.77 73.12 73.38</cell><cell>96.71 82.74 82.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Proportion of novel n-grams in the generated summaries. C2C stands for CamemBERT2CamemBERT. Note that C2C's high scores are misleading as many of the introduced words are irrelevant.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Length Repetitions (%)</cell></row><row><cell>ABSTRACT</cell><cell>Gold mBART mBARThez BARThez C2C</cell><cell>32.12 28.20 29.45 29.10 30.68</cell><cell>11.47 7.47 8.60 14.47 23.00</cell></row><row><cell></cell><cell>Gold</cell><cell>11.42</cell><cell>0.93</cell></row><row><cell>TITLE</cell><cell>mBART mBARThez BARThez</cell><cell>10.79 11.03 11.19</cell><cell>1.73 2.27 2.73</cell></row><row><cell></cell><cell>C2C</cell><cell>11.23</cell><cell>19.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Summary statistics.</figDesc><table><row><cell></cell><cell>System</cell><cell>Score</cell></row><row><cell></cell><cell>Gold</cell><cell>14.29</cell></row><row><cell>BASE</cell><cell>BARThez (ours)</cell><cell>21.43</cell></row><row><cell>LARGE</cell><cell cols="2">CamemBERT2CamemBERT -75.00 mBART 11.90 mBARThez (ours) 27.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Human evaluation using Best-Worst Scaling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Accuracy on discriminative tasks. We report the average accuracy over 3 runs, with standard deviation as subscript. † are taken fromLe et al. (2019).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.</figDesc><table><row><cell></cell><cell>Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad-</cell></row><row><cell></cell><cell>ina Williams, Samuel R Bowman, Holger Schwenk,</cell></row><row><cell></cell><cell>and Veselin Stoyanov. 2018. Xnli: Evaluating cross-</cell></row><row><cell></cell><cell>lingual sentence representations. arXiv preprint</cell></row><row><cell></cell><cell>arXiv:1809.05053.</cell></row><row><cell></cell><cell>Pieter Delobelle, Thomas Winters, and Bettina</cell></row><row><cell></cell><cell>Berendt. 2020. Robbert: a dutch roberta-based lan-</cell></row><row><cell></cell><cell>guage model. arXiv preprint arXiv:2001.06286.</cell></row><row><cell></cell><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and</cell></row><row><cell></cell><cell>Kristina Toutanova. 2018. Bert: Pre-training of deep</cell></row><row><cell></cell><cell>bidirectional transformers for language understand-</cell></row><row><cell></cell><cell>ing. arXiv preprint arXiv:1810.04805.</cell></row><row><cell></cell><cell>Andreas Eisele and Yu Chen. 2010. Multiun: A mul-</cell></row><row><cell></cell><cell>tilingual corpus from united nation documents. In</cell></row><row><cell></cell><cell>LREC.</cell></row><row><cell></cell><cell>Suchin Gururangan, Ana Marasović, Swabha</cell></row><row><cell></cell><cell>Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,</cell></row><row><cell></cell><cell>and Noah A Smith. 2020. Don't stop pretraining:</cell></row><row><cell></cell><cell>Adapt language models to domains and tasks. arXiv</cell></row><row><cell></cell><cell>preprint arXiv:2004.10964.</cell></row><row><cell></cell><cell>Karl Moritz Hermann, Tomas Kocisky, Edward</cell></row><row><cell></cell><cell>Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-</cell></row><row><cell></cell><cell>leyman, and Phil Blunsom. 2015. Teaching ma-</cell></row><row><cell></cell><cell>chines to read and comprehend. In Advances in</cell></row><row><cell></cell><cell>neural information processing systems, pages 1693-</cell></row><row><cell></cell><cell>1701.</cell></row><row><cell></cell><cell>Jeremy Howard and Sebastian Ruder. 2018. Universal</cell></row><row><cell></cell><cell>language model fine-tuning for text classification.</cell></row><row><cell></cell><cell>arXiv preprint arXiv:1801.06146.</cell></row><row><cell></cell><cell>Diederik P Kingma and Jimmy Ba. 2014. Adam: A</cell></row><row><cell></cell><cell>method for stochastic optimization. arXiv preprint</cell></row><row><cell></cell><cell>arXiv:1412.6980.</cell></row><row><cell></cell><cell>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-</cell></row><row><cell></cell><cell>ton. 2012. Imagenet classification with deep con-</cell></row><row><cell></cell><cell>volutional neural networks. In Advances in neural</cell></row><row><cell></cell><cell>information processing systems, pages 1097-1105.</cell></row><row><cell>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa-tion for Computational Linguistics, 5:135-146.</cell><cell>Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tok-enizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.</cell></row><row><cell></cell><cell>Yuri Kuratov and Mikhail Arkhipov. 2019. Adaptation</cell></row><row><cell></cell><cell>of deep bidirectional multilingual transformers for</cell></row><row><cell></cell><cell>russian language. arXiv preprint arXiv:1905.07213.</cell></row></table><note>José Cañete, Gabriel Chaperon, Rodrigo Fuentes, and Jorge Pérez. 2020. Spanish pre-trained bert model and evaluation data. In to appear in PML4DC at ICLR 2020. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott,Guillaume Lample and Alexis Conneau. 2019. Cross- lingual language model pretraining. arXiv preprint arXiv:1901.07291.mBART Cinq personnes sont mortes mercredi dans l'accident d'une usine de pesticides qui s'est produite en Inde,à la suite du confinement liéà l'épidémie de Covid- 19, ont indiqué des responsables.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 12158.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 33555.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 25148.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 34657.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 22208.</figDesc><table><row><cell></cell><cell>Jean-Paul Dufrègne a passé un sale quart d'heure sur les réseaux sociaux</cell></row><row><cell></cell><cell>mercredi soir. Cetélu communiste de l'Allier aété filmé par les caméras</cell></row><row><cell></cell><cell>de TF1, dans un reportage diffusé le 4 avril au journal de 20 heures. Mais</cell></row><row><cell></cell><cell>téléspectateurs et internautes n'ont nullement prêté attention aux arguments du</cell></row><row><cell></cell><cell>député sur les inquiétudes persistantes des territoires ruraux et la réforme in-</cell></row><row><cell></cell><cell>stitutionnelle sur laquelle planche le gouvernement. Non, ilsétaient bien trop</cell></row><row><cell></cell><cell>captivés par son compteur de vitesse, filmé le temps de quelques plans par</cell></row><row><cell></cell><cell>les caméras de la première chaîne, comme le relève LCI.Car, sur une route</cell></row><row><cell></cell><cell>départementale limitéeà 90 km/heure, Jean-Paul Dufrègne avait le pied au</cell></row><row><cell>Document</cell><cell>plancher. Son compteur affichait 124 km/heure, plus de 30 km/heure au-dessus de la limite autorisée. Une infraction que n'ont pas manqué de relever de nom-</cell></row><row><cell></cell><cell>breux internautes. " Trois points et 135 euros d'amende ", note un utilisateur de</cell></row><row><cell></cell><cell>Twitter. " Bonjour, les limitations de vitesse ne s'appliquent pas aux parlemen-</cell></row><row><cell></cell><cell>taires ? ", ironise un autre.Opposant au 80km/heureCertains ont par ailleurs</cell></row><row><cell></cell><cell>fait le lien entre les positions politiques de l'élu communiste et cet excès de</cell></row><row><cell></cell><cell>vitesse. Car Jean-Paul Dufrègne est un farouche opposant au projet du gou-</cell></row><row><cell></cell><cell>vernement de limiter le réseau français de routes secondairesà 80 km/heure.</cell></row><row><cell></cell><cell>Avec une trentaine d'autresélus du Massif Central, il avait d'ailleurs adressé</cell></row><row><cell></cell><cell>une lettre ouverteà Emmanuel Macron sur le sujet, dénonçant une mesure "</cell></row><row><cell></cell><cell>injuste et pénalisante ", et un frein au développement du Massif Central.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 :</head><label>16</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 22077.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 17 :</head><label>17</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 22168.DocumentDans un rapport adressé aux ministres de l'Intérieur, de la Justice, età la secrétaire d'Etatà l'Egalité femmes-hommes Marlène Schiappa, les cinq députés chargés d'étudier la verbalisation du harcèlement de rue recommandent la mise en place d'"une contravention de 4e classe d'outrage sexiste et sexuel". L'infraction devraêtre constatée "en flagrance" par les agents de la toute récente "police de proximité du quotidien", précise leur texte, qui, selon les informations du Huffington Post, devraitêtre remis mercredi 28 février.Jusqu'à 1.500 euros d'amendesLe montant de l'amende forfaitaire serait de 90 euros pour un paiement immédiat, 200 euros pour un paiement sous 15 jours et 350 euros en peine majorée. En cas de circonstances aggravantes (si l'auteur est dépositaire de l'autorité publique, en cas de réunion, ou de bande organisée), une Ils estiment nécessaire de "définir une nouvelle infraction visantà sanctionner, entre autres, les gestes déplacés, les sifflements, les regards insistants ou remarques obscènes, le fait de suivre volontairementà distance une personne créant ainsi une situation d'angoisse", soulignent-ils.68% des Français favorables aux amendesLe rapport souhaiteégalement que les auteurs participentà un stage de sensibilisationà l'égalité femmeshommes, et que la police municipale et les agents des services de sécurité des transports soient habilitésà constater cette infraction. D'après un sondage Opinionway réalisé pour Public Sénat, Les Echos et Radio Classique et publié le 5 février, une large majorité de Français est favorablè a la mise en place d'une amende pénalisant le harcèlement de rue.À la question "êtes-vous favorable ou pas favorableà ce que le harcèlement de rue (sifflements, remarques...Elabe pour Le Huffington Post, 54% des Français sont opposés au projet de loi sur le harcèlement de rue. Une première en soi, alors que la question de l'emprise sexuelle se pose déjà : les contraventions seront en effet posées TITLE Gold Harcèlement de rue : bientôt une amende immédiate de 90 euros ? mBART Harcèlement de rue : vers une contravention de 4e classe ? mBARThez Harcèlement de rue : vers une contravention de 4e classe ?</figDesc><table><row><cell></cell><cell></cell><cell>contravention de 5e classe (jusqu'à 1.500 euros) pourraitêtre délivrée par un tribunal de po-</cell></row><row><cell></cell><cell></cell><cell>lice.Pour Sophie Auconie (UDI, Agir et Indépendants), Laetitia Avia (LREM), Erwan Balanant</cell></row><row><cell></cell><cell></cell><cell>(Modem), Elise Fajgeles (LREM) et Marietta Karamanli (Nouvelle gauche), le harcèlement</cell></row><row><cell></cell><cell></cell><cell>subi dans l'espace public est un "fléau". ) soit passible</cell></row><row><cell></cell><cell></cell><cell>d'une amende ?", 68 % des personnes interrogées se disent favorables (40 % "plutôt favorables"</cell></row><row><cell></cell><cell></cell><cell>et 28 % "toutà fait favorables"). 30 % y sont opposés (23 % "plutôt opposés" et 7 % "toutà</cell></row><row><cell></cell><cell></cell><cell>fait opposés") et 2 % ne se prononcent pas.</cell></row><row><cell></cell><cell></cell><cell>Des parlementaires préconisent de créer une infraction d'"outrage sexiste"</cell></row><row><cell></cell><cell>Gold</cell><cell>sanctionnant d'une amende immédiate de 90 euros "tout propos, comporte-</cell></row><row><cell></cell><cell></cell><cell>ment ou pressionà caractère sexiste ou sexuel" dans l'espace public.</cell></row><row><cell></cell><cell></cell><cell>Selon un rapport, dévoilé par le Huffington Post, le gouvernement envisage</cell></row><row><cell>ABSTRACT</cell><cell>mBART mBARThez</cell><cell>une amende forfaitaire de 90 euros pour lutter contre le harcèlement de rue. En cas de circonstances aggravantes, elle pourraitêtre délivrée par un tribunal de police. Dans un rapport adressé aux ministres de l'Intérieur, de la Justice età la secrétaire d'Etatà l'Egalité femmes-hommes, les députés chargés d'étudier la verbalisation du harcèlement de rue recommandent la mise en place d'une</cell></row><row><cell></cell><cell></cell><cell>contravention de 4e classe.</cell></row><row><cell></cell><cell></cell><cell>D'après un sondage Opinionway réalisé pour Public Sénat, Les Echos et Radio</cell></row><row><cell></cell><cell>BARThez</cell><cell>Classique, une large majorité de Français sont favorablesà la mise en place</cell></row><row><cell></cell><cell></cell><cell>d'une amende pénalisant le harcèlement de rue.</cell></row><row><cell></cell><cell></cell><cell>Selon un sondage</cell></row><row><cell></cell><cell>C2C</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 18 :</head><label>18</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 22423.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 19 :</head><label>19</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 19233.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 20 :</head><label>20</label><figDesc>C2C stands for CamemBERT2CamemBERT. OrangeSum document 22060.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">named after a legendary French goalkeeper, Fabien Barthez: https://en.wikipedia.org/wiki/Fabien_Barthez</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">There is no generative task in GLUE or superGLUE(Wang et al., 2019)  either.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://actu.orange.fr/, 'Actu' means News. 9 root URLs are https://actu.orange.fr/ for all categories except https://auto.orange.fr/news/ for automotive.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/Tixierae/OrangeSum</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are thankful to the National Center for Scientific Research (CNRS) for giving us access to their Jean Zay supercomputer, under allocation 2020-AD011011499.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arabert: Transformer-based model for arabic language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wissam</forename><surname>Antoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fady</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Hajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2020 Workshop Language Resources and Evaluation Conference 11-16</title>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clle</forename><surname>Atilf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Corpus journalistique issu de l&apos;est républicain. ORTOLANG (Open Resources and TOols for LANGuage</title>
		<ptr target="www.ortolang.fr" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
