<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The NarrativeQA Reading Comprehension Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
							<email>tkocisky@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
							<email>schwarzjn@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>pblunsom@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
							<email>melisgl@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The NarrativeQA Reading Comprehension Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language understanding seeks to create models that read and comprehend text. A common strategy for assessing the language understanding capabilities of comprehension models is to demonstrate that they can answer questions about documents they read, akin to how reading comprehension is tested in children when they are learning to read. After reading a document, a reader usually can not reproduce Title: Ghostbusters II Question: How is Oscar related to Dana? Answer: her son Summary snippet: . . . Peter's former girlfriend Dana Barrett has had a son, Oscar. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Story snippet:</head><p>DANA (setting the wheel brakes on the buggy) Thank you, Frank. I'll get the hang of this eventually.</p><p>She continues digging in her purse while Frank leans over the buggy and makes funny faces at the baby, OSCAR, a very cute nine-month old boy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FRANK (to the baby)</head><p>Hiya, Oscar. What do you say, slugger?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FRANK (to Dana)</head><p>That's a good-looking kid you got there, Ms. Barrett.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Example question-answer pair. The snippets here were extracted by humans from summaries and the full text of movie scripts or books, respectively, and are not provided to the model as supervision or at test time. Instead, the model will need to read the full text and locate salient snippets based solely on the question and its reading of the document in order to generate the answer.</p><p>the entire text from memory, but often can answer questions about underlying narrative elements of the document: the salient entities, events, places, and the relations between them. Thus, testing understanding requires creation of questions that examine high-level abstractions instead of just facts occurring in one sentence at a time.</p><p>Unfortunately, superficial questions about a document may often be answered successfully (by both humans and machines) using a shallow pattern match-ing strategies or guessing based on global salience. In the following section, we survey existing QA datasets, showing that they are either too small or answerable by shallow heuristics (Section 2). On the other hand, questions which are not about the surface form of the text, but rather about the underlying narrative, require the formation of more abstract representations about the events and relations expressed in the course of the document. Answering such questions requires that readers integrate information which may be distributed across several statements throughout the document, and generate a cogent answer on the basis of this integrated information. That is, they test that the reader comprehends language, not just that it can pattern match. We present a new task and dataset, which we call NarrativeQA, which will test and reward artificial agents approaching this level of competence (Section 3).</p><p>The dataset consists of stories, which are books and movie scripts, with human written questions and answers based solely on human-generated abstractive summaries. For the RC tasks, questions may be answered using just the summaries or the full story text. We give a short example of a sample movie script from this dataset in <ref type="figure">Figure 1</ref>. Fictional stories have a number of advantages as a domain. First, they are largely self-contained: beyond the basic fundamental vocabulary of English, all the information about salient entities and concepts required to understand the narrative is present in the document, with the expectation that a reasonably competent language user would be able to understand it. 1 Second, story summaries are abstractive and generally written by independent authors who know the work only as a reader. We make the dataset available online. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Reading Comprehension Datasets and Models</head><p>There are a large number of datasets and associated tasks available for the training and evaluation of read-1 For example, new names and words may be coined by the author (e.g. "muggle" in Harry Potter novels) but the reader need only appeal to the book itself to understand the meaning of these concepts, and their place in the narrative. This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension, and is in part tested as part of the question answering tasks we present.</p><p>2 http://deepmind.com/publications ing comprehension models. We summarize the key features of a collection of popular recent datasets in <ref type="table" target="#tab_1">Table 1</ref>. In this section, we briefly discuss the nature and limitations of these datasets and their associated tasks.</p><p>MCTest <ref type="bibr" target="#b21">(Richardson et al., 2013</ref>) is a collection of short stories, each with multiple questions. Each such question has set of possible answers, one of which is labelled as correct. While this could be used as a QA task, the MCTest corpus is in fact intended as an answer selection corpus. The data is human generated, and the answers can be phrases or sentences. The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size.</p><p>In contrast, CNN/Daily Mail <ref type="bibr" target="#b12">(Hermann et al., 2015)</ref>, Children's Book Test (CBT) <ref type="bibr" target="#b14">(Hill et al., 2016)</ref>, and BookTest  each provide large amounts of question-answer pairs. Questions are Cloze-form (predict the missing word) and are produced from either short abstractive summaries (CNN/Daily Mail) or from next sentence in the document the context was taken from (CBT and Book-Test). The tasks associated with these datasets are all selecting an answer from a set of options, which is explicitly provided for CBT and BookTest, and is implicit for CNN/Daily Mail, as the answers are always entities from the document. This significantly favors models that operate by pointing to a particular token (or type). Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) , exploit precisely this bias in the data. However, these models are inappropriate for answers requiring synthesis of a new answer. This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event.</p><p>SQuAD <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> and NewsQA <ref type="bibr" target="#b25">(Trischler et al., 2016)</ref>   because no document span would contain its answer. While they provide a large number of questions, these are from a relatively small number of documents, which are themselves fairly short, thereby limiting the lexical and topical diversity models trained on this data can cope with. While the answers are multiword phrases, the spans are generally short and rarely cross sentence boundaries. Simple models scoring and/or extracting candidate spans conditioned on the question and superficial signal from the rest of the document do well <ref type="bibr">(Seo et al., 2016, e.g.)</ref>. These models will not trivially generalize to problems where the answers are not spans in the document, supervision for spans is not provided, or several discontinuous spans are needed to generate a correct answer. This restricts the scalability and applicability of models doing well on SQuAD or NewsQA to more complex problems. MS MARCO <ref type="bibr" target="#b17">(Nguyen et al., 2016</ref>) presents a bolder challenge: questions are paired with sets of snippets ("context passages") that contain the information necessary to answer the question, and answers are free-form human generated text. However, as no restriction was placed on annotators preventing them from copying answers from source documents, many answers are in fact verbatim copies of short spans from the context passages. Models which do well on SQuAD (e.g. <ref type="bibr" target="#b26">Wang and Jiang (2016)</ref>, <ref type="bibr" target="#b28">Weissenborn et al. (2017)</ref>), extracting spans or pointing, do well here too, and the same concerns as above about the general applicability of solutions to this dataset to larger reading comprehension problems applies.</p><p>SearchQA <ref type="bibr" target="#b9">(Dunn et al., 2017</ref>) is a recent dataset in which the context for each question is a set of documents retrieved by a search engine using the question as the query. However, in contrast with previous datasets neither questions nor answers were produced by annotating the context documents, but rather the context documents were retrieved after collecting pre-existing question-answer pairs. As such, it is not open to same annotation bias as the datasets discussed above. However, upon examining answers in the Jeopardy data used to construct this dataset, one finds that 80% of answers are bigrams or unigrams, and 99% are 5 tokens or fewer. Of a sample of 100 answers, 72% are named entities, all are short noun-phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Limitations.</head><p>We see several limitations of the scope and depth of the RC problems in existing datasets. First, several datasets are small (MCTest) or not overly naturalistic (bAbI; <ref type="bibr" target="#b29">Weston et al. (2015)</ref>). Second, in more naturalistic documents, a majority of questions require only a single sentence to locate supporting information for answering <ref type="bibr" target="#b7">(Chen et al., 2016;</ref><ref type="bibr" target="#b20">Rajpurkar et al., 2016)</ref>. This, we suspect, is largely an artifact of the question generation methodology, in which annotators have created questions from a context document, or where context documents that explicitly answer a question are identified using a search engine. Although the factoidlike Jeopardy questions of SearchQA also appears to favor questions answerable with local context. Finally, we see further evidence of the superficiality of the questions in the architectures that have evolved to solve them, which tend to exploit span selection based on representations derived from local context and the query <ref type="bibr">(Seo et al., 2016;</ref><ref type="bibr" target="#b27">Wang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NarrativeQA: A New Dataset</head><p>In this section, we introduce our new dataset, Nar-rativeQA, which addresses many of the limitations identified in existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Desiderata</head><p>From the above discussed features and limitations, we define our desiderata as follows. We wish to construct a dataset with a large number of questionanswer pairs based on either a large number of supporting documents or from a smaller collection of large documents. This permits the training of neural network-based models over word embeddings and provide decent lexical coverage and diversity. The questions and answers should be natural, unconstrained, and human generated, and answering questions should frequently require reference to several parts or a larger span of the context document rather than superficial representations of local context. Furthermore, we want annotators to privilege writing answers expressed in their own words, and consider higher-level relations between entities, places, and events, rather than copy short spans of the document.</p><p>Furthermore, we want to evaluate models both on the fluency and correctness of generated free-form answers, and as an answer selection problem, which requires the provision of sensible distractors to the correct answer. Finally, the scope and complexity of the QA problem should be such that current models struggle, while humans are capable of solving the task correctly, so as to motivate further research into the development of models seeking human reading comprehension ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Collection Method</head><p>We will consider complex, self-contained narratives as our documents/stories. To make the annotation tractable and lead annotators towards asking nonlocalized questions, we will only provide them human written summaries of the stories for generating the question-answer pairs. We present both books and movie scripts as stories in our dataset. Books were collected from Project Gutenberg 3 and movie scripts scraped from the web. <ref type="bibr">4</ref> We matched our stories with plot summaries from Wikipedia using titles and verified the matching with help from human annotators. The annotators were asked to determine if both the story and the summary refer to a movie or a book (as some books are made into movies), or if they are the same part in a series produced in the same year. In this way we obtained 1,567 stories. This provides with a smaller set of documents, compared to the other datasets, but the documents are long which provides us with good lexical coverage and diversity. The bottleneck for obtaining a larger number of publicly available stories was finding corresponding summaries.</p><p>Annotators on Amazon Mechanical Turk were instructed to write 10 question-answer pairs each based solely on a given summary. Reading and annotating summaries is tractable unlike writing questions and answers based on the full stories, and moreover, as the annotators never see the full stories we are much less likely to get questions and answers which are extracted from a localized context.</p><p>Annotators were instructed to imagine that they are writing questions to test students who have read the full stories but not the summaries. We required questions that are specific enough, given the length and complexity of the narratives, and to provide a diverse set of questions about characters, events, why this happened, and so on. Annotators were encouraged to use their own words and we prevented them from copying. <ref type="bibr">5</ref> We asked for answers that are grammatical, complete sentences, and explicitly allowed short answers (one word, or a few-word phrase, or a short sentence) as we think that answering with a full sentence is frequently perceived as artificial when asking about factual information. Annotators were asked to avoid extra, unnecessary information in the question or the answer, and to avoid yes/no questions or questions about the author or the actors.</p><p>About 30 question-answer pairs per summary were obtained. The result is a collection of human written natural questions and answers. As we have multiple questions per summary/story, this allows us to consider answer selection (from among the 30) as a simpler version of the QA than answer generation from scratch. Answer selection <ref type="bibr" target="#b13">(Hewlett et al., 2016)</ref> and multiple-choice question answering <ref type="bibr" target="#b21">(Richardson et al., 2013;</ref><ref type="bibr" target="#b14">Hill et al., 2016)</ref> are frequently used. We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable, given the summary, and provide an answer if it was. All but 2.3% of the questions were judged as answerable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Core Statistics</head><p>We collected 1,567 stories, evenly split between books and movie scripts. We partitioned the dataset into non-overlapping training, validation, and test portions, along stories/summaries. See <ref type="table" target="#tab_3">Table 2</ref> for detailed statistics.</p><p>The dataset contains 46,765 question-answer pairs. The questions are grammatical questions written by human annotators, average 9.8 tokens in length, and are mostly formed as 'WH'-questions (see <ref type="table" target="#tab_4">Table 3</ref>). We categorized a sample of 300 questions in <ref type="table" target="#tab_5">Table 4</ref>. We observe a good variety of question types. An interesting category are questions which ask for something related to or occurring together/before/after with an event, of which there are about 15%.</p><p>Answers in the dataset are human written, short, averaging 4.73 tokens, but not restricted to spans from the documents. There are 44.05% and 29.57% answers that appear as spans of the summaries and the stories, respectively; as expected, lower proportion of answers are spans on stories compared to summaries on which they were constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Tasks</head><p>We present tasks varying in their scope and complexity: we consider either the summary or the story as context, and for each we evaluate answer generation and answer selection.</p><p>The task of answering questions based on summaries is similar in scope to previous datasets. However, summaries contain more complex relationships and timelines than news articles or short paragraphs from the web and thus provide a task different in nature. We hope that NarrativeQA will motivate the design of architectures capable of modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents.</p><p>The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). This task is at present intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections.</p><p>We require the use of metrics for generated text. We evaluate using Bleu-1, Bleu-4 <ref type="bibr" target="#b18">(Papineni et al., 2002)</ref>, Meteor <ref type="bibr" target="#b8">(Denkowski and Lavie, 2011)</ref>, and Rouge-L (Lin, 2004), using two references for each question, 6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR). 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baselines and Oracles</head><p>In this section, we show that NarrativeQA presents a challenging problem for current approaches to reading comprehension by evaluating several baselines based on information retrieval (IR) techniques and neural models. Since neural models use quite different processes for generating answers (e.g., predicting a single word or entity, selecting a span of the document context, or open generation of the answer sequence), we present results on each. We also report the human performance by scoring the second reference answer against the first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simple IR Baselines</head><p>We consider basic IR baselines which retrieve an answer by selecting a span of tokens from the context document based on a similarity measure between the candidate span and a query. We compare two queries: the question and (as an oracle) the gold standard answer. The answer oracle provides an upper bound <ref type="bibr">6</ref> We lowercase both the candidates and the references and remove the end of sentence marker and the final full stop. <ref type="bibr">7</ref>     on the performance of span retrieval models, including the neural models discussed below. When using the question as the query, we obtain generalization results of IR methods. Test set results are computed by extracting either 4-gram, 8-gram, or full-sentence spans according to the best performance on the validation set. <ref type="bibr">8</ref> We consider three similarity metrics for extracting spans: Bleu-1, Rouge-L, and the cosine similarity between bag-of-words embedding of the query and the candidate span using pre-trained GloVe word embeddings <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Benchmarks</head><p>As a first benchmark we consider a simple bidirectional LSTM sequence to sequence (Seq2Seq) model <ref type="bibr" target="#b23">(Sutskever et al., 2014</ref>) predicting the answer directly from the query. Importantly, we provide no context information from either summary or story. Such a model might classify the question and predict an answer of similar topic or category.</p><p>Previous reading comprehension tasks such as CNN/Daily Mail motivated models constrained to predicting a single token from the input sequence. The AS Reader  considers the entire context and predicts a distribution over unique word types. We adapt the model for sequence prediction by using an LSTM sequence decoder and choosing a token from the input at each step of the 8 Note that we do not consider the span's context when computing the MRR for IR baselines, as the candidate spans (i.e. all answers to questions on the story) are given and simply ranked by their similarity to the query. output sequence.</p><p>As a span-prediction model we consider a simplified version of the Bi-Directional Attention Flow network <ref type="bibr">(Seo et al., 2016)</ref>. We omit the character embedding layer and learn a mapping from words to a vector space rather than making use of pre-trained embeddings; and we use a single layer bi-directional LSTM to model interactions among context words conditioned on the query (modelling layer). As proposed, we adopt the output-layer tailored for spanprediction and leave the rest unchanged. It was not our aim to use the state-of-the-art model for other datasets but rather to provide a strong benchmark.</p><p>Span prediction models can be trained by obtaining supervision on the training set from the oracle IR model. We use start and end indices of the span achieving the highest Rouge-L score with respect to the reference answers as labels on the training set. The model is then trained to predict these spans by maximizing the probability of the indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neural Benchmarks on Stories</head><p>The design of the NarrativeQA dataset makes the straight-forward application of the existing neural architectures computationally infeasible, as this would require running an recurrent neural network on sequences of hundreds of thousands of time steps or computing a distribution over the entire input for attention, as is common.</p><p>We split the task into two steps: first, we retrieve a small number of relevant passages from the story using an IR system, and subsequently, apply one of  the neural models above on the resulting document. The question becomes the query for retrieval. This IR problem is much harder that traditional document retrieval, as the documents, the passages here, are very similar, and the question is short and entities mentioned likely occur many times in the story.</p><p>Our retrieval system considers chunks of 200 words from story and computes representations for all chunks and the query. We then select a varying number of such chunks based on their similarity to the query. We experiment with different representations and similarity measures in Section 5. Finally, we concatenate the selected chunks in the correct temporal order and insert delimiters between them to obtain a much shorter document. For span prediction models, we then further select a span from the retrieved chunks as described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we describe the data prepraration methodology we used, and experimental results on the summary-reading task as well as the full story task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preparation</head><p>The provided narratives contain a large number of named entities (such as names of characters or places). Inspired by Hermann et al. <ref type="formula">(2015)</ref>, we replace such entities with markers, such as @entity42. These markers are permuted during training and testing so that none of their embeddings learn a specific entity's representation. This allows us to build representations for entities from stories that were never seen in training, since they are given a specific identifier (to differentiate them from other entities in the document) from a set of generic identifiers re-used across documents. Entities are replaced according to a simple heuristic based on capital first character and the respective word not appearing in lowercase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reading Summaries Only</head><p>Reading comprehension of summaries is similar to a number of previous reading comprehension tasks where questions were constructed based on the context document. However, plot summaries tend to contain more intricate event time lines and a larger number of characters, and in this sense, are more complex to follow than news articles or paragraphs from Wikipedia. See <ref type="table" target="#tab_7">Table 5</ref> for the results.</p><p>Given that questions were constructed based on the summaries, we expected that both neural models and span-selection models would perform well. This is indeed the case, with the neural span prediction model significantly outperforming all other proposed methods. However, there remains a significant room for improvement when compared with the oracle and human scores.  Both the plain sequence to sequence model and the AS Reader, successfully applied to the CNN/DailyMail reading comprehension task, also perform well on this task. We observe that the AS Reader tends to copy subsequent tokens from the context, thus behaving like a span prediction model. An additional inductive bias results in higher performance for the span prediction model. Similar observations between AS Reader and span models have also been made by <ref type="bibr" target="#b26">Wang and Jiang (2016)</ref>.</p><p>Note that we have tuned each model separately on the development set twice, once selecting the best model based on Rouge-L and report the first four metrics, and a second time selecting based on MRR. <ref type="table" target="#tab_9">Table 6</ref> summarizes the results on the full Narra-tiveQA task, where the context documents are full stories. As expected (and desired), we observe a decline in performance of the span-selection oracle IR model, compared with the results on summaries. This is unsurprising as the questions were constructed on summaries and confirms the initial motivation for designing this task. As previously, we considered all spans of a given length across the entire story for this model. For short answers of one or two wordstypically main characters in a story-the candidate, i.e. the closest span to the reference answer, is easily found due to being mentioned throughout the text. For longer answers it becomes much less likely, compared to the summaries, that a high-scoring span can be found in the story. Note that this distinguishes NarrativeQA from many of the reviewed datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reading Full Stories Only</head><p>In our IR plus neural two-step approach to the task, we first retrieve relevant chunks of the stories and then apply existing reading comprehension models. We use the questions to guide the IR system for chunk extraction, with the results of the standalone IR baselines giving an indication of the difficulty of this aspect of the task. The retrieval quality has a direct effect on the performance of all neural models; a challenge which models on summaries are not presented with. We considered several approaches to chunk selection: we retrieve chunks based on the highest Rouge-L or Bleu-1 scoring span with respect to the question in the story; comparing topic distributions from an LDA model <ref type="bibr" target="#b4">(Blei et al., 2003)</ref> between questions and chunks according to their symmetric Kullback-Leibler divergence. Finally, we also consider the cosine similarity of TF-IDF representations.</p><p>We found that this approach lead to the best performance of the subsequently applied model on the validation set, irrespective of the number of chunks. Note that we used the answer as the query on the training, and the question for validation and test.</p><p>Given the retrieved chunks, we experimented with several neural models using them as context. The AS Reader, which was the better-performing model on the summaries task, underperforms the simple no-context Seq2Seq baseline (shown in <ref type="table" target="#tab_7">Table 5</ref>) in terms of MRR. While is does slightly better on the other metrics, it clearly fails to make use of the retrieved context to gain a distinctive margin over the no-context Seq2Seq model. Increasing the number of retrieved chunks, and thereby recall of possibly relevant parts of the story, had only a minor positive effect. The span prediction model-which here also uses selected chunks for context-does especially poorly in this setup. While this model provided the best neural results on the summaries task, we suspect that its performance was particularly badly hurt by the fact that there is so little lexical and grammatical overlap between the source of the questions (summaries) and the context provided (stories). As with the AS Reader, we observed no significant differences for varying number of chunks.</p><p>These results leave us a large gap to human performance, highlighting the success of our design objective to build a task that is realistic and straightforward for humans while very difficult for current reading comprehension models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Analysis and Challenges</head><p>We find that the proposed dataset meets the desiderata we set out in Section 3.1. In particular, we constructed a dataset with a number of long documents, characterised by good lexical coverage and diversity. The questions and answers are human generated and natural sounding. And, based on a small manual examination (of 'Ghostbusters II', 'Airplane', 'Jacob's Ladder'), only a small number of questions and answers are shallow paraphrases of sentences in the full document. Most questions require reading segments at least several paragraphs long, and in some cases even multiple segments spread throughout the story.</p><p>Computational challenges identified in Section 5.3 naturally suggest a retrieval procedure as the first step. Story snippet: I should state therefore, that I, Anthony Rogers, am, so far as I know, the only man alive whose normal span of eighty-one years of life has been spread over a period of 573 years. To be precise, I lived the first twenty-nine years of my life between 1898 and 1927; the other fifty-two since 2419. The gap between these two, a period of nearly five hundred years, I spent in a state of suspended animation, free from the ravages of katabolic processes, and without any apparent effect on my physical or mental faculties. When I began my long sleep, man had just begun his real conquest of the air. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Example question-answer pair with snippets from the summary and the story.</p><p>We found that the retrieval is challenging even for humans not familiar with the presented narrative. In particular, the task often requires referring to larger parts of the story, in addition to knowing at least some background about entities. This makes the search procedure, based on only a short question, a challenging and interesting task in itself.</p><p>We show example question-answer pairs in <ref type="figure" target="#fig_0">Figures 1, 2, 3</ref>. These examples were chosen from a small set of manually annotated question-answer pairs to be representative of this collection. In particular, the examples show that larger parts of the story are required to answer questions. Consider <ref type="figure" target="#fig_0">Figure 3</ref>. While the relevant paragraph depicting the injury appears early on, it is not until the next snippet (which appears at the end of the narrative) that the lethal consequences of the injury are revealed. This illustrates an iterative reasoning process as well as extremely long temporal dependencies we encountered during manual annotation. As shown in <ref type="figure">Figure 1</ref>, reading comprehension on movie scripts requires understanding of written dialogue. This is a challenge as dialogue is typically non-descriptive, whereas the questions were asked based on descriptive summaries, requiring models to "read between the lines".</p><p>We expect that understanding narratives as complex as those presented in NarrativeQA will require transferring text understanding capability from other supervised learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels <ref type="bibr" target="#b10">(Frermann and Szarvas, 2017)</ref> or summarization of movie scripts <ref type="bibr" target="#b11">(Gorinski and Lapata, 2015)</ref>, to more fine grained processing by inducing character types <ref type="bibr" target="#b2">(Bamman et al., 2014b;</ref><ref type="bibr" target="#b1">Bamman et al., 2014a)</ref>, understanding relationships between characters <ref type="bibr" target="#b15">(Iyyer et al., 2016;</ref><ref type="bibr" target="#b6">Chaturvedi et al., 2017)</ref>, or understanding plans, goals, and narrative structure in terms of abstract narratives <ref type="bibr" target="#b22">(Schank and Abelson, 1977;</ref><ref type="bibr" target="#b30">Wilensky, 1978;</ref><ref type="bibr" target="#b3">Black and Wilensky, 1979;</ref><ref type="bibr" target="#b5">Chambers and Jurafsky, 2009</ref>). In computer vision, the MovieQA dataset <ref type="bibr" target="#b24">(Tapaswi et al., 2016</ref>) fulfills a similar role as Nar-rativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have introduced a new dataset and a set of tasks for training and evaluating reading comprehension systems, born from an analysis of the limitations of existing datasets and tasks. While our QA task resembles tasks provided by existing datasets, it exposes new challenges because of its domain: fiction. Fictional stories-in contrast to news stories-are selfcontained and describe richer set of entities, events, and the relations between them. We have a range of tasks, from simple (which requires models to read summaries of books and movie scripts, and generate or rank fluent English answers to human-generated questions) to more complex (which requires models to read the full stories to answer the questions, with no access to the summaries).</p><p>In addition to the issue of scaling neural models to large documents, the larger tasks are significantly more difficult as questions formulated based on one or two sentences of a summary might require appealing to possibly discontiguous sentences or paragraphs Title: Jacob's Ladder Question: What is the fatal injury that Jacob sustains which ultimately leads to his death ?</p><p>Answer: A bayonete stabbing to his gut. Summary snippet: A terrified Jacob flees into the jungle, only to be bayoneted in the gut by an unseen assailant.</p><p>[. . . ] In a wartime triage tent in 1971, military doctors fruitlessly treating Jacob reluctantly declare him dead Story snippet: As he spins around one of the attackers jams all eight inches of his bayonet blade into Jacob's stomach. Jacob screams. It is a loud and piercing wail.</p><p>[. . . ] Int. Vietnam Field Hospital -Day A doctor leans his head in front of the lamp and removes his mask. His expression is somber. He shakes his head. His words are simple and final.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DOCTOR</head><p>He's gone.</p><p>Cut to Jacob Singer . . . The doctor steps away. A nurse rudely pulls a green sheet up over his head. The doctor turns to one of the aides and throws up his hands in defeat. from the source text. This requires potential solutions to these tasks to jointly model the process of searching for information (possibly in several steps) to serve as support for generating an answer, alongside the process of generating the answer entailed by said support. End-to-end mechanisms for both searching for information, such as attention, do not scale beyond selecting words or n-grams in short contexts such as sentences and small documents. Likewise, neural models for mapping documents to answers, or determining entailment between supporting evidence and a hypothesis, typically operate on the scale of sentences rather than sets of paragraphs.</p><p>We have provided baseline and benchmark results for both sets of tasks, demonstrating that while existing models give sensible results out of the box on summaries, they do not get any traction on the book-scale tasks. Having given a quantitative and qualitative analysis of the difficulty of the more complex tasks, we suggest research directions that may help bridge the gap between existing models and hu-man performance. Our hope is that this dataset will serve not only as a challenge for the machine reading community, but as a driver for the development of a new class of neural models which will take a significant step beyond the level of complexity which existing datasets and tasks permit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Example question-answer pair with snippets from the summary and the story.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>offer a different challenge. A large number of a questions and answers are provided for a set of documents, where the answers are spans of the context document, i.e. contiguous sequences of words from the document. Although the answers are not just single word/entity answers, many plausible questions for assessing RC cannot be asked</figDesc><table><row><cell>Dataset</cell><cell cols="2">Documents</cell><cell></cell><cell></cell><cell>Questions</cell><cell>Answers</cell></row><row><cell>MCTest (Richardson et al., 2013)</cell><cell cols="2">660 short stories,</cell><cell></cell><cell></cell><cell>2640 human generated,</cell><cell cols="2">multiple choice</cell></row><row><cell></cell><cell cols="2">grade school level</cell><cell></cell><cell></cell><cell>based on the document</cell><cell></cell></row><row><cell>CNN/Daily Mail (Hermann et al., 2015)</cell><cell cols="3">93K+220K news articles</cell><cell></cell><cell>387K+997K Cloze-form,</cell><cell>entities</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>based on highlights</cell><cell></cell></row><row><cell cols="5">Children's Book Test (CBT) (Hill et al., 2016) 687K of 20 sentence passages from</cell><cell>Cloze-form,</cell><cell cols="2">multiple choice</cell></row><row><cell></cell><cell cols="2">108 children's books</cell><cell></cell><cell></cell><cell>from the 21st sentence</cell><cell></cell></row><row><cell>BookTest (Bajgar et al., 2016)</cell><cell cols="2">14.2M, similar to CBT</cell><cell></cell><cell></cell><cell>Cloze-form, similar to CBT</cell><cell cols="2">multiple choice</cell></row><row><cell>SQuAD (Rajpurkar et al., 2016)</cell><cell>23K</cell><cell>paragraphs</cell><cell>from</cell><cell>536</cell><cell>108K human generated,</cell><cell>spans</cell></row><row><cell></cell><cell cols="2">Wikipedia articles</cell><cell></cell><cell></cell><cell>based on the paragraphs</cell><cell></cell></row><row><cell>NewsQA (Trischler et al., 2016)</cell><cell cols="4">13K news articles from the CNN</cell><cell>120K human generated,</cell><cell>spans</cell></row><row><cell></cell><cell>dataset</cell><cell></cell><cell></cell><cell></cell><cell>based on headline, highlights</cell><cell></cell></row><row><cell>MS MARCO (Nguyen et al., 2016)</cell><cell cols="4">1M passages from 200K+ docu-</cell><cell>100K search queries</cell><cell cols="2">human generated,</cell></row><row><cell></cell><cell cols="4">ments retrieved using the queries</cell><cell></cell><cell cols="2">based on the passages</cell></row><row><cell>SearchQA (Dunn et al., 2017)</cell><cell cols="4">6.9m passages retrieved from a</cell><cell>140k human generated</cell><cell>human</cell><cell>generated</cell></row><row><cell></cell><cell cols="3">search engine using the queries</cell><cell></cell><cell>Jeopardy! questions</cell><cell cols="2">Jeopardy! answers</cell></row><row><cell>NarrativeQA (this paper)</cell><cell cols="4">1,572 stories (books, movie scripts)</cell><cell>46,765 human generated,</cell><cell cols="2">human generated,</cell></row><row><cell></cell><cell cols="3">&amp; human generated summaries</cell><cell></cell><cell>based on summaries</cell><cell cols="2">based on summaries</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>{1, 2, . . .} is the rank of the correct answer among candidates.</figDesc><table><row><cell></cell><cell>train</cell><cell>valid</cell><cell>test</cell></row><row><cell># documents</cell><cell>1,102</cell><cell>115</cell><cell>355</cell></row><row><cell>. . . books</cell><cell>548</cell><cell>58</cell><cell>177</cell></row><row><cell>. . . movie scripts</cell><cell>554</cell><cell>57</cell><cell>178</cell></row><row><cell># question-answer pairs</cell><cell>32,747</cell><cell>3,461</cell><cell>10,557</cell></row><row><cell cols="2">Avg. #tok. in summaries 659</cell><cell>638</cell><cell>654</cell></row><row><cell cols="2">Max #tok. in summaries 1,161</cell><cell>1,189</cell><cell>1,148</cell></row><row><cell>Avg. #tok. in stories</cell><cell>62,528</cell><cell>62,743</cell><cell>57,780</cell></row><row><cell>Max #tok. in stories</cell><cell cols="3">430,061 418,265 404,641</cell></row><row><cell>Avg. #tok. in questions</cell><cell>9.83</cell><cell>9.69</cell><cell>9.85</cell></row><row><cell>Avg. #tok. in answers</cell><cell>4.73</cell><cell>4.60</cell><cell>4.72</cell></row></table><note>MRR is the mean over examples of 1/r, where r ∈</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>NarrativeQA dataset statistics.</figDesc><table><row><cell>First token</cell><cell>Frequency</cell></row><row><cell>What</cell><cell>38.04%</cell></row><row><cell>Who</cell><cell>23.37%</cell></row><row><cell>Why</cell><cell>9.78%</cell></row><row><cell>How</cell><cell>8.85%</cell></row><row><cell>Where</cell><cell>7.53%</cell></row><row><cell>Which</cell><cell>2.21%</cell></row><row><cell>How many/much</cell><cell>1.80%</cell></row><row><cell>When</cell><cell>1.67%</cell></row><row><cell>In</cell><cell>1.19%</cell></row><row><cell>OTHER</cell><cell>5.57%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Category</cell><cell>Frequency</cell></row><row><cell>Person</cell><cell>30.54%</cell></row><row><cell>Description</cell><cell>24.50%</cell></row><row><cell>Location</cell><cell>9.73%</cell></row><row><cell>Why/reason</cell><cell>9.40%</cell></row><row><cell>How/method</cell><cell>8.05%</cell></row><row><cell>Event</cell><cell>4.36%</cell></row><row><cell>Entity</cell><cell>4.03%</cell></row><row><cell>Object</cell><cell>3.36%</cell></row><row><cell>Numeric</cell><cell>3.02%</cell></row><row><cell>Duration</cell><cell>1.68%</cell></row><row><cell>Relation</cell><cell>1.34%</cell></row><row><cell>: Frequency of first</cell><cell></cell></row><row><cell>token of the question in the</cell><cell></cell></row><row><cell>training set.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Question categories</cell></row><row><cell>on a sample of 300 questions</cell></row><row><cell>from the validation set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Experiments on summaries. Higher is better for all metrics. Sections 4.1 and 4.2 explain the IR and neural models, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Experiments on full stories. Each chunk contains 200 tokens. Higher is better for all metrics. Sections 4.1 and 4.2 explain the IR and neural models, respectively. Note that the human scores are based on answering questions given summaries, same as inTable 5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Title: Armageddon 2419 A.D. Question: In what year did Rogers awaken from his deep slumber? Answer: 2419 Summary snippet: . . . Rogers remained in sleep for 492 years. He awakes in 2419 and,. . .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.gutenberg.org/ 4 Mainly from http://www.imsdb.com/, but also http://www.dailyscript.com/, http: //www.awesomefilm.com/. 5 This was done both through instructions and Javascript hard limitations on the annotation site.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Embracing data abundance: Booktest dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<idno>abs/1610.00956</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning latent personas of film characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">352</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bayesian mixed effects model of literary character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An evaluation of story grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wilensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="229" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative schemas and their participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
	<note>ACL &apos;09</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of evolving relationships between literary characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">SearchQA: A new Q&amp;A dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inducing semantic micro-clusters from deep multi-view representations of novels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1874" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Movie script summarization as graph-based scene extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>John Gorinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1066" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wikireading: A novel large-scale language understanding task over wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fandrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sumit Chopra, and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feuding families and former friends: Unsupervised learning for dynamic fictional relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bajgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL workshop on Text Summarization Branches Out</title>
		<meeting>ACL workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of ACL. Chin-Yew Lin</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing of EMNLP</title>
		<meeting>essing of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
	</analytic>
	<monogr>
		<title level="m">Bidirectional attention flow for machine comprehension</title>
		<editor>L. Erlbaum, Hillsdale, NJ. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi</editor>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Antonio Torralba, Raquel Urtasun, and Sanja Fidler</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno>abs/1611.09830</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">FastQA: A simple and efficient neural architecture for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
		<idno>abs/1703.04816</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why John married Mary: Understanding stories involving recurring goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="266" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
