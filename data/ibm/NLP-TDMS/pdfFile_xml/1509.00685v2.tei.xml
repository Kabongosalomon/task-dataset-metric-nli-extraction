<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<email>srush@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Harvard SEAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
							<email>spchopra@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Summarization is an important challenge of natural language understanding. The aim is to produce a condensed representation of an input text that captures the core meaning of the original. Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques <ref type="bibr" target="#b12">(Knight and Marcu (2002)</ref>, among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering <ref type="bibr" target="#b10">(Jing, 2002)</ref>. Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints <ref type="bibr" target="#b6">(Dorr et al., 2003;</ref><ref type="bibr" target="#b25">Zajic et al., 2004)</ref> or with syntactic transformations of the input text <ref type="bibr">(Cohn and Figure 1</ref>: Example output of the attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represent the distribution over the input after generating each word. <ref type="bibr" target="#b22">Woodsend et al., 2010)</ref>. These approaches are described in more detail in Section 6.</p><p>We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of <ref type="bibr" target="#b2">Bahdanau et al. (2014)</ref> in that it learns a latent soft alignment over the input text to help inform the summary (as shown in <ref type="figure" target="#fig_3">Figure 1</ref>). Crucially both the encoder and the generation model are trained jointly on the sentence summarization task. The model is described in detail in Section 3. Our model also incorporates a beam-search decoder as well as additional features to model extractive elements; these aspects are discussed in Sections 4 and 5.</p><p>This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approaches, but can easily <ref type="bibr">Input (x1,</ref><ref type="bibr">. . . ,</ref><ref type="bibr">x18)</ref>. First sentence of article: russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism Output (y1, . . . , y8). Generated headline: russia calls for joint front against terrorism ⇐ g(terrorism, x, for, joint, front, against) <ref type="figure">Figure 2</ref>: Example input sentence and the generated summary. The score of generating yi+1 (terrorism) is based on the context yc (for . . . against) as well as the input x1 . . . x18. Note that the summary generated is abstractive which makes it possible to generalize (russian defense minister to russia) and paraphrase (for combating to against), in addition to compressing (dropping the creation of), see <ref type="bibr" target="#b10">Jing (2002)</ref> for a survey of these editing operations.</p><p>scale to train on a large amount of data. Since our system makes no assumptions about the vocabulary of the generated summary it can be trained directly on any document-summary pair. 1 This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword <ref type="bibr" target="#b8">(Graff et al., 2003)</ref> consisting of around 4 million articles. An example of generation is given in <ref type="figure">Figure 2</ref>, and we discuss the details of this task in Section 7.</p><p>To test the effectiveness of this approach we run extensive comparisons with multiple abstractive and extractive baselines, including traditional syntax-based systems, integer linear programconstrained systems, information-retrieval style approaches, as well as statistical phrase-based machine translation. Section 8 describes the results of these experiments. Our approach outperforms a machine translation system trained on the same large-scale dataset and yields a large improvement over the highest scoring system in the DUC-2004 competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We begin by defining the sentence summarization task. Given an input sentence, the goal is to produce a condensed summary. Let the input consist of a sequence of M words x 1 , . . . , x M coming from a fixed vocabulary V of size |V| = V . We will represent each word as an indicator vector x i ∈ {0, 1} V for i ∈ {1, . . . , M }, sentences as a sequence of indicators, and X as the set of possible inputs. Furthermore define the notation x <ref type="bibr">[i,j,k]</ref> to indicate the sub-sequence of elements i, j, k.</p><p>A summarizer takes x as input and outputs a shortened sentence y of length N &lt; M . We will assume that the words in the summary also come from the same vocabulary V and that the output is a sequence y 1 , . . . , y N . Note that in contrast to related tasks, like machine translation, we will assume that the output length N is fixed, and that the system knows the length of the summary before generation. <ref type="bibr">2</ref> Next consider the problem of generating summaries.</p><p>Define the set Y ⊂ ({0, 1} V , . . . , {0, 1} V ) as all possible sentences of length N , i.e. for all i and y ∈ Y, y i is an indicator. We say a system is abstractive if it tries to find the optimal sequence from this set Y,</p><formula xml:id="formula_0">arg max y∈Y s(x, y),<label>(1)</label></formula><p>under a scoring function s : X × Y → R. Contrast this to a fully extractive sentence summary 3 which transfers words from the input:</p><formula xml:id="formula_1">arg max m∈{1,...M } N s(x, x [m 1 ,...,m N ] ),<label>(2)</label></formula><p>or to the related problem of sentence compression that concentrates on deleting words from the input:</p><formula xml:id="formula_2">arg max m∈{1,...M } N ,m i−1 &lt;m i s(x, x [m 1 ,...,m N ] ).<label>(3)</label></formula><p>While abstractive summarization poses a more difficult generation challenge, the lack of hard constraints gives the system more freedom in generation and allows it to fit with a wider range of training data.</p><p>In this work we focus on factored scoring functions, s, that take into account a fixed window of previous words:</p><formula xml:id="formula_3">s(x, y) ≈ N −1 i=0 g(y i+1 , x, y c ),<label>(4)</label></formula><p>where we define y c y [i−C+1,...,i] for a window of size C.</p><p>In particular consider the conditional logprobability of a summary given the input, s(x, y) = log p(y|x; θ). We can write this as:</p><formula xml:id="formula_4">log p(y|x; θ) ≈ N −1 i=0 log p(y i+1 |x, y c ; θ),</formula><p>where we make a Markov assumption on the length of the context as size C and assume for i &lt; 1, y i is a special start symbol S .</p><p>With this scoring function in mind, our main focus will be on modelling the local conditional distribution: p(y i+1 |x, y c ; θ). The next section defines a parameterization for this distribution, in Section 4, we return to the question of generation for factored models, and in Section 5 we introduce a modified factored scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The distribution of interest, p(y i+1 |x, y c ; θ), is a conditional language model based on the input sentence x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model <ref type="bibr" target="#b0">(Banko et al., 2000;</ref><ref type="bibr" target="#b12">Knight and Marcu, 2002;</ref><ref type="bibr" target="#b5">Daumé III and Marcu, 2002)</ref>, i.e., where p(y) and p(x|y) are estimated separately. Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network. The network contains both a neural probabilistic language model and an encoder which acts as a conditional summarization model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Language Model</head><p>The core of our parameterization is a language model for estimating the contextual probability of the next word. The language model is adapted from a standard feed-forward neural network language model (NNLM), particularly the class of NNLMs described by <ref type="bibr" target="#b1">Bengio et al. (2003)</ref>. The full model is:  The</p><formula xml:id="formula_5">p(y i+1 |y c , x; θ) ∝ exp(Vh + Wenc(x, y c )),</formula><formula xml:id="formula_6">parameters are θ = (E, U, V, W) where E ∈ R D×V is a word embedding matrix, U ∈ R (CD)×H , V ∈ R V ×H , W ∈ R V ×H are weight matrices, 4</formula><p>D is the size of the word embeddings, and h is a hidden layer of size H. The black-box function enc is a contextual encoder term that returns a vector of size H representing the input and current context; we consider several possible variants, described subsequently. <ref type="figure" target="#fig_2">Figure 3a</ref> gives a schematic representation of the decoder architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoders</head><p>Note that without the encoder term this represents a standard language model. By incorporating in enc and training the two elements jointly we crucially can incorporate the input text into generation. We discuss next several possible instantiations of the encoder.</p><p>Bag-of-Words Encoder Our most basic model simply uses the bag-of-words of the input sentence embedded down to size H, while ignoring properties of the original order or relationships between neighboring words. We write this model as:</p><formula xml:id="formula_7">enc 1 (x, y c ) = p x, p = [1/M, . . . , 1/M ], x = [Fx 1 , . . . , Fx M ].</formula><p>Where the input-side embedding matrix F ∈ R H×V is the only new parameter of the encoder and p ∈ [0, 1] M is a uniform distribution over the input words.</p><p>For summarization this model can capture the relative importance of words to distinguish content words from stop words or embellishments. Potentially the model can also learn to combine words; although it is inherently limited in representing contiguous phrases.</p><p>Convolutional Encoder To address some of the modelling issues with bag-of-words we also consider using a deep convolutional encoder for the input sentence. This architecture improves on the bag-of-words model by allowing local interactions between words while also not requiring the context y c while encoding the input.</p><p>We utilize a standard time-delay neural network (TDNN) architecture, alternating between temporal convolution layers and max pooling layers.</p><formula xml:id="formula_8">∀j, enc2(x, yc)j = max ix L i,j ,<label>(5)</label></formula><formula xml:id="formula_9">∀i, l ∈ {1, . . . L},x l j = tanh(max{x l 2i−1 ,x l 2i }),<label>(6)</label></formula><formula xml:id="formula_10">∀i, l ∈ {1, . . . L},x l i = Q lxl−1 [i−Q,...,i+Q] ,<label>(7)</label></formula><p>x</p><formula xml:id="formula_11">0 = [Fx1, . . . , FxM ].<label>(8)</label></formula><p>Where F is a word embedding matrix and Q L×H×2Q+1 consists of a set of filters for each layer {1, . . . L}. Eq. 7 is a temporal (1D) convolution layer, Eq. 6 consists of a 2-element temporal max pooling layer and a pointwise non-linearity, and final output Eq. 5 is a max over time. At each layerx is one half the size ofx. For simplicity we assume that the convolution is padded at the boundaries, and that M is greater than 2 L so that the dimensions are well-defined.</p><p>Attention-Based Encoder While the convolutional encoder has richer capacity than bag-ofwords, it still is required to produce a single representation for the entire input sentence. A similar issue in machine translation inspired Bahdanau et al. <ref type="formula" target="#formula_0">(2014)</ref> to instead utilize an attention-based contextual encoder that constructs a representation based on the generation context. Here we note that if we exploit this context, we can actually use a rather simple model similar to bag-of-words:</p><formula xml:id="formula_12">enc 3 (x, y c ) = p x, p ∝ exp(xPỹ c ), x = [Fx 1 , . . . , Fx M ], y c = [Gy i−C+1 , . . . , Gy i ], ∀ix i = i+Q q=i−Qx i /Q.</formula><p>Where G ∈ R D×V is an embedding of the context, P ∈ R H×(CD) is a new weight matrix parameter mapping between the context embedding and input embedding, and Q is a smoothing window. The full model is shown in <ref type="figure" target="#fig_2">Figure 3b</ref>.</p><p>Informally we can think of this model as simply replacing the uniform distribution in bag-of-words with a learned soft alignment, P, between the input and the summary. <ref type="figure" target="#fig_3">Figure 1</ref> shows an example of this distribution p as a summary is generated. The soft alignment is then used to weight the smoothed version of the inputx when constructing the representation. For instance if the current context aligns well with position i then the words x i−Q , . . . , x i+Q are highly weighted by the encoder. Together with the NNLM, this model can be seen as a stripped-down version of the attention-based neural machine translation model. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>The lack of generation constraints makes it possible to train the model on arbitrary input-output pairs. Once we have defined the local conditional model, p(y i+1 |x, y c ; θ), we can estimate the parameters to minimize the negative loglikelihood of a set of summaries. Define this training set as consisting of J input-summary pairs (x (1) , y (1) ), . . . , (x (J) , y (J) ). The negative loglikelihood conveniently factors 6 into a term for each token in the summary:</p><formula xml:id="formula_13">NLL(θ) = − J j=1 log p(y (j) |x (j) ; θ), = − J j=1 N −1 i=1 log p(y (j) i+1 |x (j) , yc; θ).</formula><p>We minimize NLL by using mini-batch stochastic gradient descent. The details are described further in Section 7.</p><p>We now return to the problem of generating summaries. Recall from Eq. 4 that our goal is to find,</p><formula xml:id="formula_14">y * = arg max y∈Y N −1 i=0 g(y i+1 , x, y c ).</formula><p>Unlike phrase-based machine translation where inference is NP-hard, it actually is tractable in theory to compute y * . Since there is no explicit hard alignment constraint, Viterbi decoding can be applied and requires O(N V C ) time to find an exact solution. In practice though V is large enough to make this difficult. An alternative approach is to approximate the arg max with a strictly greedy or deterministic decoder.</p><p>A compromise between exact and greedy decoding is to use a beam-search decoder (Algorithm 1) which maintains the full vocabulary V while limiting itself to K potential hypotheses at each position of the summary. This has been the standard approach for neural MT models <ref type="bibr" target="#b2">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015)</ref>. The beam-search algorithm is shown here, modified for the feed-forward model: As with Viterbi this beam search algorithm is much simpler than beam search for phrase-based MT. Because there is no explicit constraint that each source word be used exactly once there is no need to maintain a bit set and we can simply move from left-to-right generating words. The beam search algorithm requires O(KN V ) time. From a computational perspective though, each round of beam search is dominated by computing p(y i |x, y c ) for each of the K hypotheses. These can be computed as a mini-batch, which in practice greatly reduces the factor of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extension: Extractive Tuning</head><p>While we will see that the attention-based model is effective at generating summaries, it does miss an important aspect seen in the human-generated references. In particular the abstractive model does not have the capacity to find extractive word matches when necessary, for example transferring unseen proper noun phrases from the input. Similar issues have also been observed in neural translation models particularly in terms of translating rare words <ref type="bibr" target="#b15">(Luong et al., 2015)</ref>.</p><p>To address this issue we experiment with tuning a very small set of additional features that tradeoff the abstractive/extractive tendency of the system. We do this by modifying our scoring function to directly estimate the probability of a summary using a log-linear model, as is standard in machine translation:</p><formula xml:id="formula_15">p(y|x; θ, α) ∝ exp(α N −1 i=0 f (y i+1 , x, y c )).</formula><p>Where α ∈ R 5 is a weight vector and f is a feature function. Finding the best summary under this distribution corresponds to maximizing a factored scoring function s,</p><formula xml:id="formula_16">s(y, x) = N −1 i=0 α f (y i+1 , x, y c ).</formula><p>where g(y i+1 , x, y c ) α f (y i+1 , x, y c ) to satisfy Eq. 4. The function f is defined to combine the local conditional probability with some additional indicator featrues:</p><formula xml:id="formula_17">f (y i+1 , x, y c ) = [ log p(y i+1 |x, y c ; θ), 1{∃j. y i+1 = x j }, 1{∃j. y i+1−k = x j−k ∀k ∈ {0, 1}}, 1{∃j. y i+1−k = x j−k ∀k ∈ {0, 1, 2}}, 1{∃k &gt; j. y i = x k , y i+1 = x j } ].</formula><p>These features correspond to indicators of unigram, bigram, and trigram match with the input as well as reordering of input words. Note that setting α = 1, 0, . . . , 0 gives a model identical to standard ABS.</p><p>After training the main neural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data <ref type="bibr" target="#b19">(Och, 2003)</ref>. This tuning step is also identical to the one used for the phrase-based machine translation baseline.</p><p>Abstractive sentence summarization has been traditionally connected to the task of headline generation. Our work is similar to early work of <ref type="bibr" target="#b0">Banko et al. (2000)</ref> who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs. We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding.</p><p>This task was standardized around the DUC-2003 and DUC-2004 competitions <ref type="bibr" target="#b20">(Over et al., 2007</ref>). The TOPIARY system <ref type="bibr" target="#b25">(Zajic et al., 2004)</ref> performed the best in this task, and is described in detail in the next section. We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task.</p><p>More recently, <ref type="bibr" target="#b4">Cohn and Lapata (2008)</ref> give a compression method which allows for more arbitrary transformations. They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm. <ref type="bibr" target="#b22">Woodsend et al. (2010)</ref> propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries. Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences. The latter system is W&amp;L in our results; we attempted to train the former system T3 on this dataset but could not train it at scale.</p><p>In addition to <ref type="bibr" target="#b0">Banko et al. (2000)</ref> there has been some work using statistical machine translation directly for abstractive summary. <ref type="bibr" target="#b23">Wubben et al. (2012)</ref> utilize MOSES directly as a method for text simplification.</p><p>Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs. Because their focus is extractive compression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly.</p><p>Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine transla-tion. The core of our model is a NNLM based on that of <ref type="bibr" target="#b1">Bengio et al. (2003)</ref>.</p><p>Recently, there have been several papers about models for machine translation <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014)</ref>. Of these our model is most closely related to the attention-based model of <ref type="bibr" target="#b2">Bahdanau et al. (2014)</ref>, which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNN-LM in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Setup</head><p>We experiment with our attention-based sentence summarization model on the task of headline generation. In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data Set</head><p>The standard sentence summarization evaluation set is associated with the DUC-2003 and DUC-2004 shared tasks <ref type="bibr" target="#b20">(Over et al., 2007)</ref>. The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes. This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task. The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence). The full data set is available by request at http://duc.nist.gov/data.html.</p><p>For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric <ref type="bibr" target="#b14">(Lin, 2004)</ref>. To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths. The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report.</p><p>In addition to the standard DUC-2014 evalu-ation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release. For this evaluation, we tune systems to generate output of the average title length.</p><p>For training data for both tasks, we utilize the annotated Gigaword data set <ref type="bibr" target="#b8">(Graff et al., 2003;</ref><ref type="bibr" target="#b18">Napoles et al., 2012)</ref>, which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools <ref type="bibr" target="#b17">(Manning et al., 2014)</ref>. Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well. Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades.</p><p>For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair. While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs. We therefore prune training based on the following heuristic filters:</p><p>(1) Are there no non-stop-words in common? (2) Does the title contain a byline or other extraneous editing marks? (3) Does the title have a question mark or colon? After applying these filters, the training set consists of roughly J = 4 million title-article pairs. We apply a minimal preprocessing step using PTB tokenization, lower-casing, replacing all digit characters with #, and replacing of word types seen less than 5 times with UNK. We also remove all articles from the time-period of the DUC evaluation. release.</p><p>The complete input training vocabulary consists of 119 million word tokens and 110K unique word types with an average sentence size of 31.3 words. The headline vocabulary consists of 31 million tokens and 69K word types with the average title of length 8.3 words (note that this is significantly shorter than the DUC summaries). On average there are 4.6 overlapping word types between the headline and the input; although only 2.6 in the first 75-characters of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Baselines</head><p>Due to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines.</p><p>From the DUC-2004 task we include the PRE-FIX baseline that simply returns the first 75characters of the input as the headline. We also report the winning system on this shared task, TOPIARY <ref type="bibr" target="#b25">(Zajic et al., 2004)</ref>. TOPIARY merges a compression system using linguisticallymotivated transformations of the input <ref type="bibr" target="#b6">(Dorr et al., 2003)</ref> with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output. <ref type="bibr" target="#b22">Woodsend et al. (2010)</ref> (described above) also report results on the DUC dataset.</p><p>The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence). We report the average inter-annotater agreement score as <ref type="bibr">REF-ERENCE.</ref> For reference, the best human evaluator scores 31.7 ROUGE-1.</p><p>We also include several baselines that have access to the same training data as our system. The first is a sentence compression baseline COM-PRESS <ref type="bibr" target="#b3">(Clarke and Lapata, 2008)</ref>. This model uses the syntactic structure of the original sentence along with a language model trained on the headline data to produce a compressed output. The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver.</p><p>To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see <ref type="bibr" target="#b16">Manning et al. (2008)</ref>).</p><p>Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, MOSES+ <ref type="bibr" target="#b13">(Koehn et al., 2007)</ref>. To improve the baseline for this task, we augment the phrase table with "deletion" rules mapping each article word to , include an additional deletion feature for these rules, and allow for an infinite distortion limit. We also explicitly tune the model using MERT to target the 75byte capped ROUGE score as opposed to standard BLEU-based tuning. Unfortunately, one remaining issue is that it is non-trivial to modify the translation decoder to produce fixed-length outputs, so we tune the system to produce roughly the expected length.  <ref type="bibr" target="#b9">(Hinton et al., 2012)</ref>. Based on the validation set, we set hyperparameters as D = 200, H = 400, C = 5, L = 3, and Q = 2.</p><p>Our implementation uses the Torch numerical framework (http://torch.ch/) and will be openly available along with the data pipeline. Crucially, training is performed on GPUs and would be intractable or require approximations otherwise. Processing 1000 mini-batches with D = 200, H = 400 requires 160 seconds. Best validation accuracy is reached after 15 epochs through the data, which requires around 4 days of training.</p><p>Additionally, as described in Section 5 we apply a MERT tuning step after training using the DUC-2003 data. For this step we use Z-MERT <ref type="bibr" target="#b24">(Zaidan, 2009)</ref>. We refer to the main model as ABS and the tuned model as ABS+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>Our main results are presented in <ref type="table">Table 1</ref>. We run experiments both using the DUC-2004 evaluation data set (500 sentences, 4 references, 75 bytes) with all systems and a randomly held-out Gigaword test set (2000 sentences, 1 reference). We first note that the baselines COMPRESS and IR do relatively poorly on both datasets, indicating that neither just having article information or language model information alone is sufficient for the task. The PREFIX baseline actually performs sur-prisingly well on ROUGE-1 which makes sense given the earlier observed overlap between article and summary.</p><p>Both ABS and MOSES+ perform better than TOPIARY, particularly on ROUGE-2 and ROUGE-L in DUC. The full model ABS+ scores the best on these tasks, and is significantly better based on the default ROUGE confidence level than TOPIARY on all metrics, and MOSES+ on ROUGE-1 for DUC as well as ROUGE-1 and ROUGE-L for Gigaword. Note that the additional extractive features bias the system towards retaining more input words, which is useful for the underlying metric.</p><p>Next we consider ablations to the model and algorithm structure. <ref type="table" target="#tab_2">Table 2</ref> shows experiments for the model with various encoders. For these experiments we look at the perplexity of the system as a language model on validation data, which controls for the variable of inference and tuning. The NNLM language model with no encoder gives a gain over the standard n-gram language model. Including even the bag-of-words encoder reduces perplexity number to below 50. Both the convolutional encoder and the attention-based encoder further reduce the perplexity, with attention giving a value below 30. We also consider model and decoding ablations on the main summary model, shown in <ref type="table" target="#tab_3">Table 3</ref>. These experiments compare to the BoW encoding models, compare beam search and greedy decoding, as well as restricting the system to be complete extractive. Of these features, the biggest impact is from using a more powerful encoder (attention versus BoW), as well as using beam search to generate summaries. The abstractive nature of the system helps, but for ROUGE even using pure extractive generation is effective.   Finally we consider example summaries shown in <ref type="figure">Figure 4</ref>. Despite improving on the baseline scores, this model is far from human performance on this task. Generally the models are good at picking out key words from the input, such as names and places. However, both models will reorder words in syntactically incorrect ways, for instance in Sentence 7 both models have the wrong subject. ABS often uses more interesting re-wording, for instance new nz pm after election in Sentence 4, but this can also lead to attachment mistakes such a russian oil giant chevron in Sentence 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have presented a neural attention-based model for abstractive summarization, based on recent developments in neural machine translation. We combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries. As a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries. Both pose additional challenges in terms of efficient alignment and consistency in generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y c = [Ey i−C+1 , . . . , Ey i ], h = tanh(Uỹ c ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) A network diagram for the NNLM decoder with additional encoder element. (b) A network diagram for the attention-based encoder enc3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Beam Search Input: Parameters θ, beam size K, input x Output: Approx. K-best summaries π[0] ← { } S = V if abstractive else {xi | ∀i} for i = 0 to N − 1 do Generate Hypotheses N ← [y, yi+1] | y ∈ π[i], yi+1 ∈ S Hypothesis Recombination H ← y ∈ N | s(y, x) &gt; s(y , x) ∀y ∈ N s.t. yc = y c Filter K-Max π[i + 1] ← K-arg max y∈H g(yi+1, yc, x) + s(y, x) end for return π[N ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Experimental results on the main summary tasks on various ROUGE metrics . Baseline models are described in detail in Section 7.2. We report the percentage of tokens in the summary that also appear in the input for Gigaword as Ext %.</figDesc><table><row><cell></cell><cell></cell><cell>DUC-2004</cell><cell></cell><cell></cell><cell>Gigaword</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">ROUGE-1 ROUGE-2 ROUGE-L</cell><cell cols="4">ROUGE-1 ROUGE-2 ROUGE-L Ext. %</cell></row><row><cell>IR</cell><cell>11.06</cell><cell>1.67</cell><cell>9.67</cell><cell>16.91</cell><cell>5.55</cell><cell>15.58</cell><cell>29.2</cell></row><row><cell>PREFIX</cell><cell>22.43</cell><cell>6.49</cell><cell>19.65</cell><cell>23.14</cell><cell>8.25</cell><cell>21.73</cell><cell>100</cell></row><row><cell>COMPRESS</cell><cell>19.77</cell><cell>4.02</cell><cell>17.30</cell><cell>19.63</cell><cell>5.13</cell><cell>18.28</cell><cell>100</cell></row><row><cell>W&amp;L</cell><cell>22</cell><cell>6</cell><cell>17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TOPIARY</cell><cell>25.12</cell><cell>6.46</cell><cell>20.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MOSES+</cell><cell>26.50</cell><cell>8.13</cell><cell>22.85</cell><cell>28.77</cell><cell>12.10</cell><cell>26.44</cell><cell>70.5</cell></row><row><cell>ABS</cell><cell>26.55</cell><cell>7.06</cell><cell>22.05</cell><cell>30.88</cell><cell>12.22</cell><cell>27.77</cell><cell>85.4</cell></row><row><cell>ABS+</cell><cell>28.18</cell><cell>8.49</cell><cell>23.81</cell><cell>31.00</cell><cell>12.65</cell><cell>28.34</cell><cell>91.5</cell></row><row><cell>REFERENCE</cell><cell>29.21</cell><cell>8.38</cell><cell>24.46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.6</cell></row><row><cell cols="2">Table 1: 7.3 Implementation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">For training, we use mini-batch stochastic gradient</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">descent to minimize negative log-likelihood. We</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">use a learning rate of 0.05, and split the learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">rate by half if validation log-likelihood does not</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">improve for an epoch. Training is performed with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">shuffled mini-batches of size 64. The minibatches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">are grouped by input length. After each epoch, we</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">renormalize the embedding tables</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Perplexity results on the Gigaword validation set comparing various language models with C=5 and endto-end summarization models. The encoders are defined in Section 3.</figDesc><table><row><cell cols="3">Decoder Model Cons.</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>Greedy</cell><cell>ABS+</cell><cell>Abs</cell><cell cols="2">26.67 6.72 21.70</cell></row><row><cell>Beam</cell><cell>BOW</cell><cell>Abs</cell><cell cols="2">22.15 4.60 18.23</cell></row><row><cell>Beam</cell><cell>ABS+</cell><cell>Ext</cell><cell cols="2">27.89 7.56 22.84</cell></row><row><cell>Beam</cell><cell>ABS+</cell><cell>Abs</cell><cell cols="2">28.48 8.91 23.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>ROUGE scores on DUC-2003 development data for various versions of inference. Greedy and Beam are described in Section 4. Ext. is a purely extractive version of the system (Eq. 2)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In contrast to a large-scale sentence compression systems like<ref type="bibr" target="#b7">Filippova and Altun (2013)</ref> which require monotonic aligned compressions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For the DUC-2004 evaluation, it is actually the number of bytes of the output that is capped. More detail is given in Section 7.3 Unfortunately the literature is inconsistent on the formal definition of this distinction. Some systems self-described as abstractive would be extractive under our definition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Each of the weight matrices U, V, W also has a corresponding bias term. For readability, we omit these terms throughout the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">To be explicit, compared to<ref type="bibr" target="#b2">Bahdanau et al. (2014)</ref> our model uses an NNLM instead of a target-side LSTM, source-side windowed averaging instead of a source-side bidirectional RNN, and a weighted dot-product for alignment instead of an alignment MLP. 6 This is dependent on using the gold standard contexts yc. An alternative is to use the predicted context within a structured or reenforcement-learning style objective.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Dzmitry Bahdanau, <ref type="bibr">Kyunghyun Cho, and Yoshua Bengio. 2014</ref>. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. I(1): a detained iranian-american academic accused of acting against national security has been released from a tehran prison after a hefty bail was posted , a to p judiciary official said tuesday . G: iranian-american academic held in tehran released on bail A: detained iranian-american academic released from jail after posting bail A+: detained iranian-american academic released from prison after hefty bail I(2): ministers from the european union and its mediterranean neighbors gathered here under heavy security on monday for an unprecedented conference on economic and political cooperation . G: european mediterranean ministers gather for landmark conference by julie bradford A: mediterranean neighbors gather for unprecedented conference on heavy security A+: mediterranean neighbors gather under heavy security for unprecedented conference I(3): the death toll from a school collapse in a haitian shanty-town rose to ## after rescue workers uncovered a classroom with ## dead students and their teacher , officials said saturday . G: toll rises to ## in haiti school unk : official A: death toll in haiti school accident rises to ## A+: death toll in haiti school to ## dead students I(4): australian foreign minister stephen smith sunday congratulated new zealand 's new prime minister-elect john key as he praised ousted leader helen clark as a " gutsy " and respected politician .  <ref type="formula">8)</ref>: thousands of kashmiris chanting pro-pakistan slogans on sunday attended a rally to welcome back a hardline separatist leader who underwent cancer treatment in mumbai . G: thousands attend rally for kashmir hardliner A: thousands rally in support of hardline kashmiri separatist leader A+: thousands of kashmiris rally to welcome back cancer treatment I(9): an explosion in iraq 's restive northeastern province of diyala killed two us soldiers and wounded two more , the military reported monday . G: two us soldiers killed in iraq blast december toll ### A: # us two soldiers killed in restive northeast province A+: explosion in restive northeastern province kills two us soldiers I(10): russian world no. # nikolay davydenko became the fifth withdrawal through injury or illness at the sydney international wednesday , retiring from his second round match with a foot injury . G: tennis : davydenko pulls out of sydney with injury A: davydenko pulls out of sydney international with foot injury A+: russian world no. # davydenko retires at sydney international I(11): russia 's gas and oil giant gazprom and us oil major chevron have set up a joint venture based in resource-rich northwestern siberia , the interfax news agency reported thursday quoting gazprom officials . G: gazprom chevron set up joint venture A: russian oil giant chevron set up siberia joint venture A+: russia 's gazprom set up joint venture in siberia <ref type="figure">Figure 4</ref>: Example sentence summaries produced on Gigaword. I is the input, A is ABS, and G is the true headline.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Headline generation based on statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vibhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentence compression beyond word deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A noisychannel model for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hedge trimmer: A parse-and-trim approach to headline generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 03 on Text summarization workshop</title>
		<meeting>the HLT-NAACL 03 on Text summarization workshop</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English gigaword. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using hidden markov modeling to decompose human-written summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="543" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: A probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m">Duc in context. Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generation with quasi-synchronous grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bbn/umd at duc-2004: Topiary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 2004 Document Understanding Workshop</title>
		<meeting>the HLT-NAACL 2004 Document Understanding Workshop<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
