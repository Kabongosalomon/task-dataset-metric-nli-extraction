<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeesawFaceNets: sparse and robust face verification model for mobile platform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Zhang Didichuxing</surname></persName>
							<email>jtzhangcas@gmail.com</email>
						</author>
						<title level="a" type="main">SeesawFaceNets: sparse and robust face verification model for mobile platform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Convolutional Neural Network (DCNNs) come to be the most widely used solution for most computer vision related tasks, and one of the most important application scenes is face verification. Due to its high-accuracy performance, deep face verification models of which the inference stage occurs on cloud platform through internet plays the key role on most prectical scenes. However, two critical issues exist: First, individual privacy may not be well protected since they have to upload their personal photo and other private information to the online cloud backend. Secondly, either training or inference stage is time-comsuming and the latency may affect customer experience, especially when the internet link speed is not so stable or in remote areas where mobile reception is not so good, but also in cities where building and other construction may block mobile signals. Therefore, designing lightweight networks with low memory requirement and computational cost is one of the most practical solutions for face verification on mobile platform. In this paper, a novel mobile network named SeesawFaceNets, a simple but effective model, is proposed for productively deploying face recognition for mobile devices. Dense experimental results have shown that our proposed model SeesawFaceNets outperforms the baseline MobilefaceNets, with only 66%(146M VS 221M MAdds) computational cost, smaller batch size and less training steps, and SeesawFaceNets achieve comparable performance with other SOTA model e.g. mobiface with only 54.2%(1.3M VS 2.4M) parameters and 31.6%(146M VS 462M MAdds) computational cost, It is also eventually competitive against large-scale deep-networks face recognition on all 5 listed public validation datasets, with 6.5%(4.2M VS 65M) parameters and 4.35%(526M VS 12G MAdds) computational cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Convolutional Neural Network (DCNNs) have revolutionized traditional machine learning applications with unprecedented performace as well as strong robustness, the performance of DCNNs exceeds human-level accuracy in more and more vision or non-vision related tasks. Most of the computer vision applications, e.g. image classification <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>, object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b42">42]</ref> image segmentation <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b24">24]</ref>, and face modeling <ref type="bibr" target="#b15">[16]</ref> etc., deploy DCNNs to achieve the state-of-theart. However, using deeper neural network with hundreds of layer and millions of parameters comes at heavy cost. Due to the high computational cost and the fact that still many operations are less optimised in existing deep learning frameworks on different hardware platforms <ref type="bibr" target="#b6">[7]</ref>, DCNNs require high computational resources beyond the capabilities of daily mobile and embedded applications.</p><p>To design such powerful neural networks, powerful GPU clusters are required so as to provide sufficient GPU memory and reduce training/inference time. Due to this hardware limitation, accordingly, there are several new advances in compressing DCNNs. Generally, most of these works utilize Mimic Networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b55">55]</ref>, Network Pruning <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b44">44]</ref>, Efficient build block designing <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b5">6]</ref>, Binary Networks <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b11">12]</ref>. These previously proposed methods improve the efficiency of the training and inference stage of the model with only small loss in accuracy. However, since face verification tasks are expected to be robust such as the very deep features of millions of facial subjects discriminated <ref type="bibr" target="#b4">[5]</ref>, speedup face recognition model with above compressed networks is still challenging task.</p><p>The main contributions of this work is a novel sparse and robust deep neural network which greatly minimizes the conputational and memory required for either training or inference stage while retaining the accuracy for mobile face verification task. Compared to previous work, we present our contributions in SeesawFaceNets approach as follows: (1) we improve the baseline face verification model MobileFaceNets <ref type="bibr" target="#b0">[1]</ref> by utilizing the Seesaw-block and SEblock to achieve further lighter-weight model but higher accuracy performance. (2) the proposed method Seesaw-FaceNets is then applied in face verification and trained an end-to-end on face verification database. In addition, we extensively experiment our proposed SeesawFaceNets on mobile-based network on face verification tasks with alternative state-of-the-art face recognition databases including LFW, AgeDB-30, CFP-FP, CPLFW and CALFW, and the results show that our SeesawFaceNets obtains stable improved efficiency compared with state-of-the-art lightweight and mobile CNNs for face verification with less computational cost, memory requirement and training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>There has been a rising interest in lightweight neural architectures designing to obtain an optimal balance between accuracy and efficiency during the past several years. Most of previous lightweight models could be categorized into efficient designed networks and pruned networks, binarized networks, quantized networks. Our proposed work focus on efficient neural networks designing for face verification tasks on mobile platforms.</p><p>Face recognition is one of the most important tasks in computer vision related area. To reduce the huge memory requirement and computational cost of classical large face verification model, MobileFaceNets <ref type="bibr" target="#b0">[1]</ref> were presented mainly based on MobileNetV2 framework. Dense face verification benchmark experiments on alternative datasets in <ref type="bibr" target="#b0">[1]</ref> show that MobileFaceNets achieve significantly superior accuracy as well as speedup on mobile devices even through it contains obviously fewer parameters and computational cost than mobile networks which directly utilize image classfication backbones <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b48">48]</ref>. MobiFace <ref type="bibr" target="#b4">[5]</ref> introduces efficient memory and low cost operators by adopting fast downsampling <ref type="bibr" target="#b46">[46]</ref> and bottleneck residual block <ref type="bibr" target="#b48">[48]</ref>, and achieves 99.7% on LFW database and 91.3% on Megaface database. ShrinkTeaNet <ref type="bibr" target="#b34">[34]</ref> presents a teacherstudent learning paradigm to train a portable student network which achieves 99.77% on LFW, 95.14% on CFP-FP and 95.64% on Megaface protocols. AirFace <ref type="bibr" target="#b35">[35]</ref> proposes a novel loss function and improved the the perfomance of MobileFaceNets with deeper and wider network and attention module to achieve better result on face verification tasks. However, still there are much work to be done to effectively run deep learning framework on variable mobile devices which will be expected to show diverse performances <ref type="bibr" target="#b65">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed SeesawFaceNets</head><p>This section Firstly introduces network design strategies we deployed in this work to construct sparse but robust model. Then, by adopting these strategies, the See-sawFaceNets architecture for face verification on mobile devices is proposed. Meanwhile, we proposed a practical issue caused by the huge GPU requirement when training large neural networks, dense further experiments results prove that our proposed SeesawFaceNets outperforms other previous mobile networks, and our deeper and wider version model -DW-SeesawFaceNet could achieve comparable performance with state of the art large networks with much less computational cost and memory requirement on face verification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Design Strategy</head><p>Seesaw block. Seesaw block is introduced in <ref type="bibr" target="#b1">[2]</ref>. however, to deploy Seesaw block for face verification tasks, we modified the original Seesaw block and add nonlinearity after the second pointwise convolution layer as MobileNetV2 and MobilefaceNets does. There are 2 key factors of this type of block compared with MobilefaceNets: (1) Seesaw block utilizes uneven pointwise group convolution; (2) channel permute/shuffle operation is applied to enable the information flow between uneven convolution groups. We use swish <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> as the non-linearity, which is slightly better for face verification than using PReLU <ref type="bibr" target="#b2">[3]</ref>.</p><p>The residual connection is employed in a bottleneck block whose input channel number equals to the output channel number. This type of build blocks is shown to have the capabilities of preventing manifold collapse during transformation and also enable the network to represent more complex functions <ref type="bibr" target="#b48">[48]</ref>. The use of Seesaw block is introduced in Seesaw-Net <ref type="bibr" target="#b1">[2]</ref> where the original pointwise convolutions within Bottleneck Residual block are replaced with uneven goup convolutions and channel permute/shuffle operation is deployed to enable information flow between convolutional groups. With Seesaw-block, Seesaw-Net achieve notable better performance and efficiency beyond mobilenet-v2 and IGCV3 on dense classification datasets. Moreover, additional ablation experiments was included in <ref type="bibr" target="#b1">[2]</ref> to verify adaptability on small expansion ratio, which implies Seesaw block may has the potential to construct lighter-weight but efficient model for mobile tasks.</p><p>Based on mobilenet-v2 <ref type="bibr" target="#b48">[48]</ref>, mobilenet-v3[6] introduce 3 main updates and achieve new SOTA on several mobile classification, detection and segmentation tasks: 1. Apply the squeeze and excite in the residual layer; 2. Use different nonlinearity depending on the layer; 3. Deploy NAS specifically on number of input/output channels for each basic blocks. The main perpose of our paper is to boost performance as well as efficiency of the baseline modelmobilefacenets, so we just adopt the SE-block and different nonlinearity to mobilefacenets and replace the mobilenet-v2 block with Seesaw-shuffle/Seesaw-share block which we expected will save notable computation cost.</p><p>Seesaw-shuffle and Seesaw-share block. With limited computational resource on mobile platform, Seesaw-Net <ref type="bibr" target="#b1">[2]</ref> introduce 2 types of build block named Seesaw-shuffle and Seesaw-share block so as to enable information flow between uneven convolutional groups in each stage. As <ref type="bibr" target="#b1">[2]</ref> mentioned, only one channel permute/shuffle operation is included in Seesaw-shuffle block and it could be avoided with low-level language implementation instead of calling high-level API of deep learning frameworks. For highlevel deep learning API development, Seesaw-share block introduce channel share operation that will enables the information flow between convolutional groups like channel shuffle operation without any channel permute/shuffle operation, which implies Seesaw-share block could achieve better efficiency compared with channel permute/shuffle based block under the limited computational budgets, since memory transfer lead to more time and power consumption on mobile platform. Generally, high efficiency light-model could be constructed with Seesaw-shuffle and Seesaw-share block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SeesawFaceNets</head><p>In this section, we propose a novel model -Seesaw-FaceNets , sparse and robust face verification. Given an 112 × 112 × 3 RGB face image, SeesawFaceNets aims at efficient facial feature embedding with low computational cost and memory requirement. Inspired by the strategies presented in previous sections, Seesaw block is adopt as the basic build block of SeesawFaceNets, compared to Mobile-FaceNets which utilize the original inverted linear bottlenecks as the basic build block. <ref type="table" target="#tab_0">Table 1</ref> represents the architecture of SeesawFaceNets.</p><p>With the squeeze-and-excitation optimization <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b5">6]</ref> achieves slightly better performance on face recognition and image classification tasks. Seesaw-Net <ref type="bibr" target="#b1">[2]</ref> achieves better performance on mobile classification tasks compared with previous work but no experiments with squeeze-andexcitation optimization included since it focus on basic build block design. We adopt squeeze-and-excitation optimization in our proposed SeesawFaceNets, since we expect the squeeze-and-excitation optimization will improves channel interdependencies of Seesaw-block with limited extra computational cost. In our implementation, Swish is used for the non-linear activation function due to its accuracy improvement over PReLU and ReLU function. The embedding size during our work is fixed to 512 rather than 128 in <ref type="bibr" target="#b0">[1]</ref>, since better performance could be obtained by 512 with only few amount of computational cost introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Batch Size and Learning Rate</head><p>Recent deep neural network training is typically based on mini-batch stochastic gradient optimization. An milestone work on relation between batch size and learning rate is the linear scaling rule adopted by Krizhevsky <ref type="bibr" target="#b7">[8]</ref>, which reported a 1% increase of error on the validation set of ILSVRC 2012 when increasing the minibatch size from 128 to 1024. <ref type="bibr" target="#b8">[9]</ref> show how to maintain accuracy across a much broader regime of minibatch sizes while also present an informal discussion of the linear scaling rule and why it may be effective.</p><p>In our experiments, we adopt similar learning schedule from the baseline model-mobilefacenets except the batch size due to limited GPU memory. However, we hypothesize the above linear scaling rule will not effect result obvously across a much narrow regime of minibatch sizes compared to <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>, and we found that the model converges very slowly if we set 0.025 as the initial learning rate which may  MS1MV2 <ref type="bibr" target="#b18">[19]</ref> is introduced as a semi-automatic refined version of the MS-Celeb-1M dataset. As given in Table 1, we employ MS1MV2 as our training data in order to conduct fair comparison with other methods. During training, we explore public face verification database (e.g. LFW <ref type="bibr" target="#b38">[38]</ref>, CFP-FP <ref type="bibr" target="#b20">[20]</ref>, AgeDB-30 <ref type="bibr" target="#b21">[21]</ref>) to check the improvement from different settings. Besides the most widely used LFW <ref type="bibr" target="#b38">[38]</ref> dataset, we also report the performance of SeesawFaceNets on the recent large-pose and large-age datasets(e.g. CPLFW <ref type="bibr" target="#b22">[22]</ref> and CALFW <ref type="bibr" target="#b23">[23]</ref>) to further verify robustness of the models. This training data has no overlapping with the testing data.</p><p>The comparisons between different models in terms of both accuracy/model size/MAdds are represented. Here we only count MAdds of the convolutional and fullly connected layers, and sigmoid operations not take into account. An key difference between sigmoid/swish and other non-linearity functions is that sigmoid/swish belongs to transcendental functions. To the best of our knowledge, mainstream deep learning hardware will include dedicated processing unit for transcendental functions implement. e.g. NVIDIA GPU since fermi series, Google TPU include activation pipeline, dedicated ASICs, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>During the face preprocessing stage, MTCNN <ref type="bibr" target="#b57">[57]</ref> face detection and alignment algorithm is applied to detect all faces and their five landmark points. Then, with the returned face landmark points, each face is aligned and cropped into a template with the size of 112 × 112 × 3. The template is normalized into [−1, 1] by subtracting the mean pixel value, i.e. 127.5, and then divided by 128.</p><p>For training, we adopt Stochastic Gradient Descent (SGD) as the optimizer with the batch size of 164/192/256 due to limited GPU memory. It is also a common but key practical issue for DCNNs training stage that lightweight model could be trained with relative large batch size with same GPU drvices compared to large model. The momentum parameter is set to 0.9, the initial learning rate is set to 0.1 and decreases by a factor of 10 periodically at 9, 13, 15 epochs for 16-epochs training schedule. All of the proposed SeesawFaceNets models in this work are trained from scratch by ArcFace loss with a angular margin m=0.5 for a fair performance comparison with other state of the art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Verification Results</head><p>According to dense experiments on various public validation dataset, our proposed SeesawFaceNets outperform LMobileNetE and MobileFaceNet on all listed Verification datasets: SeesawFaceNets always performs better than Mo-bileFaceNet with 66% of the MAdds operation even with less batch size and training steps, and Seesaw-shareFaceNet does not include extra channel permute/shuffle operations which may introduce cost when implement with high-level language or mainstream deep learning frameworks <ref type="bibr" target="#b1">[2]</ref>.  To further verify the performance of proposed improved seesaw block in face recognition tasks, we adopt the hyperparameters and network structure of MobiFace to construct a Seesaw-shuffleFaceNet(mobi)(see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>According to the verification result, the Seesaw-shuffleFaceNet(mobi) could also achieve better performance on all listed benchmark dataset than MobileFaceNet with 69.7%(154M VS 221M MAdds), and achieve comparable performance with SOTA model-mobiface on LFW dataset with only 54.2%(1.3M VS 2.4M) parameters and 31.6%(146M VS 462M MAdds) computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Adaptability on deeper and wider networks</head><p>In order to pursue ultimate performance, we construct deeper and wider version of Seesaw-shuffleFaceNet named DW-SeesawFaceNet and train the novel model with same training schedule and hyper parameters but less batch size due to limited available GPU memory, which also implies that our DW-SeesawFaceNet could be trained from scratch by mini-batch size of 128 with only 22GB GPU memory requirement with pytorch 1.0 <ref type="bibr" target="#b66">[65]</ref>.</p><p>Inverted residual bottleneck is a baseline structure of dense SOTA mobile ( <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b5">6]</ref>) first introduced in [48] since improved information flow enabled by shortcut connections in residual structure, however, when the input channel number does not equal to the output channel, shortcut connections is removed in Inverted residual bottleneck structure. To strengthen the feature reuse, firstly we add a skip connection branch comprises a 2*2 max pooling and a 1*1 convolution layers to all Inverted bottleneck blocks without residual structure whose filter stride is 2, we named this version of deeper and wider networks(see <ref type="table" target="#tab_6">Table 6</ref>) as DW-SeesawFaceNet V2 and named the baseline version without the skip connection branch as DW-SeesawFaceNet V1. The performance on listed public benchmark datasets show that the second version gives better result.</p><p>According to the verification result, the DW-SeesawFaceNet V1 and V2 could reach comparable performance on all listed public Verification datasets, with 6.5% parameters size and 4.35% computational cost compared to large-scale deep face recognition network <ref type="bibr" target="#b3">[4]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Inspired by different network design strategies, this paper has further presented a novel simple but highperformance deep network for face recognition, named See-sawFaceNets. Experiments on dense public face verification benchmarks have shown the efficiency of our Seesaw-FaceNets in terms of both accuracy and computation cost. Although the model is very small, its performance on both testing benchmarks is competitive against other large-scale deep face recognition network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Seesaw-shuffle block (b) Seesaw-share block The structure of the proposed Seesaw-shuffle and Seesaw-share block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Model Architecture of SeesawFaceNet for facial feature embedding. /2 means stride of the operator is 2. "RBlock" and "Block" respectively indicate the Seesaw Block with and without Residual learning connection.</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell></cell></row><row><cell cols="3">112 × 112 × 3 3 × 3 Conv, /2, 64</cell></row><row><cell>56 × 56 × 64</cell><cell cols="2">3 × 3 DWconv, 64</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 128</cell></row><row><cell>56 × 56 × 64</cell><cell>Block 1 ×</cell><cell>3 × 3 DWconv, /2, 128</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 64</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 128</cell></row><row><cell>28 × 28 × 64</cell><cell>RBlock 4 ×</cell><cell>3 × 3 DWconv, 128</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 64</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 256</cell></row><row><cell>28 × 28 × 64</cell><cell>Block 1 ×</cell><cell>3 × 3 DWconv, /2, 256</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 128</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 256</cell></row><row><cell cols="2">14 × 14 × 128 RBlock 6 ×</cell><cell>3 × 3 DWconv, 256</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 128</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 512</cell></row><row><cell cols="2">14 × 14 × 128 Block 1 ×</cell><cell>3 × 3 DWconv, /2, 512</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 128</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 256</cell></row><row><cell>7 × 7 × 128</cell><cell>RBlock 2 ×</cell><cell>3 × 3 DWconv, 256</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 128</cell></row><row><cell>7 × 7 × 128</cell><cell cols="2">1 × 1 Conv, 512</cell></row><row><cell>7 × 7 × 512</cell><cell cols="2">linear GD7 × 7 Conv, 512</cell></row><row><cell>1 × 1 × 512</cell><cell cols="2">linear 1 × 1 Conv</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Model Architecture of SeesawFaceNet which adopt the hyper parameters of MobiFace for facial feature embedding. /2 means stride of the operator is 2. "RBlock" and "Block" respectively indicate the Seesaw Block with and without Residual connection.</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell></cell></row><row><cell cols="4">112 × 112 × 3 3 × 3 Conv, /2, 64</cell></row><row><cell>56 × 56 × 64</cell><cell cols="3">3 × 3 DWconv, 64</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1 Conv, 128</cell></row><row><cell>56 × 56 × 64</cell><cell>Block 1 ×</cell><cell cols="2">3 × 3 DWconv, /2, 128</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1 Conv, Linear, 64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 × 1 Conv, 128</cell></row><row><cell>28 × 28 × 64</cell><cell cols="2">RBlock 2 ×</cell><cell>3 × 3 DWconv, 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 64</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1 Conv, 256</cell></row><row><cell>28 × 28 × 64</cell><cell>Block 1 ×</cell><cell cols="2">3 × 3 DWconv, /2, 256</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1 Conv, Linear, 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 × 1 Conv, 256</cell></row><row><cell cols="3">14 × 14 × 128 RBlock 3 ×</cell><cell>3 × 3 DWconv, 256</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 128</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1 Conv, 512</cell></row><row><cell cols="2">14 × 14 × 128 Block 1 ×</cell><cell cols="2">3 × 3 DWconv, /2, 512</cell></row><row><cell></cell><cell></cell><cell cols="2">1 × 1 Conv, Linear, 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 × 1 Conv, 256</cell></row><row><cell>7 × 7 × 128</cell><cell cols="2">RBlock 6 ×</cell><cell>3 × 3 DWconv, 256</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 128</cell></row><row><cell>7 × 7 × 128</cell><cell cols="2">1 × 1 Conv, 512</cell></row><row><cell>7 × 7 × 512</cell><cell cols="3">linear GD7 × 7 Conv, 512</cell></row><row><cell>1 × 1 × 512</cell><cell cols="3">linear 1 × 1 Conv</cell></row><row><cell cols="4">require longer training epochs. We set dense control exper-</cell></row><row><cell cols="4">iments which adopt same batch size for all models to make</cell></row><row><cell cols="4">fair comparisons. The experiment result also verified our</cell></row><row><cell cols="4">view, our proposed SeesawFaceNets outperforms mobile-</cell></row><row><cell cols="4">facenets and MobiFace with similar training schedule and</cell></row><row><cell cols="4">achieve state of the art on mobile face recognition tasks.</cell></row><row><cell cols="2">4. Experiments</cell><cell></cell></row><row><cell>4.1. Datasets</cell><cell></cell><cell></cell></row><row><cell cols="2">Datasets</cell><cell cols="2">#Identity #Image</cell></row><row><cell cols="2">MS1MV2</cell><cell cols="2">85K</cell><cell>5.8M</cell></row><row><cell cols="2">LFW [38]</cell><cell cols="2">5,749</cell><cell>13,233</cell></row><row><cell cols="2">CFP-FP [20]</cell><cell cols="2">500</cell><cell>7,000</cell></row><row><cell cols="2">AgeDB-30 [21]</cell><cell cols="2">568</cell><cell>16,488</cell></row><row><cell cols="2">CPLFW [22]</cell><cell cols="2">5,749</cell><cell>11,652</cell></row><row><cell cols="2">CALFW [23]</cell><cell cols="2">5,749</cell><cell>12,174</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Face datasets for training and testing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Verification results (%) of different model. Training dataset: CASIA-Webface for AirFace and MS1MV2 for other model. For AirFace, we list the best evaluation results with different loss function</figDesc><table><row><cell cols="9">Verification results (%) of different model. MobileFaceNet(*) stands for our implement of MobileFaceNet with 512D embedding</cell><cell></cell></row><row><cell cols="9">size(Training dataset of MobileFaceNet[1]: CASIA-Webface/MS-Celeb-1M(cleaned), LMobileNetE: MS-Celeb-1M(refined); Our im-</cell><cell></cell></row><row><cell cols="9">plement of MobileFaceNet/Seesaw-shuffleFaceNet/Seesaw-shareFaceNet/Seesaw-shuffleFaceNet(mobi): MS1MV2. For each validation</cell><cell></cell></row><row><cell cols="2">dataset, top-3 accuracy records in bold)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Training epoch Params MAdds Batch size</cell><cell>LFW</cell><cell cols="4">CFP-FP AgeDB-30 CPLFW CALFW</cell></row><row><cell>HUMAN-Individual[4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.27</cell><cell>-</cell><cell>-</cell><cell>81.21</cell><cell>82.32</cell></row><row><cell>HUMAN-Fusion[4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.85</cell><cell>-</cell><cell>-</cell><cell>85.24</cell><cell>86.50</cell></row><row><cell>Arcface[4]</cell><cell>16</cell><cell>65M</cell><cell>12.1G</cell><cell>512</cell><cell>99.83</cell><cell>98.37</cell><cell>98.15</cell><cell>92.08</cell><cell>95.45</cell></row><row><cell>AirFace[35]</cell><cell>61</cell><cell>-</cell><cell>1G</cell><cell>1024</cell><cell cols="2">99.267 94.514</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ShrinkTeaNet-MFNR[34]</cell><cell>-</cell><cell>3.73M</cell><cell>-</cell><cell>512</cell><cell>99.77</cell><cell>95.14</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DW-SeesawFaceNet V1</cell><cell>16</cell><cell>4.1M</cell><cell>514M</cell><cell>128</cell><cell>99.80</cell><cell>96.39</cell><cell>97.52</cell><cell>91.77</cell><cell>96.07</cell></row><row><cell>DW-SeesawFaceNet V2</cell><cell>16</cell><cell>4.2M</cell><cell>526M</cell><cell>128</cell><cell>99.80</cell><cell>97.24</cell><cell>97.55</cell><cell>91.98</cell><cell>95.98</cell></row><row><cell>5. Ablation Study</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">5.1. Adaptability on other SOTA mobile networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">on face verification tasks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Model Architecture of DW-SeesawFaceNet for facial feature embedding. /2 means stride of the operator is 2. "RBlock" and "Block" respectively indicate the Seesaw Block with and without Residual connection.</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell></cell></row><row><cell cols="3">112 × 112 × 3 3 × 3 Conv, /2, 96</cell></row><row><cell>56 × 56 × 96</cell><cell cols="2">3 × 3 DWconv, 96</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 128</cell></row><row><cell>56 × 56 × 96</cell><cell>Block 1 ×</cell><cell>3 × 3 DWconv, /2, 128</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 96</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 192</cell></row><row><cell>28 × 28 × 96</cell><cell>RBlock 8 ×</cell><cell>3 × 3 DWconv, 192</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 96</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 384</cell></row><row><cell>28 × 28 × 96</cell><cell>Block 1 ×</cell><cell>3 × 3 DWconv, /2, 384</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 192</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 384</cell></row><row><cell cols="2">14 × 14 × 192 RBlock 12 ×</cell><cell>3 × 3 DWconv, 384</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 192</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 768</cell></row><row><cell cols="2">14 × 14 × 192 Block 1 ×</cell><cell>3 × 3 DWconv, /2, 768</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 192</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, 384</cell></row><row><cell>7 × 7 × 192</cell><cell>RBlock 4 ×</cell><cell>3 × 3 DWconv, 384</cell></row><row><cell></cell><cell></cell><cell>1 × 1 Conv, Linear, 192</cell></row><row><cell>7 × 7 × 192</cell><cell cols="2">1 × 1 Conv, 512</cell></row><row><cell>7 × 7 × 512</cell><cell cols="2">linear GD7 × 7 Conv, 512</cell></row><row><cell>1 × 1 × 512</cell><cell cols="2">linear 1 × 1 Conv</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>We would like to thank Xiangyu Zhang for his helpful suggestion and Chi Nhan Duong team for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07573</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03672</idno>
		<title level="m">Seesaw-Net: Convolution Neural Network With Uneven Group Convolution</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoph</forename><forename type="middle">P B</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11080</idno>
		<title level="m">A lightweight deep learning face recognition on mobile devices</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1602.02830</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond principal components: Deep boltzmann machines for face modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Longitudinal face modeling via temporal deep restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep appearance models: A deep boltzmann machine approach for face modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal non-volume preserving approach to facial age-progression and age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Agedb: The first manually collected in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep recurrent level set for segmenting brain tumors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S H N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1510.00149</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ShrinkTeaNet: Million-scale Lightweight Face Recognition via Shrinking Teacher-Student Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10620</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12256</idno>
		<title level="m">AirFace: Lightweight and Efficient Model for Face Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on faces in&apos;Real-Life&apos;Images: detection, alignment, and recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks. In NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep contextual recurrent residual networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reformulating level sets as deep recurrent neural network approach to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Trans. on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TIP)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust hand detection in vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7341" to="7349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fdmobilenet: Improved mobilenet with a fast downsampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1363" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00178</idno>
		<title level="m">IGCV3: Interleaved low-rank group convolutions for efficient deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Quantization mimic: Towards very tiny cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1805.02152</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards a deep learning framework for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchor&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Weakly supervised facial analysis with dense hyper-column features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Enhancing interior and exterior deep facial features for face detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl Conf. on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 1mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analysis of GPU and FPGA parallel computing -example of Kirchhoff prestack time migration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Geophysics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Van Gool Ai benchmark: Running deep neural networks on android smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
