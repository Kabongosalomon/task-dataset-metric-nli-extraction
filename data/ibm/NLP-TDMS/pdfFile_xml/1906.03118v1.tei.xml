<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliable Estimation of Individual Treatment Effect with Causal Information Bottleneck</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyub</forename><surname>Kim</surname></persName>
							<email>sungyub.kim@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsu</forename><surname>Baek</surname></persName>
							<email>yongsubaek@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<email>sjhwang82@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
							<email>eunhoy@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aitrics</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">Reliable Estimation of Individual Treatment Effect with Causal Information Bottleneck</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating individual level treatment effects (ITE) from observational data is a challenging and important area in causal machine learning and is commonly considered in diverse mission-critical applications. In this paper, we propose an information theoretic approach in order to find more reliable representations for estimating ITE. We leverage the Information Bottleneck (IB) principle, which addresses the trade-off between conciseness and predictive power of representation. With the introduction of an extended graphical model for causal information bottleneck, we encourage the independence between the learned representation and the treatment type. We also introduce an additional form of a regularizer from the perspective of understanding ITE in the semi-supervised learning framework to ensure more reliable representations. Experimental results show that our model achieves the state-of-the-art results and exhibits more reliable prediction performances with uncertainty information on real-world datasets.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating individual-level treatment effect (ITE) , which infers causality between a treatment and an outcome from observational data, is one of the fundamental problems in machine learning and is essential in various applications such as healthcare <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1]</ref>, political science <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref>, and online advertisement <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> to name a few. In order to accurately deduce the causality between a treatment and an outcome, it is necessary to predict the outcome of each treatment correctly for each individual data point. However, in the usual (or almost all) observational studies, each data point is only allowed to receive one treatment out of several options, and the results for other unselected treatments do not even exist in the data. Hence, this ITE problem is connected to counterfactual questions <ref type="bibr" target="#b17">[18]</ref> like "If the patient received other prescriptions, would the patient's disease progress differently?". A naive approach for this is to predict counterfactual outcomes that have not been observed, using the model trained only on observed factual data. One implicit assumption of such an inference is that the input distributions of the treatment and control groups (in case of two options) should be identical. It is known, however, that a bias may exist in the selection of treatments, unless the data are obtained in a rigorously randomized controlled trial, leading to a covariate shift in the ITE problem.</p><p>To overcome this difficulty, previous works focus on a representation learning approach. A common central idea in this vein is to learn the feature extractor by which any discrepancy between the treatment and the control becomes smaller in the learned representation space than in the original covariate space. Almost all existing works based on this representation learning approach for ITE can be seen as a way of finding balanced and maximally expressive representation that could be simultaneously used in the prediction of factual and counterfactual outcomes. Motivated by the recent success of information bottleneck in several domains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28]</ref>, in this paper we propose a causal information bottleneck (CIB) that additionally pursue maximal compressiveness between the representation for ITE prediction and the observed covariate, on top of previously considered balance and maximal expressiveness. By learning with information bottleneck principles <ref type="bibr" target="#b31">[32]</ref>, not only can one learn an ideal representation that can better predict the counterfactual outcomes, but one can also gain additional (but more important in some sense) effects. Specifically, earlier work <ref type="bibr" target="#b2">[3]</ref> showed that models learned with information bottleneck objective are more robust to overfitting and adversarial attacks than models learned through other forms of objectives or regularizations. This occurs because the representation found through information bottleneck tries to ignore as much information as possible from the covariate and hence idiosyncratic perturbations cannot easily pass through an information bottleneck. In addition to improving generalization, models learned by IB principals are empirically shown to be well calibrated providing meaningful uncertainty about their predictions <ref type="bibr" target="#b1">[2]</ref>. In the same context, one can expect that the proposed CIB naturally inherits such benefits of IB. Toward this, we experimentally confirm that CIB yields significantly better results against baselines on the scenario where the models are allowed to say "I don't know" on instances that are different from the training data and hence they are not sure. It is one of the most important desiderata to provide accurate uncertainty on their predictions since most tasks that consider the counterfactual inference are irreversible and mission-critical problems with serious consequences.</p><p>Main contributions. The main contributions of this paper are as follows:</p><p>• We introduce a graphical model for causal inference on which we can apply the information bottleneck (IB) principle for reliable individual treatment effect (ITE) estimations. The proposed principle is formulated as a simple optimization problem that can be efficiently solved with stochastic gradient methods <ref type="bibr" target="#b18">[19]</ref> and a reparameterization trick <ref type="bibr" target="#b19">[20]</ref>.</p><p>• Along with the standard information bottleneck, we propose to use two additional regularizations, one that emerges naturally by the graph structure and another inspired by semi-supervised learning literature, to help learning representations for ITE.</p><p>• We validate our model on three standard benchmark datasets and show that our model significantly outperforms with well-calibrated uncertainty for the scenario where the models are allowed to say "I don't know".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Setup</head><p>Individual Treatment Estimation (ITE) Here, we denote the set of possible covariates by X and the set of possible outcomes by Y. We focus on a binary treatment scenario that selects one among the treat/control options, just for simplicity. For convenience of the notation, we use 0 to denote the control and 1 to denote the treat, that is, the space of treatment is T = {0, 1}. For example, let x i ∈ X be a covariate describing the features of patient i. If the patient chooses to receive some treatment, t i = 1, otherwise t i = 0. The actual patient progression variable (e.g. blood pressure, blood sugar) after selecting the medical care becomes y F i ∈ Y. Such a set of factual data can be expressed as</p><formula xml:id="formula_0">{(x i , t i , y F i )} n i=1</formula><p>for n patients. Similarly, the counterfactual data can be expressed as</p><formula xml:id="formula_1">{(x i , 1 − t i , y CF i )} n i=1</formula><p>, however there is no y CF i value available in the observational data. The individual treatment effect (ITE) that we want to estimate is a conditional expected difference between the treatment outcome y  given covariate</p><formula xml:id="formula_2">x i , τ (x i ) = E y (1) i − y (0)</formula><p>i |x i . A commonly used metric to evaluate the performance of ITE estimators is the expected Precision in Estimation of Heterogeneous Effect (PEHE) <ref type="bibr" target="#b13">[14]</ref>, which is defined as</p><formula xml:id="formula_3">P EHE = E x τ (x i ) −τ (x i ) 2 .</formula><p>Here,τ (x i ) is the predicted difference by the model.</p><p>As several works on causal inference including <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> did for justifiability of counterfactual prediction, we also assume throughout the paper the following two conditions of the Rubin-Neyman causal model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>: </p><formula xml:id="formula_4">• (Overlap) P[t|x] ∈ (0, 1), ∀x ∈ X , ∀t ∈ T , Z Y X (a) Z Y 1 X T Y 0 (b) T X Z Y (c)</formula><formula xml:id="formula_5">• (Conditional ignorability) Y |= T |X.</formula><p>The overlap assumption here requires that all patients have a strictly greater than 0 probability of receiving all treatments. This assumption is necessary to ensure that each patient has both possibilities of being treated or controlled so that so that ITE can be meaningfully estimated. The assumption of conditional ignorability implies that the covariate has sufficient information about the common effects of treatment decisions and outcomes. Therefore, it is also referred to as an unconfoundedness assumption <ref type="bibr" target="#b26">[27]</ref>. This unconfoundedness assumption is necessary to make the estimation of ITE identifiable <ref type="bibr" target="#b15">[16]</ref>.</p><p>Information Bottleneck (IB) principle The information bottleneck principle was first proposed in <ref type="bibr" target="#b31">[32]</ref>. The trade-off between maximal expressiveness and maximal compressiveness in the information bottleneck theory is expressed by the following expression, which contains two mutual information terms I(·, ·): one between input X and representation Z and the other between representation Z and output Y :</p><formula xml:id="formula_6">maximize I(Y ; Z) − β I(X; Z).</formula><p>The classical information bottleneck assumes the graphical model shown in <ref type="figure" target="#fig_2">Figure 1</ref>(a) and computes p φ (z|x) by sampling from stochastic encoder given input. In general, it is known that the information bottleneck principle is difficult to pursue since a precise estimation of the mutual information is intractable. However, recent works such as those focusing on variational approximation <ref type="bibr" target="#b2">[3]</ref> and adversarial learning <ref type="bibr" target="#b4">[5]</ref> proposed efficient techniques to approximate mutual information and confirmed that information bottleneck is an effective regularization methodology robust to observation noise, hence providing better generalization performance and resilience against adversarial attacks and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Causal information bottleneck (CIB)</head><p>It is important to note that even under the conditional ignorability above, the covariate X might have information irrelevant to both Y and T in many real problems. Such noise in covariates cause problems such as overfitting. Since in the main application fields of causal inference such as health care and political science, it is hard to collect huge number of training data, and moreover they are usually irreversible mission-critical problems, we need a systematic way of removing such noise present in the covariate and having a reliable representation for predictions.</p><p>Toward this, in this section we propose an information bottleneck framework for causal inference.</p><p>We extend the graphical model in <ref type="figure" target="#fig_2">Figure 1</ref>(a) into the form of <ref type="figure" target="#fig_2">Figure 1</ref>(b) to incorporate variables on treatment and factual/counterfactual outcomes. Unlike the graphical models to capture data generation process with hidden confounder Z (for example, CEVAE <ref type="bibr" target="#b23">[24]</ref> in <ref type="figure" target="#fig_2">Figure 1(c)</ref>), the graphical model of CIB directly extends the information bottleneck to causal inference task, to find some good representation in the discriminative setting. Note that treatment T and outcome Y are conditionally independent given covariate X, which is a sufficient condition for the conditional ignorability of the Rubin-Neyman causal model. Under the graphical model shown in Figure1(b), we propose the information bottleneck principle for the causal model:</p><formula xml:id="formula_7">• (Maximal Expressiveness) representation Z should have high mutual information with treatment outcome Y 0 , Y 1 .</formula><p>• (Maximal Compressiveness) representation Z should have low mutual information with covariate X.</p><p>These principles can be expressed as the following optimization problem, with a tunable Lagrange multiplier β:</p><formula xml:id="formula_8">maximize I(Y 0 ; Z) + I(Y 1 ; Z) − βI(X; Z).<label>(1)</label></formula><p>The first term in the objective <ref type="formula" target="#formula_8">(1)</ref>, maximal expressiveness of Z for control outcome Y 0 , can be calculated by the following variational approximation <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_9">I(Y 0 ; Z) = p(y 0 , z) log p(y 0 , z) p(y 0 )p(z) dy 0 dz = p(y 0 , z) log p(y 0 |z) p(y 0 ) dy 0 dz ≥ p(y 0 , z) log q θ (y 0 |z) p(y 0 ) dy 0 dz (2) = p(y 0 , z) log q θ (y 0 |z)dy 0 dz + H(y 0 )<label>(3)</label></formula><p>where q θ (y|z) is a variational approximation of the conditional distribution p(y|z) and H(y 0 ) is the entropy of the random variable y 0 . The inequality (2) follows from the non-negativity property of the Kullback Leibler (KL) divergence:</p><formula xml:id="formula_10">D KL (p(y 0 |z) q θ (y 0 |z)) ≥ 0 =⇒ p(y 0 |z) log p(y 0 |z)dy 0 ≥ p(y 0 |z) log q θ (y 0 |z)dy 0 , ∀z.</formula><p>Now <ref type="formula" target="#formula_9">(3)</ref> can be calculated as follows with a stochastic encoder p φ (z|x):</p><formula xml:id="formula_11">p(y 0 , z) log q θ (y 0 |z)dy 0 dz = p(y 0 |x)p φ (z|x)p(x) log q θ (y 0 |z)dy 0 dzdx.<label>(4)</label></formula><p>This follows from the fact that p(y 0 , z) = p(y 0 , z, x)dx = p(y 0 , z|x)p(x)dx = p(y 0 |x)p φ (z|x)p(x)dx under the graph structure <ref type="figure" target="#fig_2">Figure 1</ref></p><formula xml:id="formula_12">(b).</formula><p>Here, additional difficulty arises in computing (4) due to the nature of observational data; we must marginalize p(y 0 |x) over the entire population p(x), but we know y 0 only for the conditional population p(x|t = 0). To overcome this issue, we can further derive (4) under the overlap assumption p(t = 0|x) ∈ (0, 1) above:</p><formula xml:id="formula_13">p(y 0 |x)p φ (z|x)p(x) log q θ (y 0 |z)dy 0 dzdx = p(y 0 |x)p φ (z|x) p(x) p(x|t = 0) p(x|t = 0) log q θ (y 0 |z)dy 0 dzdx = p(y 0 |x)p φ (z|x) p(t = 0) p(t = 0|x) p(x|t = 0) log q θ (y 0 |z)dy 0 dzdx.</formula><p>At this point we introduce the score classifier s ν to estimate the probability of treatment from data and define the loss:</p><formula xml:id="formula_14">L 0 (φ, θ):= p(y 0 |x)p φ (z|x) p(t = 0) s ν (t = 0|x) p(x|t = 0) log q θ (y 0 |z)dy 0 dzdx.</formula><p>We can derive the variational bound for the mutual information between Z and Y 1 in a similar way.</p><p>On the other hand, the last term in (1), the maximal compressiveness, can be computed as a seamless extension of variation approximation for standard IB <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_15">I(X; Z) ≤ ! CMI . . . Stochas,c Encoder " # (%|') Conditional Mutual Information Estimator ) * (', %, ,) Treat-wise Outcome Estimator -. / 0 % , -. (/ 1 |%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned Marginal</head><p>Representation <ref type="formula">2</ref>  </p><formula xml:id="formula_16">p(x, z) log p φ (z|x)dxdz − p(z) log r ψ (z)dz=:L C (φ, ψ) where r ψ (z) is a variational ap- proximation of p(z).</formula><p>Meanwhile, the graphical model of <ref type="figure" target="#fig_2">Figure 1</ref>(b) assumes that representation Z is conditionally independent of the treatment T given covariate X. Without additional constraint, this assumption can not be satisfied with a stochastic encoder p φ (z|x). Hence, we introduce a novel technique referred to as mutual information guided disentangling that will be explained in the next subsection. Furthermore, the ITE estimation where we are asked to predict unlabeled counterfactual outcomes, may be understood under the semi-supervised learning framework. Based on this interpretation, we apply predictive variance regularization, a popular method in semi-supervised learning <ref type="bibr" target="#b16">[17]</ref>, for counterfactual prediction.</p><p>Mutual Information Guided Disentangled Representation (MIGDR) In order to encourage Z ⊥ ⊥ T |X according to our graphical structure, we minimize the conditional mutual information between them,</p><formula xml:id="formula_17">I(Z; T |X) = E p(x) D KL (p(z, t|x) p φ (z|x)p(t|x)) .</formula><p>Since applying the variational approximation technique as above is not trivial in this case due to the conditional expectation, we resort to the Donsker-Varadhan representation-based methodology <ref type="bibr" target="#b8">[9]</ref>. According to the Donsker-Varadhan representation, the above KL divergence can be expressed as follows <ref type="bibr" target="#b4">[5]</ref>,</p><formula xml:id="formula_18">E p(x) D KL (p(z, t|x) p φ (z|x)p(t|x)) = E p(x) sup f E p(z,t|x) [f ] − log E p φ (z|x)p(t|x) [e f ]</formula><p>where f : X × Z × T → R is a scalar-valued function whose two expectations are finite. As proposed in <ref type="bibr" target="#b4">[5]</ref>, this can be approximated efficiently by estimating auxiliary neural network f ρ with parameter ρ. This additional network discriminate between samples from joint distribution p(z, t|x) and samples from product distribution p(z|x)p(t|x). From f ρ , one can efficiently minimize conditional mutual information between representation Z and target label T by maximizing</p><formula xml:id="formula_19">L M (φ, ρ):= − E p(x) E p(z,t|x) [f ρ (x, z, t)] .<label>(5)</label></formula><p>More details are provided in the appendix.</p><p>Counterfactual Predictive Variance Regularization (CPVR) Predictive variance regularization, a method commonly used for semi-supervised learning, adds variance minimization to the predictive distribution of unlabeled data as a regularization term <ref type="bibr" target="#b16">[17]</ref>. In the regularized Bayesian framework, this method includes the intuition that an unlabeled sample is close to the labeled sample in the representation space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. In this paper, we provide an inductive bias so that counterfactual predictions generated by stochastic encoder are consistent to each other.</p><formula xml:id="formula_20">L V (φ, θ):= − E p(x) Var q θ (y|x) [y CF |x] .</formula><p>The predictive variance regularization is similar to the nearest neighbor methodology frequently used in counterfactual predictions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> in that both provide an approximate target for counterfactual data. However, predictive variance regularization uses the predictions of learned networks, while nearest based methods propagate factual data to unlabelled counterfactual outcomes. The nearest neighbor method can be a good approximation when the reference data is sufficient, but the predictive variance regularization can provide better inductive bias if there is little data to calculate nearest neighbors. We use the gradient information of CPVR only to update the parameters of stochastic encoder as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Overall, the final objective of CIB is described as (with graphical description in <ref type="figure" target="#fig_3">Figure 2</ref>)</p><formula xml:id="formula_21">maximize θ,φ,ψ L(θ, φ, ψ):=L 0 (φ, θ) + L 1 (φ, θ) − β · L C (φ, ψ) + λ M · L M (φ, ρ) + λ V · L V (φ)</formula><p>where L M (φ, ρ) involves the following optimization problem</p><formula xml:id="formula_22">maximize ρ E p(x) sup f E p(z,t|x) [f ] − log E p φ (z|x)p(t|x) [e f ] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Balanced representation for causal inference The idea of balancing the representation between heterogeneous samples is a commonly used idea in the field of unsupervised domain adaptation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15]</ref>. This idea comes from the concept of H-divergence in <ref type="bibr" target="#b5">[6]</ref>. It is proposed in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref> that this idea can be applied to counterfactual inference. The mutual information guided disentangled representation proposed in this paper is different to these works in that the target label is regularized to be conditionally independent when the representation and covariate are given.</p><p>Properly exploiting the local structure of each covariate in learning balanced representation is an important issue in the ITE. The most common idea is known as the nearest neighbor matching (NNM), which finds opponent treatment data for each covariate <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref>. The predictive variance regularization is also considered to find some kernel in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. In contrast to these studies, we propose to apply this predictive variance regularization to the learning of stochastic encoders. Recently, <ref type="bibr" target="#b9">[10]</ref> also proposed the balancing method in an adversarial way for causal inference.</p><p>Generative models for causal inference The stochastic encoder p φ (z|x) of CIB can be understood as a generative model in that it samples representations satisfying the information bottleneck principle for causal inference. The variational information bottleneck <ref type="bibr" target="#b2">[3]</ref> is known as the supervised learning version of β-VAE <ref type="bibr" target="#b12">[13]</ref>. Despite their similarities on network architectures and loss functions, they have different interpretations; VIB with stochastic encoders variationally approximates the marginal representation distribution p(z) and the conditional distribution p(y|z), but VAE with the prior and sample generators approximates the posterior distribution p(z|x). This difference is related to regularizations on z. Since our model is the natural extension of VIB for causal inference, it inherits this property and has a similar connection to CEVAE <ref type="bibr" target="#b23">[24]</ref>.</p><p>Information bottleneck principle is used for causal inference by recent parallel work <ref type="bibr" target="#b24">[25]</ref>. A graphical structure for CEIB <ref type="bibr" target="#b24">[25]</ref> assumes that the representation Z directly affects to outcome Y . On the other hand, CIB naturally extends the standard information bottleneck principle and inherits its benefits. This difference between CIB and CEIB influences the details of architecture selections and objective functions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We evaluate CIB on three real-world datasets used in the existing literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24]</ref>. Due to the space constraints, we defer descriptions on datasets and evaluation metrics to the appendix.</p><p>Baselines We compare our method against classical regression methods, nearest neighbor based methods and representation learning based methods. Classical regression methods involve Ordinary Least Squares with treatment as a feature (OLS/LR 1), OLS with separate regressors for each treamtment (OLS-2). Nearest neighbor based methods contain Hilbert-Schmidt Independence Criterion based Nearest Neighbor Matching (HSIC-NNM) <ref type="bibr" target="#b6">[7]</ref>, Propensity Score Match with logistic regression (PSM) <ref type="bibr" target="#b25">[26]</ref> and k-nearest neighbor (k-NN) <ref type="bibr" target="#b7">[8]</ref>. Representation learning based methods contain Balanced Neural Network (BNN) <ref type="bibr" target="#b17">[18]</ref>, Treatment-Agnostic Representation Network (TARNET), Counterfactual regression with MMD/Wasserstein metric (CFR-M/W) <ref type="bibr" target="#b28">[29]</ref>, Generative Adversarial Nets for inference of ITE (GANITE) <ref type="bibr" target="#b35">[36]</ref>, Causal Effect Variational Autoencoder (CEVAE) <ref type="bibr" target="#b23">[24]</ref> and local Similarity preserved Individual Treatment Effect (SITE) <ref type="bibr" target="#b34">[35]</ref>. We also report the results for SITE with MMD/Wasserstein metric (SITE-M/W). Results in <ref type="table" target="#tab_0">Table 1</ref> and 2 except our model are reported in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. We parenthesized the results of CEVAE and GANITE, since their results are based on slightly different setting: 1000 realization for IHDP dataset and 100 realization for Jobs dataset (GANITE).</p><p>Implementation We use a shared encoder for both treat/control data and separate regressors as in TARNET <ref type="bibr" target="#b28">[29]</ref>. Using a Gaussian reparameterization trick as proposed in VIB <ref type="bibr" target="#b2">[3]</ref>, our stochastic encoder predicts mean and standard deviation of conditional Gaussian given covariates. We use the same number of hidden layers for conditional mutual information estimator and encoder. We use single-layered networks for regressors. The score classifier did not improve empirical performance of CIB, and was omitted. More details on implementations are provided in the appendix.</p><p>Results <ref type="table" target="#tab_0">Tables 1 and 2</ref> represent means and standard errors of 10 realizations/splits on three datasets. CIB achieves the best result for in-sample error on Twins dataset and out-sample error on IHDP and Twins datasets. On Jobs datasets, CIB shows the comparable performance to other state-ofthe-art models. Tables here only show comparisons against major baselines and full comparisons are provided in the appendix due to the space constraints.</p><p>As noted in <ref type="bibr" target="#b34">[35]</ref>, the results show that the representation learning based methods outperform the classical regression methods and the nearest neighbor based methods. Our CIB method also follows this tendency. Although other representation balancing methods such as BNN <ref type="bibr" target="#b17">[18]</ref>, CFR <ref type="bibr" target="#b28">[29]</ref> and SITE <ref type="bibr" target="#b34">[35]</ref> consider group-wise balancing representation, i.e. their regularization for balancing  Uncertainty calibration with information bottleneck penalty Since the representation of CIB is modeled as conditional Gaussian, we can measure how generated representation z given covariate x is rare compared to marginal representation as log</p><formula xml:id="formula_23">p φ (z|x) r ψ (z)</formula><p>. Therefore, if we average this with sampled representation as</p><formula xml:id="formula_24">E p φ (z|x) [log p φ (z|x) r ψ (z) ]=D KL (p φ (z|x) r ψ (z)),</formula><p>we can interpret this term as an indicator for how much the covariate x is outof-distribution (OOD) sample for entire population p(x) <ref type="bibr" target="#b1">[2]</ref>. In order to explicitly benefit this property, we consider the scenario in this experiment where models are allowed to answer "I don't know" for uncertain inputs.</p><p>In <ref type="figure" target="#fig_4">Figure 3</ref>, we show our results on removing top k% uncertain samples for IHDP datasets. Since CEVAE <ref type="bibr" target="#b23">[24]</ref> can also measure D KL (p φ (z|x, t, y) r ψ (z)) for confounder Z, we also remove uncertain samples based on this KL divergence (denoted as "CEVAE KL"). However, since the predictive variances are unavailable for other methods, we just drop k% samples randomly for them. As <ref type="figure" target="#fig_2">Figure  1</ref> shows, only CIB quickly finds and drops samples on which it is not sure.</p><p>Ablation Study We proposed two additional regularizations above, MIGDR and CPVR on top of basic CIB framework. Here, we confirm our additional regularization is critical to empirical success of our model. Toward this, we compare our CIB against CIB without MIGDR, CIB without CPVR and CIB with no additional regularization on three datasets. <ref type="table" target="#tab_2">Table 3</ref> summarizes the results. The CPVR regularization is critical to results for IHDP dataset, while both MIGDR and CPVR are important for other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a novel framework CIB for estimating ITE by extending information bottleneck. On the top of information bottleneck framework, we proposed two additional regularizations to learn more reliable representation. We confirmed CIB method performs comparable to state-of-the-art models and reliable in detecting how much given covariate x is out-of-distribution (OOD) sample for entire population p(x). This property of CIB is critical to mission-critical applications of causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on MGIDR</head><p>As mentioned in main text, conditional mutual information between Z and T can be represented as</p><formula xml:id="formula_25">E p(x) D KL (p(z, t|x) p φ (z|x)p(t|x)) = E p(x) sup f E p(z,t|x) [f ] − log E p φ (z|x)p(t|x) [e f ] .</formula><p>where f : X × Z × T → R is a scalar-valued function whose two expectation are finite. Due to the universal approximation properties of the neural network, this statistic is strongly consistent when we approximate f ρ with neural networks without optimizing on the infinite dimensions <ref type="bibr" target="#b4">[5]</ref>. Learning to maximize the mean difference between the two distributions is similar to learning discriminators in generative adversarial learning. Because it is less important to calculate the exact mutual information in this paper, we optimize the statistic network f using a zero-centered gradient penalty <ref type="bibr" target="#b30">[31]</ref>, which is more stable and known to be capable of greater generalization performance outcomes. Moreover, finding the optimal f ρ for each covariate x is inefficient because doing so requires the estimation of several functions. Therefore, the statistic network is learned with amortized inference by considering x conditionally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details on Experiments</head><p>Datasets To evaluate the proposed CIB, we use three real-world datasets used to evaluate the existing methodologies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24]</ref>. The covariates of Infant Health and Development Program (IHDP) dataset are from real-world randomized controlled trial experiment. By removing some of the treated observations of RCT dataset, the IHDP dataset intentionally involves selection bias and is used as benchmark data for ITE researches. More details are given in <ref type="bibr" target="#b13">[14]</ref>. The data consists of 25 dimensions 747 covariates . We split train/validate/test 10 realization of simulated data with 63/27/10 ratio as proposed in <ref type="bibr" target="#b34">[35]</ref>. Jobs dataset <ref type="bibr" target="#b20">[21]</ref> is a mixed observations based on the National Support Work program with observational study <ref type="bibr" target="#b29">[30]</ref>. The covariate of Jobs dataset consists with 17 dimension 3212 instance. Since this dataset consists of one realization without repeated simulation, we experimented with 10 times train/validation/test split with 56/24/20 ratio as suggested in <ref type="bibr" target="#b28">[29]</ref>. The Twins dataset was used as the benchmark data of counterfactual inference in for the first time in <ref type="bibr" target="#b23">[24]</ref>. This data is based on the twins born in the USA in 1989-1991. As in <ref type="bibr" target="#b34">[35]</ref>, we focus on the same sex twin-pair less than 2000g. The covariate is a 40-dimensional data consisting of parents, their condition at the time of their pregnancy, and birth information. We used the identical data preprocessing as suggested in <ref type="bibr" target="#b34">[35]</ref>.</p><p>Performance Metrics In the case of IHDP datasets where all control/treat outcomes are known for each covariate, we use the expected Precision in Estimation of Heterogeneous Effect (PEHE). Since there exists only factual outcome for a given covariate, it is intractable to calculate the PEHE score in Jobs data. In order to evaluate the performance of the learned model, <ref type="bibr" target="#b28">[29]</ref> defined a policy π f based on the learned model f as follows.</p><formula xml:id="formula_26">π f (x) = 1, if f (x, 0) ≤ f (x, 1) 0, if f (x, 0) &gt; f (x, 1) (6) R pol (π f ) = 1 − E[Y 1 |π f (x) = 1] · p(π f = 1) + E[Y 0 |π f (x) = 0] · p(π f = 0) (7)</formula><p>We estimate this for the randomized subset of Jobs dataset aŝ</p><formula xml:id="formula_27">R pol = 1− 1 |X 1 ∩ T 1 ∩ E| xi∈X1∩T1∩E y (i) 1 |X 1 ∩ E| |E| + 1 |X 0 ∩ T 0 ∩ E| xi∈X0∩T0∩E y (i) 0 |X 0 ∩ E| |E| (8) where E = {x i : x i is from the randomized experiment.}, X 1 = {x i :ŷ (i) 1 −ŷ (i) 0 &gt; 0}, A 0 = {x i : y (i) 1 −ŷ (i) 0 ≤ 0},T 1 = {x i : t i = 1}, T 0 = {x i : t i = 0}.</formula><p>Finally, for the Twins dataset, we report area under ROC (AUC) for counterfactual prediction as proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>We measure the in-sample error with both train and valid data and out-sample error with only test data. This convention follows <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Since in-sample evaluation metric is non-trivial, as we never observe the ITE for any unit, we report in-sample results.</p><p>We measure the in-sample error with both train and valid data and out-sample error with only test data. This convention follows <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Since in-sample evaluation metric is non-trivial, as we never observe the ITE for any unit, we report in-sample results.</p><p>Implementation For cross-validation, We search to optimize the number of hidden layers in encoder ({1, 2, 3}), the dimension of layers <ref type="figure" target="#fig_2">({64, 128, 256}</ref>) and the regularization coefficients (β, λ M , λ V ∈ {0.01, 0.1, 1.0, 10.0, 100.0}). We use Adam <ref type="bibr" target="#b18">[19]</ref> with 0.0001 for learning rate and β 1 = 0.9, β 2 = 0.999. We train 2000 iterations with early stopping as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. Experiments in this paper are conducted on a cluster with Intel Xeon E5 2.2GHz CPU, 4x Nvidia Titan Xp GPU and 256GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Full Experiment Result</head><p>Bold font indicates that the mean belongs to 95% confidence interval of the best performing model on each dataset.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>(a) Graphical model of classical information bottleneck (b) Graphical model of causal information bottleneck (c) Graphical model of CEVAE<ref type="bibr" target="#b23">[24]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of Causal Information Bottleneck (CIB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Results on removing top k% "I don't know" samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of counterfactual errors: 10 repeat/in-sample case</figDesc><table><row><cell>Dataset</cell><cell>IHDP( √</cell><cell>P EHE )</cell><cell>Jobs(R pol )</cell><cell>Twins(AUC)</cell></row><row><cell>TARNET</cell><cell cols="2">0.729 ± 0.088</cell><cell>0.228 ± 0.004</cell><cell>0.849 ± 0.002</cell></row><row><cell>CFR-M</cell><cell cols="2">0.663 ± 0.068</cell><cell>0.213 ± 0.006</cell><cell>0.852 ± 0.001</cell></row><row><cell>CFR-W</cell><cell cols="2">0.649 ± 0.089</cell><cell>0.225 ± 0.004</cell><cell>0.850 ± 0.002</cell></row><row><cell>CEVAE</cell><cell cols="2">(2.7 ± 0.1)</cell><cell>0.15 ± 0.0</cell><cell>not reported</cell></row><row><cell>SITE</cell><cell cols="2">0.604 ± 0.093</cell><cell>0.224 ± 0.004</cell><cell>0.862 ± 0.002</cell></row><row><cell>CIB</cell><cell cols="2">0.663 ± 0.193</cell><cell>0.256 ± 0.006</cell><cell>0.870 ± 0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of counterfactual errors: 10 repeat/out-sample case</figDesc><table><row><cell>Dataset</cell><cell>IHDP( √</cell><cell>P EHE )</cell><cell>Jobs(R pol )</cell><cell>Twins(AUC)</cell></row><row><cell>TARNET</cell><cell cols="2">1.342 ± 0.597</cell><cell>0.234 ± 0.012</cell><cell>0.840 ± 0.006</cell></row><row><cell>CFR-M</cell><cell cols="2">1.202 ± 0.550</cell><cell>0.231 ± 0.009</cell><cell>0.840 ± 0.006</cell></row><row><cell>CFR-W</cell><cell cols="2">1.152 ± 0.527</cell><cell>0.225 ± 0.010</cell><cell>0.842 ± 0.005</cell></row><row><cell>CEVAE</cell><cell cols="2">(2.6 ± 0.1)</cell><cell>0.26 ± 0.0</cell><cell>not reported</cell></row><row><cell>SITE</cell><cell cols="2">0.656 ± 0.108</cell><cell>0.219 ± 0.009</cell><cell>0.853 ± 0.006</cell></row><row><cell>CIB</cell><cell cols="2">0.613 ± 0.118</cell><cell>0.211 ± 0.017</cell><cell>0.861 ± 0.005</cell></row></table><note>*Bold font indicates that the mean belongs to 95% confidence interval of the best performing model on each dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of CIB without MIGDR or CPVR</figDesc><table><row><cell>Dataset</cell><cell>IHDP( In-sample</cell><cell>√</cell><cell>P EHE ) Out-sample</cell><cell cols="4">Jobs(R pol ) In-sample Out-sample</cell><cell>Twins(AUC) In-sample Out-sample</cell></row><row><cell>CIB</cell><cell cols="2">0.663 ± 0.193</cell><cell>0.613 ± 0.118</cell><cell cols="2">0.256 ± 0.006</cell><cell cols="2">0.211 ± 0.017</cell><cell>0.870 ± 0.002</cell><cell>0.861 ± 0.005</cell></row><row><cell>w/o MIGDR</cell><cell cols="2">0.664 ± 0.191</cell><cell>0.614 ± 0.118</cell><cell cols="2">0.246 ± 0.004</cell><cell cols="2">0.230 ± 0.013</cell><cell>0.864 ± 0.001</cell><cell>0.858 ± 0.006</cell></row><row><cell>w/o CPVR</cell><cell cols="2">0.686 ± 0.186</cell><cell>0.649 ± 0.122</cell><cell cols="2">0.245 ± 0.013</cell><cell cols="2">0.230 ± 0.013</cell><cell>0.865 ± 0.001</cell><cell>0.858 ± 0.006</cell></row><row><cell>No regularizer</cell><cell cols="2">0.686 ± 0.186</cell><cell>0.649 ± 0.122</cell><cell cols="2">0.245 ± 0.004</cell><cell cols="2">0.230 ± 0.013</cell><cell>0.865 ± 0.001</cell><cell>0.858 ± 0.006</cell></row><row><cell cols="8">representation is defined for a group of representations, while MIGDR of CIB considers seeks</cell></row><row><cell cols="8">instance-wisely balanced representation. Since CIB adds an auxiliary network for balancing represen-</cell></row><row><cell cols="8">tations, it does not suffer from calculating the Sinkhorn divergence in CFR-W or choosing optimal</cell></row><row><cell cols="8">kernel in CFR-M. Also, CIB does not need to hold hard examples to consider local similarity unlike</cell></row><row><cell>SITE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">100</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PEHE (%)</cell><cell>60 70 80 90</cell><cell>CFR Rand SITE Rand CEVAE Rand CIB Rand CEVAE KL CIB KL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>10</cell><cell>20 Drop Rate (%)</cell><cell>30</cell><cell>40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Mean/Std Err for counterfactual error 10 repeat/in-sample ± 4.350 0.310 ± 0.017 0.660 ± 0.005 OSL/LR2 10.280 ± 3.794 0.228 ± 0.012 0.660 ± 0.004 HSIC-NNM 2.439 ± 0.445 0.291 ± 0.019 0.762 ± 0.011 PSM 7.188 ± 2.679 0.292 ± 0.019 0.500 ± 0.003 k-NN 4.432 ± 2.345 0.230 ± 0.016 0.609 ± 0.010 BNN 3.827 ± 2.044 0.232 ± 0.008 0.690 ± 0.008 TARNET 0.729 ± 0.088 0.228 ± 0.004 0.849 ± 0.002 CFR-M 0.663 ± 0.068 0.213 ± 0.006 0.852 ± 0.001 CFR-W 0.649 ± 0.089 0.225 ± 0.004 0.850 ± 0.002 ± 0.015 0.849 ± 0.003 SITE 0.604 ± 0.093 0.224 ± 0.004 0.862 ± 0.002 CIB 0.663 ± 0.193 0.256 ± 0.006 0.870 ± 0.002</figDesc><table><row><cell>Dataset</cell><cell>IHDP( √</cell><cell>P EHE )</cell><cell>Jobs(R pol )</cell><cell>Twins(AUC)</cell></row><row><cell cols="3">OSL/LR1 10.761 GANITE (1.9 ± 0.4)</cell><cell>(0.13 ± 0.01)</cell><cell>not reported</cell></row><row><cell>CEVAE</cell><cell cols="2">(2.7 ± 0.1)</cell><cell>0.15 ± 0.0</cell><cell>not reported</cell></row><row><cell>SITE-M</cell><cell cols="2">1.162 ± 0.118</cell><cell cols="2">0.194 ± 0.015 0.710 ± 0.003</cell></row><row><cell>SITE-W</cell><cell cols="2">0.993 ± 0.112</cell><cell>0.190</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Mean/Std Err for counterfactual error 10 repeat/out-sample ± 0.011 0.762 ± 0.007 SITE 0.656 ± 0.108 0.219 ± 0.009 0.853 ± 0.006 CIB 0.613 ± 0.118 0.211 ± 0.017 0.861 ± 0.005</figDesc><table><row><cell>Dataset</cell><cell>IHDP( √</cell><cell>P EHE )</cell><cell>Jobs(R pol )</cell><cell>Twins(AUC)</cell></row><row><cell>OSL/LR1</cell><cell cols="2">7.354 ± 2.914</cell><cell>0.279 ± 0.067</cell><cell>0.500 ± 0.028</cell></row><row><cell>OSL/LR2</cell><cell cols="2">5.245 ± 0.986</cell><cell>0.733 ± 0.103</cell><cell>0.500 ± 0.016</cell></row><row><cell>HSIC-NNM</cell><cell cols="2">2.401 ± 0.367</cell><cell>0.311 ± 0.069</cell><cell>0.501 ± 0.017</cell></row><row><cell>PSM</cell><cell cols="2">7.290 ± 3.389</cell><cell>0.307 ± 0.053</cell><cell>0.506 ± 0.011</cell></row><row><cell>k-NN</cell><cell cols="2">4.303 ± 2.077</cell><cell>0.262 ± 0.038</cell><cell>0.492 ± 0.012</cell></row><row><cell>BNN</cell><cell cols="2">4.874 ± 2.850</cell><cell cols="2">0.240 ± 0.012 0.676 ± 0.008</cell></row><row><cell>TARNET</cell><cell cols="2">1.342 ± 0.597</cell><cell cols="2">0.234 ± 0.012 0.840 ± 0.006</cell></row><row><cell>CFR-M</cell><cell cols="2">1.202 ± 0.550</cell><cell cols="2">0.231 ± 0.009 0.840 ± 0.006</cell></row><row><cell>CFR-W</cell><cell cols="2">1.152 ± 0.527</cell><cell cols="2">0.225 ± 0.010 0.842 ± 0.005</cell></row><row><cell>GANITE</cell><cell cols="2">(2.4 ± 0.4)</cell><cell>(0.14 ± 0.01)</cell><cell>not reported</cell></row><row><cell>CEVAE</cell><cell cols="2">(2.6 ± 0.1)</cell><cell>0.26 ± 0.0</cell><cell>not reported</cell></row><row><cell>SITE-M</cell><cell cols="2">1.242 ± 0.153</cell><cell cols="2">0.218 ± 0.010 0.705 ± 0.006</cell></row><row><cell>SITE-W</cell><cell cols="2">1.459 ± 0.481</cell><cell>0.232</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian inference of individualized treatment effects using multi-task gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3424" to="3432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00906</idno>
		<title level="m">Uncertainty in the variational information bottleneck</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Informative subspace learning for counterfactual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonparametric tests for treatment effect heterogeneity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Crump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Mitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="405" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain markov process expectations for large time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R S</forename><surname>Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">iv. Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="212" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial balancing-based representation learning for causal effect inference with observational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duivesteijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13335</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>March</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal inference in public health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Public Health</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="75" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric modeling for causal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised deep kernel learning: Regression with unlabeled data by minimizing predictive variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5322" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>Balcan, M. F. and Weinberger, K. Q.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic gradient vb and the variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating the econometric evaluations of training programs with experimental data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="624" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching on balanced nonlinear representations for treatment effects estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="929" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matching via dimensionality reduction for estimation of treatment effects in digital marketing campaigns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3768" to="3774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6446" to="6456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02326</idno>
		<title level="m">Causal deep information bottleneck</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">469</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the information bottleneck theory of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Does matching overcome lalonde&apos;s critique of nonexperimental estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="305" to="353" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving generalization and stability of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thanh-Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 37th annual Allerton Conf. on Communication, Control, and Computing</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust tree-based causal inference for complex ad effectiveness analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation learning for treatment effect estimation from observational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2633" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GANITE: Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
