<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Entity and Relation Extraction with Set Prediction Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename><forename type="middle">♠</forename><surname>Yubo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">♥</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><forename type="middle">♥</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename><forename type="middle">♠</forename><surname>Xiangrong Zeng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Unisound Information Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
							<email>liushengping@unisound.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Unisound Information Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Entity and Relation Extraction with Set Prediction Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with crossentropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-theart methods. Training code and trained models will be available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>A relational triple consists of two entities connected by a semantic relation, which is in the form of <ref type="bibr">(subject, relation, object)</ref>. The extraction of relational triples from unstructured raw texts is a key technology for automatic knowledge graph construction, which has received growing interest in recent years.</p><p>There have been several studies addressing technical solutions for relational triple extraction. Early researches, such as <ref type="bibr" target="#b25">Zelenko, Aone, and Richardella (2003)</ref>; <ref type="bibr" target="#b0">Chan and Roth (2011)</ref>, employ a pipeline manner to extract both of entities and relations, where entities are recognized first and then the relation between the extracted entities is predicted. Such a pipeline approach ignores the relevance of entity identification and relation prediction <ref type="bibr" target="#b13">(Li and Ji 2014)</ref> and tends to suffer from the error propagation problem.</p><p>To model cross-task dependencies explicitly and prevent error propagation in the pipeline approach, subsequent studies propose joint entity and relation extraction. These stud-ies can be roughly categorized into three main paradigms. The first stream of work, such as <ref type="bibr" target="#b17">Miwa and Bansal (2016)</ref>; <ref type="bibr" target="#b9">Gupta, Schütze, and Andrassy (2016)</ref>; Zhang, Zhang, and <ref type="bibr">Fu (2017)</ref>, treats joint entity and relation extraction task as an end-to-end table filling problem. Although these methods represent entities and relations with shared parameters in a single model, they extract the entities and relations separately and produce redundant information <ref type="bibr">(Zheng et al. 2017)</ref>. The second stream of work, such as <ref type="bibr">Zheng et al. (2017)</ref>; <ref type="bibr" target="#b2">Dai et al. (2019)</ref>; <ref type="bibr" target="#b23">Wei et al. (2020)</ref>, transforms joint entity and relation extraction into sequence labeling. To do this, human experts need to design a complex tagging schema. The last stream of work, including <ref type="bibr">Zeng et al. (2018</ref><ref type="bibr">Zeng et al. ( , 2019</ref>; <ref type="bibr" target="#b19">Nayak and Ng (2020)</ref>; <ref type="bibr" target="#b27">Zeng, Zhang, and Liu (2020)</ref>, is driven by the sequence-to-sequence (seq2seq) model <ref type="bibr" target="#b21">(Sutskever, Vinyals, and Le 2014)</ref> to generate relational triples directly, which is a flexible framework to handle overlapping triples and does not require the substantial effort of human experts.</p><p>We follow the seq2seq based models for joint entity and relation extraction. Despite the success of existing seq2seq based models, they are still limited by the autoregressive decoder and the cross-entropy loss. The reasons are as follows: the relational triples contained in a sentence have no intrinsic order in essence. However, in order to adapt the autoregressive decoder, whose output is a sequence, the unordered target triples must be sorted in a certain order during the training phase. Meanwhile, cross-entropy is a permutationsensitive loss function, where a penalty is incurred for every triple that is predicted out of the position. Consequently, current seq2seq base models not only need to learn how to generate triples, but also are required to consider the extraction order of multiple triples.</p><p>In this work, we formulate the joint entity and relation extraction task as a set prediction problem, avoiding considering the order of multiple triples. In order to solve the set prediction problem, we propose an end-to-end network featured by transformers with non-autoregressive parallel decoding and bipartite matching loss. In detail, there are three parts in the proposed set prediction networks (SPN): a sentence encoder, a set generator, and a set based loss function. First of all, we adopt the BERT model <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> as the encoder to represent the sentence. Then, since an autoregressive decoder must generate items one by one in order, such a decoder is not suitable for generating unordered sets. In contrast, we leverage the transformer-based non-autoregressive decoder <ref type="bibr">(Gu et al. 2018)</ref> as the set generator, which can predict all triples at once and avoid sorting triples. Finally, in order to assign a predicted triple to a unique ground truth triple, we propose bipartite matching loss function inspired by the assigning problem in operation research <ref type="bibr" target="#b10">(Kuhn 1955;</ref><ref type="bibr" target="#b18">Munkres 1957;</ref><ref type="bibr" target="#b4">Edmonds and Karp 1972)</ref>. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed loss function is invariant to any permutation of predictions; thus it is suitable for evaluating the difference between ground truth set and prediction set.</p><p>In a nutshell, our main contributions are:</p><p>• We formulate the joint entity and relation extraction task as a set prediction problem.</p><p>• We combine non-autoregressive parallel decoding with bipartite matching loss function to solve this problem.</p><p>• Our proposed method yields state-of-the-art results on two benchmark datasets, and we perform various experiments to verify the effectiveness of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Relation Extraction</head><p>Relation extraction is a long-standing natural language process task of mining factual knowledge from free texts. When giving a sentence with annotated entities, this task degenerates into a simple task, namely relation classification. Some studies, such as <ref type="bibr" target="#b26">Zeng et al. (2014)</ref>; <ref type="bibr" target="#b24">Xu et al. (2015)</ref>, leveraged CNN or RNN to solve the relation classification task. However, these methods ignore the extraction of entities from sentences and could not truly extract relational facts. When giving a sentence without any annotated entities, researchers proposed several methods to extract entities and relations jointly. Existing studies on multiple relation extraction task can be divided into four paradigms: (1) Pipeline based methods, such as <ref type="bibr" target="#b25">Zelenko, Aone, and Richardella (2003)</ref>; <ref type="bibr" target="#b0">Chan and Roth (2011)</ref>, firstly recognize entities and then conduct relation classification; (2) Table filling based methods, like <ref type="bibr" target="#b17">Miwa and Bansal (2016)</ref>; <ref type="bibr" target="#b9">Gupta, Schütze, and Andrassy (2016);</ref><ref type="bibr">Zhang, Zhang, and Fu (2017)</ref>, represent entities and relations with shared parameters, but extract the entities and relations separately; (3) Tagging based methods, such as Zheng et al. <ref type="formula" target="#formula_0">(2017)</ref>; <ref type="bibr" target="#b2">Dai et al. (2019)</ref>; <ref type="bibr" target="#b23">Wei et al. (2020)</ref>, treat this task as a sequence labeling problem and need to design complex tagging schema; (4) Seq2seq based methods, like <ref type="bibr">Zeng et al. (2018</ref><ref type="bibr">Zeng et al. ( , 2019</ref>; <ref type="bibr" target="#b19">Nayak and Ng (2020)</ref>; <ref type="bibr" target="#b27">Zeng, Zhang, and Liu (2020)</ref>, apply seq2seq model to generate relational triples directly. Our work is in line with seq2seq based methods. In contrast with the previous studies, we reckon the triples in a sentence are in the form of a set instead of a sequence, and treat the joint entity and relation extraction as a set prediction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Autoregressive Model</head><p>Non-autoregressive models <ref type="bibr">(Gu et al. 2018;</ref><ref type="bibr" target="#b11">Lee, Mansimov, and Cho 2018;</ref><ref type="bibr" target="#b16">Ma et al. 2019</ref>) generate all the tokens of a target in parallel and can speed up inference. Non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation <ref type="bibr">(Gu et al. 2018)</ref> and automatic speech recognition <ref type="bibr" target="#b1">(Chen et al. 2019)</ref>. To the best of our knowledge, this is the first work to apply non-autoregressive models to information extraction. In this work, we resort to the nonautoregressive model to generate the set of triples in one shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The goal of joint entity and relation extraction is to identify all possible relational triples in a given sentence. Formally, given an raw sentence X, the conditional probability of the target triple set Y = {(s 1 , r 1 , o 1 ), ..., (s n , r n , o n )} is:</p><formula xml:id="formula_0">P (Y |X; θ) = p L (n|X) n i=1 p(Y i |X, Y j =i ; θ)<label>(1)</label></formula><p>where p L (n|X) model the size of the target triple set, and p(Y i |X, Y j =i ; θ) means that a target triple Y i is related not only to the given sentence X, but also to the other triples Y j =i . In this paper, this conditional probability is parameterized using set prediction networks (SPN), which are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Three key components of the proposed networks will be elaborated in the following section. Concretely, we first introduce the sentence encoder, which represents each token in a given sentence based on its bidirectional context. Then, we present how to use the non-autoregressive decoder to generate a set of triples in a single pass. Finally, we describe a set-based loss, dubbed as bipartite matching loss, which forces unique matching between predicted and ground truth triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Encoder</head><p>The goal of this component is to obtain the context-aware representation of each token in a sentence. Given the impressive performance of recent deep transformers <ref type="bibr" target="#b22">(Vaswani et al. 2017</ref>) trained on variants of language modeling, we utilize the BERT model <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> as the sentence encoder. The input sentence is segmented with tokens by the byte pair encoding <ref type="bibr" target="#b20">(Sennrich, Haddow, and Birch 2016)</ref>, and then fed into the BERT model. The output of the BERT model is the context-aware embedding of tokens, and is denoted as H e ∈ R l×d , where l is the sentence length (including [CLS] and [SEP], two special start and end markers), and d is the number of hidden units in the BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Autoregressive Decoder for Triple Set Generation</head><p>We regard joint entity and relation extraction as a set prediction problem and use the transformer-based nonautoregressive decoder <ref type="bibr">(Gu et al. 2018)</ref> to directly generate the triple set. Previous studies <ref type="bibr">(Zeng et al. 2018</ref><ref type="bibr">(Zeng et al. , 2019</ref><ref type="bibr" target="#b19">Nayak and Ng 2020;</ref><ref type="bibr" target="#b27">Zeng, Zhang, and Liu 2020)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bipartite Matching Loss</head><formula xml:id="formula_1">P (Y |X; θ) = n i=1 p(Y i |X, Y j&lt;i ; θ)<label>(2)</label></formula><p>In contrast, we use the non-autoregressive decoder to direct model the Equation 1. Compared with previous seq2seq based method, the non-autoregressive decoder can not only avoid learning the extraction order of multiple triples, but also generate triples based on bidirectional information, not just left-to-right information.</p><p>Input. Before decoding starts, the decoder need to know the size of the target set, in other words, p L (n|X) in Equation 1 is required to be modeled at first. In this work, we simplify the p L (n|X) into a constant by requiring the nonautoregressive decoder to generate a fixed-size set of m predictions for each sentence, where m is set to be significantly larger than the typical number of triples in a sentence. Instead of copying tokens from the encoder side <ref type="bibr">(Gu et al. 2018)</ref>, the input of the decoder is initialized by m learnable embeddings that we refer to as triple queries. Note that all sentences share the same triple queries.</p><p>Architecture. The non-autoregressive decoder is composed of a stack of N identical transformer layers. In each transformer layer, there are multi-head self attention mechanism to model the relationship between triples, and multihead inter attention mechanism to fuse the information of the given sentence. Notably, compared with autoregressive decoder, the non-autoregressive decoder does not have the constraint of an autoregressive factorization of the output, so there is no need to prevent earlier decoding steps from accessing information from later steps. Thus, there is no casual mask used in the multi-head self attention mechanism. Instead, we use the unmasked self-attention.</p><p>The m triple queries are transformed into m output embeddings by the non-autoregressive decoder, which are denoted as H d ∈ R m×d . The output embeddings H d are then independently decoded into relation types and entities by feed forward networks (FFN), resulting m final predicted triples. Concretely, given an output embedding h d ∈ R d in H d , the predicted relation type is obtained by:</p><formula xml:id="formula_2">p r = softmax(W r h d )<label>(3)</label></formula><p>and the predicted entities (subject and object) are decoded by separately predicting the starting and ending indices with four l-class classifiers:</p><formula xml:id="formula_3">p s−start = softmax(v T 1 tanh(W 1 h d + W 2 H e )) (4) p s−end = softmax(v T 2 tanh(W 3 h d + W 4 H e )) (5) p o−start = softmax(v T 3 tanh(W 5 h d + W 6 H e )) (6) p o−end = softmax(v T 4 tanh(W 7 h d + W 8 H e )) (7) where W r ∈ R t×d , {W i ∈ R d×d } 8 i=1 and {v i ∈ R d } 4 i=1</formula><p>are learnable parameters, t is the total number of relation types (including a special relation type ∅ to indicate no triple), l is the sentence length, and H e is the output of the BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bipartite Matching Loss</head><p>The main difficulty of training is to score the predicted triples with respect to the ground truths. It is not proper to apply cross-entropy loss function to measure the difference between two sets, since cross-entropy loss is sensitive to the permutation of the predictions. Inspired by the assigning problem in operation research <ref type="bibr" target="#b10">(Kuhn 1955)</ref>, we propose a set prediction loss that can produce an optimal bipartite matching between predicted and ground truth triples.</p><p>Notations. Let us denote by Y = {Y i } n i=1 the set of ground truth triples, andŶ = {Ŷ i } m i=1 the set of m predicted triples, where m is larger than n. We consider Y also as a set of size m padded with ∅ (no triple). Each element i of the ground truth set can be seen as a</p><formula xml:id="formula_4">Y i = (r i , s start i , s end i , o start i , o end i ), where r i is the target rela- tion type (which may be ∅) and s start i , s end i , o start i , o end i</formula><p>are the starting or ending indices of subject s or object o. Each element i of the set of predicted triples is denoted aŝ</p><formula xml:id="formula_5">Y i = (p r i , p s−start i , p s−end i , p o−start i , p o−end i ), which is calculated based on Equation 3-7.</formula><p>Loss. The process of computing bipartite matching loss is divided into two steps: finding an optimal matching and computing the loss function.</p><p>To find an optimal matching between the set of ground truth triples Y and the set of predicted triplesŶ, we search for a permutation of elements π with the lowest cost:</p><formula xml:id="formula_6">π = arg min π∈Π(m) m i=1 C match (Y i ,Ŷ π(i) ) (8) where Π(m) is the space of all m-length permutations. C match (Y i ,Ŷ π(i) )</formula><p>is a pair-wise matching cost between the ground truth Y i and the predicted triple with index π(i). By taking into account both the prediction of relation type and the predictions of entity spans, we define</p><formula xml:id="formula_7">C match (Y i ,Ŷ π(i) ) as: C match (Y i ,Ŷ π(i) ) = −1 {ri =∅} [p r π(i) (r i ) + p s−start π(i) (s start i ) + p s−end π(i) (s end i ) + p o−start π(i) (o start i ) + p o−end π(i) (o end i )]<label>(9)</label></formula><p>This optimal assignment π is computed in polynomial time (O(m 3 )) via the Hungarian algorithm 1 . In detail, we can view the set of ground truth Y as a set of people in the assignment problem, the set of predicted triplesŶ as a set of jobs. The cost of assigning Y i (the people i) withŶ j (the job j) is defined as C match (Y i ,Ŷ j ). The optimal matching with the minimum total cost is easy to be computed via the classical Hungarian algorithm.</p><p>The second step is to compute the loss function for all pairs matched in the previous step. We define the loss as:</p><formula xml:id="formula_8">L(Y,Ŷ) = m i=1 {− log p r π (i) (r i ) + 1 {ri =∅} [− log p s−start π (i) (s start i ) − log p s−end π (i) (s end i ) − log p o−start π (i) (o start i ) − log p o−end π (i) (o end i )]}<label>(10)</label></formula><p>where π is the optimal assignment computed in the first step <ref type="formula" target="#formula_0">(Equation 11</ref>).</p><p>1 https://en.wikipedia.org/wiki/Hungarian algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we carry out an extensive set of experiments with the aim of answering the following research questions:</p><p>• RQ1: What is the overall performance of the proposed set prediction networks (SPN) in joint entity and relation extraction? • RQ2: How does each design of our model matter?</p><p>• RQ3: What is the performance of the proposed model in sentences annotated with different numbers of triples? • RQ4: How does our proposed model adapt to different overlapping patterns? In the remainder of this section, we describe the datasets, experimental setting, and all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We evaluate the proposed method on two widely used joint entity and relation extraction datasets: New York Times (NYT) <ref type="bibr" target="#b20">(Riedel, Yao, and McCallum 2010)</ref> and WebNLG <ref type="bibr" target="#b6">(Gardent et al. 2017)</ref> 2 . The statistics of these datasets are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>NYT. This dataset is produced by the distantly supervised method, which automatically aligns Freebase with 1987-2007 New York Times news articles. There are 24 predefined relation types in total. Following previous studies <ref type="bibr">(Zheng et al. 2017;</ref><ref type="bibr">Zeng et al. 2018)</ref>, we ignore the noise in this dataset and treat it as a supervised dataset. Since there are many versions of the NYT dataset, we adopted the preprocessed dataset used in <ref type="bibr">Zeng et al. (2018)</ref>, which is publicly available.</p><p>WebNLG. This data is originally created for Natural Language Generation (NLG) task. In this dataset, an instance includes a set of triples and several standard sentences written by humans. Every standard sentence contains all triples of this instance. There are 246 predefined relation types in this dataset.</p><p>Overlap Between Triples. According to different overlapping patterns of triples, sentences are split into three categories <ref type="bibr">(Zeng et al. 2018)</ref>: Normal, Entity Pair Overlap (EPO) and Single Entity Overlap (SEO). A sentence belongs to Normal class if none of its triples have overlapped entities. A sentence belongs to EPO class if some of its triples have overlapped entity pairs. And a sentence belongs to SEO class if some of its triples have an overlapped entity and these triples don't have overlapped entity pair. Note that a sentence can belongs to both EPO class and SEO class.   <ref type="bibr">(Zeng et al. 2018)</ref> 61.0 56.6 58.7 ---GraphRel-1p <ref type="bibr" target="#b5">(Fu, Li, and Ma 2019)</ref> 62.9 57.3 60.0 ---GraphRel-2p <ref type="bibr" target="#b5">(Fu, Li, and Ma 2019)</ref> 63.9 60.0 61.9 ---CopyRRL <ref type="bibr">(Zeng et al. 2019)</ref> 77.9 67.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We adopt standard micro-F1 to evaluate the performance. A triple is regarded as correct if the relation type and the two corresponding entities are all correct. Notably, there are two ways to judge whether the extracted entities are correct. One is Partial Matching, where the extracted entities are regarded as correct if the predictions of subject and object are the same as the head words of the ground truth. Since the head word of entity is not annotated in most situations, the last word of entity is treated as the head word. The other is Exact Matching. In this way, only if the predictions of subject and object are identical to the ground truth, the extracted entities are treated as correct. Note that the training data under partial matching is different from the one under exact matching. Under partial matching, only head words of entities are annotated, while the whole entities are annotated under exact matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>To conduct a fair comparison, we use the cased base version of BERT in our experiments, which contains 110M parameters. The initial learning rate of BERT is set to 0.00001, and the initial learning of the non-autoregressive decoder is set to 0.00002. The number of stacked bidirectional transformer blocks in non-autoregressive decoder is set to 3. We use the dropout strategy to mitigate overfitting, the dropout rate is set to 0.1. Meanwhile, We apply gradient clipping to prevent exploding gradients. The set prediction networks are trained by minimizing the loss function defined in Equation 10 through stochastic gradient descent over shuffled minibatch with the AdamW update rule <ref type="bibr" target="#b15">(Loshchilov and Hutter 2017)</ref>. All experiments are conducted with an NVIDIA GeForce RTX 2080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>The following state-of-the-art (SoTA) models have been compared in the experiments.</p><p>• NovelTagging (Zheng et al. 2017) introduces a novel tagging scheme that transforms the joint entity and relation extraction task into a sequence labeling problem. • CopyRE <ref type="bibr">(Zeng et al. 2018)</ref> is a seq2seq based model with copy mechanism, which can effectively extract overlapping triples. • GraphRel <ref type="bibr" target="#b5">(Fu, Li, and Ma 2019)</ref> is a two phases model based on graph convolutional networks (GCN), where a relation-weighted GCN is utilized to model the interaction between entities and relations. • CopyRRL <ref type="bibr">(Zeng et al. 2019</ref>) combines the reinforcement learning with a seq2seq model to automatically learn the extraction order of triples. In such a way, the interactions among triples can be considered. • CopyMTL <ref type="bibr" target="#b27">(Zeng, Zhang, and Liu 2020</ref>) is a multi-task learning framework, where conditional random field is used to identify entities, and a seq2seq model is adopted to extract relational triples. • WDec (Nayak and Ng 2020) fuses a seq2seq model with a new representation scheme, which enables the decoder to generate one word at a and can handle full entity names of different length and overlapping entities.  • PNDec (Nayak and Ng 2020) is a modification of seq2seq model. Pointer networks are used in the decoding framework to identify the entities in the sentence using their start and end locations.</p><p>• Attention as Relation ) contains a conditional random field based entity extraction module and a supervised multi-head self attention based relation detection module.</p><p>• CasRel <ref type="bibr" target="#b23">(Wei et al. 2020</ref>) is a novel cascade binary tagging framework, where all possible subjects are identified in the first stage, and then for each identified subject, all possible relations and the corresponding objects are simultaneously identified by a relation specific tagger. This work achieves the SoTA results.</p><p>Note that <ref type="bibr" target="#b12">Li and Tian (2020)</ref> claimed that they have achieved the SoTA results in WebNLG, but we find that the model they proposed is only designed for relation classification. Therefore, we do not compare our method with REDN (Li and Tian 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>To start, we address the research question RQ1. <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table" target="#tab_5">Table 3</ref> show the results of our model against baselines on two benchmark datasets. Overall, our proposed model significantly outperforms baselines on these datasets.</p><p>In NYT dataset, Our proposed model outperforms all the baselines in both partial matching and exact matching and achieves 2.9% and 3.0% improvements in F1 score respectively over the current SoTA method <ref type="bibr" target="#b23">(Wei et al. 2020)</ref>. Combined with <ref type="table" target="#tab_7">Table 4</ref>, we find that our proposed model outperforms the SoTA model <ref type="bibr" target="#b23">(Wei et al. 2020)</ref> in both entity pair extraction and relation type extraction, demonstrating the effectiveness of our proposed model. We also find that there is an obvious gap between the F1 score on relation type extraction and entity pair extraction, but a trivial gap between entity pair extraction and overall extraction. It reveals that entity pair extraction is the main bottleneck of joint entity and relation extraction in NYT dataset.</p><p>In WebNLG dataset, since there is no unified version of data under exact matching, we only compare our proposed networks with baselines under partial matching metric. In general, our proposed model achieves the best F1 score under partial matching, which is 93.4%. There is 1.6% improvement compared with the result of the SoTA model <ref type="bibr" target="#b23">(Wei et al. 2020</ref>   <ref type="table" target="#tab_7">Table  4</ref> shows that our proposed model exhibits balanced results of precision and recall in both entity pair extraction and relation type extraction, while there is an obvious gap between precision and recall in the results of CasRel <ref type="bibr" target="#b23">(Wei et al. 2020)</ref>.</p><p>We conjecture that this is largely due to that the number of triple queries is set to be significantly larger than the typical number of triples in a sentence, which ensures that enough triples can be recalled. In addition, we observe that the results of entity pair extraction and relation type extraction are much higher than that of joint entity and relation extraction, which means that the accurate combination of entity pair extraction and relation type extraction is the key to improve the joint entity and relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Next, we turn to the research question RQ2. We conduct ablation studies to investigate the importance of the nonautoregressive decoder and the bipartite matching loss. To evaluate the importance of the non-autoregressive decoder, we change the number of decoder layers. To evaluate the importance of bipartite matching loss, we replacing bipartite matching loss with cross-entropy loss. <ref type="table" target="#tab_9">Table 5</ref> shows the results. We find that increasing the number of layers of the non-autoregressive decoder can achieve better results. When the number of decoder layers is set to 1, 2, and 3, the best results are 91.4%, 92.0% and 92.15%, respectively. We conjecture that this is largely due to that with the deepening of the non-autoregressive decoder layers, more multi-head self attention modules allow for better modeling of relationships between triple queries, and more multi-head inter attention modules allow for more Models NYT WebNLG N=1 N=2 N=3 N=4 N≥5 N=1 N=2 N=3 N=4 N≥5 CopyRE-One <ref type="bibr">(Zeng et al. 2018)</ref> 66.6 52.6 49.7 48.7 20.3 65.2 33.0 22.2 14.2 13.2 CopyRE-Mul <ref type="bibr">(Zeng et al. 2018)</ref> 67.1 58.6 52.0 53.6 30.0 59.2 42.5 31.7 24.2 30.0 GraphRel-1p <ref type="bibr">(Fu, Li, and Ma 2019) 69.1 59.5 54.4 53.9 37.5 63.8 46.3 34.7 30.8 29.4</ref> GraphRel-2p <ref type="bibr" target="#b5">(Fu, Li, and Ma 2019)</ref>   <ref type="table">Table 6</ref>: Partial matching F1 score of conducting extraction in sentences that contains different numbers of triples. We divide the sentences of the test sets into 5 sub-classes. Each class contains sentences that have 1,2,3,4 or ≥5 triples.</p><p>complete integration of sentence information into triple queries. Compared our proposed bipartite matching loss with widely used cross-entropy loss, we find that there is 8.9% improvement, which indicates bipartite matching loss is very suitable for joint entity and relation extraction. We hypothesize that in order to adopt to cross-entropy loss, the model is required to consider the generative order of triples, which places an unnecessary burden on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Results on Sentences with Different Number of Triples</head><p>In this section, we answer the research question RQ3. To do this, we compare the models' ability of extracting relational facts from sentences annotated with different numbers of triples. We divide the sentences in test sets into 5 sub-classes. Each class contains sentences that have 1, 2, 3, 4 or ≥5 triples. The results are shown in <ref type="table">Table 6</ref>. In general, our proposed model achieves the best results in all sub-classes. Besides, we observe that though the our proposed model gains considerable improvements on all five sub-classes compared to the SoTA model <ref type="bibr" target="#b23">(Wei et al. 2020)</ref>, the greatest improvement of F1 score on the two datasets both come from the most difficult sub-class (N≥5), in particular, there is 6.9% improvement in the NYT data. Such results indicate that our proposed model is more suitable for complicated scenarios than the current SoTA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Results on Different Overlapping Patterns</head><p>Finally, to address the research question RQ4, we conduct further experiments on NYT dataset to verify the ability of our proposed model in handling the overlapping problem. <ref type="figure" target="#fig_1">Figure 2</ref> shows the results of our proposed model and baseline models in Normal, Single Entity Overlap (SEO) and Entity Pair Overlap (EPO) classes.</p><p>Overall, our proposed model performs better than all baseline models in all three classes. In details, there are 3.5%, 2.6% and 2.1% improvements compared with the SoTA model <ref type="bibr" target="#b23">(Wei et al. 2020)</ref> in Normal, SEO and EPO classes, respectively. Such results show that our proposed model is very effective in handling the overlapping problem, which is widely existed in joint entity and relation extraction. In addition, We also observe that the performance of all seq2seq based models, such as CopyRE (Zeng et al.  <ref type="bibr">(Zeng et al. 2019</ref>), on Normal, EPO and SEO presents a decreasing trend, which indicates that the Normal class presents a relatively easiest pattern while EPO and SEO classes are the relatively harder ones for these seq2seq based models to extract. In contrast, our proposed model attains consistently strong performance over all three overlapping patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2018) and CopyRRL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we introduce set prediction networks for joint entity and relation extraction. Compared with previous seq2seq based models, We formulate the joint entity and relation extraction task as a set prediction problem. In such a way, the extraction model will be relieved of predicting the extraction order of multiple triples. To solve the set prediction problem, We combine non-autoregressive parallel decoding with bipartite matching loss function. We conduct extensive experiments on two widely used datasets to validate the effectiveness of the proposed set prediction networks. Experimental results show that our proposed networks outperforms state-of-the-art baselines over different scenarios. This challenging task is far from being solved. We find that relation types exhibit an imbalanced or long-tailed distribution in NYT dataset and WebNLG dataset. Our future work will concentrate on how to combine cost-sensitive learning with the proposed set prediction networks. <ref type="bibr">Zeng, X.;</ref><ref type="bibr">He, S.;</ref><ref type="bibr" target="#b26">Liu, K.;</ref><ref type="bibr">and Zhao, J. 2019</ref>. Learning the extraction order of multiple relational facts in a sentence with reinforcement learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.</p><p>Zeng, X.; Zeng, D.; He, S.; <ref type="bibr" target="#b26">Liu, K.;</ref><ref type="bibr">and Zhao, J. 2018</ref>. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hungarian Algorithm</head><p>The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem in polynomial time. The Hungarian algorithm consists of the four steps below. The first two steps are executed once, while Steps 3 and 4 are repeated until an optimal assignment is found. The input of the algorithm is an m × m by m square matrix, called cost matrix.</p><p>Step 1: Subtract row minima.</p><p>For each row, find the lowest element and subtract it from each element in that row.</p><p>Step 2: Subtract column minima.</p><p>Similarly, for each column, find the lowest element and subtract it from each element in that column.</p><p>Step 3: Cover all zeros with a minimum number of lines.</p><p>Cover all zeros in the resulting matrix using a minimum number of horizontal and vertical lines. If m lines are required, an optimal assignment exists among the zeros. The algorithm stops.</p><p>If less than m lines are required, continue with Step 4.</p><p>Step 4: Create additional zeros Find the smallest element (call it k) that is not covered by a line in Step 3. Subtract k from all uncovered elements, and add k to all elements that are covered twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Example of Bipartite Matching Loss</head><p>We use an example to illustrate how bipartite matching loss function works. In this example, we assume that there are only three relation types that we are interested in. These three relation types are leader name, located in and capital Of. As shown in this paper, a special relation type ∅ is also required. Furthermore, we assume that the non-autoregressive decoder generate 3 triples for each sentence, in other words, m is set to 3. The sentence, the ground-truths and the predictions are presented as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>Aarhus airport serves the city of Aarhus , which is led by Jacob Bundsgaard .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truths</head><p>(Aarhus, leader name, Jacob Bundsgaard) (Aarhus Airport, located in, Aarhus) ∅ (padding a no triple to match m)</p><p>The indices of Aarhus Airport, Aarbus and Jacob Bundsgaard are (0, 1), (6, 6) and (12, 13) respectively. We also convert each relations type to a unique integer. In detail, leader name, located in, capital Of and ∅ are converted to 0, 1, 2, 3 respectively. In such a way, ground-truths are represented as:</p><formula xml:id="formula_9">Y 0 = {0, 6, 6, 12, 13} Y 1 = {1, 0, 1, 6, 6} Y 2 = {3}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions</head><p>We assume that the outputs of the set prediction networks are as follow:</p><formula xml:id="formula_10">Y 0 = { p r 0</formula><p>: (0.1, 0.3, 0.4, 0.2) p s−start 0 : (0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) p s−end 0 : (0.2, 0.8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) p o−start 0 : (0.1, 0.1, 0, 0, 0, 0, 0.7, 0, 0, 0, 0, 0, 0.1, 0, 0) p o−end 0 : (0, 0.1, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0, 0.1, 0.2, 0) } Y 1 = { p r 1 : (0.5, 0.25, 0.15, 0.1) p s−start 1 : (0.1, 0, 0, 0, 0, 0, 0.8, 0, 0, 0, 0, 0, 0.1, 0, 0) p s−end 1 : (0.2, 0.3, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0) p o−start 1 : (0.2, 0.1, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0.5, 0, 0) p o−end 1 : (0, 0.4, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0, 0, 0.3, 0) } Y 2 = { p r 2 : (0.1, 0.3, 0.4, 0.2) p s−start 2 : (0.4, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0.1, 0, 0) p s−end 2 : (0.1, 0.4, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0) p o−start 2 : (0.3, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7, 0, 0) p o−end 2 : (0, 0.2, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0, 0.4, 0) }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bipartite Matching Loss</head><p>The process of computing bipartite matching loss is divided into two steps: finding an optimal matching and computing the loss function.</p><p>(1). Finding an optimal matching We search for a permutation of elements π with the lowest cost to find an optimal matching between the set of ground truth triples Y and the set of predicted triplesŶ:</p><formula xml:id="formula_11">π = arg min π∈Π(m) m i=1 C match (Y i ,Ŷ π(i) )<label>(11)</label></formula><p>To do this, we need to determine the cost matrix, which is the input of Hungarian algorithm. The (i, j)-entry of cost matrix is C match (Y i ,Ŷ j ), where C match (Y i ,Ŷ j ) is define as: The cost matrix is denoted as follows:</p><formula xml:id="formula_12">C match (Y i ,Ŷ j ) = −1 {ri =∅} [p</formula><p>C =Ŷ 0Ŷ1Ŷ2 Y 0 -0.4 -2.6 -2.1 Y 1 -3.3 -1.15 -1.5 Y 2 0 0 0 Via Hugarian algorithm, the optimal assignment π corresponding to the cost matrix is ground − truths :[0, 1, 2] −→ predictions : <ref type="bibr">[1,</ref><ref type="bibr">0,</ref><ref type="bibr">2]</ref> which has the minimum total cost (-5.9 = (-2.6)+(-3.3)+0).</p><p>(2). Computing the loss function based on the optimal assignment.</p><p>Recall that the loss is defined as: </p><formula xml:id="formula_13">L</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The main architecture of set prediction networks. The set prediction networks predict the final set of triples in parallel by combining a BERT encoder with a non-autoregressive decoder. In the training phrase, bipartite matching uniquely assigns predictions with ground truths to provide accurate training signals. a way, the conditional probability of the target triple set in Equation 1 is modified into:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>F1 score of extracting relational triples from sentences with different overlapping pattern in NYT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Zhang, M.; Zhang, Y.; and Fu, G. 2017. End-to-end neural relation extraction with global optimization. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Zheng, S.; Wang, F.; Bao, H.; Hao, Y.; Zhou, P.; and Xu, B. 2017. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Each entry of cost matrix is computed as follows:C match (Y 0 ,Ŷ 0 ) = −[0.1 + 0 + 0 + 0.1 + 0.2] = −0.4 C match (Y 0 ,Ŷ 1 ) = −[0.5 + 0.8 + 0.5 + 0.5 + 0.3] = −2.6 C match (Y 0 ,Ŷ 2 ) =−[0.1 + 0.5 + 0.5 + 0.7 + 0.3] = −2.1 C match (Y 1 ,Ŷ 0 ) = −[0.3 + 0.9 + 0.8 + 0.7 + 0.6] = −3.3 C match (Y 1 ,Ŷ 1 ) = −[0.25 + 0.1 + 0.3 + 0.2 + 0.3] = −1.15 C match (Y 1 ,Ŷ 2 ) = −[0.3 + 0.4 + 0.4 + 0 + 0.4] = −1.5C match (Y 2 ,Ŷ 0 ) = 0 C match (Y 2 ,Ŷ 1 ) = 0 C match (Y 2 ,Ŷ 2 ) = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>Based on the optimal assignment π , the value of loss function is:L(Y,Ŷ) ={− log p r 1 (0) − log p s−start 1 log p r 2 (3)} ={− log(0.5) − log(0.8) − log(0.5) − log(0.5) − log(0.3)} + {− log(0.3) − log(0.9) − log(0.8) − log(0.7) − log(0.6)} + {− log(0.2)} =7.52</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The statistics of NYT and WebNLG.</figDesc><table><row><cell>Models</cell><cell cols="2">Partial Matching Precision Recall</cell><cell>F1</cell><cell cols="2">Exact Matching Precision Recall</cell><cell>F1</cell></row><row><cell>NovelTagging (Zheng et al. 2017)</cell><cell>62.4</cell><cell>31.7</cell><cell>42</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CopyRE-One (Zeng et al. 2018)</cell><cell>59.4</cell><cell>53.1</cell><cell>56.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CopyRE-Mul</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Precision (%) , Recall (%) and F1 score (%) of our proposed SPN and state-of-the-art mehtods on the NYT test set. † indicates that the result is reproduced by us.</figDesc><table><row><cell>Models</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>NovelTagging</cell><cell>52.5</cell><cell>19.3</cell><cell>28.3</cell></row><row><cell>CopyRE-One</cell><cell>32.2</cell><cell>28.9</cell><cell>30.5</cell></row><row><cell>CopyRE-Mul</cell><cell>37.7</cell><cell>36.4</cell><cell>37.1</cell></row><row><cell>GraphRel-1p</cell><cell>42.3</cell><cell>39.2</cell><cell>40.7</cell></row><row><cell>GraphRel-2p</cell><cell>44.7</cell><cell>41.1</cell><cell>42.9</cell></row><row><cell>CopyRRL</cell><cell>63.3</cell><cell>59.9</cell><cell>61.6</cell></row><row><cell>CasRel</cell><cell>93.4</cell><cell>90.1</cell><cell>91.8</cell></row><row><cell>SPN (Ours)</cell><cell>93.1</cell><cell>93.6</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Precision (%) , Recall (%) and F1 score (%) of our proposed SPN and state-of-the-art mehtods on the WebNLG test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on extracting elements of relational triples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>), which is 91.8%. Compared with the SoTA</figDesc><table><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Bipartite Matching Loss, Num of Decoder Layers = 3</cell><cell>93.3</cell><cell>91.7</cell><cell>92.5</cell></row><row><cell>Bipartite Matching Loss, Num of Decoder Layers = 2</cell><cell>92.7</cell><cell>91.3</cell><cell>92.0</cell></row><row><cell>Bipartite Matching Loss, Num of Decoder Layers = 1</cell><cell>91.9</cell><cell>90.9</cell><cell>91.4</cell></row><row><cell>Cross-Entropy Loss, Num of Decoder Layers = 3</cell><cell>87.2</cell><cell>80.9</cell><cell>84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results of ablation studies on NYT dataset.</figDesc><table /><note>model, We find our model to be more balanced in terms of precision and recall. A further analysis combined with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>71.0 61.5 57.4 55.1 41.1 66.0 48.3 37.0 32.1 32.1 CopyRRL (Zeng et al. 2019) 71.7 72.6 72.5 77.9 45.9 63.4 62.2 64.4 57.2 55.7</figDesc><table><row><cell>CasRel (Wei et al. 2020)</cell><cell>88.2 90.3 91.9 94.2 83.7 89.3 90.8 94.2 92.4 90.9</cell></row><row><cell>SPN (Ours)</cell><cell>90.9 93.4 94.2 95.5 90.6 89.5 91.3 96.4 94.7 93.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These datasets are available at https://github.com/ xiangrongzeng/copy re</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting syntacticosemantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04908</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Non-Autoregressive Transformer Automatic Speech Recognition. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and overlapping relations using position-attentive sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theoretical improvements in algorithmic efficiency for network flow problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="248" to="264" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Creating training corpora for nlg micro-planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Non-Autoregressive</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Downstream Model Design of Pre-trained Language Model for Relation Extraction Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03786</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention as Relation: Learning Supervised Multihead Self-Attention for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>The Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
	<note>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
