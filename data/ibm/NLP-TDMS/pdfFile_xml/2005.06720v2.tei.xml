<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Streaming keyword spotting on mobile devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
							<email>rybakov@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Kononenko</surname></persName>
							<email>natashaknk@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
							<email>sniranjan@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirkó</forename><surname>Visontai</surname></persName>
							<email>mirkov@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Speech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
							<email>laurenzo@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Streaming keyword spotting on mobile devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>keyword spotting</term>
					<term>on-device inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we explore the latency and accuracy of keyword spotting (KWS) models in streaming and non-streaming modes on mobile phones. NN model conversion from non-streaming mode (model receives the whole input sequence and then returns the classification result) to streaming mode (model receives portion of the input sequence and classifies it incrementally) may require manual model rewriting. We address this by designing a Tensorflow/Keras based library which allows automatic conversion of non-streaming models to streaming ones with minimum effort. With this library we benchmark multiple KWS models in both streaming and non-streaming modes on mobile phones and demonstrate different tradeoffs between latency and accuracy. We also explore novel KWS models with multi-head attention which reduce the classification error over the state-of-art by 10% on Google speech commands data sets V2. The streaming library with all experiments is open-sourced. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Research and development of neural networks has many steps: data collection, model design and training, model latency or memory footprint optimization, model conversion to inference mode and execution of the model on different hardware, including mobile devices. In this work we are focused on the last three steps: model optimization, model conversion to inference mode and running it on mobile devices. A common method of model optimization is quantization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> allowing to reduce both model size and latency. It can be applied to many problems, including computer vision <ref type="bibr" target="#b0">[1]</ref> and speech recognition <ref type="bibr" target="#b2">[3]</ref>. Another optimization approach is the transformation of the NN model (which was used during training) into another NN streaming model which can be more efficient for inference/prediction mode <ref type="bibr" target="#b3">[4]</ref>. In several applications, such as image classification, model representation in the training and inference modes are the same, while in others, such as sequence classification problems (for example, KWS), it can be different. In a KWS application we do not know when the keyword starts or ends, so we need to process every audio packet and <ref type="bibr" target="#b0">1</ref> Preprint. Submitted to INTERSPEECH. return the classification results in real time every 20ms (for example) -it is called streaming inference. It will not be efficient to use a non-streaming model representation <ref type="figure" target="#fig_1">(Fig 1a)</ref> in streaming inference mode because the same convolution will be recomputed on the input window multiple times. A standard approach to optimize latency in this case is to transform the non-streaming model <ref type="figure" target="#fig_1">(Fig 1 a)</ref> to a streaming one <ref type="bibr" target="#b3">[4]</ref>. The resulting model will receive input samples with dimensions 3x1 incrementally process it (keeping the previously computed convolutions in a buffer to avoid unnecessary computations) and return the classification results in a streaming fashion, as shown on <ref type="figure" target="#fig_1">Fig 1b,</ref> c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Model streaming example</head><p>There are two options of implementing streaming inference: with internal state <ref type="figure" target="#fig_1">(Fig 1b)</ref> and with external state <ref type="figure" target="#fig_1">(Fig 1c)</ref>. A model with internal state receives a new input sample with dimensions 3x1 (grey box 3x1 on <ref type="figure" target="#fig_1">Fig 1b)</ref>, appends it to the ring buffer State1i, and at the same time removes the oldest sample (marked by blue with dashed lines on State1i) from State1i. This way, State1i always has a shape of 3x3. In the next step State1i is used by convolution 3x3 (on <ref type="figure" target="#fig_1">Fig 1b)</ref>. Then conv output (1x1 grey box) is concatenated with buffer State2i. At the same time we remove the oldest sample (marked by blue with dashed lines on State2i) from State2i. In the end, State2i is fed into a dense layer. As a result, the convolution will be computed on the new input data only and all previous computations will be buffered, as shown on <ref type="figure" target="#fig_1">Fig 1 b</ref>. In this case states are internal variables of the model which have to be updated with every prediction. The same approach is shown on <ref type="figure" target="#fig_1">Fig 1c,</ref> with one difference: states State1e and State2e are inputs and outputs of the model. In this case model does not keep any internal states and developers will have to feed them into neural network as additional inputs and then in addition to classification results receive the updated states and feed them back on the next prediction cycle.</p><p>In the above example, model representation in training and inference models can be different, so developers have to reimplement the NN model in streaming mode (for model latency optimization). In this work we would like to automate it, so that developers could write a NN model once, train it and then automatically convert it to streaming mode. For this purpose we designed a Keras streaming wrapper layer, described in section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">About speech frontend and modeling pipeline</head><p>KWS NN models use a speech feature extractor based on MFCC <ref type="bibr" target="#b5">[6]</ref>. Our implementation of speech feature extractor supports both FFT and DFT. If DFT is selected, then it will increase model size, because of DFT weights. We implemented all the components of the speech feature extractor in Keras layers, so that they will be part of the Keras NN model and will be automatically converted to TFLite <ref type="bibr">[7]</ref>. Since the speech feature extractor is a part of the model it will be easier to deploy it on a mobile device. The overall modeling pipeline (with given data) will have several steps: </p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model streaming</head><p>We designed a streaming Keras layer wrapper which allows automatic conversion of Keras models to streaming inference (with internal or external states, <ref type="figure" target="#fig_1">Fig 1b,</ref>  Now we can train this model (with no impact on training time) and convert it to streaming inference mode automatically. Stream wrapper creates states and manages in inference mode. In addition, the Stream wrapper needs to know the effective time filter size which is different for different layers, that is why inside of the Stream wrapper we have different logic for extracting the effective time filter size for a particular layer. We designed the Stream wrapper with several requirements. First, it shouldn't impact the original model training or default non-streaming inference and will be used only for streaming inference. In training mode it uses the cell as it is, so the training time will be the same as without the Stream wrapper. Next, we would like to support cells with internal and external states as it is shown in <ref type="figure" target="#fig_1">Fig 1b (</ref>with internal state) and <ref type="figure" target="#fig_1">Fig 1c (</ref>with external state). So that we can use different inference engines and compare them with each other.</p><p>RNN layers also require states during streaming inference, so we build streaming-aware RNN layers with streaming function. It behaves as standard RNN during training, but after model conversion to streaming inference it will only call the RNN cell (with internal or external state defined by the user).</p><p>Automatic conversion to streaming inference mode has several steps: 1) set input layer feature size equal one frame; 2) traverse Keras NN representation and insert ring buffer for layers which have to be streamed or call streaming function in streaming-aware layers such as RNN. This approach is not specific to KWS models and can be used in other applications.</p><p>Current version of Stream wrapper does not support striding nor pooling more than 1 in the time dimension, but it can be implemented in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model architectures</head><p>The standard end to end model architecture applied for KWS <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b11">14]</ref> consists of a speech feature extractor(optional) and a neural network based classifier. An input audio signal of length 1sec is framed into overlapping frames of length 40ms with an overlap of 20ms as in <ref type="bibr" target="#b4">[5]</ref>. Each frame is fed into a speech feature extractor which is based on MFCC <ref type="bibr" target="#b5">[6]</ref>. The extracted features are classified by a neural network which produces the probabilities of the output classes. We use cross entropy loss function with Adam optimizer for model training. In this section we overview the neural network architectures evaluated in this work, We implemented popular models from <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[27]</ref>, <ref type="bibr" target="#b31">[28]</ref>, in our library for benchmarking streaming and non streaming inference on mobile phone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Neural Network (DNN)</head><p>The DNN model applies fully-connected layers with rectified linear unit (ReLU) activation function on every input speech feature. Then the outputs are stacked over 49 frames and processed by a pooling layer followed by another sequence of fullyconnected layers with ReLU. We observed that this architecture with a pooling layer gives higher accuracy than the standard DNN model published in <ref type="bibr" target="#b4">[5]</ref>, as shown in <ref type="table" target="#tab_2">Table 1</ref>. We use a similar number of model parameters with DNN model in <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional Neural Network (CNN)</head><p>CNN <ref type="bibr" target="#b13">[15]</ref> is a popular model which is used in many applications including KWS <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b22">22]</ref>. As in <ref type="bibr" target="#b4">[5]</ref> it is composed of sequence of 2D colvolutions with ReLU non linearities followed by fully connected layers in the end. Below we benchmarked several variations of CNN: one with striding equal 2 (CNN+strd on <ref type="table" target="#tab_2">Table 1 and Table 2</ref>) and another with no striding (CNN on <ref type="table" target="#tab_3">Table 2</ref>, it can be automatically converted to streaming mode).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recurrent Neural Networks: LSTM, GRU</head><p>RNNs <ref type="bibr" target="#b14">[16]</ref> are successfully applied in KWS problems <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18]</ref>. It receives speech features as a sequence and processes it sequentially with updating the internal states of the model. This approach is well suited for streaming inference mode. We explore two versions of RNN: LSTM <ref type="bibr" target="#b18">[19]</ref> (in <ref type="table" target="#tab_2">Table 1</ref> called LSTM) and a GRU-based model <ref type="bibr" target="#b19">[20]</ref> and compare them with baseline <ref type="bibr" target="#b4">[5]</ref> on <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Convolutional Recurrent Neural Network (CRNN)</head><p>CRNN <ref type="bibr" target="#b15">[17]</ref> combines properties of both CNN, which captures short term dependencies, and RNN, which uses longer range context. It applies a set of 2D convolutions on speech features followed by a GRU layer with fully connected layers. We compare CRNN model in our library with baseline [5] on <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Depthwise Separable Convolutional Neural Network (DSCNN)</head><p>DSCNN <ref type="bibr" target="#b20">[21]</ref> models are well applied on KWS <ref type="bibr" target="#b4">[5]</ref>. This model processes speech features by using a sequence of 2D convolutional and 2D depthwise layers followed by batch normalization with average pooling (as in <ref type="bibr" target="#b4">[5]</ref>) and finished by applying fully connected layers. Below we benchmarked several variations of DSCNN: one with striding equal 2 (it is similar to DSCNN in <ref type="bibr" target="#b4">[5]</ref> and shown as DSCNN+strd on <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>) and another with no striding (DSCNN on <ref type="table" target="#tab_3">Table 2</ref>, it can be automatically converted to streaming mode).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Multihead attention RNN (MHAtt-RNN)</head><p>The development of the Attention mechanism <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref> improved the accuracy on multiple tasks including KWS <ref type="bibr" target="#b29">[27]</ref>. In <ref type="bibr" target="#b29">[27]</ref> the authors build a model Att-RNN which takes a mel-scale spectrogram and convolves it with a set of 2D convolutions. Then two bidirectional LSTM <ref type="bibr" target="#b14">[16]</ref> layers are used to capture twoway long term dependencies in the audio data. The feature in the center of the bidirectional LSTM's output sequence is projected using a dense layer and is used as a query vector for the attention mechanism. Finally, the weighted (by attention score) average of the bidirectional LSTM output is processed by a set of fully connected layers for classification <ref type="bibr" target="#b29">[27]</ref>. We extended this approach with multi-head attention (4 heads) and replaced LSTM with GRU (and call this version MHAtt-RNN). It allows us to reduce classification error by 10% in comparison to the state of the art (shown in <ref type="table" target="#tab_2">Table 1</ref>). Both Att-RNN and MHAtt-RNN models are using bidirectional RNN, so they cannot be converted to streaming mode and have to receive the whole sequence before producing classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Singular value decomposition filter (SVDF)</head><p>We implemented a simplified version of <ref type="bibr" target="#b31">[28]</ref>, so that it does not require aligned annotation of audio data for training. This model is composed of several SVDF and bottleneck layers with one softmax layer in the end. The SVDF block is a sequence of one dimensional convolution and one dimensional depthwise convolution layers <ref type="bibr" target="#b31">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Temporal Convolution ResNet (TC-ResNet)</head><p>The TC-ResNet model <ref type="bibr" target="#b22">[22]</ref> is composed of sequence of residual blocks which use one dimensional convolution. In this paper we use neural network topology called TC-ResNet14 <ref type="bibr" target="#b22">[22]</ref>. To </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and accuracy metrics</head><p>On <ref type="table" target="#tab_2">Table 1</ref> we compare the accuracy of published models with our implementation on a Google dataset V1 <ref type="bibr" target="#b33">[29]</ref> and V2 <ref type="bibr" target="#b6">[9]</ref>. We use the standard data set up from TensorFlow speech commands example code, proposed at <ref type="bibr" target="#b7">[10]</ref>. The NN is trained on twelve labels: ten words "yes", "no", "up", "down", "left", "right", "on", "off", "stop", and "go" with additional two words: "silence" and "unknown". The "unknown" category contains remaining 20 keywords from the dataset. As in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">10]</ref> we use an algorithm from [30] for splitting the data into training, validation and testing set with ratio 80:10:10. The length of the one training speech sample is 1 sec, and the sampling rate is 16kHz.</p><p>After applying the standard data set up (described above) on data sets V1 <ref type="bibr" target="#b33">[29]</ref> we have 22246, 3093, 3081 samples for training validation and testing respectively. With data set V2 <ref type="bibr" target="#b6">[9]</ref> we have 36923, 4445, 4890 samples for training validation and testing respectively.</p><p>The training data is augmented with:</p><p>• time shift in range -100ms...100ms (as in [30], <ref type="bibr" target="#b4">[5]</ref>); • signal resampling with resampling factor in range 0.85...1.15; • background noise (as in [30], <ref type="bibr" target="#b4">[5]</ref>); • frequency/time masking, based on SpecAugment <ref type="bibr" target="#b28">[26]</ref> (except time warping).</p><p>For side-by-side comparison purposes we use classification accuracy metric as in <ref type="bibr" target="#b4">[5]</ref>. It is calculated by running the model on the testing data, and comparing the classification result against the expected label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with baseline</head><p>We implemented popular KWS approaches DNN <ref type="bibr" target="#b4">[5]</ref>, CNN <ref type="bibr" target="#b4">[5]</ref>, LSTM <ref type="bibr" target="#b4">[5]</ref>, GRU <ref type="bibr" target="#b4">[5]</ref>, CRNN <ref type="bibr" target="#b4">[5]</ref>, DSCNN <ref type="bibr" target="#b4">[5]</ref>, TC-ResNet [22], described above, using our library for benchmarking streaming and non streaming models. We improved their accuracy on datasets V1 and V2, as shown on <ref type="table" target="#tab_2">Table 1</ref>, by applying SpecAugment <ref type="bibr" target="#b28">[26]</ref>(except time warping) with hyper-parameters optimization of both neural net and speech feature extractor parameters. After model is trained on datasets V1/V2 we convert it to TFLite format (to be able to run it on a mobile phone), then run inference with TFLite and report its accuracy on <ref type="table" target="#tab_2">Table 1</ref> (columns V1, V2). The baseline accuracy with reference to paper is shown on <ref type="table" target="#tab_2">Table 1</ref> (columns V1*, V2*).</p><p>One of the best KWS models is Embed+head <ref type="bibr" target="#b10">[13]</ref>. It achieves the state of the art accuracy by using additional data sets from YouTube to train embedding layer. We introduced MHAtt-RNN model. It reduces classification error on datasets V2 by 10% in comparison to Embed+head <ref type="bibr" target="#b10">[13]</ref>, as shown on <ref type="table" target="#tab_2">Table 1</ref>. The cost of this improvement is MHAtt-RNN model has two times more parameters than Embed+head <ref type="bibr" target="#b10">[13]</ref>. Another recently published promising approach is Matchbox <ref type="bibr" target="#b24">[23]</ref>, but authors use different training testing data set up, so we could not compare it side by side.</p><p>In addition we implemented and benchmarked SVDF model <ref type="bibr" target="#b31">[28]</ref> on both data sets V1 and V2 shown at <ref type="table" target="#tab_2">Table 1</ref> and demonstrated that it has good properties for streaming at Table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Streaming and non-streaming latency with accuracy</head><p>In the real production environment we do not know neither the beginning nor ending of the speech command produced by the user. Also we need to provide real time responses for a good user experience. As a result, the speech command detector is running in streaming mode by classifying every 20 milliseconds (for example) of the input audio stream.</p><p>We trained all the models on datasets V2 and converted DNN, CNN no stride, CRNN, DSCNN no stride, SVDF to a streaming inference mode and benchmarked them on datasets V2. The models DSCNN with stride, CNN with stride and MHAtt-RNN are not streamable with our library. To emulate the streaming environment during accuracy evaluation we did not reset the RNN model states between testing sequences. We observed up to 2x accuracy reduction on such models (also baseline RNN models in <ref type="table" target="#tab_2">Table 1</ref> have the same issue). We addressed it by re-training RNN models GRU, CRNN) with stateful argument=True <ref type="bibr">[31]</ref>. The last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch (the model will learn how to process cell states on its own). To distinguish such RNN models on Table 2 we append (S). We observed accuracy reduction of statefully trained models GRU (S) CRNN (S), shown on <ref type="table" target="#tab_3">Table 2</ref> in comparison to their non statefully trained versions GRU, CRNN shown <ref type="table" target="#tab_2">Table 1</ref> (column V2).</p><p>The latency and accuracy of non-streaming and streaming models with the number of parameters are presented in <ref type="table" target="#tab_3">Table 2</ref>. We use a Pixel 4 mobile phone [8] and TFlite benchmarking tools [32] to measure the models latency. Non-streaming latency is the processing time of the whole 1 sec speech sequence by the non-streaming model representation. Streaming latency is the processing time of one audio frame by the streaming model (it receives 20ms of audio and returns classification result). Processing time includes both feature extraction and neural network classification (end to end).</p><p>In <ref type="table" target="#tab_3">Table 2</ref> we observe that the most effective and accurate streaming models are SVDF, CRNN and GRU. Layers with striding/pooling are not streamable in our library now, but it can be implemented in the future. With support of striding/pooling in streaming mode, models such as TC-ResNet, Embed+head and Matchbox can be more preferable. The most accurate nonstreaming model is MHAtt-RNN. It is based on bidirectional LSTM, so non streamable by default. <ref type="table" target="#tab_3">Table 2</ref> shows that the average latency ratio of non-  <ref type="figure">DSCNN)</ref> is around 10x. The same ratio for RNN models (GRU(S), CRNN(S)) is around 20x. We explain this difference by the fact that non-streaming RNN models still have to be executed sequentially frame by frame over all frames belonging to 1 second of input audio, whereas non-streaming convolutions can be computed over the whole sequence in one batch. In an ideal case this ratio has to be around 50x (there are 50 frames in one second). As a result, there are opportunities for latency speed-up of streaming models: reduce memory allocations in the ring buffers and enable support of internal state in the inference engine. At the same time the latency of non-streaming models also can be optimized further. Detailed profiling of non-streaming models showed that the speech feature extraction latency is around 3.7ms. It can be reduced by using FFT and model quantization (after enabling both we observe almost 2x latency reduction). As expected non-streaming models with striding are several times faster than the same model with no striding: CNN and DSCNN with striding are two times faster than the same models without striding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We built a library allowing end-to-end model conversion to streaming inference on mobile phones using Keras and TFLite. The converted model encapsulates speech feature extraction. It simplifies model deployment on mobile devices. We reduced classification error by 10% relative, in comparison to the state of the art models on datasets V2. It was achieved by extending the Att-RNN <ref type="bibr" target="#b29">[27]</ref> model with multi-head attention and applying SpecAugment <ref type="bibr" target="#b28">[26]</ref>. For benchmarking purpose we implemented several popular models using our streaming library and measured streaming and non streaming latency on a Pixel 4 mobile phone and demonstrated different tradeoffs between accuracy and latency. All code with experimentation results are open-sourced and available at [11].</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Let's consider an example of convolutional NN (shown onFig 1a) applied on KWS. A standard approach for model training is to use a non-streaming model representation, shown on Fig 1a. It receives the whole input sequence and then returns the classification result (on Fig 1 the whole input sequence has length 6 with single sample feature size 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Convolutional model: a) non-streaming b) streaming with internal state c) streaming with external state</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Baseline models accuracy on data V1* and V2* with paper references and our models accuracy on data V1 and V2</figDesc><table><row><cell>Models</cell><cell>V1*[%]</cell><cell>V1[%] V2*[%]</cell><cell>V2[%]</cell></row><row><cell>DNN</cell><cell>86.7 [5]</cell><cell>91.2</cell><cell>90.6</cell></row><row><cell>CNN+strd</cell><cell>92.7 [5]</cell><cell>95.4</cell><cell>95.6</cell></row><row><cell>SVDF</cell><cell></cell><cell>96.3</cell><cell>96.9</cell></row><row><cell>DSCNN+strd</cell><cell>95.4 [5]</cell><cell>97.0</cell><cell>97.1</cell></row><row><cell>GRU</cell><cell>94.7 [5]</cell><cell>96.6</cell><cell>97.2</cell></row><row><cell>LSTM</cell><cell>94.8 [5]</cell><cell>96.9</cell><cell>97.5</cell></row><row><cell>CRNN</cell><cell>95.0 [5]</cell><cell>97.0</cell><cell>97.5</cell></row><row><cell>Att-RNN</cell><cell>95.6 [27]</cell><cell>96.9 [27]</cell><cell></cell></row><row><cell>TC-ResNet</cell><cell cols="2">96.6 [22] 97.1</cell><cell>97.4</cell></row><row><cell>Embed+head</cell><cell></cell><cell>97.7 [13]</cell><cell></cell></row><row><cell>MHAtt-RNN</cell><cell></cell><cell>97.2</cell><cell>98.0</cell></row><row><cell cols="4">improve accuracy of this model we increase number of param-</cell></row><row><cell cols="4">eters from 305K to 365K and use SpecAugment [26]. Baseline</cell></row><row><cell cols="4">TC-ResNet accuracy with its improvement on data sets V1 and</cell></row><row><cell cols="2">V2 are shown on Table 1.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on data V2 with latency and model size</figDesc><table><row><cell>Models</cell><cell>accuracy,</cell><cell>non</cell><cell>stream</cell><cell>model</cell></row><row><cell></cell><cell>[%]</cell><cell>stream</cell><cell>latency,</cell><cell>size,</cell></row><row><cell></cell><cell></cell><cell>latency,</cell><cell>[ms]</cell><cell>[K]</cell></row><row><cell></cell><cell></cell><cell>[ms]</cell><cell></cell><cell></cell></row><row><cell>DNN</cell><cell>90.6</cell><cell>4</cell><cell>0.6</cell><cell>447</cell></row><row><cell>CNN+strd</cell><cell>95.6</cell><cell>6</cell><cell>N/I</cell><cell>529</cell></row><row><cell>CNN</cell><cell>96.0</cell><cell>15</cell><cell>1.5</cell><cell>606</cell></row><row><cell>GRU (S)</cell><cell>96.3</cell><cell>11</cell><cell>0.5</cell><cell>593</cell></row><row><cell>CRNN (S)</cell><cell>96.5</cell><cell>9</cell><cell>0.5</cell><cell>467</cell></row><row><cell>SVDF</cell><cell>96.9</cell><cell>5</cell><cell>0.6</cell><cell>354</cell></row><row><cell>DSCNN</cell><cell>96.9</cell><cell>19</cell><cell>1.6</cell><cell>490</cell></row><row><cell cols="2">DSCNN+strd 97.0</cell><cell>9</cell><cell>N/I</cell><cell>485</cell></row><row><cell>TC-ResNet</cell><cell>97.4</cell><cell>5</cell><cell>N/I</cell><cell>365</cell></row><row><cell cols="2">MHAtt-RNN 98.0</cell><cell>10</cell><cell>N/A</cell><cell>743</cell></row><row><cell cols="5">streaming to streaming convolutional models (CNN, SVDF,</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>The authors would like to thank Pete Warden, Karolis Misiunas, Yukun Zhu, Ben Vanik, Robert Suderman, Rohit Prabhavalkar, Kevin Kilgour and BDI team for valuable discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<idno>abs/1412.6115</idno>
		<ptr target="http://arxiv.org/abs/1412.6115" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Streaming End-to-end Speech Recognition For Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<idno>abs/1811.06621</idno>
		<ptr target="https://arxiv.org/abs/1811.06621" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient keyword spotting using dilated convolutions and gating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chlieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poumeyrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.07684" />
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>abs/1711.07128</idno>
		<ptr target="http://arxiv.org/abs/1711.07128" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Speech commands dataset v2</title>
		<ptr target="https://storage.googleapis.com/download.tensorflow.org/data/speechcommandsv0.02.tar.gz" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno>abs/1804.03209</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://arxiv.org/abs/1804.03209" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2014.6854370</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2014.6854370" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-05-04" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training Keyword Spotters with Limited and Synthesized Speech Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9053193</idno>
		<ptr target="https://doi.org/10.1109/ICASSP40776.2020.9053193" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7474" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="http://www.isca-speech.org/archive/interspeech2015/i151478.html" />
		<title level="m">Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1478" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.05390" />
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Max-pooling loss training of long short-term memory networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Spoken Language Technology Workshop, SLT 2016</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="474" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/SLT.2016.7846306</idno>
		<ptr target="https://doi.org/10.1109/SLT.2016.7846306" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LSTM recurrent networks learn simple context-free and context-sensitive languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.963769</idno>
		<ptr target="https://doi.org/10.1109/72.963769" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1340" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1406.1078" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<ptr target="http://arxiv.org/abs/1704.04861" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal Convolution for Real-time Keyword Spotting on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<ptr target="http://arxiv.org/abs/1904.03814" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531v2</idno>
		<ptr target="https://arxiv.org/abs/2004.08531v2" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Interspeech 2019, [Online</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L D S</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno>abs/1808.08929</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<ptr target="http://arxiv.org/abs/1808.08929" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end streaming keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-12" />
			<biblScope unit="page" from="6336" to="6340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICASSP.2019.8683557</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2019.8683557" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Speech commands dataset v1</title>
		<ptr target="http://download.tensorflow.org/data/speechcommandsv0.01.tar.gz" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
