<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Warped Guidance for Blind Face Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
							<email>yeyuting.jlu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<email>wmzuo@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Data and Computer Science</orgName>
								<orgName type="department" key="dep2">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
							<email>ryang@cs.uky.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Warped Guidance for Blind Face Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Face hallucination · blind image restoration · flow field · convolutional neural networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net [1]) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face restoration aims to reconstruct high quality face image from degraded observation for better display and further analysis <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. In the ubiquitous imaging era, imaging sensors are embedded into many consumer products and surveillance devices, and more and more images are acquired under unconstrained arXiv:1804.04829v2 [cs.CV] <ref type="bibr" target="#b15">16</ref> Apr 2018 scenarios. Consequently, low quality face images cannot be completely avoided during acquisition and communication due to the introduction of low-resolution, defocus, noise and compression. On the other hand, high quality face images are sorely needed for human perception, face recognition <ref type="bibr" target="#b12">[13]</ref> and other face analysis <ref type="bibr" target="#b13">[14]</ref> tasks. All these make face restoration a very challenging yet active research topic in computer vision. Many studies have been carried out to handle specific face restoration tasks, such as denoising <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, hallucination <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> and deblurring <ref type="bibr" target="#b11">[12]</ref>. Most existing methods, however, are proposed for handling a single specific face restoration task in a non-blind manner. In practical scenario, it is more general that both the degradation types and degradation parameters are unknown in advance. Therefore, more attentions should be given to blind face restoration. Moreover, most previous works produce the restoration results purely relying on a single degraded observation. It is worth noting that the degradation process generally is highly ill-posed. By learning a direct mapping from degraded observation, the restoration result inclines to be over-smoothing and cannot faithfully retain fine and identity-aware facial details.</p><p>In this paper, we study the problem of guided blind face restoration by incorporating the degraded observation and a high-quality guided face image. Without loss of generality, the guided image is assumed to have the same identity with the degraded observation, and is frontal with eyes open. We note that such guided restoration setting is practically feasible in many real world applications. For example, most smartphones support to recognize and group the face images according to their identities <ref type="bibr" target="#b0">1</ref> . In each group, the high quality face image can thus be exploited to guide the restoration of low quality images. In film restoration, it is also encouraging to use the high quality portrait of an actor to guide the restoration of low-resolution and corrupted face images of the same actor from an old film. For these tasks, further incorporation of guided image not only can ease the difficulty of blind restoration, but also is helpful in faithfully recovering fine and identity-aware facial details.</p><p>Guided blind face restoration, however, cannot be addressed well by simply taking the degraded observation and guided image as input to plain convolutional networks (CNNs), due to that the two images generally are of different poses, expressions and lighting conditions. <ref type="figure" target="#fig_0">Fig. 1(c)</ref> shows the results obtained using the U-Net <ref type="bibr" target="#b0">[1]</ref> by only taking degraded observation as input, while <ref type="figure" target="#fig_0">Fig. 1(d)</ref> shows the results by taking both two images as input. It can be seen that direct incorporation of guided image brings very limited improvement on the restoration result. To tackle this issue, we develop a guided face restoration network (GFRNet) consisting of a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). Here, the WarpNet is firstly deployed to predict a flow field for warping the guided image to obtain the warped guidance, which is required to have the same pose and expression with degraded observation. Then, the RecNet takes both degraded observation and warped guidance as input to produce the final restoration result. To train GFRNet, we adopt the reconstruction learning to constrain the restoration result to be close to the target image (i.e., ground-truth), and further employ the adversarial learning for visually realistic restoration.</p><p>Nonetheless, even though the WarpNet can be end-to-end trained with reconstruction and adversarial learning, we empirically find that it cannot converge to the desired solution and fails to align the guided image to the correct pose and expression. <ref type="figure" target="#fig_0">Fig. 1</ref>(e) gives the results of our GFRNet trained by reconstruction and adversarial learning. One can see that its improvement over U-Net is still limited, especially when the degraded observation and guided images are distinctly different in pose. Moreover, the ground-truth flow field is unavailable, and the target and guided images may be of different lighting conditions, making it infeasible to directly use the target image to guide the WarpNet learning. Instead, we adopt the face alignment method <ref type="bibr" target="#b16">[17]</ref> to detect the face landmarks of the target and guided images, and then introduce the landmark loss as well as the total variation (TV) regularizer to train the WarpNet. As in <ref type="figure" target="#fig_0">Fig. 1(f)</ref>, our full GFRNet achieves the favorable visual quality, and is effective in recovering fine facial details. Furthermore, to make the learned GFRNet applicable to blind face restoration, our model is trained on the synthetic data generated by a general degradation model with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor.</p><p>Extensive experiments are conducted to evaluate the proposed GFRNet for guided blind face restoration. The results show that the WarpNet is effective in aligning the guided image to the desired pose and expression. The proposed GFRNet achieves significant performance gains over the state-of-the-art restoration methods, e.g., SRCNN <ref type="bibr" target="#b17">[18]</ref>, VDSR <ref type="bibr" target="#b18">[19]</ref>, SRGAN <ref type="bibr" target="#b19">[20]</ref>, DCP <ref type="bibr" target="#b20">[21]</ref>, DeepDeblur <ref type="bibr" target="#b21">[22]</ref>, DeblurGAN <ref type="bibr" target="#b22">[23]</ref>, DnCNN <ref type="bibr" target="#b23">[24]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, ARCNN <ref type="bibr" target="#b25">[26]</ref>, CBN <ref type="bibr" target="#b3">[4]</ref>, WaveletSRNet <ref type="bibr" target="#b8">[9]</ref>, TDAE <ref type="bibr" target="#b10">[11]</ref>, SCGAN <ref type="bibr" target="#b9">[10]</ref> and MCGAN <ref type="bibr" target="#b9">[10]</ref> in terms of both quantitative metrics (i.e., PSNR and SSIM) and visually perceptual quality. Moreover, our GFRNet also performs favorably on real degraded images as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(f). To sum up, the main contribution of this work includes:</p><p>-The GFRNet architecture for guided blind face restoration, which includes a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). -The incorporation of landmark loss and TV regularization for training the WarpNet. -The promising results of GFRNet on both synthetic and real face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have witnessed the unprecedented success of deep learning in many image restoration tasks such as super-resolution <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, denoising <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, compression artifact removal <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, compressed sensing <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, and deblurring <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. As to face images, several CNN architectures have been developed for face hallucination <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, and the adversarial learning is also introduced to enhance the visual quality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Most of these methods, however, are suggested for non-blind restoration and are restricted by the specialized tasks. Benefitted from the powerful modeling capability of deep CNNs, recent studies have shown that it is feasible to train a single model for handling multiple instantiations of degradation (e.g., different noise levels) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref>. As for face hallucination, Yu et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> suggest one kind of transformative discriminative networks to super-resolve different unaligned tiny face images. Nevertheless, blind restoration is a more challenging problem and requires to learn a single model for handling all instantiations of one or more degradation types.</p><p>Most studies on deep blind restoration are given to blind deblurring, which aims to recover the latent clean image from noisy and blurry observation with unknown degradation parameters. Early learning-based or CNN-based blind deblurring methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> usually follow traditional framework which includes a blur kernel estimation stage and a non-blind deblurring stage. With the rapid progress and powerful modeling capability of CNNs, recent studies incline to bypass blur kernel estimation by directly training a deep model to restore clean image from degraded observation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. As to blind face restoration, Chrysos and Zafeiriou <ref type="bibr" target="#b11">[12]</ref> utilize a modified ResNet architecture to perform face deblurring, while Xu et al. <ref type="bibr" target="#b9">[10]</ref> adopt the generative adversarial network (GAN) framework to super-resolve blurry face image. It is worth noting that the success of such kernel-free end-to-end approaches depends on both the modeling capability of CNN and the sufficient sampling on clean images and degradation parameters, making it difficult to design and train. Moreover, the highly ill-posed degradation further increases the difficulty of recovering the correct fine details only from degraded observation <ref type="bibr" target="#b37">[38]</ref>. In this work, we elaborately tackle this issue by incorporating a high quality guided image and designing appropriate network architecture and learning objective.</p><p>Several learning-based and CNN-based approaches are also developed for color-guided depth image enhancement <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>, where the structural interde-pendency between intensity and depth image is modeled and exploited to reconstruct high quality depth image. For guided depth image enhancement, Hui et al. <ref type="bibr" target="#b39">[40]</ref> present a CNN model to learn multi-scale guidance, while Gu et al. <ref type="bibr" target="#b40">[41]</ref> incorporate weighted analysis representation and truncated inference for dynamic guidance learning. For general guided filtering, Li et al. <ref type="bibr" target="#b38">[39]</ref> construct CNN-based joint filters to transfer structural details from guided image to reconstructed image. However, these approaches assume that the guided image is spatially well aligned with the degraded observation. Due to that the guided image and degraded observation usually are different in pose and expression, such assumption generally does not hold true for guided face restoration. To address this issue, a WarpNet is introduced in our GFRNet to learn a flow field for warping the guided image to the desired pose and expression.</p><p>Recently, spatial transformer networks (STNs) are suggested to learn a spatial mapping for warping an image <ref type="bibr" target="#b41">[42]</ref>, and appearance flow networks (AFNs) are presented to predict a dense flow field to move pixels <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. Deep dense flow networks have been applied to view synthesis <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref>, gaze manipulation <ref type="bibr" target="#b43">[44]</ref>, expression editing <ref type="bibr" target="#b45">[46]</ref>, and video frame synthesis <ref type="bibr" target="#b46">[47]</ref>. In these approaches, the target image is required to have the similar lighting condition with the input image to be warped, and the dense flow networks can thus be trained via reconstruction learning. However, in our guided face restoration task, the guided image and the target image usually are of different lighting conditions, making it less effective to train the flow network via reconstruction learning. Moreover, the ground-truth dense flow field is not available, further increasing the difficulty to train WarpNet. To tackle this issue, we use the face alignment method <ref type="bibr" target="#b16">[17]</ref> to extract the face landmarks of guided and target images. Then, the landmark loss and TV regularization are incorporated to facilitate the WarpNet training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>This section presents our GFRNet to recover high quality face image from degraded observation with unknown degradation. Given a degraded observation I d and a guided image I g , our GFRNet model produces the restoration result I = F(I d , I g ) to approximate the ground-truth target image I. Without loss of generality, I g and I are of the same identity and image size 256 × 256. Moreover, to provide richer guidance information, I g is assumed to be of high quality, frontal, non-occluded with eyes open. Nonetheless, we empirically find that our GFRNet is robust when the assumption is violated. For simplicity, we also assume I d also has the same size with I g . When such assumption does not hold, e.g., in face hallucination, we simply apply the bicubic scheme to upsample I d to the size of I g before inputting it to the GFRNet.</p><p>In the following, we first describe the GFRNet model as well as the network architecture. Then, a general degradation model is introduced to generate synthetic training data. Finally, we present the model objective of our GFRNet. Overview of our GFRNet. The WarpNet takes the degraded observation I d and guided image I g as input to predict the dense flow field Φ, which is adopted to deform I g to the warped guidance I w . I w is expected to be spatially well aligned with ground-truth I. Thus the RecNet takes I w and I d as input to produce the restoration resultÎ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Guided Face Restoration Network</head><p>The degraded observation I d and guided image I g usually vary in pose and expression. Directly taking I d and I g as input to plain CNNs generally cannot achieve much performance gains over taking only I d as input (See <ref type="figure" target="#fig_0">Fig. 1</ref>(c)(d)).</p><p>To address this issue, the proposed GFRNet consists of two subnetworks: (i) the warping subnetwork (WarpNet) and (ii) reconstruction subnetwork (RecNet). <ref type="figure">Fig. 2</ref> illustrates the overall architecture of our GFRNet. The WarpNet takes I d and I g as input to predict the flow field for warping guided image,</p><formula xml:id="formula_0">Φ = F w (I d , I g ; Θ w ),<label>(1)</label></formula><p>where Θ w denotes the WarpNet model parameters. With Φ, the output pixel value of the warped guidance I w at location (i, j) is given by</p><formula xml:id="formula_1">I w i,j = (h,w)∈N I g h,w max(0, 1 − |Φ y i,j − h|) max(0, 1 − |Φ x i,j − w|),<label>(2)</label></formula><p>where Φ x i,j and Φ y i,j denote the predicted x and y coordinates for the pixel I w i,j , respectively. N stands for the 4-pixel neighbors of (Φ x i,j , Φ y i,j ). From Eqn.</p><p>(2), we note that I w is subdifferentiable to Φ <ref type="bibr" target="#b41">[42]</ref>. Thus, the WarpNet can be end-to-end trained by minimizing the losses defined either on I w or on Φ.</p><p>The predicted warping guidance I w is expected to have the same pose and expression with the ground-truth I. Thus, the RecNet takes I d and I w as input to produce the final restoration result,</p><formula xml:id="formula_2">I = F r (I d , I w ; Θ r ),<label>(3)</label></formula><p>where Θ r denotes the RecNet model parameters.</p><p>Warping Subnetwork (WarpNet). The WarpNet adopts the encoder-decoder structure and is comprised of two major components:  <ref type="figure">Fig. 3</ref>. Architecture of our WarpNet. It takes the degraded observation I d and guided image I g as input to predict the dense flow field Φ, which is adopted to deform I g to the warped guidance I w . I w is expected to be spatially well aligned with ground-truth I. Landmark loss, TV regularization as well as gradient from RecNet are deployed to facilitate the learning of WarpNet.</p><p>-The input encoder extracts feature representation from I d and I g , consisting of eight convolution layers and each one with size 4 × 4 and stride 2. -The flow decoder predicts the dense flow field for warping I g to the desired pose and expression, consisting of eight deconvolution layers.</p><p>Except the first layer in encoder and the last layer in decoder, all the other layers adopt the convolution-BatchNorm-ReLU form. The detailed structure of WarpNet is shown in <ref type="figure">Fig 3.</ref> Finally, some explanations are given to the design of WarpNet. (i) Instead of the U-Net architecture, we adopt the standard encoder-decoder structure by removing the skip connections. It is worth noting that the input to encoder is two color images I d and I g while the output of decoder is a dense flow field Φ. Due to the heterogeneity of the input and output, it is unappropriate to concatenate the encoder features to the corresponding decoder features as in U-Net. (ii) It is also improper to directly output the warped guidance instead of the flow field. I w is of different pose and expression with I g , making the U-Net architecture still suffer from the heterogeneity issue. Due to the effect of the bottleneck (i.e., the fully connected layer), the encoder-decoder structure inclines to produce oversmoothing I w . Instead of directly predicting I w , predicting the dense flow field Φ usually results in realistic facial image with fine details.</p><p>Reconstruction Subnetwork (RecNet). For the RecNet, the input (I d and I w ) are of the same pose and expression with the output (Î), and thus the U-Net can be adopted to produce the final restoration resultÎ. The RecNet also includes two components, i.e., an encoder and a decoder. The encoder and decoder of RecNet are of the same structure with those adopted in WarpNet. To circumvent the information loss, the i-th layer is concatenated to the (L − i)th layer via skip connections (L is the depth of the U-Net), which has been demonstrated to benefit the rich and fine details of the generated image <ref type="bibr" target="#b47">[48]</ref>. The detailed structure of RecNet is shown in   <ref type="figure">Fig. 4</ref>. Architecture of our RecNet. It takes I w and I d as input to produce the restoration resultÎ. Reconstruction loss and global adversarial loss are adopted across entire image (labeled in purple), while local adversarial loss is adopted across face region (labeled in green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Degradation Model and Synthetic Training Data</head><p>To train our GFRNet, a degradation model is required to generate realistic degraded images. We note that real low quality images can be the results of either defocus, long-distance sensing, noise, compression, or their combinations. Thus, we adopt a general degradation model to generate degraded image I d,s ,</p><formula xml:id="formula_3">I d,s = ((I ⊗ k ) ↓ s + n σ ) JP EGq ,<label>(4)</label></formula><p>where ⊗ denotes the convolution operator. k stands for the Gaussian blur kernel with the standard deviation . ↓ s denotes the downsampling operator with scale factor s. n σ denotes the additive white Gaussian noise (AWGN) with the noise level σ. (·) JP EGq denotes the JPEG compression operator with quality factor q.</p><p>In our general degradation model, (I ⊗ k ) ↓ s + n σ characterizes the degradation caused by long-distance acquisition, while (·) JP EGq depicts the degradation caused by JPEG compression. We also note that Xu et al. <ref type="bibr" target="#b9">[10]</ref> adopt the degradation model (I ⊗ k + n σ ) ↓ s . However, to better simulate the longdistance image acquisition, it is more appropriate to add the AWGN on the downsampled image. When s = 1, the degraded image I d,s is of different size with the ground-truth I. So we use bicubic interpolation to upsample I d,s with scale factor s, and then take I d = (I d,s ) ↑ s and I g as input to our GFRNet.</p><p>In the following, we explain the parameter settings for these operations:</p><p>-Blur kernel. In this work, only the isotropic Gaussian blur kernel k is considered to model the defocus effect. We sample the standard deviation of Gaussian blur kernel from the set ∈ {0, 1 : 0.1 : 3}. -Downsampler. We adopt the bicubic downsampler as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. The scale factor s is sampled from the set s ∈ {1 : 0.1 : 8}. -Noise. As for the noise level σ, we adopt the set σ ∈ {0 : 1 : 7} <ref type="bibr" target="#b9">[10]</ref>.</p><p>-JPEG compression. For economic storage and communication, JPEG compression with quality factor q is further operated on the degraded image, and we sample q from the set q ∈ {0, 10 : 1 : 40}. When q = 0, the image is only losslessly compressed.</p><p>By including = 0, s = 1, σ = 0 and q = 0 in the set of degradation parameters, the general degradation model can simulate the effect of either the defocus, long-distance acquisition, noising, compression or their versatile combinations. Given a ground-truth image I i together with the guided image I g i , we can first sample i , s i , σ i and q i from the parameter set, and then use the degradation model to generate a degraded observation I d i . Furthermore, the face alignment method <ref type="bibr" target="#b16">[17]</ref> is adopted to extract the landmarks {(x Ii j , y Ii j )| 68 j=1 } for I i and {(x</p><formula xml:id="formula_4">I g i j , y I g i j )| 68 j=1 } for I g i . Therefore, we define the synthetic training set as X = {(I i , I g i , I d i , {(x Ii j , y Ii j )| 68 j=1 }, {(x I g i j , y I g i j )| 68 j=1 })| N i=1 },</formula><p>where N denotes the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Objective</head><p>Losses on Restoration ResultÎ. To train our GFDNet, we define the reconstruction loss on the restoration resultÎ, and the adversarial loss is further incorporated onÎ to improve the visual perception quality.</p><p>Reconstruction loss. The reconstruct loss is used to constrain the restoration resultÎ to be close to the ground-truth I, which includes two terms. The first term is the 2 loss defined as the squared Euclidean distance betweenÎ and I, i.e., 0 r (I,Î) = I −Î 2 . Due to the inherent irreversibility of image restoration, only the 2 loss inclines to cause over-smoothing result. Following <ref type="bibr" target="#b48">[49]</ref>, we define the second term as the perceptual loss on the pre-trained VGG-Face <ref type="bibr" target="#b49">[50]</ref>. Denote by ψ the VGG-Face network, ψ l (I) the feature map of the l-th convolution layer. The perceptual loss on the l-th layer (i.e., Conv-4 in this work) is defined as</p><formula xml:id="formula_5">ψ,l p (I,Î) = 1 C l H l W l ψ l (Î) − ψ l (I) 2 2<label>(5)</label></formula><p>where C l , H l and W l denote the channel numbers, height and width of the feature map, respectively. Finally, we define the reconstruction loss as </p><p>where λ r,0 and λ r,l are the tradeoff parameters for the 2 and the perceptual losses, respectively. Adversarial Loss. Following <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>, both global and local adversarial losses are deployed to further improve the perceptual quality of the restoration result. Let p data (I) be the distribution of ground-truth image, p d (I d ) be the distribution of degraded observation. Using the global adversarial loss <ref type="bibr" target="#b52">[53]</ref> as an example, the adversarial loss can be formulated as,  <ref type="figure">Θ)</ref>))], <ref type="bibr" target="#b6">(7)</ref> where D(I) denotes the global discriminator which predicts the possibility that I is from the distribution p data (I). F(I d , I g ; Θ) denotes the restoration result by our GFRNet with the model parameters Θ = (Θ w , Θ r ).</p><p>Following the conditional GAN <ref type="bibr" target="#b47">[48]</ref>, the discriminator has the same architecture with pix2pix <ref type="bibr" target="#b47">[48]</ref>, and takes the degraded observation, guided image and restoration result as the input. The network is trained in an adversarial manner, where our GFRNet is updated by minimizing the loss a,g while the discriminator is updated by maximizing a,g . To improve the training stability, we adopt the improved GAN <ref type="bibr" target="#b53">[54]</ref>, and replace the labels 0/1 with the smoothed 0/0.9 to reduce the vulnerability to adversarial examples. The local adversarial loss a,l adopts the same settings with the global one but its discriminator is defined only on the minimal bounding box enclosing all facial landmarks. To sum up, the overall adversarial loss is defined as L a = λ a,g a,g + λ a,l a,l .</p><p>where λ a,g and λ a,l are the tradeoff parameters for the global and local adversarial losses, respectively.</p><p>Losses on Flow Field Φ. Although the WarpNet can be end-to-end trained based on the reconstruction and adversarial losses, it cannot be learned to correctly align I w with I in terms of pose and expression (see <ref type="figure" target="#fig_0">Fig. 13</ref>). In <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>, the appearance flow network is trained by minimizing the MSE loss between the output and the ground-truth of the warped image. But for guided face restoration, I generally has different illumination with I g , and cannot serve as the groundtruth of the warped image. To circumvent this issue, we present the landmark loss as well as the TV regularization to facilitate the learning of WarpNet. Landmark loss. Using the face alignment method TCDCN <ref type="bibr" target="#b16">[17]</ref>, we detect the 68 landmarks {(x I g j , y I g j )| 68 j=1 } for I g and {(x I j , y I j )| 68 j=1 } for I. In order to align I w and I, it is natural to require that the landmarks of I w are close to those of I, i.e., Φ x (x I j , y I j ) ≈ x I g j and Φ y (x I j , y I j ) ≈ y I g j . Thus, the landmark loss is defined as</p><formula xml:id="formula_8">lm = i (Φ x (x I i , y I i ) − x I g i ) 2 + (Φ y (x I i , y I i ) − y I g i ) 2 .<label>(9)</label></formula><p>In our implementation, all the coordinates (including x, y, Φ x and Φ y ) are normalized to the range [−1, 1]. TV regularization. The landmark loss can only be imposed on the locations of the 68 landmarks. For better learning WarpNet, we further take the TV regularization into account to require that the flow field should be spatially smooth. Given the 2D dense flow field (f x , f y ), the TV regularizer is defined as</p><formula xml:id="formula_9">T V = ∇ x Φ x 2 + ∇ y Φ x 2 + ∇ x Φ y 2 + ∇ y Φ y 2 ,<label>(10)</label></formula><p>where ∇ x (∇ y ) denotes the gradient operator along the x (y) coordinate.</p><p>Combining landmark loss with TV regularizer, we define the flow loss as</p><formula xml:id="formula_10">L f low = λ lm lm + λ T V T V ,<label>(11)</label></formula><p>where λ lm and λ T V denote the tradeoff parameters for landmark loss and TV regularizer, respectively.</p><p>Overall Objective. Finally, we combine the reconstruction loss, adversarial loss, and flow loss to give the overall objective,</p><formula xml:id="formula_11">L = L r + L a + L f low .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Extensive experiments are conducted to assess our GFRNet for guided blind face restoration. Peak Signal-to-Noise Ratio (PSNR) and structural similarity (SSIM) indices are adopted for quantitative evaluation with the related state-of-the-arts (including image super-resolution, deblurring, denoising, compression artifact removal and face hallucination). As for qualitative evaluation, we illustrate the results by our GFRNet and the competing methods. Results on real low quality images are also given to evaluate the generalization ability of our GFRNet. More results and testing code are available at: https://github.com/csxmli2016/ GFRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We adopt the CASIA-WebFace <ref type="bibr" target="#b54">[55]</ref> and VggFace2 <ref type="bibr" target="#b55">[56]</ref> datasets to constitute our training and test sets. The WebFace contains 10,575 identities and each has about 46 images with the size 256 × 256. The VggFace2 contains 9,131 identities (8,631 for training and 500 for testing) and each has an average of 362 images with different sizes. The images in the two datasets are collected in the wild and cover a large range of pose, age, illumination and expression. For each identity, at most five high quality images are selected, in which a frontal image with eyes open is chosen as the guided image and the others are used as the ground-truth to generate degraded observations. By this way, we build our training set of 20,273 pairs of ground-truth and guided images from the VggFace2 training set. Our test set includes two subsets: (i) 1,005 pairs from the VggFace2 test set, and (ii) 1,455 pairs from WebFace. The images whose identities have appeared in our training set are excluded from the test set. Furthermore, low quality images are also excluded in training and testing, which include: (i) lowresolution images, (ii) images with large occlusion, (iii) cartoon images, and (iv) images with obvious artifacts. The face region of each image in VGGFace2 is cropped and resized to 256 × 256 based on the bounding box detected by MTCNN <ref type="bibr" target="#b56">[57]</ref>. All training and test images are not aligned to keep their original pose and expression. Facial landmarks of the ground-truth and guided images are detected by TCDCN <ref type="bibr" target="#b16">[17]</ref> and are only used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details and Parameter Setting</head><p>Our model is trained using the Adam algorithm <ref type="bibr" target="#b57">[58]</ref> with the learning rate of 2 × 10 −4 , 2 × 10 −5 , 2 × 10 −6 and β 1 = 0.5. In each learning rate, the model is trained until the reconstruction loss becomes non-decreasing. Then a smaller learning rate is adopted to further fine-tune the model. The tradeoff parameters are set as λ r,0 = 100, λ r,l = 0.001, λ a,g = 1, λ a,l = 0.5, λ lm = 10, and λ T V = 1. We first pre-train the WarpNet for 5 epochs by minimizing the flow loss L f low , and then both WarpNet and RecNet are end-to-end trained by using the objective L. The batch size is 1 and the training is stopped after 100 epochs. Data augmentation such as flipping is also adopted during training. <ref type="table" target="#tab_1">Table 1</ref> lists the PSNR and SSIM results on the two test subsets, where our GFRNet achieves significant performance gains over all the competing methods. Using the 4× SR on WebFace as an example, in terms of PSNR, our GFRNet outperforms the SR and blind deblurring methods by more than 4 dB, the denoising methods by more than 3.5 dB, the compression artifact removal (AR) methods by more than 8 dB, the non-blind and blind FH methods by more than 5 dB. To the best of our knowledge, guided blind face restoration remains an uninvestigated issue in literature. Thus, we compare our GFRNet with several relevant state-of-the-arts, including three non-blind image super-resolution (SR) methods (SRCNN <ref type="bibr" target="#b17">[18]</ref>, VDSR <ref type="bibr" target="#b18">[19]</ref>, SRGAN <ref type="bibr" target="#b19">[20]</ref>), three blind deblurring methods (DCP <ref type="bibr" target="#b20">[21]</ref>, DeepDeblur <ref type="bibr" target="#b21">[22]</ref>, DeblurGAN <ref type="bibr" target="#b22">[23]</ref>), two denoising methods (DnCNN <ref type="bibr" target="#b23">[24]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>), one compression artifact removal method (AR-CNN <ref type="bibr" target="#b25">[26]</ref>), three non-blind face hallucination (FH) methods (CBN <ref type="bibr" target="#b3">[4]</ref>, Wavelet-SRNet <ref type="bibr" target="#b8">[9]</ref>, TDAE <ref type="bibr" target="#b10">[11]</ref>), and two blind FH methods (SCGAN <ref type="bibr" target="#b9">[10]</ref>, MCGAN <ref type="bibr" target="#b9">[10]</ref>). To keep consistent with the SR and FH methods, only two scale factors, i.e., 4 and 8, are considered for the test images. As for non-SR methods, we take the bicubic upsampling result as the input to the model. SRCNN <ref type="bibr" target="#b17">[18]</ref> and VDSR <ref type="bibr" target="#b18">[19]</ref> are only trained to perform 2×, 3× and 4× SR. To handle 8× SR, we adopt the strategy in <ref type="bibr" target="#b58">[59]</ref> by applying the 2× model to the result produced by the 4× model. For SCGAN <ref type="bibr" target="#b9">[10]</ref> and MCGAN <ref type="bibr" target="#b9">[10]</ref>, only the 4× models are available. For TDAE <ref type="bibr" target="#b10">[11]</ref>, only the 8× model is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Synthetic Images</head><p>Quantitative evaluation. It is worth noting that the promising performance of our GFRNet cannot be solely attributed to the use of our training data and the simple incorporation of guided image. To illustrate this point, we retrain four competing methods (i.e., SRGAN, DeblurGAN, DnCNN, and ARCNN) by using our training data and taking both degraded observation and guided image as input. For the sake of distinction, the retrained models are represented as MSRGAN, MDeblurGAN, MDnCNN, MARCNN. From <ref type="table" target="#tab_1">Table 1</ref>, the retrained models do achieve better PSNR and SSIM results than the original ones, but still perform inferior to our GFRNet with a large margin, especially on WebFace. Therefore, the performance gains over the retrained models should be explained by the network architecture and model objective of our GFRNet.</p><p>Qualitative evaluation. In Figs. 5 and 6, we compare results of all the competing methods in 4× SR and 8× SR. For better comparison, we select three competing methods with top quantitative performance, and compare their results with those by our GFRNet shown in Figs. 7 and 8. It is obvious that our GFRNet is more effective in restoring fine details while suppressing visual artifacts. In comparison with the competing methods, the results by GFRNet are visually photo-realistic and can correctly recover more fine and identity-aware details especially in eyes, nose, and mouth regions. <ref type="figure">Fig. 9</ref> shows the results on real low quality images by all the competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Real Low Quality Images</head><p>As for pose problem, <ref type="figure" target="#fig_0">Fig. 10</ref> shows more restoration results of our GFRNet compared with the top-3 performance methods on real low quality images with different poses. One can see that our GFRNet can also show great robustness in restoring facial images with different poses. The real images are selected from VGGFace2 with the resolution lower than 60 × 60. Even the degradation is unknown, our method yields visually realistic and pleasing results in face region with more fine details, while the competing methods can only achieve moderate improvement on visual quality.</p><p>(a) Input (b) SRCNN <ref type="bibr" target="#b17">[18]</ref> (c) VDSR <ref type="bibr" target="#b18">[19]</ref> (d) SRGAN <ref type="bibr" target="#b19">[20]</ref> (e) MSRGAN (f) DCP <ref type="bibr" target="#b20">[21]</ref> (g) DeepDeblur <ref type="bibr" target="#b21">[22]</ref> (h) DeblurGAN <ref type="bibr" target="#b22">[23]</ref> (i) MDeblurGAN (j) MemNet <ref type="bibr" target="#b24">[25]</ref> (k) DnCNN <ref type="bibr" target="#b23">[24]</ref> (l) MDnCNN (m) ARCNN <ref type="bibr" target="#b25">[26]</ref> (n) MARCNN (o) CBN <ref type="bibr" target="#b3">[4]</ref> (p) WaveletSRNet <ref type="bibr" target="#b8">[9]</ref> (q) SCGAN <ref type="bibr" target="#b9">[10]</ref> (r) MCGAN <ref type="bibr" target="#b9">[10]</ref> (s) Ours (t) Ground-truth   dom guidance (i.e., Ours(R)) achieves the second best results, indicating that GFRNet is robust to the misuse of identity. <ref type="figure" target="#fig_0">Figs. 1, 11 and 12</ref> give the restoration results by GFRNet variants. Ours(F ull) can generate much sharper and richer details, and achieves better perceptual quality than its variants. Moreover, Ours(R) also achieves the second best performance in qualitative results, but it may introduce the fine details of the other identity to the result (e.g., eye regions in <ref type="figure" target="#fig_0">Fig. 11(h)</ref>). Furthermore, to illustrate the effectiveness of flow loss, <ref type="figure" target="#fig_0">Fig. 13</ref>  Finally, four GFRNet models are trained based on four settings of our general degradation model: (i) only blurring, (ii) blurring+downsampling, (iii) blur-ring+downsampling+AWGN, (iv) our full degradation model in Eqn. <ref type="bibr" target="#b3">(4)</ref>. Due to that the four models are trained using different degradation settings, it is unfair to compared them using any synthetic test data. Thus, we test them on a real low quality image in <ref type="figure" target="#fig_0">Fig. 14.</ref> It can be seen that the results by the settings (i)∼(ii) are still blurry, while the results by the settings (i)∼(iii) contain visual artifacts. In comparison, the model by our full degradation model can produce sharp and clean result while suppressing most artifacts. The results indicate that our full degradation model is effective in simulating real low quality images which usually have unknown and complex degradation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a guided blind face restoration model, i.e., GFRNet, by taking both the degraded observation and a high-quality guided image from the same identity as input. Besides the reconstruction subnetwork, our GFRNet also includes a warping subnetwork (WarpNet), and incorporates the landmark loss as well as TV regularizer to align the guided image to the desired pose and expression. To make our GFRNet be applicable to blind restoration, we further introduce a general image degradation model to synthesize realistic low quality face image. Quantitative and qualitative results show that our GFRNet not only performs favorably against the relevant state-of-the-arts but also generates visually pleasing results on real low quality face images.</p><p>(a) Input (b) SRCNN <ref type="bibr" target="#b17">[18]</ref> (c) VDSR <ref type="bibr" target="#b18">[19]</ref> (d) SRGAN <ref type="bibr" target="#b19">[20]</ref> (e) MSRGAN (f) DCP <ref type="bibr" target="#b20">[21]</ref> (g) DeepDeblur <ref type="bibr" target="#b21">[22]</ref> (h) DeblurGAN <ref type="bibr" target="#b22">[23]</ref> (i) MDeblurGAN (j) MemNet <ref type="bibr" target="#b24">[25]</ref> (k) DnCNN <ref type="bibr" target="#b23">[24]</ref> (l) MDnCNN (m) ARCNN <ref type="bibr" target="#b25">[26]</ref> (n) MARCNN (o) CBN <ref type="bibr" target="#b3">[4]</ref> (p) WaveletSRNet <ref type="bibr" target="#b8">[9]</ref> (q) TDAE <ref type="bibr" target="#b10">[11]</ref> (r) SCGAN <ref type="bibr" target="#b9">[10]</ref> (s) MCGAN <ref type="bibr" target="#b9">[10]</ref> (t) Ours   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Restoration results on real low quality images: (a) real low quality image, (b) guided image, and the results by (c) U-Net [1] by taking low quality image as input, (d) U-Net [1] by taking both guided image and low quality image as input, (e) our GFRNet without landmark loss, and (f) our full GFRNet model. Best viewed by zooming in the screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Overview of our GFRNet. The WarpNet takes the degraded observation I d and guided image I g as input to predict the dense flow field Φ, which is adopted to deform I g to the warped guidance I w . I w is expected to be spatially well aligned with ground-truth I. Thus the RecNet takes I w and I d as input to produce the restoration resultÎ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>L</head><label></label><figDesc>r (I,Î) = λ r,0 0 r (I,Î) + λ r,l ψ,l p (I,Î),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>E</head><label></label><figDesc>a,g = min Θ max D I∼p data (I) [log D(I)] + E I d ∼p d (I d ) [log(1−D(F(I d , I g ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The 4× SR results compared with all the competing methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The 8× SR results compared with all the competing methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>The 8× SR results: (a) synthetic low quality image (Close-up in right bottom is the guided image), (b) MDnCNN [24], (c) MARCNN [26], (d) MDeblurGAN [23], (e) Ours, and (f) ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .</head><label>9101112</label><figDesc>Restoration on real low quality images compared with all the competing methods. Restoration results on real low quality images: (a) real low quality images (Close-up in right bottom is the guided image), (b) MDnCNN [24], (c) MARCNN [26], (d) MDeblurGAN [23], and (e) Ours. Restoration results of our GFRNet variants: (a) input, (b) guided image. (c) Ours(−W G), (d) Ours(−W G2), (e) Ours(−W ), (f) Ours(−W 2), (g) Ours(−F ), (h) Ours(R) (Close-up in right bottom is the random guided image), (i) Ours(F ull), and (j) ground-truth. Best viewed by zooming in the screen. Restoration results of our GFRNet variants: (a) input, (b) guided image. (c) Ours(−W G), (d) Ours(−W G2), (e) Ours(−W ), (f) Ours(−W 2), (g) Ours(−F ), (h) Ours(R) (Close-up in right bottom is the random guided image), (i) Ours(F ull), and (j) ground-truth. Best viewed by zooming in the screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Warped guidance by Ours(F ull) and Ours(−F ): (a) input, (b) guided image, (c) Ours(−F ), (d) Ours(F ull), and (e) ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Results on real low quality images by our GFRNet trained with different degradation settings: (a) real low quality image (Close-up in right bottom is the guided image), (b) only blurring, (c) blurring+downsampling, (d) blur-ring+downsampling+AWGAN, and (e) full degradation model. Best viewed by zooming in the screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Flow Decoder Landmark Loss TV Regularization Gradient from RecNet Degraded Observation I d Guided Image I g</head><label></label><figDesc></figDesc><table><row><cell>256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>256</cell><cell>128</cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell>128</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell></row><row><cell></cell><cell></cell><cell></cell><cell>16 8 4</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell>512</cell><cell>1024 1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024 1024</cell><cell>512</cell></row><row><cell></cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>256</cell></row><row><cell></cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>128</cell></row><row><cell>64</cell><cell></cell><cell cols="2">Input Encoder</cell><cell></cell><cell></cell><cell></cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">WarpNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on two test subsets. Numbers in the parentheses indicate SSIM and the remaining represents PSNR (dB). The best results are highlighted in red and second best ones except our GFRNet variants are highlighted in blue. (.842) 22.30 (.802) 26.11 (.872) 23.50 (.842) VDSR [19] 25.36 (.858) 22.50 (.807) 26.60 (.884) 23.65 (.</figDesc><table><row><cell cols="2">Methods</cell><cell></cell><cell cols="2">VggFace2 [56] 4× 8×</cell><cell></cell><cell></cell><cell cols="2">WebFace [55] 4× 8×</cell></row><row><cell>SR</cell><cell>SRCNN [18] SRGAN [20]</cell><cell cols="7">24.57 847) 25.85 (.911) 23.01 (.874) 27.65 (.941) 24.49 (.913)</cell></row><row><cell></cell><cell>MSRGAN</cell><cell cols="7">26.55 (.906) 23.45 (.862) 28.10 (.934) 24.92 (.908)</cell></row><row><cell></cell><cell>DCP [21]</cell><cell cols="7">24.42 (.894) 21.54 (.848) 24.97 (.895) 23.05 (.887)</cell></row><row><cell>Deblurring</cell><cell cols="8">DeepDeblur [22] 26.31 (.917) 22.97 (.873) 28.13 (.934) 24.63 (.910) DeblurGAN [23] 24.65 (.889) 22.06 (.846) 24.63 (.910) 23.38 (.896)</cell></row><row><cell></cell><cell>MDeblurGAN</cell><cell cols="7">25.32 (.918) 22.46 (.867) 29.41 (.952) 23.49 (.900)</cell></row><row><cell></cell><cell>DnCNN [24]</cell><cell cols="7">26.73 (.920) 23.29 (.877) 28.35 (.933) 24.75 (.912)</cell></row><row><cell>Denoising</cell><cell>MemNet [25]</cell><cell cols="7">26.85 (.923) 23.31 (.877) 28.57 (.934) 24.77 (.909)</cell></row><row><cell></cell><cell>MDnCNN</cell><cell cols="7">27.05 (.925) 23.33 (.879) 29.40 (.942) 24.84 (.912)</cell></row><row><cell>AR</cell><cell>ARCNN [26] MARCNN</cell><cell cols="7">22.05 (.863) 20.84 (.827) 23.39 (.876) 20.47 (.858) 25.43 (.923) 23.16 (.876) 28.40 (.938) 25.15 (.914)</cell></row><row><cell></cell><cell>CBN [4]</cell><cell cols="7">24.52 (.867) 21.84 (.817) 25.43 (.899) 23.10 (.852)</cell></row><row><cell>Non-blind FH</cell><cell cols="8">WaveletSRNet [9] 25.66 (.909) 20.87 (.831) 27.10 (.937) 21.63 (.869)</cell></row><row><cell></cell><cell>TDAE [11]</cell><cell>-</cell><cell>(-)</cell><cell cols="2">20.19 (.729)</cell><cell>-</cell><cell>(-)</cell><cell>20.24 (.741)</cell></row><row><cell>Blind FH</cell><cell>SCGAN [10] MCGAN [10]</cell><cell cols="2">25.16 (.905) 25.26 (.912)</cell><cell>--</cell><cell>--</cell><cell cols="2">26.37 (.923) 26.35 (.931)</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>Ours(−W G)</cell><cell cols="7">25.97 (.915) 22.91 (.838) 28.73 (.928) 24.76 (.884)</cell></row><row><cell></cell><cell>Ours(−W G2)</cell><cell cols="7">27.20 (.932) 23.22 (.863) 29.45 (.945) 25.93 (.914)</cell></row><row><cell></cell><cell>Ours(−W )</cell><cell cols="7">26.03 (.923) 23.29 (.843) 29.66 (.934) 25.20 (.897)</cell></row><row><cell>Ours</cell><cell>Ours(−W 2)</cell><cell cols="7">27.25 (.933) 23.24 (.864) 29.73 (.948) 25.95 (.917)</cell></row><row><cell></cell><cell>Ours(−F )</cell><cell cols="7">26.61 (.927) 23.17 (.863) 31.43 (.920) 26.00 (.922)</cell></row><row><cell></cell><cell>Ours(R)</cell><cell cols="7">27.90 (.943) 24.05 (.890) 31.46 (.962) 26.88 (.922)</cell></row><row><cell></cell><cell>Ours(F ull)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>28.55 (.947) 24.10 (.898) 32.31 (.973) 27.21 (.935)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Three groups of ablative experiments are conducted to assess the components of our GFRNet. First, we consider five variants of our GFRNet:(i) Ours(F ull): the full GFRNet, (ii) Ours(−F ): GFRNet by removing the flow loss L f low , (iii) Ours(−W ): GFRNet by removing WarpNet (RecNet takes both I d and I g as input), (iv) Ours(−W G): GFRNet by removing WarpNet and guided image (RecNet only takes I d as input), and (v) Ours(R): GFRNet by using a random I g with different identity to I d .Table 1also lists the PSNR and SSIM results of these variants, and we have the following observations. (i) All the three components, i.e., guided image, WarpNet and flow loss, contribute to the performance improvement. (ii) GFRNet cannot be well trained without the help of flow loss.</figDesc><table><row><cell>(a) Input</cell><cell cols="2">(b) Guided image (c) SRCNN [18]</cell><cell>(d) VDSR [19]</cell><cell>(e) SRGAN [20]</cell></row><row><cell>(f) MSRGAN</cell><cell>(g) DCP [21]</cell><cell cols="3">(h) DeepDeblur [22] (i) DeblurGAN [23] (j) MDeblurGAN</cell></row><row><cell cols="2">(k) MemNet [25] (l) DnCNN [24]</cell><cell>(m) MDnCNN</cell><cell>(n) ARCNN [26]</cell><cell>(o) MARCNN</cell></row><row><cell cols="3">(p) CBN [4] (q) WaveletSRNet [9] (r) TDAE [11]</cell><cell>(s) Ours</cell><cell>(t) Ground-truth</cell></row></table><note>As a result, although Ours(−F ) outperforms Ours(−W ) in most cases, some- times Ours(−W ) can perform slightly better than Ours(−F ) by average PSNR, e.g., for 8× SR on VggFace2. (iii) It is worth noting that GFRNet with ran-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>shows the warped guidance by Ours(F ull) and Ours(−F ). Without the help of flow loss, Ours(−F ) cannot converge to stable solution and results in unreasonable warped guidance. In contrast, Ours(F ull) can correctly align guided image to the desired pose and expression, indicating the necessity and effectiveness of flow loss.Second, it is noted that the parameters of Ours(F ull) are nearly two times of Ours(−W ) and Ours(−W G). To show that the gain of Ours(F ull) does not from the increase of parameter number, we include two other variants of GFRNet, i.e., Ours(−W 2) and Ours(−W G2), by increasing the channels of Ours(−W ) and Ours(−W G) to 2 times, respectively. FromTable 1, in terms of PSNR, Ours(F ull) also outperforms Ours(−W 2) and Ours(−W G2) with a large margin. Instead of the increase of model parameters, the performance improvement of Ours(F ull) should be mainly attributed to the incorporation of both WarpNet and flow loss.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell></row><row><cell cols="6">Fig. 7. The 4× SR results: (a) synthetic low quality image (Close-up in right bottom</cell></row><row><cell cols="6">is the guided image), (b) MDnCNN [24], (c) MARCNN [26], (d) MDeblurGAN [23],</cell></row><row><cell cols="2">(e) Ours, and (f) ground-truth.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>come</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://support.apple.com/HT207103</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hallucinating faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face hallucination: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="134" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">FSRNet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10703</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face hallucination with tiny unaligned images by transformative discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4327" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wavelet-SRNet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1689" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to superresolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3760" to="3768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep face deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2015" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of the effect of image resolution on automatic face gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andreu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>López-Centelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Mollineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>García-Sevilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Category-specific object image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5506" to="5518" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combined internal and external category-specific image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07064</idno>
		<title level="m">DeblurGAN: Blind motion deblurring using conditional adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep generative adversarial compression artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Alberto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4826" to="4835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reconnet: Noniterative reconstruction of images from compressively sensed measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kerviche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using deep neural networks for inverse problems in imaging: Beyond analytical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="36" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Blur-invariant deep learning for blinddeblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4752" to="4760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09056</idno>
		<title level="m">Image denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning high-order filters for efficient blind deconvolution of document photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="734" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Motion deblurring in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for direct text deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradiš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Limits of learning-based superresolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="406" to="420" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Depth map super-resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning dynamic guidance for depth image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3769" to="3778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepwarp: Photorealistic image resynthesis for gaze manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="311" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3500" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09961</idno>
		<title level="m">Semantic facial expression editing using autoencoded flow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>41.1-41.12</idno>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08092</idno>
		<title level="m">Vggface2: A dataset for recognising faces across pose and age</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07235</idno>
		<title level="m">Global-local face upsampling network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
