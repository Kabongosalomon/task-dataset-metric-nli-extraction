<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Attention and Generalization in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
							<email>bknyazev@uoguelph.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<email>gwtaylor@uoguelph.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CIFAR AI Chair</orgName>
								<orgName type="institution">University of Guelph Vector Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Attention and Generalization in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to better understand attention over nodes in graph neural networks (GNNs) and identify factors influencing its effectiveness. We particularly focus on the ability of attention GNNs to generalize to larger, more complex or noisy graphs. Motivated by insights from the work on Graph Isomorphism Networks, we design simple graph reasoning tasks that allow us to study attention in a controlled environment. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 60% in some of our classification tasks. Satisfying these conditions in practice is challenging and often requires optimal initialization or supervised training of attention. We propose an alternative recipe and train attention in a weakly-supervised fashion that approaches the performance of supervised models, and, compared to unsupervised models, improves results on several synthetic as well as real datasets. Source code and datasets are available at https://github. com/bknyaz/graph_attention_pool.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Attention meets pooling in graph neural networks</head><p>The practical importance of attention in deep learning is well-established and there are many arguments in its favor <ref type="bibr" target="#b0">[1]</ref>, including interpretability <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. In graph neural networks (GNNs), attention can be defined over edges <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> or over nodes <ref type="bibr" target="#b5">[6]</ref>. In this work, we focus on the latter, because, despite being equally important in certain tasks, it is not as thoroughly studied <ref type="bibr" target="#b6">[7]</ref>. To begin our description, we first establish a connection between attention and pooling methods. In convolutional neural networks (CNNs), pooling methods are generally based on uniformly dividing the regular grid (such as one-dimensional temporal grid in audio) into local regions and taking a single value from that region (average, weighted average, max, stochastic, etc.), while attention in CNNs is typically a separate mechanism that weights C-dimensional input X ∈ R N ×C :</p><formula xml:id="formula_0">Z = α X,<label>(1)</label></formula><p>where Z i = α i X i -output for unit (node in a graph) i, N i α i = 1, -element-wise multiplication, N -the number of units in the input (i.e. number of nodes in a graph).</p><p>In GNNs, pooling methods generally follow the same pattern as in CNNs, but the pooling regions (sets of nodes) are often found based on clustering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, since there is no grid that can be uniformly divided into regions in the same way across all examples (graphs) in the dataset. Recently, top-k pooling <ref type="bibr" target="#b10">[11]</ref> was proposed, diverging from other methods: instead of clustering "similar" nodes, it propagates only part of the input and this part is not uniformly sampled from the input. Top-k pooling can thus select some local part of the input graph, completely ignoring the rest. For this reason at first glance it does not appear to be logical.  <ref type="figure">Figure 1</ref>: Three tasks with a controlled environment we consider in this work. The values inside the nodes are ground truth attention coefficients, α GT i , which we find heuristically (see Section 3.1).</p><p>However, we can notice that pooled feature maps in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">Eq. 2]</ref> are computed in the same way as attention outputs Z in Eq. 1 above, if we rewrite their Eq. 2 in the following way:</p><formula xml:id="formula_1">Z i = α i X i , ∀i ∈ P ∅, otherwise,<label>(2)</label></formula><p>where P is a set of indices of pooled nodes, |P | ≤ N , and ∅ denotes the unit is absent in the output.</p><p>The only difference between Eq. 2 and Eq. 1 is that Z ∈ R |P |×C , i.e. the number of units in the output is smaller or, formally, there exists a ratio r = |P |/N ≤ 1 of preserved nodes. We leverage this finding to integrate attention and pooling into a unified computational block of a GNN. In contrast, in CNNs, it is challenging to achieve this, because the input is defined on a regular grid, so we need to maintain resolution for all examples in the dataset after each pooling layer. In GNNs, we can remove any number of nodes, so that the next layer will receive a smaller graph. When applied to the input layer, this form of attention-based pooling also brings us interpretability of predictions, since the network makes a decision only based on pooled nodes.</p><p>Despite the appealing nature of attention, it is often unstable to train and the conditions under which it fails or succeeds are unclear. Motivated by insights of <ref type="bibr" target="#b11">[12]</ref> recently proposed Graph Isomorphism Networks (GIN), we design two simple graph reasoning tasks that allow us to study attention in a controlled environment where we know ground truth attention. The first task is counting colors in a graph (COLORS), where a color is a unique discrete feature. The second task is counting the number of triangles in a graph (TRIANGLES). We confirm our observations on a standard benchmark, MNIST <ref type="bibr" target="#b12">[13]</ref>  <ref type="figure">(Figure 1</ref>), and identify factors influencing the effectiveness of attention.</p><p>Our synthetic experiments also allow us to study the ability of attention GNNs to generalize to larger, more complex or noisy graphs. Aiming to provide a recipe to train more effective, stable and robust attention GNNs, we propose a weakly-supervised scheme to train attention, that does not require ground truth attention scores, and as such is agnostic to a dataset and the choice of a model. We validate the effectiveness of this scheme on our synthetic datasets, as well as on MNIST and on real graph classification benchmarks in which ground truth attention is unavailable and hard to define, namely COLLAB <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, PROTEINS <ref type="bibr" target="#b16">[16]</ref>, and D&amp;D <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We study two variants of GNNs: Graph Convolutional Networks (GCN) <ref type="bibr" target="#b18">[18]</ref> and Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b11">[12]</ref>. One of the main ideas of GIN is to replace the MEAN aggregator over nodes, such as the one in GCN, with a SUM aggregator, and add more fully-connected layers after aggregating neigboring node features. The resulting model can distinguish a wider range of graph structures than previous models [12, <ref type="figure" target="#fig_1">Figure 3</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Thresholding by attention coefficients</head><p>To pool the nodes in a graph using the method from <ref type="bibr" target="#b10">[11]</ref> a predefined ratio r = |P |/N (Eq. 2) must be chosen for the entire dataset. For instance, for r = 0.8 only 80% of nodes are left after each pooling layer. Intuitively, it is clear that this ratio should be different for small and large graphs. Therefore, we propose to choose thresholdα, such that only nodes with attention values α i &gt;α are propagated:</p><formula xml:id="formula_2">Z i = α i X i , ∀i : α i &gt;α ∅, otherwise.<label>(3)</label></formula><p>Note, that dropping nodes from a graph is different from keeping nodes with very small, or even zero, feature values, because a bias is added to node features after the following graph convolution layer affecting features of neighbors. An important potential issue of dropping nodes is the change of graph structure and emergence of isolated nodes. However, in our experiments we typically observe that the model predicts similar α for nearby nodes, so that an entire local neighborhood is pooled or dropped, as opposed to clustering-based methods which collapse each neighborhood to a single node. We provide a quantitative and qualitative comparison in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention subnetwork</head><p>To train an attention model that predicts the coefficients for nodes, we consider two approaches:</p><p>(1) Linear Projection <ref type="bibr" target="#b10">[11]</ref>, where a single layer projection p ∈ R C is trained: α pre = Xp; and (2) DiffPool <ref type="bibr" target="#b9">[10]</ref>, where a separate GNN is trained:</p><formula xml:id="formula_3">α pre = GNN(A, X),<label>(4)</label></formula><p>where A is the adjacency matrix of a graph. In all cases, we use a softmax activation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> instead of tanh in <ref type="bibr" target="#b10">[11]</ref>, because it provides more interpretable results and ecourages sparse outputs: α = softmax(α pre ). To train attention in a supervised or weakly-supervised way, we use the Kullback-Leibler divergence loss (see Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ChebyGIN</head><p>In some of our experiments, the performance of both GCNs and GINs is quite poor and, consequently, it is also hard for the attention subnetwork to learn. By combining GIN with ChebyNet <ref type="bibr" target="#b7">[8]</ref>, we propose a stronger model, ChebyGIN. ChebyNet is a multiscale extension of GCN <ref type="bibr" target="#b18">[18]</ref>, so that for the first scale, K = 1, node features are node features themselves, for K = 2 features are averaged over one-hop neighbors, for K = 3 -over two-hop neighbors and so forth. To implement the SUM aggregator in ChebyGIN, we multiply features by node degrees D i = j A ij starting from K = 2.</p><p>We also add more fully-connected layers after feature aggregation as in GIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We introduce the color counting task (COLORS) and the triangle counting task (TRIANGLES) in which we generate synthetic training and test graphs. We also experiment with MNIST images <ref type="bibr" target="#b12">[13]</ref> and three molecule and social datasets. In COLORS, TRIANGLES and MNIST tasks ( <ref type="figure">Figure 1</ref>), we assume to know ground truth attention, i.e. for each node i we heuristically define its importance in solving the task correctly, α GT i ∈ [0, 1], which is necessary to train (in the supervised case) and evaluate our attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>COLORS. We introduce the color counting task. We generate random graphs where features for each node are assigned to one of the three one-hot values (colors): [1,0,0] (red), [0,1,0] (green), [0,0,1] (blue). The task is to count the number of green nodes, N green . This is a trivial task, but it lets us study the influence of initialization of the attention model p ∈ R 3 on the training dynamics. In this task, graph structure is unimportant and edges of graphs act like a medium to exchange node features. Ground truth attention is α GT i = 1/N green , when i corresponds to green nodes and α GT i = 0 otherwise. We also extend this dataset to higher n-dimensional cases p ∈ R n to study how model performance changes with n. In these cases, node features are still one-hot vectors and we classify the number of nodes where the second feature is one.</p><p>TRIANGLES. Counting the number of triangles in a graph is a well-known task which can be solved analytically by computing trace(A 3 )/6, where A is an adjacency matrix. This task turned out to be hard for GNNs, so we add node degree features as one-hot vectors to all graphs, so that the model can exploit both graph structure and features. Compared to the COLORS task, here it is more challenging to study the effect of initializing p, but we can still calculate ground truth attention as α GT i = T i / i T i , where T i is the number of triangles that include node i, so that α GT i = 0 for nodes that are not part of triangles.</p><p>MNIST-75SP. MNIST <ref type="bibr" target="#b12">[13]</ref> contains 70k grayscale images of size 28×28 pixels. While each of 784 pixels can be represented as a node, we follow <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> and consider an alternative approach to highlight the ability of GNNs to work on irregular grids. In particular, each image can be represented as a small set of superpixels without losing essential class-specific information (see <ref type="figure">Figure 2</ref>). We compute SLIC <ref type="bibr" target="#b21">[21]</ref> superpixels for each image and build a graph, in which each node corresponds to a superpixel with node features being pixel intensity values and coordinates of their centers of masses. We extract N ≤ 75 superpixels, hence the dataset is denoted as MNIST-75SP. Edges are formed based on spatial distance between superpixel centers as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Eq. 8]</ref>. Each image depicts a handwritten digit from 0 to 9 and the task is to classify the image. Ground truth attention is considered to be α GT i = 1/N nonzero for superpixels with nonzero intensity, and N nonzero is the total number of such superpixels. The idea is that only nonzero superpixels determine the digit class.</p><p>Molecule and social datasets. We extend our study to more practical cases, where ground truth attention is not available, and experiment with protein datasets: PROTEINS <ref type="bibr" target="#b16">[16]</ref> and D&amp;D <ref type="bibr" target="#b17">[17]</ref>, and a scientific collaboration dataset, COLLAB <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. These are standard graph classification benchmarks. A standard way to evaluate models on these datasets is to perform 10-fold cross-validation and report average accuracy <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b9">10]</ref>. In this work, we are concerned about a model's ability to generalize to larger and more complex or noisy graphs, therefore, we generate splits based on the number of nodes. For instance, for PROTEINS we train on graphs with N ≤ 25 nodes and test on graphs with 6 ≤ N ≤ 620 nodes (see <ref type="table">Table 2</ref> for details about splits of other datasets and results).</p><p>A detailed description of tasks and model hyperparameters is provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalization to larger and noisy graphs</head><p>One of the core strengths of attention is that it makes it easier to generalize to unseen, potentially more complex and/or noisy, inputs by reducing them to better resemble certain inputs in the training set. To examine this phenomenon, for COLORS and TRIANGLES tasks we add test graphs that can be several times larger (TEST-LARGE) than the training ones. For COLORS we further extend it by adding unseen colors to the test set (TEST-LARGEC) in the format [c 1 , c 2 , c 3 , c 4 ], where c i = 0 for i = 2 if c 2 = 1 and c i ∈ [0, 1] for i = 2 if c 2 = 0, i.e. there is no new colors that have nonzero values in a green channel. This can be interpreted as adding mixtures of red, blue and transparency channels, with nine possible colors in total as opposed to three in the training set ( <ref type="figure">Figure 2</ref>).</p><formula xml:id="formula_4">COLORS TRAIN (N ≤ 25) TEST-ORIG (N ≤ 25) TEST-LARGE (25 &lt; N ≤ 200) TEST-LARGEC (25 &lt; N ≤ 200) TRIANGLES 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 TRAIN (N ≤ 25) TEST-ORIG (N ≤ 25) TEST-LARGE (25 &lt; N ≤ 100) MNIST-75SP TRAIN(N = 64) TEST-ORIG(N = 63) TEST-NOISY(N = 63) TEST-NOISYC(N = 63) Figure 2:</formula><p>Examples from training and test sets. For COLORS, the correct label is N green = 4 in all cases; for TRIANGLES N tri = 3 and color intensities denote ground truth attention values α GT . The range of the number of nodes, N , is shown in each case. For MNIST-75SP, we visualize graphs for digit 7 by assigning an average intensity value to all pixels within a superpixel. Even though superpixels have certain shapes and borders between each other (visible only on noisy graphs), we feed only superpixel intensities and coordinates of their centers of masses to our GNNs.</p><p>Neural networks (NNs) have been observed to be brittle if they are fed with test samples corrupted in a subtle way, i.e. by adding a noise <ref type="bibr" target="#b23">[23]</ref> or changing a sample in an adversarial way <ref type="bibr" target="#b24">[24]</ref>, such that a human can still recognize them fairly well. To study this problem, test sets of standard image benchmarks have been enlarged by adding corrupted images <ref type="bibr" target="#b25">[25]</ref>.</p><p>Graph neural networks, as a particular case of NNs, inherit this weakness. The attention mechanism, if designed and trained properly, can improve a net's robustness by attending to only important and ignoring misleading parts (nodes) of data. In this work, we explore the ability of GNNs with and without attention to generalize to noisy graphs and unseen node features. This should help us to understand the limits of GNNs, and potentially NNs in general, with attention and conditions when it succeedes and when it does not. To this end, we generate two additional test sets for MNIST-75SP.</p><p>In the first set, TEST-NOISY, we add Gaussian noise, drawn from N (0, 0.4), to superpixel intensity features, i.e. the shape and coordinates of superpixels are the same as in the original clean test set. In the second set, TEST-NOISY-C, we colorize images by adding two more channels and add independent Gaussian noise, drawn from N (0, 0.6), to each channel ( <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network architectures and training</head><p>We build 2 layer GNNs for COLORS and 3 layer GNNs for other tasks with 64 filters in each layer, except for MNIST-75SP where we have more filters. Our baselines are GNNs with global sum or max pooling (gpool), DiffPool <ref type="bibr" target="#b9">[10]</ref> and top-k pooling <ref type="bibr" target="#b10">[11]</ref>. We add two layers of our pooling for TRIANGLES, each of which is a GNN with 3 layers and 32 filters (Eq. 4); whereas a single pooling layer in the form of vector p is used in other cases. We train all models with Adam <ref type="bibr" target="#b26">[26]</ref>, learning rate 1e-3, batch size 32, weight decay 1e-4 (see the Appendix for details).</p><p>For COLORS and TRIANGLES we minimize the regression loss (MSE) and cross entropy (CE) for other tasks, denoted as L M SE/CE . For experiments with supervised and weakly-supervised (described below in Section 3.4) attention, we additionally minimize the Kullback-Leibler (KL) divergence loss between ground truth attention α GT and predicted coefficients α. The KL term is weighted by scale β, so that the total loss for some training graph with N nodes becomes:</p><formula xml:id="formula_5">L = L M SE/CE + β N i α GT i log( α GT i α i ).<label>(5)</label></formula><p>We repeat experiments at least 10 times and report an average accuracy and standard deviation in <ref type="table" target="#tab_0">Tables 1 and 2</ref>. For COLORS we run experiments 100 times, since we observe larger variance. In <ref type="table" target="#tab_0">Table 1</ref> we report results on all test subsets independently. In all other experiments on COLORS, TRIANGLES and MNIST-75SP, we report an average accuracy on the combined test set. For COLLAB, PROTEINS and D&amp;D, we run experiments 10 times using splits described in Section 3.1.</p><p>The only hyperparameters that we tune in our experiments are thresholdα in our method (Eq. 3), ratio r in top-k (Eq. 2) and β in Eq. 5. For synthetic datasets, we tune them on a validation set generated in the same way as TEST-ORIG. For MNIST-75SP, we use part of the training set. For COLLAB, PROTEINS and D&amp;D, we tune them using 10-fold cross-validation on the training set.</p><p>Attention correctness. We evaluate attention correctness using area under the ROC curve (AUC) as an alternative to other methods, such as <ref type="bibr" target="#b27">[27]</ref>, which can be overoptimistic in some extreme cases, such as when all attention is concentrated in a single node or attention is uniformly spread over all nodes. AUC allows us to evaluate the ranking of α instead of their absolute values. Compared to ranking metrics, such as rank correlation, AUC enables us to directly choose a pooling threshold α from the ROC curve by finding a desired balance between false-positives (pooling unimportant nodes) and false-negatives (dropping important nodes).</p><p>To evaluate attention correctness of models with global pooling, we follow the idea from convolutional neural networks <ref type="bibr" target="#b28">[28]</ref>. After training a model, we remove node i ∈ [1, N ] and compute an absolute difference from prediction y for the original graph:</p><formula xml:id="formula_6">α W S i = |y i − y| N j=1 |y j − y| ,<label>(6)</label></formula><p>where y i is a model's prediction for the graph without node i. While this method shows surprisingly high AUC in some tasks, it is not built-in in training and thus does not help to train a better model and only implicitly interprets a model's prediction ( <ref type="figure" target="#fig_3">Figures 5 and 7</ref>). However, these results inspired us to design a weakly-supervised method described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Weakly-supervised attention supervision</head><p>Although for COLORS, TRIANGLES and MNIST-75SP we can define ground truth attention, so that it does not require manual labeling, in practice it is usually not the case and such annotations are hard to define and expensive, or even unclear how to produce. Based on results in <ref type="table" target="#tab_0">Table 1</ref>, supervision of attention is necessary to reveal its power. Therefore, we propose a weakly-supervised approach, agnostic to the choice of a dataset and model, that does not require ground truth attention labels, but can improve a model's ability to generalize. Our approach is based on generating attention coefficients α W S i (Eq. 6) and using them as labels to train our attention model with the loss defined in Eq 5. We apply this approach to COLORS, TRIANGLES and MNIST-75SP and observe peformance and robustness close to supervised models. We also apply it to COLLAB, PROTEINS and D&amp;D, and in all cases we are able to improve results compared to unsupervised attention.</p><p>Training weakly-supervised models. Assume we want to train model A with "weak-sup" attention on a dataset without ground truth attention. We first need to train model B that has the same architecture as A, but does not have any attention/pooling between graph convolution layers. So, model B has only global pooling. After training B with the L M SE/CE loss, we need to evaluate training graphs on B in the same way as during computation of α W S in Eq. 6. In particular, for each training graph G with N nodes, we first make a prediction y for the entire G. Then, for each i ∈ [1, N ], we remove node i from G, and feed this reduced graph with N − 1 nodes to model B recording the model's prediction y i . We then use Eq. 6 to compute α W S based on y and y i . Now, we can train A and use α W S instead of ground truth α GT in Eq. 5 to optimize both MSE/CE and KL losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of results</head><p>In this work, we aim to better understand attention and generalization in graph neural networks, and, based on our empirical findings, below we provide our analysis for the following questions.</p><p>How powerful is attention over nodes in GNNs? Our results on the COLORS, TRIANGLES and MNIST-75SP datasets suggest that the main strength of attention over nodes in GNNs is the ability to generalize to more complex or noisy graphs at test time. This ability essentially transforms a model that fails to generalize into a fairly robust one. Indeed, a classification accuracy gap for COLORS-LARGEC between the best model without supervised attention (GIN with global pooling) and a  similar model with supervised attention (GIN, sup) is more than 60%. For TRIANGLES-LARGE this gap is 18% and for MNIST-75SP-NOISY it is more than 12%. This gap is even larger if compared to upper bound cases indicating that our supervised models can be further tuned and improved. Models with supervised or weakly-supervised attention also have a more narrow spread of results ( <ref type="figure" target="#fig_1">Figure 3</ref>). The nodes that should be pooled according to our ground truth prior, must have larger attention values α. However, in the unsupervised cases, only the model with an optimal initialization (c) reaches a high accuracy, while other models (a,b) are stuck in a suboptimal state and wrong nodes are pooled, which degrades performance. In these experiments, we train models longer to see if they can recover from a bad initialization.</p><p>What are the factors influencing performance of GNNs with attention? We identify three key factors influencing performance of GNNs with attention: initialization of the attention model (i.e. vector p or GNN in Eq. 4), strength of the main GNN model (i.e. the model that actually performs classification), and finally other hyperparameters of the attention and GNN models.</p><p>We highlight initialization as the critical factor. We ran 100 experiments on COLORS with random initializations <ref type="figure" target="#fig_1">(Figure 3</ref>, (a-e)) of the vector p and measured how performance of both attention and classification is affected depending on how close (in terms of cosine similarity) the initialized p was to the optimal one, p = [0, 1, 0]. We disentangle the dependency between the classification accuracy and cos. sim. into two functions to make the relationship clearer ( <ref type="figure" target="#fig_1">Figure 3, (a, c)</ref>). Interestingly, we found that classification accuracy depends exponentially on attention correctness and becomes close to 100% only when attention is also close to being perfect. In the case of slightly worse attention, even starting from 99%, classification accuracy drops significantly. This is an important finding that can also be valid for other more realistic applications. In the TRIANGLES task we only partially confirm this finding, because our attention models could not achieve AUC high enough to boost classification. However, by observing the upper bound results obtained by training with ground truth attention, we assume that this boost potentially should happen once attention becomes accurate enough.  Why is the variance of some results so high? In <ref type="table" target="#tab_0">Table 1</ref> we report high variance of results, which is mainly due to initialization of the attention model as explained above. This variance is also caused by initialization of other trainable parameters of a GNN, but we show that once the attention model is perfect, other parameters can recover from a bad initialization leading to better results. The opposite, however, is not true: we never observed recovery of a model with poorly initialized attention <ref type="figure" target="#fig_2">(Figure 4)</ref>.</p><p>How top-k compares to our threshold-based pooling method? Our method to attend and pool nodes (Eq. 3) is based on top-k pooling <ref type="bibr" target="#b10">[11]</ref> and we show that the proposed threshold-based pooling is superior in a principle way. When we use supervised attention our results are better by more than 40% on COLORS-LARGEC, by 9% on TRIANGLES-LARGE and by 3% on MNIST-75SP. In <ref type="figure" target="#fig_1">Figure 3  ((a,b)</ref>-zoomed) we show that GIN and ChebyGIN models with supervised top-k pooling never reach an average accuracy of more than 80% as opposed to our method which reaches 100% in many cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How results change with increase of attention model input dimensionality or capacity?</head><p>We performed experiments using ChebyGIN-h -a model with higher dimensionality of an input to the attention model (see the Appendix for details). In such cases, it becomes very unlikely to initialize it in a way close to optimal <ref type="figure" target="#fig_1">(Figure 3</ref>, (c-e)), and attention accuracy is concentrated in the 60-80% region. Effect of the attention model of such low accuracy is neglible or even harmful, especially on the large and noisy graphs. We also experimented with a deeper attention model (ChebyGIN-h), i.e. a 2 layer fully-connected layer with 32 hidden units for COLORS and MNIST-75SP, and a deeper GNN (Eq. 4) for TRIANGLES. This has a positive effect overall, except for TRIANGLES, where our attention models were already deep GNNs.</p><p>Can we improve initialization of attention? In all our experiments, we initialize p from the Normal distribution, N (0, 1). To verify if the performance can be improved by choosing another distribution, we evaluate GIN and GCN models on a wide range of random distributions, Normal N (0, σ) and Uniform U (−σ, σ), by varying scale σ ( <ref type="figure">Figure 6</ref>). We found out that for unsupervised training  <ref type="figure">Figure 6</ref>: Influence of distribution parameters used to initialize the attention model p in the COL-ORS task with n = 3 dimensional features. We show points corresponding to the commonly used initialization strategies of Xavier <ref type="bibr" target="#b29">[29]</ref> and Kaiming <ref type="bibr" target="#b29">[29]</ref>. ( <ref type="figure">Figure 6, (a)</ref>), larger initial values and the Normal distribution should be used to make it possible to converge to an optimal solution, which is still unlikely and greatly depends on cosine similarity with GT attention <ref type="figure">(Figure 6, (d,e)</ref>). For supervised and "weak-sup" attention, smaller initial weights and either the Normal or Uniform distribution should be used ( <ref type="figure">Figure 6, (b,c)</ref>). <ref type="table">Table 2</ref>: Results on the social (COLLAB) and molecule (PROTEINS and D&amp;D) datasets. We use 3 layer GCNs <ref type="bibr" target="#b18">[18]</ref> or ChebyNets <ref type="bibr" target="#b7">[8]</ref> (see Appendix for architecture details). Dataset subscripts denote the maximum number of nodes in the training set according to our splits (Section 3.1). What is the recipe for more powerful attention GNNs? We showed that GNNs with supervised training of attention are significantly more accurate and robust, although in case of a bad initialization it can take a long time to reach the performance of a better initialization. However, supervised attention is often infeasible. We suggested an alternative approach based on weakly-supervised training and validated it on our synthetic <ref type="table" target="#tab_0">(Table 1</ref>) and real ( <ref type="table">Table 2)</ref> datasets. In case of COLORS, TRI-ANGLES and MNIST-75SP we can compare to both unsupervised and supervised models and conclude that our approach shows performance, robustness and relatively low variation (i.e. sensitivity to initialization) similar to supervised models and much better than unsupervised models. In case of COLLAB, PROTEINS and D&amp;D we can only compare to unsupervised and global pooling models and confirm that our method can be effectively employed for a wide diversity of graph classification tasks and attends to more relevant nodes ( <ref type="figure" target="#fig_3">Figures 5 and 7)</ref>. Tuning the distribution and scale σ for the initialization of attention can further improve results. For instance, on PROTEINS for the weakly-supervised case, we obtain 76.4% as opposed to 76.2%.</p><p>GLOBAL POOL UNSUP UNSUP POOLED WEAK-SUP WEAK-SUP POOLED COLLAB35 PROTEINS25 D&amp;D200 <ref type="figure">Figure 7</ref>: Qualitative results. In COLLAB, a graph represents an ego-network of a researcher, therefore center nodes are important. In PROTEINS and D&amp;D, a graph is a protein and nodes are amino acids, so it is important to attend to a connected chain of amino acids to distinguish an enzyme from a non-enzyme protein. Our weakly-supervised method attends to and pools more relevant nodes compared to global and unsupervised models, leading to better classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown that learned attention can be extremely powerful in graph neural networks, but only if it is close to optimal. This is difficult to achieve due to the sensitivity of initialization, especially in the unsupervised setting where we do not have access to ground truth attention. Thus, we have identified initialization of attention models for high dimensional inputs as an important open issue. We also show that attention can make GNNs more robust to larger and noisy graphs, and that the weakly-supervised approach proposed in our work brings advantages similar to the ones of supervised models, yet at the same time can be effectively applied to datasets without annotated attention.  <ref type="figure">Figure 8</ref>: Influence of initialization on training dynamics for COLORS using GIN trained in the supervised ways. In the supervised cases, models converge to a perfect accuracy and initialization only affects the speed of convergence. In these experiments, we train models longer to see if they can recover from a bad initialization. For the unsupervised cases, see <ref type="figure" target="#fig_2">Figure 4</ref>.  <ref type="figure">Figure 9</ref>: Influence of distribution parameters used to initialize the attention model p in the COL-ORS task with n = 16 dimensional features and the GIN model. We show points corresponding to the commonly used initialization strategies of (Xavier <ref type="bibr" target="#b29">[29]</ref>) and (Kaiming <ref type="bibr" target="#b29">[29]</ref>). (a-c) Shaded areas show range, bars show ±1 std. For n = 3 see <ref type="figure">Figure 6</ref>. For the GCN model we observe similar trends, but with lower accuracies. <ref type="table">Table 3</ref>: Additional results on three tasks for different test subsets. ChebyGIN-d -deeper attention model. ChebyGIN-h -higher dimensionality of the input fed to the attention model (see <ref type="table" target="#tab_4">Table 4</ref> for architectures). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COLORS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Additional analysis</head><p>How results differ depending on to which layer we apply the attention model? When an attention model is attached to deeper layers (as we do for TRIANGLES and MNIST-75SP), the signal that it receives is much stronger compared to the first layers, which positively influences overall performance. But in terms of computational cost, it is desirable to attach an attention model closer to the input layer to reduce graph size in the beginning of a forward pass. Using this strategy is also more reasonable when we know that attention weights can be determined solely by input features (as we do in our COLORS task), or when the goal is to interpret model's predictions. In contrast, deeper features contain information about a large neighborhood of nodes, so importance of a particular node represents the importance of an entire neighborhood making attention less interpretable.</p><p>Why is initialization of attention important? One of the reasons that initialization is so important is because training GNNs with attention is a chicken or the egg sort of problem. In order to attend to important nodes, the model needs to have a clear understanding of the graph. Yet, in order to gain that level of understanding, the model needs strong attention to avoid focusing on noisy nodes. During training, the attention model predicts attention coefficients α and they might be wrong, especially at the beginning of training, but the rest of the GNN model assumes those predictions to be correct and updates its parameters according to those α. This problem is revealed by taking the gradient of an attention function (Eq. 1): Z = α X, where X = f (w, ·) are node features, and f is some differentiable function with parameters w used to propagate node features:</p><p>∂Z ∂w = ∂Z ∂f ∂f ∂w = α ∂f ∂w . Gradients ∂Z ∂w , that are used to update parameters w in gradient descent, reinforce potentially wrong predictions α, since they depend on α, and the model solution can diverge from the optimal one, which we observe in <ref type="figure" target="#fig_2">Figure 4 (a,b)</ref>. Hence, the performance of such a model largely depends on the initial state, i.e. how accurate were α after the first forward pass. Default initialization N (0, 1) U (−a, a) for linear layers according to <ref type="bibr" target="#b29">[29]</ref> in PyTorch N (0, 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dataset statistics and model hyperparameters</head><p>Optimal weights of attention model collinear to p = [0, 1, 0] Training params 100 epochs (lr decay after 90) 2 Models with attn: 300 epochs (lr decay after 280) 100 epochs (lr decay after 85 and 95 epochs) 30 epochs (lr decay after 20 and 25 epochs) <ref type="bibr" target="#b2">3</ref> We found that using the SUM aggregator and 2 layer MLPs is not necessary for COLORS and MNIST-75SP, since the tasks are relatively easy and the standard ChebyNet models performed comparably. For MNIST-75SP, the SUM aggregator and 2 layer MLPs were also unstable during training. <ref type="bibr" target="#b3">4</ref> Since perfect attention weights can be predicted solely based on input features. <ref type="bibr" target="#b4">5</ref> Attention applied to a hidden layer receives a stronger signal compared when applied to the input layer, which improves results and makes it unnecessary to the use a GNN to predict attention weights as we do for TRIANGLES. <ref type="bibr" target="#b5">6</ref> For supervised and weakly-supervised models, we found it useful to set α GT i = 0 for nodes with superpixel intensity smaller than 0.5. <ref type="table">Table 5</ref>: Dataset statistics and model hyperparameters for experiments with unavailable ground truth attention. Dataset subscripts denote the maximum number of nodes in the training set according to our splits. * In COLLAB nodes do not have any features and a common practice is to add one-hot node degree features, in the same way as we do for TRIANGLES. The range of node degrees is from 0 to 491, hence the input dimensionality is 492. Results are reported after repeating the experiments 100 times: 10 seeds defining train/test splits × 10 seeds defining model parameters. Hyperparametersα and β are chosen based on 10-fold cross-validation on the training sets. Since the training sets are small in these datasets, it is challenging to tune hyperparameters this way. Therefore, in some cases, we adopt a strategy as in <ref type="bibr" target="#b11">[12]</ref> and fix hyperparameters for all folds. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Disentangling factors influencing attention and classification accuracy for COLORS (a-e) and TRIANGLES (f). Accuracies are computed over all test subsets. Notice the exponential growth of classification accuracy depending on attention correctness (a,b), see zoomed plots (a)-zoomed, (b)-zoomed for cases when attention AUC&gt;95%. (d) Probability of a good initialization is estimated as the proportion of cases when cosine similarity &gt; 0.5; error bars indicate standard deviation. (c-e) show results using a higher dimensional attention model, p ∈ R n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>attention coeff., bad initialization (cos. sim.=-0.75) good initialization (cos. sim.=0.75) optimal initialization (cos. sim.=1.00) Influence of initialization on training dynamics for COLORS using GIN trained in the unsupervised way. For the supervised cases, see the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative analysis. For MNIST-75SP (on the left) we show examples of input test images (top row), results of DiffPool<ref type="bibr" target="#b9">[10]</ref> (second row), attention weights α W S generated using a model with global pooling based on Eq. 6 (third row), and α predicted by our weakly-supervised model (bottom row). Both our attention-based pooling and DiffPool can be strong and interpretable depending on the task, but in our tasks DiffPool was inferior (see the Appendix). For TRIANGLES (on the right) we show an example of a test graph with N = 93 nodes with six triangles and the results of pooling based on ground truth attention weights α GT (top row); in the bottom row we show attention weights predicted by our weakly-supervised model and results of our threshold-based pooling (Eq. 3). Note that during training, our model has not encountered noisy images (MNIST-75SP) nor graphs larger than with N = 25 nodes (TRIANGLES).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a-c) Shaded areas show range, bars show ±1 std. For n = 16 see the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. accuracy of "should be pooled" nodes of "should be dropped" nodes Ratio of pooled nodes, r ., bad initialization (cos. sim.=-0.75) good initialization (cos. sim.=0.75) optimal initialization (cos. sim.=1.00)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>=</head><label></label><figDesc>Ti/ i Ti, Ti is the number of triangles that in-, where i -indices of superpixels (nodes) with nonzero intensity, Nnonzero -total number of such superpixels; the input instead of 4 128 filters in the first layer instead of 64 32 filters in the first layer instead of 4 Optimal threshold,α chosen in the range from 0.0001 to 0.1 (usually values around 1/N are the best) r chosen in the range from 0.05 to 1.0 with step 0.02-0.05 (usually values close to 1.0 are the best) Example of used r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on three tasks for different test subsets. ± denotes standard deviation, not shown in case of small values (large values are explained in Section 4). ATTN denotes attention accuracy in terms of AUC and is computed for the combined test set. The best result in each column (ignoring upper bound results) is bolded. denotes poor results with relatively low accuracy and/or high variance;denotes failed cases with accuracy close to random and/or extremely high variance. † For COLORS and MNIST-75SP, ChebyNets are used instead of ChebyGINs as described in the Appendix.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">COLORS</cell><cell></cell><cell cols="2">TRIANGLES</cell><cell></cell><cell cols="2">MNIST-75SP</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">ORIG LARGE LARGEC ATTN ORIG LARGE ATTN ORIG</cell><cell>NOISY</cell><cell>NOISYC ATTN</cell></row><row><cell>Global</cell><cell>pool</cell><cell>GCN GIN ChebyGIN  †</cell><cell cols="3">97 96±10 71±22 26±11 72±15 20±3 100 93±12 15±7</cell><cell>99.6 99.2 99.8</cell><cell>46±1 23±1 50±1 22±1 66±1 30±1</cell><cell>79 77 79</cell><cell>78.3±2 87.6±3 97.4</cell><cell>38±4 55±11 80±12</cell><cell>36±4 51±12 79±11</cell><cell>72±2 71±5 72±3</cell></row><row><cell cols="2">Unsuperv.</cell><cell cols="4">GIN, top-k GIN, ours ChebyGIN  † , top-k 100 99.6 94±18 13±7 17±4 11±7 ChebyGIN  † , ours 80±30 16±10 11±6 9±3 11±6 6±6</cell><cell cols="2">75±6 72±15 47±3 20±2 47±2 18±1 79±20 64±5 25±2 67±31 67±3 26±2</cell><cell cols="2">63±5 86±6 68±3 82.6±8 76±6 92.9±4 77±4 94.6±3</cell><cell>59±26 51±28 68±26 80±23</cell><cell>55±23 47±24 67±25 77±22</cell><cell>65±34 58±31 52±37 78±31</cell></row><row><cell cols="2">Supervised</cell><cell cols="4">GIN, topk GIN, ours ChebyGIN  † , topk 100 87±1 39±18 28±8 100 96±9 89±18 86±15 31±15 ChebyGIN  † , ours 100 94±8 75±17</cell><cell>99.9 99.8 99.8 99.8</cell><cell>49±1 20±1 49±1 22±1 83±1 39±1 88±1 48±1</cell><cell cols="4">88 76±1 90.9±0.4 85.0±1 90.5±1 85.5±2 97 95.1±0.3 90.6±0.8 83±16 79±5 80±3 96 95.4±0.2 92.3±0.4 86±16</cell><cell>99.3 99.3 100 100</cell></row><row><cell>Weak</cell><cell>sup.</cell><cell cols="2">ChebyGIN  † , ours 100</cell><cell>90±6</cell><cell>73±14</cell><cell>99.9</cell><cell>68±1 30±1</cell><cell>88</cell><cell cols="2">95.8±0.4 88.8±4</cell><cell>86±9</cell><cell>96.5±1</cell></row><row><cell>Upper</cell><cell>bound</cell><cell>GIN ChebyGIN  †</cell><cell>100 100</cell><cell>100 100</cell><cell>100 100</cell><cell>100 100</cell><cell cols="2">94±1 85±2 99.8 99.4±1 100 100</cell><cell cols="3">93.6±0.4 90.8±1 96.9±0.1 94.8±0.3 95.1±0.3 100 90.8±1 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics and model hyperparameters for our controlled environment experiments. Hyperparametersα and r are chosen based on the validation sets.</figDesc><table><row><cell></cell><cell>COLORS</cell><cell>TRIANGLES</cell><cell>MNIST-75SP</cell></row><row><cell># train graphs</cell><cell>500</cell><cell>30,000</cell><cell>60,000</cell></row><row><cell># val graphs</cell><cell>2,500</cell><cell>5,000</cell><cell>5,000 (from the training set)</cell></row><row><cell># test graphs ORIG</cell><cell>2,500</cell><cell>5,000</cell><cell>10,000</cell></row><row><cell># test graphs LARGE/NOISY</cell><cell>2,500</cell><cell>5,000</cell><cell>10,000</cell></row><row><cell># test graphs LARGEC/NOISYC</cell><cell>2,500</cell><cell>−</cell><cell>10,000</cell></row><row><cell># classes</cell><cell>11</cell><cell>10</cell><cell>10</cell></row><row><cell># nodes (N ) train/val</cell><cell>4-25</cell><cell>4-25</cell><cell>&lt;=75</cell></row><row><cell># nodes (N ) test</cell><cell>4-200</cell><cell>4-100</cell><cell>&lt;=75</cell></row><row><cell># layers and filters</cell><cell>2 layers, 64 filters in each</cell><cell>3 layers, 64 filters in each</cell><cell>3 layers: 4, 64, 512 filters</cell></row><row><cell>Dropout</cell><cell>0</cell><cell>0</cell><cell>0.5</cell></row><row><cell>Nonlinearity</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell># pooling layers</cell><cell>1</cell><cell>2</cell><cell>1</cell></row><row><cell>READOUT layer</cell><cell>global sum</cell><cell>global max</cell><cell>global max</cell></row><row><cell>GIN aggregator</cell><cell>SUM 2 layer MLP with 256 hid. units</cell><cell>SUM 2 layer MLP with 64 hid. units</cell><cell>SUM 2 layer MLP with 64 hid. units</cell></row><row><cell>ChebyGIN aggregator</cell><cell>MEAN 1 layer MLP 3</cell><cell>SUM 2 layer MLP with 64 hid. units</cell><cell>MEAN 1 layer MLP 3</cell></row><row><cell>ChebyGIN max scale, K</cell><cell>2</cell><cell>7</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>Same arch. as the class. GNN,</cell><cell></cell></row><row><cell>Attention model</cell><cell>p applied to input layer 4</cell><cell>but K = 2 for ChebyGIN,</cell><cell>p applied to hidden layer 5</cell></row><row><cell></cell><cell></cell><cell>applied to hidden layer (Eq. 4)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In DiffPool, the number of clusters returned after pooling must be fixed before we start training. While this number can be smaller or larger than the number of nodes in the graph, we still did not find it beneficial to use DiffPool with a number of clusters larger than 4 (the minimal number of nodes in training graphs). Part of the issue is that we train on small graphs and test on large ones and it is hard to choose the number of clusters suitable for graphs of all sizes.<ref type="bibr" target="#b1">2</ref> Fewer than for attention models, since they converged faster.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. The authors also acknowledge support from the Canadian Institute for Advanced Research and the Canada Foundation for Innovation. We are also thankful to Angus Galloway for feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04757</idno>
		<title level="m">Attentive explanations: Justifying decisions and pointing to the evidence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive cross-modal paratope prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Sormanni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In UAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph classification using structural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention models in graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyee</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07984</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectralnet: Spectral clustering using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph evolution: Densification and shrinking diameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new space for comparing graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 26th international conference on computer communication and networks (ICCCN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
