<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Practical Full Resolution Learned Lossless Image Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
							<email>mentzerf@vision.ee.ethz.chaeirikur@vision.ee.ethz.chmichaelt@nari.ee.ethz.chtimofter@vision.ee.ethz.chvangool@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Timofte</forename><surname>Luc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">Practical Full Resolution Learned Lossless Image Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the first practical learned lossless image compression system, L3C, and show that it outperforms the popular engineered codecs, PNG, WebP and JPEG2000. At the core of our method is a fully parallelizable hierarchical probabilistic model for adaptive entropy coding which is optimized end-to-end for the compression task. In contrast to recent autoregressive discrete probabilistic models such as PixelCNN, our method i) models the image distribution jointly with learned auxiliary representations instead of exclusively modeling the image distribution in RGB space, and ii) only requires three forward-passes to predict all pixel probabilities instead of one for each pixel. As a result, L3C obtains over two orders of magnitude speedups when sampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN). Furthermore, we find that learning the auxiliary representation is crucial and outperforms predefined auxiliary representations such as an RGB pyramid significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since likelihood-based discrete generative models learn a probability distribution over pixels, they can in theory be used for lossless image compression <ref type="bibr" target="#b39">[40]</ref>. However, recent work on learned compression using deep neural networks has solely focused on lossy compression <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref>. Indeed, the literature on discrete generative models <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref> has largely ignored the application as a lossless compression system, with neither bitrates nor runtimes being compared with classical codecs such as PNG <ref type="bibr" target="#b30">[31]</ref>, WebP <ref type="bibr" target="#b46">[47]</ref>, JPEG2000 <ref type="bibr" target="#b37">[38]</ref>, and FLIF <ref type="bibr" target="#b38">[39]</ref>. This is not surprising as (lossless) entropy coding using likelihood-based discrete generative models amounts to a decoding complexity essentially identical to the sampling complexity of the model, which renders many of the recent state-of-the-art autoregressive models such as PixelCNN <ref type="bibr" target="#b45">[46]</ref>, PixelCNN++ <ref type="bibr" target="#b34">[35]</ref>, and Multiscale-PixelCNN <ref type="bibr" target="#b31">[32]</ref> impractical, requiring minutes or hours on a GPU to generate moderately large images, typi- </p><formula xml:id="formula_0">D (1) E (1) D (2) D (3) E (3)</formula><p>x z <ref type="bibr" target="#b0">(1)</ref> z <ref type="bibr" target="#b1">(2)</ref> </p><formula xml:id="formula_1">z (3) Q Q Q E (2) f (3) f (2) f (1) p(z (2) |f (3) ) p(z (1) |f (2) ) p(x|f (1) ) Figure 1:</formula><p>Overview of the architecture of L3C. The feature extractors E (s) compute quantized (by Q) auxiliary hierarchical feature representation z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (S) whose joint distribution with the image x, p(x, z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (S) ), is modeled using the non-autoregressive predictors D (s) . The features f (s) summarize the information up to scale s and are used to predict p for the next scale. cally &lt;256 × 256px (see <ref type="table" target="#tab_3">Table 2</ref>). The computational complexity of these models is mainly caused by the sequential nature of the sampling (and thereby decoding) operation, where a forward pass needs to be computed for every single (sub) pixel of the image in a raster scan order.</p><p>In this paper, we address these challenges and develop a fully parallelizeable learned lossless compression system, outperforming the popular classical systems PNG, WebP and JPEG2000.</p><p>Our system (see <ref type="figure" target="#fig_4">Fig. 1</ref> for an overview) is based on a hierarchy of fully parallel learned feature extractors and predictors which are trained jointly for the compression task. Our code is available online 1 . The role of the feature extractors is to build an auxiliary hierarchical feature representation which helps the predictors to model both the image and the auxiliary features themselves. Our experiments show that learning the feature representations is crucial, and heuristic (predefined) choices such as a multiscale RGB pyramid lead to suboptimal performance.</p><p>In more detail, to encode an image x, we feed it through the S feature extractors E (s) and predictors D (s) . Then, we obtain the predictions of the probability distributions p, for both x and the auxiliary features z (s) , in parallel in a single forward pass. These predictions are then used with an adaptive arithmetic encoder to obtain a compressed bitstream of both x and the auxiliary features (Sec. 3.1 provides an introduction to arithmetic coding). However, the arithmetic decoder now needs p to be able to decode the bitstream. Starting from the lowest scale of auxiliary features z (S) , for which we assume a uniform prior, D (S) obtains a prediction of the distribution of the auxiliary features of the next scale, z (S−1) , and can thus decode them from the bitstream. Prediction and decoding is alternated until the arithmetic decoder obtains the image x. The steps are visualized in <ref type="figure">Fig. A4</ref>.</p><p>In practice, we only need to use S = 3 feature extractors and predictors for our model, so when decoding we only need to perform three parallel (over pixels) forward passes in combination with the adaptive arithmetic coding.</p><p>The parallel nature of our model enables it to be orders of magnitude faster for decoding than autoregressive models, while learning enables us to obtain compression rates competitive with state-of-the-art engineered lossless codecs.</p><p>In summary, our contributions are the following:</p><p>• We propose a fully parallel hierarchical probabilistic model, learning both the feature extractors that produce an auxiliary feature representation to help the prediction task, as well as the predictors which model the joint distribution of all variables (Sec. 3).</p><p>• We show that entropy coding based on our nonautoregressive probabilistic model optimized for discrete log-likelihood can obtain compression rates outperforming WebP, JPEG2000 and PNG, the latter by a large margin. We are only marginally outperformed by the stateof-the-art, FLIF, while being conceptually much simpler (Sec. 5.1).</p><p>• At the same time, our model is practical in terms of runtime complexity and orders of magnitude faster than PixelCNNbased approaches. In particular, our model is 5.31 · 10 4 × faster than PixelCNN++ <ref type="bibr" target="#b34">[35]</ref> and 5.06 · 10 2 × faster than the highly speed-optimized MS-PixelCNN <ref type="bibr" target="#b31">[32]</ref> (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Likelihood-Based Generative Models As previously mentioned, essentially all likelihood-based discrete generative models can be used with an arithmetic coder for lossless compression. A prominent group of models that obtain stateof-the-art performance are variants of the auto-regressive PixelRNN/PixelCNN <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref>. PixelRNN and PixelCNN organize the pixels of the image distribution as a sequence and predict the distribution of each pixel conditionally on (all) previous pixels using an RNN and a CNN with masked convolutions, respectively. These models hence require a number of network evaluations equal to the number of predicted sub-pixels 2 (3 ·W ·H). PixelCNN++ <ref type="bibr" target="#b34">[35]</ref> improves on this in various ways, including modeling the joint distribution of each pixel, thereby eliminating conditioning on previous channels and reducing to W · H forward passes. MS-PixelCNN <ref type="bibr" target="#b31">[32]</ref> parallelizes PixelCNN by reducing dependencies between blocks of pixels and processing them in parallel with shallow PixelCNNs, requiring O(log W H) forward passes. <ref type="bibr" target="#b19">[20]</ref> equips PixelCNN with auxiliary variables (grayscale version of the image or RGB pyramid) to encourage modeling of high-level features, thereby improving the overall modeling performance. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> propose autoregressive models similar to PixelCNN/PixelRNN, but they additionally rely on attention mechanisms to increase the receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Engineered Codecs</head><p>The well-known PNG <ref type="bibr" target="#b30">[31]</ref> operates in two stages: first the image is reversibly transformed to a more compressible representation with a simple autoregressive filter that updates pixels based on surrounding pixels, then it is compressed with the deflate algorithm <ref type="bibr" target="#b10">[11]</ref>. WebP <ref type="bibr" target="#b46">[47]</ref> uses more involved transformations, including the use of entire image fragments to encode new pixels and a custom entropy coding scheme. JPEG2000 <ref type="bibr" target="#b37">[38]</ref> includes a lossless mode where tiles are reversibly transformed before the coding step, instead of irreversibly removing frequencies. The current state-of-the-art (non-learned) algorithm is FLIF <ref type="bibr" target="#b38">[39]</ref>. It relies on powerful preprocessing and a sophisticated entropy coding method based on CABAC <ref type="bibr" target="#b32">[33]</ref> called MANIAC, which grows a dynamic decision tree per channel as an adaptive context model during encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Models in Lossy Compression</head><p>In lossy compression, context models have been studied as a way to efficiently losslessly encode the obtained image representations. Classical approaches are discussed in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48]</ref>. Recent learned approaches include <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, where shallow autoregressive models over latents are learned. <ref type="bibr" target="#b4">[5]</ref> presents a model somewhat similar to L3C: Their autoencoder is similar to our fist scale, and the hyper encoder/decoder is similar to our second scale. However, since they train for lossy image compression, their autoencoder predicts RGB pixels directly. Also, they predict uncertainties σ for z <ref type="bibr" target="#b0">(1)</ref> instead of a mixture of logistics. Finally, instead of learning a probability distribution for z <ref type="bibr" target="#b1">(2)</ref> , they assume the entries to be i.i.d. and fit a uni-variate non-parametric density model, whereas in our model, many more stages can be trained and applied recursively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Likelihood Models for Compression</head><p>The objective of continuous likelihood models, such as VAEs <ref type="bibr" target="#b18">[19]</ref> and RealNVP <ref type="bibr" target="#b11">[12]</ref>, where p(x ) is a continuous distribution, is closely related to its discrete counterpart. In particular, by setting x = x + u where x is the discrete image and u is uniform quantization noise, the continuous likelihood of p(x ) is a lower bound on the likelihood of the discrete <ref type="bibr" target="#b39">[40]</ref>. However, there are two challenges for deploying such models for compression. First, the discrete likelihood q(x) needs to be available (which involves a non-trivial integration step). Additionally, the memory complexity of (adaptive) arithmetic coding depends on the size of the domain of the variables of the factorization of q (see Sec. 3.1 on (adaptive) arithmetic coding). Since the domain grows exponentially in the number of pixels in x, unless q is factorizable, it is not feasible to use it with adaptive arithmetic coding.</p><formula xml:id="formula_2">q(x) = E u [p(x )]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lossless Compression</head><p>In general, in lossless compression, some stream of symbols x is given, which are drawn independently and identically distributed (i.i.d.) from a set X = {1, . . . , |X |} according to the probability mass functionp. The goal is to encode this stream into a bitstream of minimal length using a "code", s.t. a receiver can decode the symbols from the bitstream. Ideally, an encoder minimizes the expected bits per symbolL = j∈Xp (j) (j), where (j) is the length of encoding symbol j (i.e., more probable symbols should obtain shorter codes). Information theory provides (e.g., <ref type="bibr" target="#b8">[9]</ref>) the boundL ≥ H(p) for any possible code, where H(p) = E j∼p [− logp(j)] is the Shannon entropy <ref type="bibr" target="#b35">[36]</ref>.</p><p>Arithmetic Coding A strategy that almost achieves the lower bound H(p) (for long enough symbol streams) is arithmetic coding <ref type="bibr" target="#b48">[49]</ref>. <ref type="bibr" target="#b2">3</ref> It encodes the entire stream into a single number a ∈ [0, 1), by subdividing [0, 1) in each step (encoding one symbol) as follows: Let a, b be the bounds of the current step (initialized to a = 0 and b = 1 for the initial interval [0, 1)). We divide the interval [a, b) into |X | sections where the length of the j-th section isp(j)/(b − a). Then we pick the interval corresponding to the current symbol, i.e., we update a, b to be the boundaries of this interval. We proceed recursively until no symbols are left. Finally, we transmit a , which is a rounded to the smallest number of bits s.t. a ≥ a.</p><p>Receiving a together with the knowledge of the number of encoded symbols andp uniquely specifies the stream and allows the receiver to decode.</p><p>Adaptive Arithmetic Coding In contrast to the i.i.d. setting we just described, in this paper we are interested in losslessly encoding the pixels of a natural image, which are known to be heavily correlated and hence not i.i.d. at all. Let x t be the sub-pixels 2 of an image x, andp img (x) the joint distribution of all sub-pixels. We can then consider the factorizationp img (x) = tp (x t |x t−1 , . . . , x 1 ). Now, to encode x, we can consider the sub-pixels x t as our symbol stream and encode the t-th symbol/sub-pixel usingp(x t |x t−1 , . . . , x 1 ). Note that this corresponds to varying thep(j) of the previous paragraph during encoding, and is in general referred to as adaptive arithmetic coding (AAC) <ref type="bibr" target="#b48">[49]</ref>. For AAC the receiver also needs to know the varyingp at every step, i.e., they must either be known a priori or the factorization must be causal (as above) so that the receiver can calculate them from already decoded symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy</head><p>In practice, the exactp is usually unknown, and instead is estimated by a model p. Thus, instead of using length log 1/p(x) to encode a symbol x, we use the suboptimal length log 1/p(x). Then</p><formula xml:id="formula_3">H(p, p) = E j∼p [− log p(j)] = − j∈Xp (j) log p(j)<label>(1)</label></formula><p>is the resulting expected (sub-optimal) bits per symbol, and is called cross-entropy <ref type="bibr" target="#b8">[9]</ref>. Thus, given some p, we can minimize the bitcost needed to encode a symbol stream with symbols distributed according top by minimizing Eq. (1). This naturally generalizes to the non-i.i.d. case described in the previous paragraph by using differentp(x t ) and p(x t ) for each symbol x t and minimizing t H(p(x t ), p(x t )). The following sections describe how a hierarchical causal factorization of p img for natural images can be used to efficiently do learned lossless image compression (L3C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture</head><p>A high-level overview of the architecture is given in <ref type="figure" target="#fig_4">Fig. 1</ref>, while <ref type="figure" target="#fig_1">Fig. 2</ref> shows a detailed description for one scale s. Unlike autoregressive models such as PixelCNN and Pixel-RNN, which factorize the image distribution autoregressively over sub-pixels x t as p(x) = T t=1 p(x t |x t−1 , . . . , x 1 ), we jointy model all the sub-pixels and introduce a learned hierarchy of auxiliary feature representations z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (S) to simplify the modeling task.We fix the dimensions of z (s) to be C×H ×W , where the number of channels C is a hyperparameter (C = 5 in our reported models), and H = H/2 s , W = W/2 s given a H×W -dimensional image. 4 </p><formula xml:id="formula_4">Q C E (s) in E (s+1) in 4Cf s2f 5 s1f 1 s1f 1 s1f 1 E (s) D (s) f (s) p(z (s 1) |f (s) ) z (s) U A⇤ ReLU C (s 1) p ⇡, µ, f (s+1) f (s) + + + +</formula><p>in is the RGB image x normalized to [−1, 1]. All vertical black lines are convolutions, which have C f = 64 filters, except when denoted otherwise beneath. The convolutions are stride 1 with 3×3 filters, except when denoted otherwise above (using sSfF = stride s, filter f ). We add the features f (s+1) from the predictor D (s+1) to those of the first layer of D (s) (a skip connection between scales). The gray blocks are residual blocks, shown once on the right side. C is the number of channels of z (s) , C (s−1) p is the final number of channels, see Sec. 3.4. Special blocks are denoted in red: U is pixelshuffling upsampling <ref type="bibr" target="#b36">[37]</ref>. A * is the "atrous convolution" layer described in Sec. 3.2. We use a heatmap to visualize z (s) , see Sec. A.5. Specifically, we model the joint distribution of the image x and the feature representations z (s) as</p><formula xml:id="formula_6">p(x, z (1) , . . . , z (S) ) = p(x|z (1) , . . . , z (S) ) S s=1 p(z (s) |z (s+1) , . . . , z (S) )</formula><p>where p(z (S) ) is a uniform distribution. The feature representations can be hand designed or learned. Specifically, on one side, we consider an RGB pyramid with z (s) = B 2 s (x), where B 2 s is the bicubic (spatial) subsampling operator with subsampling factor 2 s . On the other side, we consider a learned representation z (s) = F (s) (x) using a feature extractor F (s) . We use the hierarchical model shown in <ref type="figure" target="#fig_4">Fig. 1</ref> using the composition F (s) = Q • E (s) • · · · • E (1) , where the E (s) are feature extractor blocks and Q is a scalar differentiable quantization function (see Sec. 3.3). The D (s) in <ref type="figure" target="#fig_4">Fig. 1</ref> are predictor blocks, and we parametrize E (s) and D (s) as convolutional neural networks.</p><p>Letting z (0) = x, we parametrize the conditional distributions for all s ∈ {0, . . . , S} as</p><formula xml:id="formula_7">p(z (s) |z (s+1) , . . . , z (S) ) = p(z (s) |f (s+1) ), using the predictor features f (s) = D (s) (f (s+1) , z (s) ). 5 Note that f (s+1) summarizes the information of z (S) , . . . , z (s+1) .</formula><p>The predictor is based on the super-resolution architecture from EDSR <ref type="bibr" target="#b22">[23]</ref>, motivated by the fact that our prediction task is somewhat related to super-resolution in that both are dense prediction tasks involving spatial upsampling. We mirror the predictor to obtain the feature extractor, and follow <ref type="bibr" target="#b22">[23]</ref> in not using BatchNorm <ref type="bibr" target="#b15">[16]</ref>. Inspired by the "atrous spatial pyramid pooling" from <ref type="bibr" target="#b5">[6]</ref>, we insert a similar layer at the end of D (s) : In A * , we use three atrous convolutions in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Quantization</head><p>We use the scalar quantization approach proposed in <ref type="bibr" target="#b24">[25]</ref> to quantize the output of E (s) : Given levels L = { 1 , . . . , L } ⊂ R, we use nearest neighbor assignments to quantize each entry z ∈ z (s) as</p><formula xml:id="formula_8">z = Q(z ) := arg min j z − j ,<label>(2)</label></formula><p>but use differentiable "soft quantization"</p><formula xml:id="formula_9">Q(z ) = L j=1 exp(−σ q z − j ) L l=1 exp(−σ q z − l ) j<label>(3)</label></formula><p>to compute gradients for the backward pass, where σ q is a hyperparameter relating to the "softness" of the quantization. For simplicity, we fix L to be L = 25 evenly spaced values in [−1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mixture Model</head><p>For ease of notation, let z (0) = x again. We model the conditional distributions p(z (s) |z (s+1) , . . . , z (S) ) using a generalization of the discretized logistic mixture model with K components proposed in <ref type="bibr" target="#b34">[35]</ref>, as it allows for efficient training: The alternative of predicting logits per (sub-)pixel has the downsides of requiring more memory, causing sparse gradients (we only get gradients for the logit corresponding to the ground-truth value), and does not model that neighbouring values in the domain of p should have similar probability.</p><p>Let c denote the channel and u, v the spatial location. For all scales, we assume the entries of z (s) cuv to be independent across u, v, given f (s+1) . For RGB (s = 0), we define</p><formula xml:id="formula_10">p(x|f (1) ) = u,v p(x 1uv , x 2uv , x 3uv |f (1) ),<label>(4)</label></formula><p>where we use a weak autoregression over RGB channels to define the joint probability distribution via a mixture p m (dropping the indices uv for shorter notation):</p><formula xml:id="formula_11">p(x 1 , x 2 , x 3 |f (1) ) = p m (x 1 |f (1) ) · p m (x 2 |f (1) , x 1 ) · p m (x 3 |f (1) , x 2 , x 1 ).<label>(5)</label></formula><p>We define p m as a mixture of logistic distributions p l (defined in Eq. (10) below). To this end, we obtain mixture weights 6 π k cuv , means µ k cuv , variances σ k cuv , as well as coefficients λ k cuv from f (1) (see further below), and get</p><formula xml:id="formula_12">p m (x 1uv |f (1) ) = k π k 1uv p l (x 1uv |μ k 1uv , σ k 1uv ) p m (x 2uv |f (1) , x 1uv ) = k π k 2uv p l (x 2uv |μ k 2uv , σ k 2uv ) p m (x 3uv |f (1) , x 1uv , x 2uv ) = k π k 3uv p l (x 3uv |μ k 3uv , σ k 3uv ),<label>(6)</label></formula><p>where we use the conditional dependency on previous x cuv to obtain the updated meansμ, as in</p><formula xml:id="formula_13">[35, Sec. 2.2], µ k 1uv = µ k 1uvμ k 2uv = µ k 2uv + λ k αuv x 1uṽ µ k 3uv = µ k 3uv + λ k βuv x 1uv + λ k γuv x 2uv .<label>(7)</label></formula><p>Note that the autoregression over channels in Eq. <ref type="formula" target="#formula_11">(5)</ref> is only used to update the means µ toμ.</p><p>For the other scales (s &gt; 0), the formulation only changes in that we use no autoregression at all, i.e.,μ cuv = µ cuv for all c, u, v. No conditioning on previous channels is needed, and Eqs. (4)-(6) simplify to</p><formula xml:id="formula_14">p(z (s) |f (s+1) ) = c,u,v p m (z (s) cuv |f (s+1) ) (8) p m (z (s) cuv |f (s+1) ) = k π k cuv p l (x cuv |µ k cuv , σ k cuv ).<label>(9)</label></formula><p>For all scales, the individual logistics p l are given as</p><formula xml:id="formula_15">p l (z|µ, σ) = sigmoid((z + b/2 − µ)/σ)− sigmoid((z − b/2 − µ)/σ) .<label>(10)</label></formula><p>Here, b is the bin width of the quantization grid (b = 1 for s = 0 and b = 1/12 otherwise). The edge-cases z = 0 and z = 255 occurring for s = 0 are handled as described in [35,</p><formula xml:id="formula_16">Sec. 2.1].</formula><p>For all scales, we obtain the parameters of p(z (s−1) |f (s) ) from f (s) with a 1×1 convolution that has C (s−1) p output channels (see <ref type="figure" target="#fig_1">Fig. 2</ref>). For RGB, this final feature map must contain the three parameters π, µ, σ for each of the 3 RGB <ref type="bibr" target="#b5">6</ref> Note that in contrast to <ref type="bibr" target="#b34">[35]</ref> we do not share mixture weights π k across channels. This allows for easier marginalization of Eq. (5). channels and K mixtures, as well as λ α , λ β , λ γ for every mixture, thus requiring C (0) p = 3 · 3 · K + 3 · K channels. For s &gt; 0, C (s) p = 3 · C · K, since no λ are needed. With the parameters, we can obtain p(z (s) |f (s+1) ), which has dimensions 3×H×W ×256 for RGB and C×H ×W ×L otherwise (visualized with cubes in <ref type="figure" target="#fig_4">Fig. 1</ref>).</p><p>We emphasize that in contrast to <ref type="bibr" target="#b34">[35]</ref>, our model is not autoregressive over pixels, i.e., z (s) cuv are modelled as independent across u, v given f (s+1) (also for z (0) = x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss</head><p>We are now ready to define the loss, which is a generalization of the discrete logistic mixture loss introduced in <ref type="bibr" target="#b34">[35]</ref>. Recall from Sec. 3.1 that our goal is to model the true joint distribution of x and the representations z (s) , i.e., p(x, z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (s) ) as accurately as possible using our model p(x, z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (s) ). Thereby, the z (s) = F (s) (x) are defined using the learned feature extractor blocks E (s) , and p(x, z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (s) ) is a product of discretized (conditional) logistic mixture models with parameters defined through the f (s) , which are in turn computed using the learned predictor blocks D (s) . As discussed in Sec. 3.1, the expected coding cost incurred by coding x, z <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_17">, . . . , z (s) w.r.t. our model p(x, z (1) , . . . , z (s) ) is the cross entropy H(p, p).</formula><p>We therefore directly minimize H(p, p) w.r.t. the parameters of the feature extractor blocks E (s) and predictor blocks D (s) over samples. Specifically, given N training samples</p><formula xml:id="formula_18">x 1 , . . . , x N , let F (s) i = F (s) (x i ) be the feature representa- tion of the i-th sample. We minimize L(E (1) , . . . , E (S) , D (1) , . . . , D (S) ) = − N i=1 log p x i , F (1) i , . . . , F (S) i = − N i=1 log p x i |F (1) i , . . . , F (S) i · S s=1 p F (s) i |F (s+1) i , . . . , F (S) i = − N i=1 log p(x i |F (1) i , . . . , F (S) i ) + S s=1 log p(F (s) i |F (s+1) i , . . . , F (S) i ) .<label>(11)</label></formula><p>Note that the loss decomposes into the sum of the crossentropies of the different representations. Also note that this loss corresponds to the negative log-likelihood of the data w.r.t. our model which is typically the perspective taken in the generative modeling literature (see, e.g., <ref type="bibr" target="#b45">[46]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Relationship to MS-PixelCNN</head><p>When the auxiliary features z (s) in our approach are restricted to a non-learned RGB pyramid (see baselines in Sec. 4), this is somewhat similar to MS-PixelCNN <ref type="bibr" target="#b31">[32]</ref>. In particular, <ref type="bibr" target="#b31">[32]</ref> combines such a pyramid with upscaling networks which play the same role as the predictors in our architecture. Crucially however, they rely on combining such predictors with a shallow PixelCNN and upscaling one dimension at a time (W ×H→2W ×H→2W ×2H). While their complexity is reduced from O(W H) forward passes needed for PixelCNN <ref type="bibr" target="#b45">[46]</ref> to O(log W H), their approach is in practice still two orders of magnitude slower than ours (see Sec. 5.2). Further, we stress that these similarities only apply for our RGB baseline model, whereas our best models are obtained using learned feature extractors trained jointly with the predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Models We compare our main model (L3C) to two learned baselines: For the RGB Shared baseline (see <ref type="figure" target="#fig_1">Fig. A2</ref>) we use bicubic subsampling as feature extractors, i.e., z (s) = B 2 s (x), and only train one predictor D <ref type="bibr" target="#b0">(1)</ref> . During testing, we obtain multiple z (s) using B and apply the single predictor D <ref type="bibr" target="#b0">(1)</ref> to each. The RGB baseline (see <ref type="figure" target="#fig_0">Fig. A3</ref>) also uses bicubic subsampling, however, we train S = 3 predictors D (s) , one for each scale, to capture the different distributions of different RGB scales. For our main model, L3C, we additionally learn S = 3 feature extractors E (s) . <ref type="bibr" target="#b6">7</ref> Note that the only difference to the RGB baseline is that the representations z (s) are learned. We train all these models until they converge at 700k iterations.</p><p>Datasets We train our models on 362 551 images randomly selected from the Open Images training dataset <ref type="bibr" target="#b20">[21]</ref>. We randomly downscale the images to at least 512 pixels on the longer side to remove potential artifacts from previous compression, discarding images where rescaling does not result in at least 1.25× downscaling. Further, following <ref type="bibr" target="#b4">[5]</ref> we discard high saturation/ non-photographic images, i.e., images with mean S&gt;0.9 or V &gt;0.8 in the HSV color space. We evaluate on 500 images randomly selected from the Open Images validation set, preprocessed like the training set (available on our github 1 ), the 100 images from the commonly used superresolution dataset DIV2K <ref type="bibr" target="#b1">[2]</ref>, as well as on RAISE-1k <ref type="bibr" target="#b9">[10]</ref>, a "real-world image dataset" with 1000 images. We automatically split images into equally-sized crops if they do not fit into GPU memory, and process crops sequentially. Note that this is a bias against our method.</p><p>In order to compare to the PixelCNN literature, we additionally train L3C on the ImageNet32 and ImageNet64 datasets <ref type="bibr" target="#b7">[8]</ref>, each containing 1 281 151 training images and 50 000 validation images, of 32 × 32 resp. 64 × 64 pixels.</p><p>Training We use the RMSProp optimizer <ref type="bibr" target="#b14">[15]</ref>, with a batch size of 30, minimizing Eq. (11) directly (no regularization). We train on 128 × 128 random crops, and apply random horizontal flips. We start with a learning rate λ = 1·10 −4 and decay it by a factor of 0.75 every 5 epochs. On ImageNet32/64, we decay λ every epoch, due to the smaller images.</p><p>Architecture Ablations We find that adding Batch-Norm <ref type="bibr" target="#b16">[17]</ref> slightly degrades performance. Furthermore, replacing the stacked atrous convolutions A * with a single convolution, slightly degrades performance as well. By stopping H×W , the last bottleneck has 5×H/8×W/8 dimensions, each quantized to L = 25 values. Encoding this with a uniform prior amounts to ≈4% of the total bitrate. For the RGB Shared baseline, we apply D (1) 4 times, as only one encoder is trained.  gradients from propagating through the targets of our loss, we get significantly worse performance -in fact, the optimizer does not manage to pull down the cross-entropy of any of the learned representations z (s) significantly.</p><p>We find the choice of σ q for Q has impacts on training: <ref type="bibr" target="#b24">[25]</ref> suggests setting it s.t.Q resembles identity, which we found to be good starting point, but found it beneficial to let σ q be slightly smoother (this yields better gradients for the encoder). We use σ q = 2.</p><p>Additionally, we explored the impact of varying C (number of channels of z (s) ) and the number of levels L and found it more beneficial to increase L instead of increasing C, i.e., it is beneficial for training to have a finer quantization grid.</p><p>Other Codecs We compare to FLIF and the lossless mode of WebP using the respective official implementations <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>, for PNG we use the implementation of Pillow <ref type="bibr" target="#b29">[30]</ref>, and for the lossless mode of JPEG2000 we use the Kakadu implementation <ref type="bibr" target="#b17">[18]</ref>. See Sec. 2 for a description of these codecs. <ref type="table" target="#tab_1">Table 1</ref> shows a comparison of our approach (L3C) and the learned baselines to the other codecs, on our testsets, in terms of bits per sub-pixel (bpsp) <ref type="bibr" target="#b7">8</ref> All of our methods outperform the widely-used PNG, which is at least 43% larger on all datasets. We also outperform WebP and JPEG2000 everywhere by a smaller margin of up to 3.3%. We note that FLIF still marginally outperforms our model but remind the reader of the many hand-engineered highly specialized techniques involved in FLIF (see <ref type="bibr">Section 2)</ref>. In contrast, we use a simple convolutional feed-forward neural network architecture. The <ref type="bibr" target="#b7">8</ref> We follow the likelihood-based generative modelling literature in measuring bpsp; X bits per pixel (bpp) = X/3 bpsp, see also footnote 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Compression</head><p>[bpsp] ImageNet32 Learned L3C (ours) 4.76 PixelCNN <ref type="bibr" target="#b45">[46]</ref> 3.83 MS-PixelCNN <ref type="bibr" target="#b31">[32]</ref> 3.95 PNG 6.42 JPEG2000 6.35 WebP 5.28 FLIF 5.08 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with PixelCNN</head><p>While PixelCNN-based approaches are not designed for lossless image compression, they learn a probability distribution over pixels and optimize for the same log-likelihood objective. Since they thus can in principle be used inside a compression algorithm, we show a comparison here. <ref type="table" target="#tab_3">Table 2</ref> shows a speed comparison to three PixelCNN-based approaches (see Sec. 2 for details on these approaches). We compare time spent when sampling from the model, to be able to compare to the PixelCNN literature. Actual decoding times for L3C are given in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Runtimes</head><p>While the runtime for PixelCNN <ref type="bibr" target="#b45">[46]</ref> and MS-PixelCNN <ref type="bibr" target="#b31">[32]</ref> is taken from the table in <ref type="bibr" target="#b31">[32]</ref>, we can compare with L3C by assuming that PixelCNN++ is not slower than PixelCNN to get a conservative estimate 9 , and by considering that MS-PixelCNN reports a 105× speedup over PixelCNN. When comparing on 320×320 crops, we thus observe massive speedups compared to the original PixelCNN: &gt;1.63 · 10 5 × for batch size (BS) 1 and &gt;5.31 · 10 4 × for BS <ref type="bibr" target="#b29">30</ref>. We see that on 320 × 320 crops, L3C is at least 5.06 · 10 2 × faster than MS-PixelCNN, the fastest PixelCNNtype approach. Furthermore, <ref type="table" target="#tab_3">Table 2</ref> makes it obvious that the PixelCNN based approaches are not practical for lossless compression of high-resolution images.</p><p>We emphasize that it is impossible to do a completely fair comparison with PixelCNN and MS-PixelCNN due to the unavailability of their code and the different hardware. Even if the same hardware was available to us, differences in frameworks/framework versions (PyTorch vs. Tensorflow) can not be accounted for. See also Sec. A.4 for notes on the influence of the batch size.</p><p>Bitcost To put the runtimes reported in <ref type="table" target="#tab_3">Table 2</ref> into perspective, we also evaluate the bitcost on ImageNet32, for which PixelCNN and MS-PixelCNN were trained, in <ref type="table" target="#tab_4">Table 3</ref>. We observe our outputs to be 20.6% larger than MS-PixelCNN and 24.4% larger than the original PixelCNN, but smaller than all classical approaches. However, as shown above, this increase in bitcost is traded against orders of magnitude in speed. We obtain similar results for ImageNet64, see Sec. A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Encoding / Decoding Time</head><p>To encode/decode images with L3C (and other methods outputting a probability distribution), a pass with an entropy coder is needed. We implemented a relatively simple pipeline to encode and decode images with L3C, which we describe in the supplementary material, in Section A.2. The results are shown in Tables 4 and A1. As noted in Section A.2, we did not optimize our code for speed, yet still obtain practical runtimes. We also note that to use other likelihood-based methods for lossless compression, similar steps are required. While our encoding time is in the same order as for classical approaches, our decoder is slower than that of the other approaches. This can be attributed to more optimized code and offloading complexity to the encoder -while in our approach, decoding essentially mirrors encoding. However, combining encoding and decoding time we are either faster (FLIF) or have better bitrate (PNG, WebP, JPEG2000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Sampling Representations</head><p>We stress that we study image compression and not image generation. Nevertheless, our method produces models from which x and z (s) can be sampled. Therefore, we visualize the output when sampling part of the representations from our model in <ref type="figure" target="#fig_0">Fig. 3</ref>: the top left shows an image from Open Images, when we store all scales (losslessly). When we store z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> , z <ref type="bibr" target="#b2">(3)</ref> but not x and instead sample from p(x|f <ref type="bibr" target="#b0">(1)</ref> ), we only need 27.8% of the total bits without noticeably degrading visual quality. Sampling z <ref type="bibr" target="#b0">(1)</ref> and x leads to some blur while reducing the number of stored bits to 10.2% of the   full bitcost. Finally, only storing z (3) (containing 64 × 64 × 5 values from L and 3.00% of the full bitcost) and sampling z <ref type="bibr" target="#b1">(2)</ref> , z <ref type="bibr" target="#b0">(1)</ref> , and x produces significant artifacts. However, the original image is still recognizable, showing the ability of our networks to learn a hierarchical representation capturing global image structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed and evaluated a fully parallel hierarchical probabilistic model with auxiliary feature representations. Our L3C model outperforms PNG, JPEG2000 and WebP on all datasets. Furthermore, it significantly outperforms the RGB Shared and RGB baselines which rely on predefined heuristic feature representations, showing that learning the representations is crucial. Additionally, we observed that using PixelCNN-based methods for losslessly compressing full resolution images takes two to five orders of magnitude longer than L3C.</p><p>To further improve L3C, future work could investigate weak forms of autoregression across pixels and/or dynamic adaptation of the model network to the current image. Moreover, it would be interesting to explore domain-specific applications, e.g., for medical image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Practical Full Resolution Learned Lossless Image Compression -Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Changes in Version 3</head><p>Previously, our preprocessing script saved all training images and validation images as JPGs with a high quality factor of Q = 95, downscaled by a factor 0.75. It turns out that the resulting images have a specific enough distribution that the neural network picks up on it, and the images are also easier to compress for the non-learned codecs.</p><p>For correctness, we have thus re-created the training and validation sets. The new preprocessing script and more details is available on github 1 . The important differences are:</p><p>• All images are saved as PNGs.</p><p>• We do not rescale validation sets in any way, and instead divide the images into crops such that everything fits into memory. • For the training set, we use a random downscaling factor, instead of fixed 0.75x: this provides a wider variety of downscaling artefacts. <ref type="table" target="#tab_1">Table A1</ref> shows the time required to decode each scale s. We first obtain the CDF as a matrix on the CPU to be able to use the arithmetic decoder (see below), and then do a pass with the arithmetic decoder. We did not optimize either part for speed, as noted in Sec. A.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Encoding and Decoding Details</head><p>The following shows detailed steps, using again z (0) = x. The steps are also visualized in <ref type="figure">Fig. A4</ref>  <ref type="table" target="#tab_1">Table A1</ref>: We show the time to obtain CDF, including all forward passes through the different stages, as well as the time required by the arithmetic decoder. We measured on a Titan X (Pascal), and took the average over 500 crops of 512 × 512 pixels. For s = 3, we assume a uniform prior, and thus do not need to calculate a CDF.</p><p>3. Update the means µ predicted for the RGB scale (s = 0) toμ, given the input x (see Eq. <ref type="formula" target="#formula_13">(7)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>In practice, the division into intervals [a, b) required for arithmetic coding described in Sec. 3.1 is most efficiently done by having access to the cumulative distribution function (CDF) of the symbol to encode. Thus, for the RGB scale (s = 0), we obtain the CDF analogously to Eq. <ref type="formula" target="#formula_12">(6)</ref>:</p><formula xml:id="formula_19">C(x 1uv |f (1) ) = k π k 1uv C l (x 1uv |μ k 1uv , σ k 1uv ) C(x 2uv |f (1) , x 1uv ) = k π k 2uv C l (x 2uv |μ k 2uv , σ k 2uv ) C(x 3uv |f (1) , x 1uv , x 2uv ) = k π k 3uv C l (x 3uv |μ k 3uv , σ k 3uv ).<label>(12)</label></formula><p>And, analogously to Eq. (9), the CDF for s &gt; 0 for each channel c is</p><formula xml:id="formula_20">C(z (s) cuv |f (s+1) ) = k π k cuv C l (z (s) c |µ k cuv , σ k cuv ). (13)</formula><p>C l in Eqs. <ref type="bibr" target="#b11">(12)</ref>, <ref type="formula" target="#formula_3">(13)</ref> is the CDF of the logistic distribution, with the predicted C(z (s) c |f (s+1) ), using adaptive arithmetic coding (see Sec. 3.1). To be able to uniquely decode, the sub-bitstream for z (s) always starts with a triplet encoding its dimensions C, H , W as UINT16. The final bitstream is the concatenation of all sub-bitstreams. 4. Given f <ref type="bibr" target="#b0">(1)</ref> , which contains all parameters for the RGB scale (i.e., we know ∀k, c, u, v: π k cuv , µ k cuv , σ k cuv as well as λ k αuv , λ k αuv , λ k αuv , see Sec. 3.4), we can obtain the CDF for the first channel of x (x 1 , red channel), C(x 1 |f (1) ), and decode this first channel from the bitstream. Now we know x 1 , and with µ k 2uv , λ k αuv we can obtainμ k 2 via Eq. <ref type="bibr" target="#b6">(7)</ref>. With this, we also know the CDF of the next channel, C(x 2 |f (1) , x 1 ), and can decode x 2 from the bitstream. In the same fashion, we can then obtainμ k 3 , then C(x 3 |f (1) , x 1 , x 2 ), and thus x 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Concatenating the channels x 1 , x 2 , x 3 , we finally obtain the decoded image x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Hardware Used</head><p>Our timings were obtained on a machine with a Titan X (Pascal) GPU and Intel Xeon E5-2680 v3 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Notes on Code Optimization</head><p>The encoder can be run in parallel over all scales, as all CDFs are known after one forward pass. Further, we do not need to know the CDF for all symbols, but only for the symbols z we encode and z + 1, since this specifies the interval [a, b).</p><p>The decoder is sequential in the scales since z (s) is required to predict the distribution of z (s−1) . Still, for s &gt; 0, the decoding of the channels of the z (s) could be parallelized, as the channels are modelled fully independently. However, we did not implement either of these improvements, keeping the code simple. For both encoder and decoder, the CDFs must be available to the CPU, as the arithmetic coder runs there. However, the CDFs are huge tensors for real-world images (H × W × 257 for RGB, which amounts to 257MB for each channel of a 512 × 512 image). To save the expensive copying from GPU to CPU, we implemented our own CUDA kernel to store the claculated C directly into "managed memory", which can be accessed from both CPU and GPU. However, we did not optimize this CUDA kernel for speed.</p><p>Finally, while state-of-the-art adaptive entropy coders typically require on the order of milliseconds per MB (see <ref type="bibr" target="#b12">[13]</ref> and in particular <ref type="bibr" target="#b13">[14]</ref> for benchmarks on adaptive entropy coding), we implemented a simple arithmetic coding module to obtain the times in our tables. Please see the code 1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Comparison on ImageNet64</head><p>We show a bpsp comparison on ImageNet64 in <ref type="table" target="#tab_3">Table A2</ref>. Similar to what we observed on ImageNet32 (see Section 5.2), our outputs are 23.8% larger than MS-PixelCNN and 19.4% larger than the original PixelCNN, but smaller than all classical approaches. We note again that increase in bitcost is traded against orders of magnitude in speed.</p><p>We also note that the gap between classical approaches and PixelCNN becomes smaller compared to ImageNet32. <ref type="bibr">[</ref>  In <ref type="table" target="#tab_3">Table 2</ref>, we report run times for batch size 30 to be able to compare with the run times reported in <ref type="bibr" target="#b31">[32]</ref>. However, this comparison is biased against us, as can be seen in <ref type="table" target="#tab_4">Table A3</ref>: Since our network is fairly small, we can process up to 480 images of size 32×32 in parallel. We observe that the time to sample one image drops as the batch size increases, indicating that for BS=30, some overhead dominates.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Visualizing Representations</head><p>We visualize the representations z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> , z <ref type="bibr" target="#b2">(3)</ref> in <ref type="figure" target="#fig_4">Fig. A1</ref>. It can be seen that the global image structure is preserved over scales, with representations corresponding to smaller s modeling more detail. This shows potential for efficiently performing image understanding tasks on partially decoded images similarly as described in <ref type="bibr" target="#b42">[43]</ref> for lossy learned compression: instead of training a feature extractor for a given task on x, one could directly use the features z (s) from our network. <ref type="bibr" target="#b0">1</ref> 25 <ref type="figure" target="#fig_4">Figure A1</ref>: Heatmap visualization of the first three channels for each of the representations z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> , z <ref type="bibr" target="#b2">(3)</ref> , each containing values in L = {1, . . . , 25}, as indicated by the scale underneath.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Architectures of Baselines</head><p>Figs. A2, A3 show the architectures for the RGB Shared and RGB baselines. The dots in <ref type="figure" target="#fig_1">Fig. A2</ref> indicate that the model could in theory be applied more since D <ref type="bibr" target="#b0">(1)</ref> is used for every scale.    <ref type="figure" target="#fig_1">Figure A2</ref>: Architecture for the RGB Shared baseline. Note that we train only one predictor D <ref type="bibr" target="#b0">(1)</ref> .    </p><formula xml:id="formula_21">D (1) x z (1) z (2) z (3) f (3) f (2) f (1) p(z (2) |f (3) ) p(z (1) |f (2) ) p(x|f (1) ) B 2 &lt;</formula><formula xml:id="formula_22">D (1) D (2) D (3) x z (1) z (2) z (3) f (3) f (2) f (1) p(z (2) |f (3) ) p(z (1) |f (2) ) p(x|f (1) ) B 2 &lt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Encoding and Decoding Visualized</head><p>We visualize the steps needed to encode the different z (s) in <ref type="figure">Fig. A4</ref> on the next page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding</head><p>Decoding </p><formula xml:id="formula_23">E (1) E (1) E (3) x z (3)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Version 3 :</head><label>3</label><figDesc>This is an updated version of the paper. See suppl. A.1. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 6 r E W v W P E + m R 3 c L r a / x J 6 0 a k b / c = " &gt; A A A K d H i c v V Z b b 9 M w G M 3 G Z V 0 Y s A F v 8 G C o J n W s q 5 J y m z Q q T d o D P A 6 J X a Q k m h z H a c 0 c J 3 I c t t b K z + G V R / 4 L j / w J n r G T D b r e N E C N q 0 q f v / P Z x + f E 0 m c / o S Q V l v V 9 Y f H G z V u 3 l 2 r L 5 p 2 V u / f u r 6 4 9 O E z j j C N 8 g G I a 8 2 M f p p g S h g 8 E E R Q f J x z D y K f 4 y D / d 0 / j R Z 8 x T E r O P o p 9 g L 4 J d R k K C o F C p k 7 W l L 6 b r 4 y 5 h U p D T Q U K Q y D j O H b 0 d + B Q T 1 u F x x g L P d J N u G E H R S 7 G I I O K x d F H m 4 / N c 2 v k U r D 8 D G + T S a l m v J s F x G K q J x t v T Y V / j L 6 f j S O O v p + O B x r c V H n B 4 5 m R U c A h E j 7 B m S C j t 7 K k T F s 5 5 o G E 1 1 W 8 D b G 2 B z c 3 G V i l 6 O G U 1 y 2 T / T 2 q k C P U R x T t / S 6 U n 5 c 6 D G V S 6 a q j o 3 6 g m q h p j / x 9 V F 6 5 X Z + M I 4 f z N v I 7 C e V j q V + 6 p X 7 m p s z X O w 1 V U u a u o c l d n a 5 y H q 0 H l r g a V u z p b 4 3 V d x S y 4 0 o 5 N 8 2 S 1 r h p m M c B 4 Y F 8 E 9 d 1 6 6 9 H g x 7 u v + y d r C 9 / c I E Z Z h J l A F K a p Y 1 u J 8 C T k g i g S 1 f q y F C c Q n c I u d l T I Y I R T T x Y P i h y s q 0 w A w p i r P x O g y A 6 v k D B K 0 3 7 k q 8 q i v Y 5 i O j k R O 7 8 k G I f 8 a F L a y U S 4 7 U n C k k x g h s q j h R k F I g b 6 P Q M C w j E S t K 8 C 1 e G J U g d Q D 3 K I h H r 1 X C H Q j i r d D J + h O I o g C 5 6 7 i H B l R u D Y n n Q 1 7 F w + p T o N v U l L T z c 8 a Y K h 4 b I 4 w E 7 a g w n u l O v L G 3 H W I w I 3 9 W V p E s Y w B 4 q 5 0 1 a e g 2 K v D S D r d r 6 T q x M U H l A s 5 O 9 L l E u f q l M + t a 1 c f W l 7 9 L u O B 4 f t l v 2 i 1 f 5 g 1 3 f b R j l q x m P j m d E w b O O N s W u 8 N / a N A w P V V m r t 2 k 7 t 7 f J P 8 4 l Z N 9 f L 0 s W F i z U P j S v D b P 0 C 1 i A z R g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 uG U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E A a l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 6 r E W v W P E + m R 3 c L r a / x J 6 0 a k b / c = " &gt; A A A K d H i c v V Z b b 9 M w G M 3 G Z V 0 Y s A F v 8 G C o J n W s q 5 J y m z Q q T d o D P A 6 J X a Q k m h z H a c 0 c J 3 I c t t b K z + G V R / 4 L j / w J n r G T D b r e N E C N q 0 q f v / P Z x + f E 0 m c / o S Q V l v V 9 Y f H G z V u 3 l 2 r L 5 p 2 V u / f u r 6 4 9 O E z j j C N 8 g G I a 8 2 M f p p g S h g 8 E E R Q f J x z D y K f 4 y D / d 0 / j R Z 8 x T E r O P o p 9 g L 4 J d R k K C o F C p k 7 W l L 6 b r 4 y 5 h U p D T Q U K Q y D j O H b 0 d + B Q T 1 u F x x g L P d J N u G E H R S 7 G I I O K x d F H m 4 / N c 2 v k U r D 8 D G + T S a l m v J s F x G K q J x t v T Y V / j L 6 f j S O O v p + O B x r c V H n B 4 5 m R U c A h E j 7 B m S C j t 7 K k T F s 5 5 o G E 1 1 W 8 D b G 2 B z c 3 G V i l 6 O G U 1 y 2 T / T 2 q k C P U R x T t / S 6 U n 5 c 6 D G V S 6 a q j o 3 6 g m q h p j / x 9 V F 6 5 X Z + M I 4 f z N v I 7 C e V j q V + 6 p X 7 m p s z X O w 1 V U u a u o c l d n a 5 y H q 0 H l r g a V u z p b 4 3 V d x S y 4 0 o 5 N 8 2 S 1 r h p m M c B 4 Y F 8 E 9 d 1 6 6 9 H g x 7 u v + y d r C 9 / c I E Z Z h J l A F K a p Y 1 u J 8 C T k g i g S 1 f q y F C c Q n c I u d l T I Y I R T T x Y P i h y s q 0 w A w p i r P x O g y A 6 v k D B K 0 3 7 k q 8 q i v Y 5 i O j k R O 7 8 k G I f 8 a F L a y U S 4 7 U n C k k x g h s q j h R k F I g b 6 P Q M C w j E S t K 8 C 1 e G J U g d Q D 3 K I h H r 1 X C H Q j i r d D J + h O I o g C 5 6 7 i H B l R u D Y n n Q 1 7 F w + p T o N v U l L T z c 8 a Y K h 4 b I 4 w E 7 a g w n u l O v L G 3 H W I w I 3 9 W V p E s Y w B 4 q 5 0 1 a e g 2 K v D S D r d r 6 T q x M U H l A s 5 O 9 L l E u f q l M + t a 1 c f W l 7 9 L u O B 4 f t l v 2 i 1 f 5 g 1 3 f b R j l q x m P j m d E w b O O N s W u 8 N / a N A w P V V m r t 2 k 7 t 7 f J P 8 4 l Z N 9 f L 0 s W F i z U P j S v D b P 0 C 1 i A z R g = = &lt; / l a t e x i t &gt; D (1) E (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture details for a single scale s. For s = 1, E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of generating representations instead of storing them, given different z (s) of a 512 × 512 image. Below each generated image, we show the required bitcost and which scales are stored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>C</head><label></label><figDesc>l (z|µ, σ) = sigmoid((z − µ)/σ). For each s, c the CDF C(z (s) c |f (s+1) ) is a H × W × Ldimensional matrix, where L = 257 for RGB and L = 26 otherwise, and H = H/2 s , W = W/2 s . 5. For each s ∈ {S+1, . . . , 0}, encode each channel c of z (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Decoding 1 .</head><label>1</label><figDesc>Obtain the final z (S) from the bitstream, which was encoded with a uniform prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 u G U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E A a l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X 5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 uG U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E Aa l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X 5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 u G U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E A a l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X 5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; D (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " t f q u M u F I o t k 0 4 W n W Y y A 7 6 A + 2 + m I = " &gt; A A A C u n i c b V F b a x N B F J 6 s t x p v r Q o i v g w G o Z U Q d u N D C x o I + u J j B d M W d p d w d v Z s M 2 Q u 6 8 x s Y 1 z 2 X w h 9 1 Z / l v 3 E 2 a a F t e m D g m + 8 7 9 5 O V g l s X h v 8 6 w Z 2 7 9 + 4 / 2 H r Y f f T 4 y d N n 2 z v P j 6 y u D M M J 0 0 K b k w w s C q 5 w 4 r g T e F I a B J k J P M 7 m X 1 r 9 + A y N 5 V p 9 d 8 s S U w m n i h e c g f N U m k h w M w a i / t x M h 9 P t X j g I V 0 Y 3 Q X Q B e u N X r 3 / H Y h 8 O p z u d 8 y T X r J K o H B N g b R y F p U t r M I 4 z g U 0 3 q S y W w O Z w i r G H C i T a t F 5 1 3 d B 3 n s l p o Y 1 / y t E V e z W i B m n t U m b e s + 3 S 3 t R a 8 l b t 5 2 W B T S m T t 9 F x 5 Y q D t O a q r B w q t m 6 t q A R 1 m r Z L o z k 3 y J x Y e g D M c D 8 d Z T M w w J x f 7 b U C j s 9 / + b k V L p i W E l T + P m H c + G X k c Z T W S S v H l / c a 7 b Z J B u 1 3 L 6 2 7 9 I o l S u c Y 2 x m U O F r H 9 w s u x G g x 4 w 7 7 u Y F F n y u F h v r K o 6 H f O V 3 l 2 q N 1 L 2 o + N o 0 / Z X T z c J v g a D i I P g y G 3 6 L e + C V Z 2 x Z 5 Q 9 6 S X R K R f T I m X 8 k h m R B G f p B z 8 o f 8 D T 4 F W c C D + d o 1 6 F z E v C D X L H D / A X f r 2 b 4 = &lt; / l a t e x i t &gt; B 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t f q u M u F I o t k 0 4 W n W Y y A 7 6 A + 2 + m I = " &gt; A A A C u n i c b V F b a x N B F J 6 s t x p v r Q o i v g w G o Z U Q d u N D C x o I + u J j B d M W d p d w d v Z s M 2 Q u 6 8 x s Y 1 z 2 X w h 9 1 Z / l v 3 E 2 a a F t e m D g m + 8 7 9 5 O V g l s X h v 8 6 w Z 2 7 9 + 4 / 2 H r Y f f T 4 y d N n 2 z v P j 6 y u D M M J 0 0 K b k w w s C q 5 w 4 r g T e F I a B J k J P M 7 m X 1 r 9 + A y N 5 V p 9 d 8 s S U w m n i h e c g f N U m k h w M w a i / t x M h 9 P t X j g I V 0 Y 3 Q X Q B e u N X r 3 / H Y h 8 O p z u d 8 y T X r J K o H B N g b R y F p U t r M I 4 z g U 0 3 q S y W w O Z w i r G H C i T a t F 5 1 3 d B 3 n s l p o Y 1 / y t E V e z W i B m n t U m b e s + 3 S 3 t R a 8 l b t 5 2 W B T S m T t 9 F x 5 Y q D t O a q r B w q t m 6 t q A R 1 m r Z L o z k 3 y J x Y e g D M c D 8 d Z T M w w J x f 7 b U C j s 9 / + b k V L p i W E l T + P m H c + G X k c Z T W S S v H l / c a 7 b Z J B u 1 3 L 6 2 7 9 I o l S u c Y 2 x m U O F r H 9 w s u x G g x 4 w 7 7 u Y F F n y u F h v r K o 6 H f O V 3 l 2 q N 1 L 2 o + N o 0 / Z X T z c J v g a D i I P g y G 3 6 L e + C V Z 2 x Z 5 Q 9 6 S X R K R f T I m X 8 k h m R B G f p B z 8 o f 8 D T 4 F W c C D + d o 1 6 F z E v C D X L H D / A X f r 2 b 4 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>B 2 &lt;</head><label>2</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " t f q u M u F I o t k 0 4 W n W Y y A 7 6 A + 2 + m I = " &gt; A A A C u n i c b V F b a x N B F J 6 s t x p v r Q o i v g w G o Z U Q d u N D C x o I + u J j B d M W d p d w d v Z s M 2 Q u 6 8 x s Y 1 z 2 X w h 9 1 Z / l v 3 E 2 a a F t e m D g m + 8 7 9 5 O V g l s X h v 8 6 w Z 2 7 9 + 4 / 2 H r Y f f T 4 y d N n 2 z v P j 6 y u D M M J 0 0 K b k w w s C q 5 w 4 r g T e F I a B J k J P M 7 m X 1 r 9 + A y N 5 V p 9 d 8 s S U w m n i h e c g f N U m k h w M w a i / t x M h 9 P t X j g I V 0 Y 3 Q X Q B e u N X r 3 / H Y h 8 O p z u d 8 y T X r J K o H B N g b R y F p U t r M I 4 z g U 0 3 q S y W w O Z w i r G H C i T a t F 5 1 3 d B 3 n s l p o Y 1 / y t E V e z W i B m n t U m b e s + 3 S 3 t R a 8 l b t 5 2 W B T S m T t 9 F x 5 Y q D t O a q r B w q t m 6 t q A R 1 m r Z L o z k 3 y J x Y e g D M c D 8 d Z T M w w J x f 7 b U C j s 9 / + b k V L p i W E l T + P m H c + G X k c Z T W S S v H l / c a 7 b Z J B u 1 3 L 6 2 7 9 I o l S u c Y 2 x m U O F r H 9 w s u x G g x 4 w 7 7 u Y F F n y u F h v r K o 6 H f O V 3 l 2 q N 1 L 2 o + N o 0 / Z X T z c J v g a D i I P g y G 3 6L e + C V Z 2 x Z 5 Q 9 6 S X R K R f T I m X 8 k h m R B G f p B z 8 o f 8 D T 4 F W c C D + d o 1 6 F z E v C D X L H D / A X f r 2 b 4 = &lt; / l a t e x i t &gt; D (1) D (1) . . .&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l E G R e n f 4 i p a B X 1 yR 4 N 2 c M D x g u f g = " &gt; A A A B 7 H i c b V D L S g M x F L 1 T X 7 W + 6 m P n J l g E V 2 W m C r o s u n F Z w W k L 7 V A y a a Y N z W S G 5 I 5 Q S r / B j Q t F 3 P p B 7 v w b 0 2 k X 2 n o g c D j n H n L v C V M p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H T Z N k m n G f J T L R 7 Z A a L o X i P g q U v J 1 q T u N Q 8 l Y 4 u p v 5 r S e u j U j U I 4 5 T H s R 0 o E Q k G E U r + d 1 + g q Z X r r h V N w d Z J d 6 C V O o n k K P R K 3 / ZH M t i r p B J a k z H c 1 M M J l S j Y J J P S 9 3 M 8 J S y E R 3 w j q W K x t w E k 3 z Z K T m 3 S p 9 E i b Z P I c n V 3 4 k J j Y 0 Z x 6 G d j C k O z b I 3 E / / z O h l G N 8 F E q D R D r t j 8 o y i T B B M y u 5 z 0 h e Y M 5 d g S y r S w u x I 2 p J o y t P 2 U b A n e 8 s m r p F m r e p f V 2 s N V p X 4 7 b w O K c A p n c A E e X E M d 7 q E B P j A Q 8 A y v 8 O Y o 5 8 V 5 d z 7 m o w V n k T m G P 3 A + f w B k 4 Y 8 a &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l E G R e n f 4 i p a B X 1 y R 4 N 2 c M D x g u f g = " &gt; A A A B 7 H i c b V D L S g M x F L 1 T X 7 W + 6 m P n J l g E V 2 W m C r o s u n F Z w W k L 7 V A y a a Y N z W S G 5 I 5 Q S r / B j Q t F 3 P p B 7 v w b 0 2 k X 2 n o g c D j n H n L v C V M p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H T Z N k m n G f J T L R 7 Z A a L o X i P g q U v J 1 q T u N Q 8 l Y 4 u p v 5 r S e u j U j U I 4 5 T H s R 0 o E Q k G E U r + d 1 + g q Z X r r h V N w d Z J d 6 C V O o n k K P R K 3 / Z H M t i r p B J a k z H c 1 M M J l S j Y J J P S 9 3 M 8 J S y E R 3 w j q W K x t w E k 3 z Z K T m 3 S p 9 E i b Z P I c n V 3 4 k J j Y 0 Z x 6 G d j C k O z b I 3 E / / z O h l G N 8 F E q D R D r t j 8 o y i T B B M y u 5 z 0 h e Y M 5 d g S y r S w u x I 2 p J o y t P 2 U b A n e 8 s m r p F m r e p f V 2 s N V p X 4 7 b w O K c A p n c A E e X E M d 7 q E B P j A Q 8 A y v 8 O Y o 5 8 V 5 d z 7 m o w V n k T m G P 3 A + f w B k 4 Y 8 a &lt; / l a t e x i t &gt; … . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l E G R e n f 4 i p a B X 1 y R 4 N 2 c M D x g u f g = " &gt; A A A B 7 H i c b V D L S g M x F L 1 T X 7 W + 6 m P n J l g E V 2 W m C r o s u n F Z w W k L 7 V A y a a Y N z W S G 5 I 5 Q S r / B j Q t F 3 P p B 7 v w b 0 2 k X 2 n o g c D j n H n L v C V M p D L r u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H T Z N k m n G f J T L R 7 Z A a L o X i P g q U v J 1 q T u N Q 8 l Y 4 u p v 5 r S e u j U j U I 4 5 T H s R 0 o E Q k G E U r + d 1 + g q Z X r r h V N w d Z J d 6 C V O o n k K P R K 3 / Z H M t i r p B J a k z H c 1 M M J l S j Y J J P S 9 3 M 8 J S y E R 3 w j q W K x t w E k 3 z Z K T m 3 S p 9 E i b Z P I c n V 3 4 k J j Y 0 Z x 6 G d j C k O z b I 3 E / / z O h l G N 8 F E q D R D r t j 8 o y i T B B M y u 5 z 0 h e Y M 5 d g S y r S w u x I 2 p J o y t P 2 U b A n e 8 s m r p F m r e p f V 2 s N V p X 4 7 b w O K c A p n c A E e X E M d 7 q E B P j A Q 8 A y v 8 O Y o 5 8 V 5 d z 7 m o w V n k T m G P 3 A + f w B k 4 Y 8 a &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>&lt; l a t</head><label></label><figDesc>e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 u G U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E A a l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X 5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 uG U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E Aa l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X 5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3 Q 1 u G U f K w t L n 7 P s O v n F 1 u O c u V k = " &gt; A A A H V n i c t V X d b t M w F M 7 G u o 7 C Y A V u J m 4 M F V L H u i r p k E A a l S Z x w + W Q 2 I + U R J P j n L R m j h 0 5 D l 0 X 5 T k Q P A D P B C + D s J t N 6 9 a u Y m M 7 U a T j 8 x 2 f z + e L d R I k j K b K t n / P z d 9 b q C x W l + 7 X H j x c f v R 4 p f 5 k L x W Z J L B L B B P y I M A p M M p h V 1 H F 4 C C R g O O A w X 5 w 9 M H g + 1 9 B p l T w z 2 q Y g B / j H q c R J V j p 0 G F 9 4 W f N C 6 B H e a 7 o 0 U l C i c o k F K 4 p h 7 4 I y r t S Z D z 0 a 1 7 S i 2 K s + i m o G B M p c o 9 k A R w X u V N c g Q 1 n Y C d F b r e n o i K K 9 M L A n a v h w O B v N B 5 K P H A z p i R G q k 9 5 K 6 K M d Q O W w Q v H 9 l H T b u l n D W 1 s o P X 1 5 k Z 5 4 v G Q 3 S q D w / P Q p S Q y J A y 2 r k d k F m X d k x l E J m s s 6 S Z E U z u a 4 L 5 O R z 0 J w M + Z T u W + O w V n 8 9 2 + k D f o 7 7 / 0 l B B O s A V 3 J + d M u t t X 8 / r d / a u Y w M M L 8 6 h W O 1 x p 2 G 1 7 Z G j S c U 6 d x v b q 6 n d t P 3 Y O 6 3 P f v F C Q L A a u C M N p 6 j p 2 o v w c S 0 U 1 i R 4 e W Q o J J k e 4 B 6 5 2 O Y 4 h 9 f P R R C 3 Q K x 0 J U S S k f r l C o + j 4 j h z H a T q M A 5 0 5 G l C X M R O c i h 2 f E U x C Q T w t 7 G Y q e u f n l C e Z A k 7 K o 0 U Z Q 0 o g M 9 B R S C U Q x Y b a 0 T O S 6 u 4 Q 6 W O J i d J j / w K B U V T 3 z W F A R B x j H r 7 2 C J V a j N B 1 / N w z s H v 2 L + k 2 T Z G 2 W a 7 5 e Q 2 N m c d F C G 7 a x w l 0 y / 3 l d R j 0 q Y K W u S k t y j l I p J m 7 H a 0 5 G t V a Q 3 n D K b a K Q n 9 K 5 / K H m 3 T 2 O m 1 n s 9 3 5 5 D S 2 n 1 m l L V n P r Z d W 0 3 K s t 9 a 2 9 d H a s X Y t U l m u b F b e V 7 q L v x b / V C v V a p k 6 P 3 e 6 5 6 l 1 w a o r f w E 0 m T s w &lt; / l a t e x i t &gt; D (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " t f q u M u F I o t k 0 4 W n W Y y A 7 6 A + 2 + m I = " &gt; A A A C u n i c b V F b a x N B F J 6 s t x p v r Q o i v g w G o Z U Q d u N D C x o I + u J j B d M W d p d w d v Z s M 2 Q u 6 8 x s Y 1 z 2 X w h 9 1 Z / l v 3 E 2 a a F t e m D g m + 8 7 9 5 O V g l s X h v 8 6 w Z 2 7 9 + 4 / 2 H r Y f f T 4 y d N n 2 z v P j 6 y u D M M J 0 0 K b k w w s C q 5 w 4 r g T e F I a B J k J P M 7 m X 1 r 9 + A y N 5 V p 9 d 8 s S U w m n i h e c g f N U m k h w M w a i / t x M h 9 P t X j g I V 0 Y 3 Q X Q B e u N X r 3 / H Y h 8 O p z u d 8 y T X r J K o H B N g b R y F p U t r M I 4 z g U 0 3 q S y W w O Z w i r G H C i T a t F 5 1 3 d B 3 n s l p o Y 1 / y t E V e z W i B m n t U m b e s + 3 S 3 t R a 8 l b t 5 2 W B T S m T t 9 F x 5 Y q D t O a q r B w q t m 6 t q A R 1 m r Z L o z k 3 y J x Y e g D M c D 8 d Z T M w w J x f 7 b U C j s 9 / + b k V L p i W E l T + P m H c + G X k c Z T W S S v H l / c a 7 b Z J B u 1 3 L 6 2 7 9 I o l S u c Y 2 x m U O F r H 9 w s u x G g x 4 w 7 7 u Y F F n y u F h v r K o 6 H f O V 3 l 2 q N 1 L 2 o + N o 0 / Z X T z c J v g a D i I P g y G 3 6 L e + C V Z 2 x Z 5 Q 9 6 S X R K R f T I m X 8 k h m R B G f p B z 8 o f 8 D T 4 F W c C D + d o 1 6 F z E v C D X L H D / A X f r 2 b 4 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>B 2 &lt;</head><label>2</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " t f q u M u F I o t k 0 4 W n W Y y A 7 6 A + 2 + m I = " &gt; A A A C u n i c b V F b a x N B F J 6 s t x p v r Q o i v g w G o Z U Q d u N D C x o I + u J j B d M W d p d w d v Z s M 2 Q u 6 8 x s Y 1 z 2 X w h 9 1 Z / l v 3 E 2 a a F t e m D g m + 8 7 9 5 O V g l s X h v 8 6 w Z 2 7 9 + 4 / 2 H r Y f f T 4 y d N n 2 z v P j 6 y u D M M J 0 0 K b k w w s C q 5 w 4 r g T e F I a B J k J P M 7 m X 1 r 9 + A y N 5 V p 9 d 8 s S U w m n i h e c g f N U m k h w M w a i / t x M h 9 P t X j g I V 0 Y 3 Q X Q B e u N X r 3 / H Y h 8 O p z u d 8 y T X r J K o H B N g b R y F p U t r M I 4 z g U 0 3 q S y W w O Z w i r G H C i T a t F 5 1 3 d B 3 n s l p o Y 1 / y t E V e z W i B m n t U m b e s + 3 S 3 t R a 8 l b t 5 2 W B T S m T t 9 F x 5 Y q D t O a q r B w q t m 6 t q A R 1 m r Z L o z k 3 y J x Y e g D M c D 8 d Z T M w w J x f 7 b U C j s 9 / + b k V L p i W E l T + P m H c + G X k c Z T W S S v H l / c a 7 b Z J B u 1 3 L 6 2 7 9 I o l S u c Y 2 x m U O F r H 9 w s u x G g x 4 w 7 7 u Y F F n y u F h v r K o 6 H f O V 3 l 2 q N 1 L 2 o + N o 0 / Z X T z c J v g a D i I P g y G 3 6 L e + C V Z 2 x Z 5 Q 9 6 S X R K R f T I m X 8 k h m R B G f p B z 8 o f 8 D T 4 F W c C D + d o 1 6 F z E v C D X L H D / A X f r 2 b 4 = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>B 2 &lt;</head><label>2</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " t f q u M u F I o t k 0 4 W n W Y y A 7 6 A + 2 + m I = " &gt; A A A C u n i c b V F b a x N B F J 6 s t x p v r Q o i v g w G o Z U Q d u N D C x o I + u J j B d M W d p d w d v Z s M 2 Q u 6 8 x s Y 1 z 2 X w h 9 1 Z / l v 3 E 2 a a F t e m D g m + 8 7 9 5 O V g l s X h v 8 6 w Z 2 7 9 + 4 / 2 H r Y f f T 4 y d N n 2 z v P j 6 y u D M M J 0 0 K b k w w s C q 5 w 4 r g T e F I a B J k J P M 7 m X 1 r 9 + A y N 5 V p 9 d 8 s S U w m n i h e c g f N U m k h w M w a i / t x M h 9 P t X j g I V 0 Y 3 Q X Q B e u N X r 3 / H Y h 8 O p z u d 8 y T X r J K o H B N g b R y F p U t r M I 4 z g U 0 3 q S y W w O Z w i r G H C i T a t F 5 1 3 d B 3 n s l p o Y 1 / y t E V e z W i B m n t U m b e s + 3 S 3 t R a 8 l b t 5 2 W B T S m T t 9 F x 5 Y q D t O a q r B w q t m 6 t q A R 1 m r Z L o z k 3 y J x Y e g D M c D 8 d Z T M w w J x f 7 b U C j s 9 / + b k V L p i W E l T + P m H c + G X k c Z T W S S v H l / c a 7 b Z J B u 1 3 L 6 2 7 9 I o l S u c Y 2 x m U O F r H 9 w s u x G g x 4 w 7 7 u Y F F n y u F h v r K o 6 H f O V 3 l 2 q N 1 L 2 o + N o 0 / Z X T z c J v g a D i I P g y G 3 6 L e + C V Z 2 x Z 5 Q 9 6 S X R K R f T I m X 8 k h m R B G f p B z 8 o f 8 D T 4 F W c C D + d o 1 6 F z E v C D X L H D / A X f r 2 b 4 = &lt; / l a t e x i t &gt; Figure A3: Architecture for the RGB baseline. Multiple predictors are trained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " G G p A x k D 4 4 Z J H F d P M h 1 8 W T w W s x H s = " &gt; A A A C I H i c b V D L S g N B E O z 1 G e M r P m 5 6 W A x C T m E 3 C n q M 5 O I x g l E h C W F 2 0 j G D s w 9 m e j V h 2 Y u f 4 s m r f o U 3 8 a j / 4 D 8 4 2 S h o Y k F D U d U 9 P V 1 e J I U m x 3 m 3 Z m b n 5 h c W c 0 v 5 5 Z X V t f X C x u a F D m P F s c F D G a o r j 2 m U I s A G C Z J 4 F S l k v i f x 0 r u p j f z L W 1 R a h M E 5 D S N s + + w 6 E D 3 B G R m p U 9 h t E Q 4 o e y e 5 6 w v C N M m U 5 K S W p p 1 C 0 S k 7 G e x p 4 n 6 T Y n U b M t Q 7 h c 9 W N + S x j w F x y b R u u k 5 E 7 Y Q p E l x i m m / F G i P G b 9 g 1 N g 0 N m I + 6 n W T b U 3 v f K F 2 7 F y p T A d m Z + n s i Y b 7 W Q 9 8 z n T 6 j v p 7 0 R u K / 3 u B n w Z T V j K l 3 3 E 5 E E M W E A R / / o R d L m 0 J 7 l J b d F Q o 5 y a E h j C t h z r B 5 n y n G y W S a N / m 4 k 2 l M k 4 t K 2 T 0 o V 8 4 O i 9 X S O C j I w Q 7 s Q Q l c O I I q n E I d G s D h H h 7 h C Z 6 t B + v F e r X e x q 0 z 1 v f M F v y B 9 f E F B k K l A Q = = &lt; / l a t e x i t &gt; U &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z q l h q 4 b z i F s t G 2 Y n + a 0 w + R 1 0 p 2 Y = " &gt; A A A C D X i c b V D L S s N A F L 3 x W e u r P n Z u g k X o q i R V 0 G X B j c s K p i 2 k o U y m k 3 b o Z C b M T M Q S + g 2 u 3 O p X u B O 3 f o M f 4 T 8 4 S S t o 6 4 G B w z n 3 c s + c M G F U a c f 5 t F Z W 1 9 Y 3 N k t b 5 e 2 d 3 b 3 9 y s F h W 4 l U Y u J h w Y T s h k g R R j n x N N W M d B N J U B w y 0 g n H 1 7 n f u S d S U c H v 9 C Q h Q Y y G n E Y U I 2 0 k v x c j P c K I Z d 6 0 X 6 k 6 d a e A v U z c O a k 2 j 6 F A q 1 / 5 6 g 0 E T m P C N W Z I K d 9 1 E h 1 k S G q K G Z m W e 6 k i C c J j N C S + o R z F R A V Z E X l q n x l l Y E d C m s e 1 X a i / N z I U K z W J Q z O Z R 1 S L X i 7 + 6 z 3 8 H F i y / F R H V 0 F G e Z J q w v E s Q 5 Q y W w s 7 r 8 Y e U E m w Z h N D E J b U f M P G I y Q R 1 q b A s u n H X W x j m b Q b d f e 8 3 r i 9 q D Z r s 6 K g B C d w C j V w 4 R K a c A M t 8 A C D g C d 4 h h f r 0 X q 1 3 q z 3 2 e i K N d 8 5 g j + w P r 4 B f x q c u w = = &lt; / l a t e x i t &gt; bitstream &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m c f U m n e l r y t 5 l x 5 v N c I B M R y K Y e o = " &gt; A A A C F H i c b V D L S s N A F L 3 x W e s r P n Z u g k V w V Z I q 2 G X B j c s K 9 g F t K J P p p B 0 6 e T B z U 1 p C f 8 O V W / 0 K d + L W v R / h P z h J K 2 j r g Y H D O f d w 7 x w v F l y h b X 8 a a + s b m 1 v b h Z 3 i 7 t 7 + w a F 5 d N x U U S I p a 9 B I R L L t E c U E D 1 k D O Q r W j i U j g S d Y y x v d Z n 5 r z K T i U f i A 0 5 i 5 A R m E 3 O e U o J Z 6 p t l F N s H U 4 6 g w y 8 1 6 Z s k u 2 z m s V e I s S K l 2 C j n q P f O r 2 4 9 o E r A Q q S B K d R w 7 R j c l E j k V b F b s J o r F h I 7 I g H U 0 D U n A l J v m l 8 + s C 6 3 0 L T + S + o V o 5 e r v R E o C p a a B p y c D g k O 1 7 G X i v 9 7 k Z 8 G K 1 U n Q r 7 o p D + M E W U j n N / i J s D C y s o a s P p e M o p h q Q q j k + h s W H R J J K O o e i 7 o f Z 7 m N V d K s l J 2 r c u X + u l S r z o u C A p z B O V y C A z d Q g z u o Q w M o j O E J n u H F e D R e j T f j f T 6 6 Z i w y J / A H x s c 3 s T a f j w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 6 r E W v W P E + m R 3 c L r a / x J 6 0 a k b /c = " &gt; A A A K d H i c v V Z b b 9 M w G M 3 G Z V 0 Y s A F v 8 G C o J n W s q 5 J y m z Q q T d o D P A 6 J X a Q k m h z H a c 0 c J 3 I c t t b K z + G V R / 4 L j / w J n r G T D b r e N E C N q 0 q f v / P Z x + f E 0 m c / o S Q V l v V 9 Y f H G z V u 3 l 2 r L 5 p 2 V u / f u r 6 4 9 O E z j j C N 8 g G I a 8 2 M f p p g S h g 8 E E R Q f J x z D y K f 4 y D / d 0 / j R Z 8 x T E r O P o p 9 g L 4 J d R k K C o F C p k 7 W l L 6 b r 4 y 5 h U p D T Q U K Q y D j O H b 0 d + B Q T 1 u F x x g L P d J N u G E H R S 7 G I I O K x d F H m 4 / N c 2 v k U r D 8 D G + T S a l m v J s F x G K q J x t v T Y V / j L 6 f j S O O v p + O B x r c VH n B 4 5 m R U c A h E j 7 B m S C j t 7 K k T F s 5 5 o G E 1 1 W 8 D b G 2 B z c 3 G V i l 6 O G U 1 y 2 T / T 2 q k C P U R x T t / S 6 U n 5 c 6 D G V S 6 a q j o 3 6 g m q h p j / x 9 V F 6 5 X Z + M I 4 f z N v I 7 C e V j q V + 6 p X 7 m p s z X O w 1 V U u a u o c l d n a 5 y H q 0 H l r g a V u z p b 4 3 V d x S y 4 0 o 5 N 8 2 S 1 r h p m M c B 4 Y F 8 E 9 d 1 6 6 9 H g x 7 u v + y d r C 9 / c I E Z Z h J l A F K a p Y 1 u J 8 C T k g i g S 1 f q y F C c Q n c I u d l T I Y I R T T x Y P i h y s q 0 w A w p i r P x O g y A 6 v k D B K 0 3 7 k q 8 q i v Y 5 i O j k R O 7 8 k G I f 8 a F L a y U S 4 7 U n C k k x g h s q j h R k F I g b 6 P Q M C w j E S t K 8 C 1 e G J U g d Q D 3 K I h H r 1 X C H Q j i r d D J + h O I o g C 5 6 7 i H B l R u D Y n n Q 1 7 F w + p T o N v U l L T z c 8 a Y K h 4 b I 4 w E 7 a g w n u l O v L G 3 H W I w I 3 9 W V p E s Y w B 4 q 5 0 1 a e g 2 K v D S D r d r 6 T q x M U H l A s 5 O 9 L l E u f q l M + t a 1 c f W l 7 9 L u O B 4 f t l v 2 i 1 f 5 g 1 3 f b R j l q x m P j m d E w b O O N s W u 8 N / a N A w P V V m r t 2 k 7 t 7 f J P 8 4 l Z N 9 f L 0 s W F i z U P j S v D b P 0 C 1 i A z R g = = &lt; / l a t e x i t &gt; E (1) t e x i t s h a 1 _ b a s e 6 4 = " G G p A x k D 4 4 Z J H F d P M h 1 8 W T w W s x H s = " &gt; A A A C I H i c b V D L S g N B E O z 1 G e M r P m 5 6 W A x C T m E 3 C n q M 5 O I x g l E h C W F 2 0 j G D s w 9 m e j V h 2 Y u f 4 s m r f o U 3 8 a j / 4 D 8 4 2 S h o Y k F D U d U 9 P V 1 e J I U m x 3 m 3 Z m b n 5 h c W c 0 v 5 5 Z X V t f X C x u a F D m P F s c F D G a o r j 2 m U I s A G C Z J 4 F S l k v i f x 0 r u p j f z L W 1 R a h M E 5 D S N s + + w 6 E D 3 B G R m p U 9 h t E Q 4 o e y e 5 6 w v C N M m U 5 K S W p p 1 C 0 S k 7 G e x p 4 n 6 T Y n U b M t Q 7 h c 9 W N + S x j w F x y b R u u k 5 E 7 Y Q p E l x i m m / F G i P G b 9 g 1 N g 0 N m I + 6 n W T b U 3 v f K F 2 7 F y p T A d m Z + n s i Y b 7 W Q 9 8 z n T 6 j v p 7 0 R u K / 3 u B n w Z T V j K l 3 3 E 5 E E M W E A R / / o R d L m 0 J 7 l J b d F Q o 5 y a E h j C t h z r B 5 n y n G y W S a N / m 4 k 2 l M k 4 t K 2 T 0 o V 8 4 O i 9 X S O C j I w Q 7 s Q Q l c O I I q n E I d G s D h H h 7 h C Z 6 t B + v F e r X e x q 0 z 1 v f M F v y B 9 f E F B k K l A Q = = &lt; / l a t e x i t &gt; bitstream &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m c f U m n e l r y t 5 l x 5 v N c I B M R y K Y e o = " &gt; A A A C F H i c b V D L S s N A F L 3 x W e s r P n Z u g k V w V Z I q 2 G X B j c s K 9 g F t K J P p p B 0 6 e T B z U 1 p C f 8 O V W / 0 K d + L W v R / h P z h J K 2 j r g Y H D O f d w 7 x w v F l y h b X 8 a a + s b m 1 v b h Z 3 i 7 t 7 + w a F 5 d N x U U S I p a 9 B I R L L t E c U E D 1 k D O Q r W j i U j g S d Y y x v d Z n 5 r z K T i U f i A 0 5 i 5 A R m E 3 O e U o J Z 6 p t l F N s H U 4 6 g w y 8 1 6 Z s k u 2 z m s V e I s S K l 2 C j n q P f O r 2 4 9 o E r A Q q S B K d R w 7 R j c l E j k V b F b s J o r F h I 7 I g H U 0 D U n A l J v m l 8 + s C 6 3 0 L T + S + o V o 5 e r v R E o C p a a B p y c D g k O 1 7 G X i v 9 7 k Z 8 G K 1 U n Q r 7 o p D + M E W U j n N / i J s D C y s o a s P p e M o p h q Q q j k + h s W H R J J K O o e i 7 o f Z 7 m N V d K s l J 2 r c u X + u l S r z o u C A p z B O V y C A z d Q g z u o Q w M o j O E J n u H F e D R e j T f j f T 6 6 Z i w y J / A H x s c 3 s T a f j w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 6 r E W v W P E + m R 3 c L r a / x J 6 0 a k b / c = " &gt; A A A K d H i c v V Z b b 9 M w G M 3 G Z V 0 Y s A F v 8 G C o J n W s q 5 J y m z Q q T d o D P A 6 J X a Q k m h z H a c 0 c J 3 I c t t b K z + G V R / 4 L j / w J n r G T D b r e N E C N q 0 q f v / P Z x + f E 0 m c / o S Q V l v V 9 Y f H G z V u 3 l 2 r L 5 p 2 V u / f u r 6 4 9 O E z j j C N 8 g G I a 8 2 M f p p g S h g 8 E E R Q f J x z D y K f 4 y D / d 0 / j R Z 8 x T E r O P o p 9 g L 4 J d R k K C o F C p k 7 W l L 6 b r 4 y 5 h U p D T Q U K Q y D j O H b 0 d + B Q T 1 u F x x g L P d J N u G E H R S 7 G I I O K x d F H m 4 / N c 2 v k U r D 8 D G + T S a l m v J s F x G K q J x t v T Y V / j L 6 f j S O O v p + O B x r c V H n B 4 5 m R U c A h E j 7 B m S C j t 7 K k T F s 5 5 o G E 1 1 W 8 D b G 2 B z c 3 G V i l 6 O G U 1 y 2 T / T 2 q k C P U R x T t / S 6 U n 5 c 6 D G V S 6 a q j o 3 6 g m q h p j / x 9 V F 6 5 X Z + M I 4 f z N v I 7 C e V j q V + 6 p X 7 m p s z X O w 1 V U u a u o c l d n a 5 y H q 0 H l r g a V u z p b 4 3 V d x S y 4 0 o 5 N 8 2 S 1 r h p m M c B 4 Y F 8 E 9 d 1 6 6 9 H g x 7 u v + y d r C 9 / c I E Z Z h J l A F K a p Y 1 u J 8 C T k g i g S 1 f q y F C c Q n c I u d l T I Y I R T T x Y P i h y s q 0 w A w p i r P x O g y A 6 v k D B K 0 3 7 k q 8 q i v Y 5 i O j k R O 7 8 k G I f 8 a F L a y U S 4 7 U n C k k x g h s q j h R k F I g b 6 P Q M C w j E S t K 8 C 1 e G J U g d Q D 3 K I h H r 1 X C H Q j i r d D J + h O I o g C 5 6 7 i H B l R u D Y n n Q 1 7 F w + p T o N v U l L T z c 8 a Y K h 4 b I 4 w E 7 a g w n u l O v L G 3 H W I w I 3 9 W V p E s Y w B 4 q 5 0 1 a e g 2 K v D S D r d r 6 T q x M U H l A s 5 O 9 L l E u f q l M + t a 1 c f W l 7 9 L u O B 4 f t l v 2 i 1 f 5 g 1 3 f b R j l q x m P j m d E w b O O N s W u 8 N / a N A w P V V m r t 2 k 7 t 7 f J P 8 4 l Z N 9 f L 0 s W F i z U P j S v D b P 0 C 1 i A z R g = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Gradients through Targets We emphasize that in contrast to the generative model literature, we learn the representations, propagating gradients to both E (s)</figDesc><table><row><cell>Method L3C RGB Shared RGB PNG JPEG2000 WebP Propagating [bpsp] Open Images DIV2K Ours 2.991 3.094 Learned Baselines 4.314 +44% 4.429 +43% 3.298 +10% 3.418 +10% Non-Learned Approaches 4.005 +34% 4.235 +37% 3.055 +2.1% 3.127 +1.1% 3.047 +1.9% 3.176 +2.7% FLIF 2.867 −4.1% 2.911 −5.9%</cell><cell>RAISE-1k 2.387 3.779 +58% 2.572 +7.8% 3.556 +49% 2.465 +3.3% 2.461 +3.1% 2.084 −13%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Compression performance of our method (L3C) and learned baselines (RGB Shared and RGB) to previous (non-learned) approaches, in bits per sub-pixel (bpsp). We emphasize the difference in percentage to our method for each other method in green if L3C outperforms the other method and in red otherwise. Numbers are updated, see suppl. A.1 for details. and D (s) , since each component of our loss depends on D (s+1) , . . . , D (S) via the parametrization of the logistic distribution and on E (s) , . . . , E (1) because of the differentiable</figDesc><table /><note>Q. Thereby, our network can autonomously learn to navigate the trade-off between a) making the output z (s) of feature ex- tractor E (s) more easily estimable for the predictor D (s+1) and b) putting enough information into z (s) for the predictor D (s) to predict z (s−1) .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Method 32 × 32px 320 × 320px</figDesc><table><row><cell>BS=1</cell><cell>L3C (Ours) PixelCNN++ [35]</cell><cell>0.0168 s 47.4 s  *</cell><cell>0.0291 s ≈ 80 min  ‡</cell></row><row><cell></cell><cell>L3C (Ours)</cell><cell cols="2">0.000624 s 0.0213 s</cell></row><row><cell>BS=30</cell><cell cols="2">PixelCNN++ PixelCNN [46] MS-PixelCNN [32] 1.17 s  † 11.3 s  *  120 s  †</cell><cell>≈ 18 min  ‡ ≈ 8 hours  ‡ ≈ 2 min  ‡</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Sampling times for our method (L3C), compared to the PixelCNN literature. The results in the first two rows were obtained with batch size (BS) 1, the other times with BS=30, since this is what is reported in<ref type="bibr" target="#b31">[32]</ref>. [ * ]: Times obtained by us with code released of PixelCNN++<ref type="bibr" target="#b34">[35]</ref>, on the same GPU we used to evaluate L3C (Titan X Pascal).[ †]: times reported in<ref type="bibr" target="#b31">[32]</ref>, obtained on a Nvidia Quadro M4000 GPU (no code available).[ ‡]: To put the numbers into perspective, we compare our runtime with linearly extrapolated runtimes for for the other approaches on 320 × 320 crops.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparing bits per sub-pixel (bpsp) on the 32 × 32 images from ImageNet32 of our method (L3C) vs. PixelCNN-based approaches and classical approaches.</figDesc><table /><note>RGB baseline with S = 3 learned predictors outperforms the RGB Shared baseline on all datasets, showing the impor- tance of learning a predictor for each scale. Using our main model (L3C), where we additionally learn the feature extrac- tors, we outperform both baselines: The outputs are at least 7.8% larger everywhere, showing the benefits of learning the representation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>· 10 −5 4.733 JPEG2000 1.48 · 10 −2 2.26 · 10 −4 3.471</figDesc><table><row><cell>Codec</cell><cell cols="3">Encoding [s] Decoding [s] [bpsp] GPU CPU</cell></row><row><cell cols="2">L3C (Ours) 0.242</cell><cell>0.374</cell><cell>3.386</cell></row><row><cell cols="4">PNG 6.09 WebP 0.213 0.157 7.12 · 10 −2 3.447 FLIF 1.72 0.133 3.291</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>.207 bpsp</cell><cell>stored: 0,1,2,3 1.209 bpsp</cell><cell>stored: 1,2,3</cell></row><row><cell>0.355 bpsp</cell><cell>stored: 2,3 0.121 bpsp</cell><cell>stored: 3</cell></row></table><note>Encoding and Decoding times compared to classical ap- proaches, on 512 × 512 crops from DIV2K, as well as bpsp and required devices.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>.</head><label></label><figDesc>Encoding 1. Forward pass through network to obtain ∀s : z (s) , f (s) .2. Encode z (S) assuming a uniform prior, i.e., assuming each of the L symbols is equally likely. This requires log 2 (L) bits per symbol.</figDesc><table><row><cell>Decoding Time</cell><cell cols="2">Obtaining CDF Arithmetic</cell></row><row><cell></cell><cell cols="2">for Decoder [s] Decoding [s]</cell></row><row><cell cols="2">-s = 2, 128 × 128 0.00737 s = 3, 64 × 64 s = 1, 256 × 256 0.0219 s = 0, 512 × 512 0.143</cell><cell>0.00179 0.00759 0.0234 0.169</cell></row><row><cell>Total</cell><cell>0.172</cell><cell>0.202</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A2 :</head><label>A2</label><figDesc>Comparing bits per sub-pixel (bpsp) on the 64 × 64 images from ImageNet64 of our method (L3C) vs. PixelCNN-based approaches and classical approaches.</figDesc><table /><note>A.4. Note on Comparing Times for 32 × 32 Images</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A3 :</head><label>A3</label><figDesc>Effect of varying the batch size.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A RGB "pixel" has 3 "sub-pixels", one in each channel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use (adaptive) arithmetic coding for simplicity of exposition, but any adaptive entropy-achieving coder can be used with our method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Considering that z (s) is quantized, this conveniently upper bounds the information that can be contained within each z (s) , however, other dimensions could be explored.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The final predictor only sees z (S) , i.e., we let f (S+1) = 0. parallel, with rates 1, 2, and 4, then concatenate the resulting feature maps to a 3C f -dimensional feature map.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We chose S = 3 because increasing S comes at the cost of slower training, while yielding negligible improvements in bitrate. For an image of size</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">PixelCNN++ is in fact around 3× faster than PixelCNN due to modelling the joint directly, see Sec. 2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. Feed z (S) to D (S) to obtain f (S) , and thereby also C(z (S−1) cuv |f (S) ) for all c. Since the decoder now has access to the same CDF as the encoder, we can decode z (S−1) from the bitstream with our adaptive arithmetic decoder.3. Analogously, we repeat the previous step to obtain z (S) , . . . , z<ref type="bibr" target="#b0">(1)</ref> , as well as f (S) , . . . , f<ref type="bibr" target="#b0">(1)</ref> using the accompanying CDFs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors would like to thank Sergi Caelles for the insightful discussions and feedback. This work was partly supported by ETH General Fund (OK) and Nvidia through the hardware grant.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>x z <ref type="bibr" target="#b0">(1)</ref> z <ref type="bibr" target="#b1">(2)</ref> z <ref type="bibr" target="#b2">(3)</ref> Q Q Q E <ref type="bibr" target="#b1">(2)</ref> f <ref type="bibr" target="#b2">(3)</ref> f <ref type="bibr" target="#b1">(2)</ref> p(z(2)|f (3)) p(z <ref type="bibr" target="#b0">(1)</ref> |f <ref type="bibr" target="#b1">(2)</ref>    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Challenge on Single Image Super-Resolution: Dataset and Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks for Extreme Learned Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02958</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end Optimized Image Compression. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational Image Compression with a Scale Hyperprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<title level="m">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pixel-SNAIL: An Improved Autoregressive Generative Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<title level="m">A downsampled variant of ImageNet as an alternative to the CIFAR datasets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RAISE: A Raw Images Dataset for Digital Image Forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pasquini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Conotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MMSys</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DEFLATE compressed data format specification version 1.3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deutsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The use of asymmetric numeral systems as an accurate replacement for huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Gadgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giesen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3392</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Interleaved entropy coders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural Networks for Machine Learning Lecture 6a Overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<ptr target="http://kakadusoftware.com.7" />
		<title level="m">Kakadu JPEG2000 implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PixelCNN Models with Auxiliary Variables for Natural Image Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Open-Images: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Convolutional Networks for Content-weighted Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-based adaptive binary arithmetic coding in the h. 264/avc video compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="620" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional Probability Models for Deep Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TMW -a new method for lossless image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCS</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glicbawls -Grey Level Image Compression by Adaptive Weighted Least Squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Tischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint Autoregressive and Hierarchical Priors for Learned Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Transformer. ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pillow Library for Python</title>
		<ptr target="https://python-pillow.org.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<ptr target="http://libpng.org/pub/png/libpng.html.1" />
		<title level="m">Portable Network Graphics (PNG)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parallel Multiscale Autoregressive Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">264 and MPEG-4 video compression: video coding for next-generation multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-Time Adaptive Image Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pixel-CNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The JPEG 2000 still image compression standard. IEEE Signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skodras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="36" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FLIF: Free lossless image format based on MANIAC compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sneyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A note on the evaluation of generative models. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<title level="m">Lossy Image Compression with Compressive Autoencoders. ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Full Resolution Image Compression with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Torfason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Towards Image Understanding from Deep Compression without Decoding. ICLR</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Generative Models for Distribution-Preserving Lossy Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conditional Image Generation with PixelCNN Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixel Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<ptr target="https://developers.google.com/speed/webp/.1" />
	</analytic>
	<monogr>
		<title level="j">WebP Image format</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Applications of Universal Context Modeling to Lossless Compression of Gray-Scale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Arps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="586" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Arithmetic coding for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="540" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Piecewise 2D autoregression for predictive image coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barthel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
