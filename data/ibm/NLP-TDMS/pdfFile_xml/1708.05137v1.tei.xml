<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
							<email>shshin@rcv.kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<email>iskweon@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Minneapolis, MN</roleName><forename type="first">†</forename><surname>Umn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Kaist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel video object segmentation algorithm based on pixel-level matching using Convolutional Neural Networks (CNN). Our network aims to distinguish the target area from the background on the basis of the pixel-level similarity between two object units. The proposed network represents a target object using features from different depth layers in order to take advantage of both the spatial details and the category-level semantic information. Furthermore, we propose a feature compression technique that drastically reduces the memory requirements while maintaining the capability of feature representation. Two-stage training (pretraining and fine-tuning) allows our network to handle any target object regardless of its category (even if the object's type does not belong to the pre-training data) or of variations in its appearance through a video sequence. Experiments on large datasets demonstrate the effectiveness of our model -against related methods -in terms of accuracy, speed, and stability. Finally, we introduce the transferability of our network to different domains, such as the infrared data domain.</p><p>Recently, Convolutional Neural Networks (CNN) have been extensively applied to various vision tasks given their ability to encapsulate semantic information in the object representation task itself. In spite of this advantage, however, only a few attempts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref> have been made to solve video object segmentation problems using a CNN. arXiv:1708.05137v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction &amp; Related Works</head><p>Video object segmentation refers to the propagation of the mask of an initial object(s) from the first to the last frame of a video sequence. With it, users can determine the pixel-level foreground masks of every image from single key frame supervision. Separating the foreground from the background in a video is a fundamental problem with high applicability to many video based tasks including video summarization, stabilization, retrieval, and scene understanding. For these reasons, video object segmentation has been studied intensively, but it still demonstrates poor results in real world scenarios.</p><p>Most recent approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> separate discriminative objects from a background by optimizing an energy equation under various pixel graph relationships. For instance, fully connected graphs have been proposed in <ref type="bibr" target="#b21">[22]</ref> to construct a long range spatio-temporal graph structure robust to challenging situations such as occlusion. In another study <ref type="bibr" target="#b8">[9]</ref>, the higher potential term in a supervoxel cluster unit was used to enforce the steadiness of a graph structure. More recently, non-local graph connections were effectively approximated in the bilateral space <ref type="bibr" target="#b16">[17]</ref>, which drastically improved the accuracy of segmentation. However, many recent methods are too computationally expensive to deal with long video sequences. They are also greatly affected by cluttered backgrounds, resulting in a drifting effect. Furthermore, many challenges remain partly unsolved, such as large scale variations and dynamic appearance changes. The main reason behind these failure cases is likely poor target appearance representations which do not encompass any semantic level information.</p><p>Three major causes can be considered: The lack of a training dataset is the primary problem. Indeed, CNN generally requires training with abundant and representative datasets for specific classes. However, it is difficult to account for all types of target object classes given that the class type is always assumed to be undefined for video object segmentation purposes. Many recent benchmarks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref> (including pixel-level ground-truth labeling) provide a partial solution to this problem but nonetheless cannot cover all possible classes. Secondly, training a network exclusively with the initial frame of a video usually leads to over-fitting. If the network is over-fitted to a specific object appearance in the first image, it cannot deal with appearance changes of the target object in the subsequent frames. Finally, the localization of the target object using a CNN is also a challenging issue, especially in relation to pixel-level accuracy. Most CNN structures encode category-level semantic information using features from deep layers generally exploited for object classification. However, they cannot preserve the spatial details of the target object, which are the key components for object localization. For this reason, the idea of combining higher-level features with lower-level ones has attracted much attention in relation to object localization but a unified and elegant method has yet to be proposed.</p><p>In order to solve the aforementioned problems, our method is mainly inspired by recent breakthroughs in visual tracking. In a sense, visual tracking shares many common features with video object segmentation (i.e. propagating the initial object mask through a series of images). CNN based visual tracking is also a relatively new trend, but various approaches have already been proposed. We can distinguish two types of visual tracking methods using a CNN, as described below: 1) Generative model based tracking aims statistically to describe the appearance of a target and to track the bounding box with the best score compared to previous ones. Some works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref> exploit hierarchical features from different layers to create robust correlation filters. However, these approaches cannot integrate this idea into an end-to-end CNN structure. Nam et al. <ref type="bibr" target="#b17">[18]</ref> pre-train a network using tracking datasets and fine-tune the model using the initial appearance of the target (in the first image), such that their tracker can be adapted to any type of object. Tao et al. <ref type="bibr" target="#b26">[27]</ref> validated that matching with Siamese structure can be robust to various challenging scenarios without model updates.</p><p>2) Discriminative tracking involves the learning of a model that separates distinguishable target objects from the surrounding background. Wang et al. <ref type="bibr" target="#b29">[30]</ref> effectively deal with the identification problems using the features from conv4 and conv5 layers simultaneously. The criterion used to select the layers, however, causes confusion to the network. In other work <ref type="bibr" target="#b30">[31]</ref>, a pre-trained network is frequently updated with the appearance of the object to track. For this purpose, the authors reshape the last fully connected layer encoded with the target objectness and calculate the element-wise loss with a conventional regression model. In consideration of the above tracking approaches, the proposed network embodies the features from lower to higher levels in an end-to-end process. Our learning method also consists of two steps to propose an arbitrary target class adaptation and to cope with appearance variations. The feature extraction part of our network is designed with a Siamese structure to improve the robustness against challenging scenarios. Using the proposed network, our video object segmentation strategy consists of the matching of an object of interest (initialized in the first frame) with subsequent images until the end of a sequence. Therefore, the proposed network must be trained to perform semantic matching between non-successive frames which contain target appearance variations as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Overall descriptions of the proposed network and the target object mask propagation strategy are described in <ref type="figure">Fig. 2</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref> respectively. The main contributions of this paper are three folded as presented below:</p><p>• We propose a novel pixel-level matching network for video object segmentation. Our method shows good computational efficiency as well as higher performance capabilities compared to previous graph based approaches. • We propose a feature compression technique that drastically reduces the memory requirements of the network but that also maintains its representation ability. • We experimentally validate the transferability of our network pre-trained on RGB data wiith different domain like infrared data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head><p>If we directly train the proposed network from scratch (random weight parameters) using only the initial frame of a new sequence, the network weights are over-fitted to the first frame object and cannot handle appearance changes. To prevent this, we split the training into two stages. The first involves network pre-training (off-line). In this stage, training involves semantic matching inference in order to deal with appearance variations using a dataset consisting of 300,000 image pairs. The second stage is network finetuning (on-line) using the appearance of the object in the first image. This step is indispensable because the target object does not necessarily belong to any pre-trained object class. This makes our network versatile enough to deal with any arbitrary target.</p><p>In light of this two-stage learning process, we present specific methods by which to train our pixel-level matching network including its architecture details. We then explain how our model can be applied directly to video object segmentation tasks.  <ref type="figure">Figure 2</ref>. The architecture of the proposed pixel-level matching network. Multi-layered features are extracted from a Siamese structure (blue). Here, all convolutional layers share their weights with mirrored layers including compressors. The pixel-level similarity is then encoded via three more hidden fully connected layers (red). Finally, we discriminatively enforce the object coherency through multiple usages of convolutional layers, finally classifying each pixel into the background and the foreground (green). Zero-padding is properly used to fit the output size, while the pooling size is 2×2 with a stride of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pretraining Pixel-Level Matching Network</head><p>The proposed network is composed of two main parts: pixel-level similarity encoding and target objectness decoding. Our model is pre-trained using 30 video sequences from the DAVIS <ref type="bibr" target="#b20">[21]</ref> dataset which contains various challenging scenarios. A global description of our network is available in <ref type="figure">Fig. 2</ref>.</p><p>Network inputs Since the first part of our network is a Siamese structure, it requires two types of inputs: a reference for the query stream and a target for the search stream. To make our network robust to dynamic appearance variations during the learning process, we choose the target and reference frames randomly (in the same sequence) in order to avoid successive frames (with high similarity).</p><p>To generate the query stream dataset, we randomly select 20 reference frames in each training video. For each frame, we crop the bounding box containing the reference object leaving margins around it (25% larger than the original object box size). This region of interest is then completely segmented using the ground-truth label and resized into a 100×100p image patch.</p><p>Regarding the search stream data, we randomly select six target frames associated with each reference frame in the same sequence. We then generate the search stream data by cropping and resizing (into 100×100p) multiple bounding boxes at various locations around the target object with three different margins sizes (ρ 1 , ρ 2 and ρ 3 ). By doing this, we can train the model to deal with localization and scale variation. The generated search stream data is further augmented by flipping and rotating. Here, the target object is always fully or partially included in the cropped box (overlapping minimum: 50%). In our experiments, training with hard negative data which does not contain any part of a target object has proved to be rather inefficient.</p><p>Pixel-level matching To encode the pixel-wise similarity between the search and query processes, our network initially extracts the features from inputs through several combinations of convolution, max pooling and Rectified Linear Units (ReLU). In the field of visual tracking, one of the major issues is to determine the optimal number of layers to ensure an effective representation of a target object. It was recently found that too many deep layers can be redundant because visual tracking is a binary classification task (foreground or background). Therefore, many works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref> have exploited VGGNet <ref type="bibr" target="#b25">[26]</ref> or proposed much lighter structures <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18]</ref> which demonstrate high performance in terms of accuracy and processing time. Given that our task also belongs to binary classification, we designed our feature extractor similarly. <ref type="figure">Fig. 2</ref>-(blue) shows our feature extraction part. Because our model starts with a Siamese structure, the weights for feature extraction are shared between the search and query streams.</p><p>In the fully connected (FC) layers ( <ref type="figure">Fig. 2-(red)</ref>), the similarity between the query and search inputs is globally encoded using the extracted features. Here, directly feeding the output features from the last layer in the Siamese structure to the initial FC layer is a good strategy to generate the semantic information of a target object. It can, however, also cause critical localization and identification issues due to the lack of spatial details. On the other hand, the use of lower features alone makes it easy to lose the semantic information. To solve this dilemma, we exploit all of the feature instances from the lower to the higher layers and stack them on the initial FC layer. We then use three more hidden FC layers to globally combine the multi-level feature instances which encode the similarity between two object units. The size and number of FC layers are similar to that in earlier work <ref type="bibr" target="#b30">[31]</ref>, which reduces the strong correlation between neighboring pixels to prevent over-fitting problems. A 50×50 matching score table is finally generated by reshaping the last FC layer. However, the use of a large volume of features in the initial FC layer incurs heavy memory requirements which can lead to a critical computational bottleneck. Hence, we compress the output features from each layer (with a data compression ratio of 16) and feed them to the initial FC layer. We call this process compression and the associated components compressors.</p><p>A compressor is composed of three types of layers: 3×3 convolution, ReLU, and Local Response Normalization (LRN). The number of convolution filters is sixteen times smaller than the input channel size. The filter weights of a compressor are also shared between the query and search stream. Note that the output value of ReLU at each layer shows a different scale due to the unbounded nonlinearity. These unbalanced scales can cause confusion when the feature instances are globally combined in the FC layers. Moreover, pre-normalized features before a loss function induce more effective network convergence as validated in Sec. 3.2. We thus include LRN layer at a compressor. Finally, we vectorize and concatenate all of the features from each compressor and feed them to the initial FC layer.</p><p>The inspiration behind the concept of a compressor comes from Wang et al. <ref type="bibr" target="#b29">[30]</ref>. They fix the number of channels exploited simply by discarding redundant features from the outputs of the conv4 and conv5 layers. Using such a technique, they proved that a target object can be effectively represented without noise. This can, however, cause a critical loss of valuable information depending on the type of data or the complexity of a scene. Therefore, we adaptively compress our features in order to reduce redundancy without losing reliable information.</p><p>Objectness decoding The 50×50 matching score table reshaped from the last FC layer encodes the similarity between two input objects. However, direct classification of this matching table at the pixel-level is ineffective due to outliers and inconsistent similarity scores. We thus enforce objectness coherency and reject outliers through several instances of the use of convolutional layers (shaded in green in <ref type="figure">Fig. 2</ref>). Note that one zero padded input is always convoluted by 3×3 filters to maintain the spatial resolution. An example of a clearly separated target object area is available in <ref type="figure" target="#fig_2">Fig. 3</ref>-(h), and the performance according to the number of decoding layers is validated in Sec. 3.2.</p><p>Loss To constraint the network outputs to binary values (background: 1, target: 0) in each pixel, we use the basic element-wise Euclidean distance as a loss function L. Let the P indexes be the probability of the output and let L denote the value of the ground-truth label. The score is then estimated by</p><formula xml:id="formula_0">L = 1 N 2 N x=1 N y=1 P (x, y) − L(x, y) 2 ,<label>(1)</label></formula><p>where L ∈ {0, 1}, N is the output size, which is set to 50, and (x, y) is the pixel location in the probability map. It should be noted that Sigmoid activation can be used before L2 loss in order to prevent a "strong positive probability" pixel which results in a large penalty. In our case, however, the normalization layer in each compressor effectively preadjusts the feature scale to [0, 1], resulting in stable loss convergence with the simple L2 function only ( <ref type="figure" target="#fig_3">Fig. 4-(b)</ref>). <ref type="figure" target="#fig_2">Fig. 3</ref> shows the pipeline of the proposed video object segmentation method for an arbitrary video. Our method to propagate the initial object mask consists of two parts: candidates sampling and optimal area extraction. Before using our pixel-level matching network, however, it must be fine-tuned (second training stage) during the first frame because the network parameters at the end of the pre-training process are optimized exclusively for the objects in the 30 training videos. Thanks to the supervision of the initial target appearance, the model can adapt to any type of object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Object Segmentation</head><p>Initial frame fine-tuning In order to generate the finetuning data, we use only the first frame of a new video (not utilized for training). For the query input, we completely segment the target object and crop the bounding box around it with small margins (this step assumes a manually labeled image). For the search input, we sample multiple boxes around the target object with different margins (ρ 1 , ρ 2 , ρ 3 ) to deal with various scales. It is further augmented by translation and flipping. Here, the target object is always fully or partially included (at least 50% overlapped) for more efficient weight convergence. The input data (about 150 pairs) are finally resized to 100×100p while the corresponding labels are adjusted to 50×50p. We believe that the feature extraction parts are not highly dependent on the appearance of an arbitrary target. Therefore, the weights for the feature extraction parts (the blue boxes in <ref type="figure">Fig. 2)</ref>, except for the compressors, are fixed, whereas the others are updated.</p><p>In the field of visual tracking, many previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31</ref>] use a frequent model update strategy in the course of a new video sequence. In our case, however, we do not update the network after fine-tuning for the following reasons. First, we need to consider the computational efficiency for video processing. In consideration of this, the model update is particularly heavy (about ten times slower than one single feed forward process). Secondly, the model update strategy has a rather negative effect on video segmentation because we cannot obtain a perfect pixel-level label in the middle of the sequence. Therefore, updating the model using slightly mislabeled data can increase the drift.  Therefore, we match the initial frame with current frame until the end of the sequence without any model update.</p><p>Candidates sampling In order to predict reliable target object area in the following frame, we investigate the network responses to multiple candidates and aggregate their information. To do this, we initially sample nine bounding boxes in the next frame in light of the current target object area with certain margins <ref type="figure" target="#fig_2">(Fig. 3 -(a)</ref>). They are then resized into 100×100p patches to fit the input size of the network ( <ref type="figure" target="#fig_2">Fig. 3 -(b)</ref>). Finally, the patches are fed to the fine-tuned network to obtain the probability maps as shown in <ref type="figure" target="#fig_2">Fig. 3 -(c, d, e</ref>).</p><p>Optimal area extraction Because the size of each probability map is 50×50p, the outputs are reshaped back to the original frame size as described in <ref type="figure" target="#fig_2">Fig. 3-(f)</ref>. We then accumulate all of the response maps aligning the pixel locations. Using the combined information, an optimal target area is finally extracted by thresholding with a pre-defined parameter ( <ref type="figure" target="#fig_2">Fig. 3-(h)</ref>). This strategy is particularly efficient because the output of the network differs slightly according to the location of the box. Therefore, we can extract a more reliable target object area through the aggregation of multiple pixel responses, especially in the edge area. An investigation of the network responses to a larger number of sample candidates may lead to greater performances. In practice, however, we determined that using nine samples is a good compromise between speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we describe implementation details, and we demonstrate the validity of the proposed network structure. Comparative evaluations on three different benchmarks show the effectiveness of the proposed method. Finally, we apply our method to region tracking tasks using infrared data to demonstrate the transferability of our network to a different domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation details</head><p>Our CNN pixel-level matching structure is designed and pre-trained using the Caffe toolbox. Our video seg-mentation algorithm (including fine-tuning) is implemented through the MATLAB wrapper of Caffe. All of the experiments are conducted on a desktop computer equipped with a 3.60 GHz CPU and a TITAN X graphic card.</p><p>We pre-trained our network using a Stochastic Gradient Descent (SGD) method with a learning rate of 10 −5 . The momentum and weight decay parameters are fixed to 0.9 and 5×10 −4 respectively. The learning rate is reduced at every 10 epochs, while the parameters of the local size, α and β for the LRN are set to 5, 10 −4 and 7.5×10 −1 respectively. For fine-tuning using the initial frame, the learning rate is twice as large (2×10 −5 ) for more rapid convergence, while the other parameters are identical to the pre-training parameters. In our experiments, ρ 1 , ρ 2 , and ρ 3 are fixed at 10, 30 and 50 respectively. After fine-tuning, online model update does not occur during the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Network Validation</head><p>To demonstrate the efficiency of our pixel-level matching network, we conducted a self-structure evaluation on the DAVIS [21] benchmark with two different metrics: region similarity (intersection-over-union: IoU) and contour accuracy (calculated from the F-measure score). In order to evaluate the system stability, we also report the mean standard deviation of each metric. Here, the speed for one feed forward task in our network is approximately 8 × 10 −3 s, while it takes about 1.5×10 −1 s per frame to deal with the entire process. The major bottleneck (except for the feed forward time) comes from the image resizing step. In Table 1, the results from the proposed pixel-level matching network and its post-processed results are denoted as PLM and PLM P , respectively. PLM P is simply obtained by inserting a mask from PLM with the corresponding image into a publicly available weighted median filter <ref type="bibr" target="#b33">[34]</ref> code (filter size 5, iteration 3). It makes the PLM results to be sharper and effectively rejects outliers, while it does not have a significant impact on the computational time.</p><p>Ablation test for decoding convolutional (DC) layers We conduct an ablation test to find the optimal combina- tion of DC layers (the green parts in <ref type="figure">Fig. 2</ref>). In this assessment, we add DC layers one by one and stop the layer addition process when the accuracy reaches its maximum as depicted in <ref type="figure" target="#fig_3">Fig. 4-(a)</ref>. The performance (in terms of the average score) for each experimental setting (PLM 1˜P LM 10 ) is evaluated on the DAVIS 20 test sequences without finetuning. Here, the first DC layer increases the channel size from 1 to 2 (x−1) , where x is the total number of DC layers, while the subsequent DC layers reduce the size of the previous channel by a factor of two. From this graph, we can also demonstrate the effect of fine-tuning (second stage training) by comparing PLM 9 with PLM because they have identical network structures. Normalization vs. without normalization <ref type="figure" target="#fig_3">Fig. 4-(b)</ref> depicts the effectiveness of the normalization layers in the compressors. The loss severely fluctuates without normalization, while the normalized features lead to stable loss convergence even without sigmoid activation. We further noted that this convergence tendency is similar to the case when using "sigmoid activation + L2 loss" function.</p><p>Use and disuse of compressor As described in Sec. 2.1, owing to the compressor, we could shorten the computational time while also boosting the memory efficiency of the network. In order to highlight the advantages offered by the compression technique, we attempted to train the network without compressors. However, the memory requirement exceeded 12GB which beyond the capacity of our GPU (even for a batch size of 1). This implies that compressors are indispensable when training the network (about 3.7GB with a batch size of 32).</p><p>Features from multiple layers vs. single layer To consider the spatial details and semantic information at the same time, we exploit the features from different layers. To underline the importance of this, we compare our network against the same network but using the features from a single layer only denoted as PLM S . In this case, we directly feed the output of the last convolutional layer (in the Siamese structure) to the initial FC layer. We then train and test the network on the same environment. In <ref type="table">Table 1</ref>, PLM S does not work for every sequence due to ineffective pixel-wise object localization, and it is highly affected by cluttered background causing critical drifting effects.  <ref type="table">Table 1</ref>. Performance evaluation on the DAVIS [21] benchmark. First: IoU score for region similarity (higher is better). Second: F-measure (higher is better) for contour accuracy. Stability is denoted by the mean standard deviation of each metric (lower is better). At the end of the table, we present the approximate processing time for each method as reported in earlier work <ref type="bibr" target="#b16">[17]</ref>. Here, the left part is for a comparative evaluation, while the right one is for the self-structure evaluation. Red: best, blue: second best. For comparison, the following baseline algorithms are assessed: SEA: SeamSeg <ref type="bibr" target="#b24">[25]</ref>, JMP: JumpCut <ref type="bibr" target="#b6">[7]</ref>, NLC: Non-Local Consensus Voting <ref type="bibr" target="#b5">[6]</ref>, HVS: Efficient Hierarchical Graph-Based Video Segmentation <ref type="bibr" target="#b7">[8]</ref>, BVS: Bilateral Space Video Segmentation <ref type="bibr" target="#b16">[17]</ref>, OFL: Video Segmentation via Object Flow <ref type="bibr" target="#b27">[28]</ref>.</p><p>Initial frame matching without a model update vs. successive frames matching with a model updates To prevent the background drift problem and ensure good computational efficiency, our strategy consists of matching the initially fine-tuned query input with the entire sequence. In order to validate the effectiveness of our strategy, we compare it against successive frame matching with a model update scheme. To do so, the query input for frame t + 1 is generated using the output mask of the previous frame t. The network model is updated every ten frames through 50  <ref type="figure">Figure 5</ref>. The average IoU graph at each sequence as evaluated on the DAVIS <ref type="bibr" target="#b20">[21]</ref> benchmark. The performances of most baselines decrease except for NLC as the initial object mask is propagated. Due to the low dependency between successive frames, our method shows stable graph appearance with high accuracy despite the fact that the initial mask reaches the back part of the sequence.</p><p>iterations. Here, the model update is done in a manner similar to that in our fine-tuning scheme (described in Sec. 2.2). The performance of the pixel-level matching model with updating is denoted as PLM U . In <ref type="table">Table 1</ref>, PLM U is comparable to PLM, but it never outperforms PLM. This is mainly due to the fact that model updating and matching between two successive frames using continuously drifted outputs lead to rather degenerated results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparative Evaluation</head><p>In our comparative experiments, we measure how much the key frame object mask is precisely propagated through the sequences on four different benchmarks.</p><p>DAVIS <ref type="bibr" target="#b20">[21]</ref> supplies 30 training datasets and 20 test datasets. This database provides a wide range of challenging scenarios, such as occlusions, dynamic deformations, illumination changes, and scale variations. Note that we did not use any additional datasets to train the network even when testing on different benchmarks.</p><p>As described in <ref type="table">Table 1</ref>, the proposed method generally shows better performance on average compared to all of the other approaches. Among the baselines, OFL is highly comparable but our method is much more efficient in terms of the processing time and system stability which are crucial components in real-world situations. Overall, there are two reasons for these promising results. First, our pixel-level matching network can encompass semantic-level informa-   tion as well as spatial details. Therefore, our method robustly responds to many challenging scenarios such as occlusions, illumination changes, scale variations, and nonrigid motions. Secondly, unlike previous spatio-temporal graph optimization based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, our method handles each frame independently. Therefore, a slight mis-segmentation in the current frame does not have induce critical effects on the following frames. The performance gap between the first and last image of the sequences (see <ref type="figure">Fig. 5</ref>) and the stability in <ref type="table">Table 1</ref> highlight the robustness of our system to the drifting effect problem. Nonetheless, however, background objects of the same class and with an appearance similar to the target object are highly distractive in our system. Furthermore, the small size of the network output shows difficulty in handling thin objects. SegTrack <ref type="bibr" target="#b12">[13]</ref> To validate the performance of the proposed method more thoroughly, we also conducted experiments on this dataset using the following additional baseline algorithms: FST: Fast Object Segmentation in Unconstrained Video <ref type="bibr" target="#b18">[19]</ref>, DAG: Video Object Segmentation through spatially accurate and temporally dense extraction of primary object regions <ref type="bibr" target="#b32">[33]</ref>, TMF: Video segmentation by tracking many figure-ground segments <ref type="bibr" target="#b13">[14]</ref>, and KEY: Key-segments for video object segmentation <ref type="bibr" target="#b11">[12]</ref>. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results. Here, our method shows comparable results but does not always outperform the other approaches. This is mainly due to the fact that our network has not been trained on the same dataset. In addition, the lowquality videos have caused the confusion for our network to distinguish the target object from the background.</p><p>JumpCut In order to validate the effectiveness of our model on non-successive scenarios, we use 22 medium resolution videos published by Fan et al. <ref type="bibr" target="#b6">[7]</ref>. We compare    our algorithm to the following methods: RB: RotoBrush tool from Adobe AfterEffect <ref type="bibr" target="#b1">[2]</ref> based on the SnapCut, and DA: Discontinuity-aware video object cutout <ref type="bibr" target="#b34">[35]</ref>. We measured the performance using the same error metric described in <ref type="bibr" target="#b6">[7]</ref>. Thus, we investigate the transferred mask from the i th key frame (i= 0, 16,..,96) to the (i+d) th frame.</p><p>To do this, we fine-tune the network at the key frame and propagate mask skipping for 16 frames (d=16). We then calculate the average error score in each sequence according to the following equation: Err = 100 N i i # of mislabeled pixels at (i + d) th frame # of foreground pixels at (i + d) th frame ,</p><p>where N i denotes the number of key frames. Overall, the proposed method outperforms all other methods, even without the active contour refinement process introduced in an earlier study <ref type="bibr" target="#b6">[7]</ref>.</p><p>Thermal-Road To demonstrate the applicability of our network to region based tasks such as road tracking, we present an evaluation of the thermal-infrared based road scene data from Yoon et al. <ref type="bibr" target="#b31">[32]</ref>, which contains three different scenarios for a total of approximately 6000 manually annotated images. To generate query and search data on this benchmark, we use the full frame (instead of bounding boxes), as the road occupies nearly the entire image. We conduct a comparison with one hand-crafted feature based method (TD: Thermal-infrared based drivable region detection <ref type="bibr" target="#b31">[32]</ref>) and three different CNN based approaches (AlexNet <ref type="bibr" target="#b10">[11]</ref>, CN24 <ref type="bibr" target="#b2">[3]</ref>, and FCN <ref type="bibr" target="#b14">[15]</ref>). We follow the same error rate metric with <ref type="bibr" target="#b31">[32]</ref> calculated by:</p><formula xml:id="formula_2">ErrorRate = N F P + N F N N P + N N × 100,<label>(3)</label></formula><p>where N F P , N F N , N P , and N N are the number of pixels which are respectively associated with incorrectly detected drivable and non-drivable region, and their groundtruth. As summarized in <ref type="table" target="#tab_5">Table 4</ref>, our method outperforms the state-of-the-art hand-crafted feature based approach (TD), and performs second best overall, closely following the FCN approach. Note that, however, we use only an initial frame for fine-tuning our network pre-trained with color images, whereas FCN is fully supervised with the Thermal-Road datasets. From these experiments, we can validate that the proposed network is transferable to a different domain or task using only a single frame. The results also demonstrate that the proposed network is readily trainable with a simple fine-tuning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we proposed a deep learning based video object segmentation algorithm. Our network is composed of encoding and decoding models which are suitable for pixellevel object matching. Two-stage training allows our network to handle appearance variations while also preventing over-fitting problem. We extensively evaluated our method on three widely used benchmark datasets. The obtained results demonstrate that our method performs better than previous related methods in terms of accuracy, speed, and stability. We also verified the importance of using multilayer features and a compression technique to make the network compact while maintaining its object representation capability. Finally, we proved the transferability of our network to different domains using the thermal infrared database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of a result of the proposed pixel-level matching network. Here, a completely segmented reference frame is fixed to the query and a sample target frame is fed to the search input. Each colored box is associated with the same color inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The proposed video object segmentation strategy for an online sequence: (a) Candidates sampling, (b) Image resizing, (c, d) Feeding search and query inputs, (e) Probability maps for each candidate, (f) Size restoration, (g) Stacking, and (h) Thresholding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) The performance graph according to the number of decoding convolutional layers. The structure PLM9 is the identical to the PLM. (b) The loss graphs for three different conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>HVS NLC JMP SEA BVS OFL PLM PLM P PLM S PLM U blackswan 0.91 0.87 0.93 0.93 0.94 0.94 0.86 0.89 0.27 0.86 0.91 0.82 0.94 0.95 0.96 0.98 0.87 00.01 0.73 0.53 0.66 0.86 0.76 0.77 0.05 0.74 0.54 0.13 0.61 0.47 0.58 0.84 0.63 0.69 0.05 0.60 horse 0.76 0.83 0.58 0.63 0.80 0.86 0.72 0.77 0.38 0.71 jump 0.80 0.88 0.65 0.65 0.80 0.90 0.73 0.78 0.24 0.70 kite 0.40 0.45 0.50 0.48 0.42 0.70 0.59 0.64 0.20 0.25 surf 0.37 0.45 0.31 0.28 0.64 0.49 0.45 0.45 0.15 0.17 libby 0.55 0.63 0.29 0.22 0.77 0.55 0.52 0.54 0.23 0.43 0.64 0.74 0.36 0.21 0.84 0.61 0.57 0.58 0.21 0.45 motorcross 0.09 0.25 0.58 0.38 0.34 0.60 0.49 0.51 0.27 0.44 jump 0.13 0.30 0.54 0.40 0.37 0.47 0.32 0.37 0.06 0.27 paragliding 0.53 0.62 0.59 0.57 0.64 0.63 0.55 0.57 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>HSV FST DAG TMF KEY NLC BVS PLM PLM P birdfall 0.57 0.59 0.71 0.62 0.49 0.74 0.66 0.64 0.65 cheetah 0.19 0.28 0.40 0.37 0.44 0.69 0.10 0.65 0.65 girl 0.32 0.73 0.82 0.89 0.88 0.91 0.89 0.77 0.78 monkeydog 0.68 0.79 0.75 0.71 0.74 0.78 0.41 0.61 0.72 parachute 0.69 0.91 0.94 0.93 0.96 0.94 0.94 0.85 0.88 Average 0.49 0.66 0.72 0.70 0.70 0.81 0.60 0.70 0.73</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Representative results of the proposed method on challenging scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance evaluation on the SegTrack [13] benchmark using IoU score (higher is better). Red: best , blue: second best.</figDesc><table><row><cell></cell><cell>RB</cell><cell>DA</cell><cell>SEA</cell><cell>JMP</cell><cell>PLM</cell><cell>PLMp</cell></row><row><cell>animation</cell><cell>11.9</cell><cell>6.38</cell><cell>6.78</cell><cell>4.55</cell><cell>9.38</cell><cell>5.86</cell></row><row><cell>bball</cell><cell>18.4</cell><cell>8.47</cell><cell>8.89</cell><cell>3.90</cell><cell>12.8</cell><cell>8.04</cell></row><row><cell>bear</cell><cell>4.58</cell><cell>4.48</cell><cell>4.21</cell><cell>4.00</cell><cell>8.60</cell><cell>3.45</cell></row><row><cell>car</cell><cell>1.76</cell><cell>5.93</cell><cell>5.08</cell><cell>2.26</cell><cell>4.12</cell><cell>2.18</cell></row><row><cell>cheetah</cell><cell>31.5</cell><cell>16.6</cell><cell>7.68</cell><cell>8.16</cell><cell>14.1</cell><cell>11.8</cell></row><row><cell>couple</cell><cell>17.5</cell><cell>16.0</cell><cell>23.4</cell><cell>5.13</cell><cell>13.1</cell><cell>9.14</cell></row><row><cell>cup</cell><cell>5.45</cell><cell>12.9</cell><cell>9.31</cell><cell>2.15</cell><cell>8.63</cell><cell>6.04</cell></row><row><cell>dance</cell><cell>56.1</cell><cell>50.8</cell><cell>43.0</cell><cell>18.7</cell><cell>31.5</cell><cell>14.7</cell></row><row><cell>fish</cell><cell>51.8</cell><cell>21.7</cell><cell>25.7</cell><cell>17.5</cell><cell>9.39</cell><cell>7.42</cell></row><row><cell>giraffe</cell><cell>22.0</cell><cell>11.2</cell><cell>17.4</cell><cell>7.40</cell><cell>19.7</cell><cell>17.4</cell></row><row><cell>goat</cell><cell>13.1</cell><cell>13.3</cell><cell>8.22</cell><cell>4.14</cell><cell>17.8</cell><cell>15.2</cell></row><row><cell>hiphop</cell><cell>67.5</cell><cell>51.1</cell><cell>33.7</cell><cell>14.2</cell><cell>19.9</cell><cell>13.6</cell></row><row><cell>horse</cell><cell>8.39</cell><cell>45.1</cell><cell>37.8</cell><cell>6.80</cell><cell>11.5</cell><cell>7.94</cell></row><row><cell>kongfu</cell><cell>40.2</cell><cell>40.8</cell><cell>17.9</cell><cell>8.00</cell><cell>9.74</cell><cell>6.25</cell></row><row><cell>park</cell><cell>11.8</cell><cell>6.54</cell><cell>6.91</cell><cell>5.39</cell><cell>16.5</cell><cell>10.2</cell></row><row><cell>pig</cell><cell>9.22</cell><cell>9.85</cell><cell>10.3</cell><cell>3.43</cell><cell>9.09</cell><cell>5.15</cell></row><row><cell>pot</cell><cell>2.43</cell><cell>5.03</cell><cell>2.98</cell><cell>2.95</cell><cell>5.46</cell><cell>2.66</cell></row><row><cell>skater</cell><cell>38.7</cell><cell>40.8</cell><cell>29.6</cell><cell>22.8</cell><cell>16.8</cell><cell>12.6</cell></row><row><cell>station</cell><cell>8.85</cell><cell>20.9</cell><cell>21.3</cell><cell>9.01</cell><cell>7.31</cell><cell>4.68</cell></row><row><cell cols="2">supertramp 129</cell><cell>60.5</cell><cell>57.4</cell><cell>42.9</cell><cell>30.4</cell><cell>20.7</cell></row><row><cell>toy</cell><cell>1.28</cell><cell>3.19</cell><cell>2.16</cell><cell>1.30</cell><cell>5.07</cell><cell>2.25</cell></row><row><cell>tricking</cell><cell>79.4</cell><cell>70.9</cell><cell>35.8</cell><cell>21.3</cell><cell>21.9</cell><cell>15.7</cell></row><row><cell>Average</cell><cell>28.6</cell><cell>23.7</cell><cell>18.8</cell><cell>9.81</cell><cell>13.7</cell><cell>9.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Errors on the JumpCut [7] benchmark using a transfer distance of sixteen (lower is better). Red: best , blue: second best.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Each color of the output mask is associated with each benchmark: green, red and blue come from DAVIS<ref type="bibr" target="#b20">[21]</ref>, SegTrack<ref type="bibr" target="#b12">[13]</ref> and Thermal-Road<ref type="bibr" target="#b31">[32]</ref> benchmark respectively.</figDesc><table><row><cell></cell><cell>TD</cell><cell cols="2">AlexNet CN24</cell><cell>FCN</cell><cell>PLM</cell><cell>PLMp</cell></row><row><cell>campus</cell><cell>10.7</cell><cell>42.1</cell><cell>36.4</cell><cell>10.3</cell><cell>11.3</cell><cell>9.89</cell></row><row><cell>mountain</cell><cell>14.0</cell><cell>41.1</cell><cell>21.1</cell><cell>6.34</cell><cell>11.4</cell><cell>10.4</cell></row><row><cell>city</cell><cell>12.6</cell><cell>29.1</cell><cell>28.0</cell><cell>9.77</cell><cell>11.95</cell><cell>11.5</cell></row><row><cell>Average</cell><cell>11.8</cell><cell>39.0</cell><cell>36.7</cell><cell>9.45</cell><cell>11.5</cell><cell>10.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Error rate (lower is better) evaluation on the Thermal-Road<ref type="bibr" target="#b31">[32]</ref> benchmark. Red: best , blue: second best.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video snapcut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Convolutional patch networks with spatial prior for road detection and urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-A</forename><surname>Brust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06344</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive video segmentation using occlusion boundaries and temporally coherent superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dondera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="784" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jumpcut: non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">195</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07945</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-T</forename><forename type="middle">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3227" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Livecut: Learningbased interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05863</idno>
		<title level="m">Siamese instance search for tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Transferring rich feature hierarchies for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04587</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Thermal-infrared based drivable region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="978" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">100+ times faster weighted median filter (wmf)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2830" to="2837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discontinuityaware video object cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
