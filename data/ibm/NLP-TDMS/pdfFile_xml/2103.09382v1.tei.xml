<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPICE: Semantic Pseudo-labeling for Image Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Niu</surname></persName>
							<email>niuc@rpi.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>110 8th Street</addrLine>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
							<email>wangg6@rpi.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>110 8th Street</addrLine>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPICE: Semantic Pseudo-labeling for Image Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents SPICE, a Semantic Pseudo-labeling framework for Image ClustEring. Instead of using indirect loss functions required by the recently proposed methods, SPICE generates pseudo-labels via self-learning and directly uses the pseudo-label-based classification loss to train a deep clustering network. The basic idea of SPICE is to synergize the discrepancy among semantic clusters, the similarity among instance samples, and the semantic consistency of local samples in an embedding space to optimize the clustering network in a semantically-driven paradigm. Specifically, a semantic-similarity-based pseudo-labeling algorithm is first proposed to train a clustering network through unsupervised representation learning. Given the initial clustering results, a local semantic consistency principle is used to select a set of reliably labeled samples, and a semi-pseudo-labeling algorithm is adapted for performance boosting. Extensive experiments demonstrate that SPICE clearly outperforms the state-of-the-art methods on six common benchmark datasets including STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet. On average, our SPICE method improves the current best results by about 10% in terms of adjusted rand index, normalized mutual information, and clustering accuracy. Codes are available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image clustering aims to group images into different clusters without human annotations, and is an essential task in unsupervised learning especially for computer vision. The characterization of similarity and discrepancy is at the core of all clustering methods. Particularly, the similarity measurement of images depends on the representation features that are semantically and contextually dependent. Given an imperfect representation, how to effectively cluster samples with high-level semantics is another challenge, especially for clustering images of complicated contents.</p><p>By combining deep learning techniques with traditional clustering algorithms, some deep clustering methods were proposed to learn representation features and perform clustering simultaneously and alternatively <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref>. However, these encoder-decoder-based methods hardly capture discriminitative features of complex images. Thus, a number of methods were proposed to learn discriminitative label features under various constraints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. However, these methods have limited performance when directly using the label features to measure the similarity among samples. This is because the category-level features lose too much instance-level information to accurately measure the relations of instances. Recently, SCAN <ref type="bibr" target="#b33">[34]</ref> was proposed to leverage the embedding features of a representation learning model to search for similar samples across the whole dataset, and then en-courage the model to output the same labels for similar instances, achieving significantly better results. However, the local nearest samples in the embedding space do not always have the same semantics especially when the samples lie around the borderlines of different clusters as shown in <ref type="figure" target="#fig_0">Fig.  1-(a)</ref> and (c), which may compromise the performance. Essentially, these label-feature-based methods aim to train a classification model with an objective function of multiple indirect losses mainly based on the instance similarity.</p><p>In this paper, we propose a Semantic Pseudo-labeling Image ClustEring (SPICE) framework that explicitly leverages both the discrepancy among semantic clusters and the similarity among instance samples to adaptively label training samples in batch-wise. In contrast to the instance similarity among individual samples, we call the similarity between the cluster prototype and instance samples as the semantic similarity, as shown in <ref type="figure" target="#fig_0">Fig. 1-(a)</ref>. Thus, constraining semantically similar samples to have the same label can reduce the semantic inconsistency of borderline samples when the semantic clusters are well captured. To this end, we propose a semantic pseudo-labeling method that assigns the same cluster label to semantically similar samples. Specifically, for each batch of samples, we use the most confident samples predicted by the classification model to compute the prototype of each cluster, and then spread the semantic labels of prototypes to their neighbors based on the semantic similarity. Given these pseudo labels, the classification model can be optimized using the classification loss directly, which is then used to compute prototypes in the next iteration. On the other hand, the semantic inconsistency may appear around borderlines and other places. To reduce the inconsistency of similar samples, we design a local consistency principle to select a set of reliably labeled images from the clustering results as in <ref type="figure" target="#fig_0">Fig. 1-(b)</ref>, and then reformulate the unsupervised task into a semi-supervised pseudo-labeling process for performance boosting.</p><p>Our main contributions are as follows: First, a novel SPICE framework is proposed for image clustering, which directly uses the classification loss to train the classification network through the pseudo-labeling processes that synergize both the discrepancy among semantic clusters and the similarity among instance samples. Second, a pseudo-labeling method is designed to utilize the semanticsimilarity measurement during training and reduce the semantic inconsistency of the samples around borderlines. In practice, it is operated batch-wise to optimize a light-weight classifier such that it is scalable to large datasets. Third, a double softmatx cross-entropy loss function is designed to enforce the model making confident predictions, which effectively boosts the clustering performance. Fourth, the designed local consistency principle effectively reduces the semantic inconsistency, and boosts the clustering performance by transforming the original clustering problem into a semi-supervised learning paradigm. Fifth, SPICE clearly outperforms the state-of-the-art approaches on six image clustering benchmarks, and codes have been made publicly available at https://github.com/niuchuangnn/SPICE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Unsupervised Deep Clustering methods have shown significant superiority over traditional clustering algorithms, especially in computer vision. In a data-driven fashion, deep clustering can effectively utilize the representation ability of deep neural networks. Initially, several methods were proposed to combine the stacked auto-encoders (SAE) with the traditional clustering algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref>, such as k-means <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6]</ref> and spectral clustering <ref type="bibr" target="#b19">[20]</ref>. However, since the pixel-wise reconstruction loss of SAE tends to over-emphasize low-level features, these methods have an inferior performance when clustering images of complex contents due to lack of object-level semantics. Instead of using SAE, JULE <ref type="bibr" target="#b38">[39]</ref> and Deep-Clustering <ref type="bibr" target="#b2">[3]</ref> alternately perform the traditional clustering algorithms, such as agglomerative clustering and k-means respectively, for clustering, and then train the convolutional neural network (CNN) with the cluster indices for representation learning. However, the performance of these methods can be compromised by the errors accumulated during the alternation, and their successes in online scenarios are limited as they need to perform clustering on the entire dataset.</p><p>Recently, novel methods emerged that directly learn to map images into label features, which are used as the representation features during training and as the one-hot encoded cluster indices during testing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27]</ref>. Actually, these methods aim to train the classification model in the unsupervised setting while using multiple indirect loss functions, such as sample relations <ref type="bibr" target="#b4">[5]</ref>, invariant information <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>, mutual information <ref type="bibr" target="#b34">[35]</ref>, partition confidence maximisation <ref type="bibr" target="#b16">[17]</ref>, attention <ref type="bibr" target="#b27">[28]</ref>, and entropy <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27]</ref>. However, the performance of these methods may be sub-optimal when using such label features to compute the similarity and discrepancy between samples, as the category-level label features can hardly reflect the relations of instance-level samples accurately. Therefore, SCAN <ref type="bibr" target="#b33">[34]</ref> was proposed to use embedding features of the representation learning model for computing the instance similarity, based on which the label features are learned by encouraging nearest samples to have the same label. However, since the embedding features are not perfect, similar instances do not always have the same semantics especially when the samples lie near the borderlines of different clusters, which may degrade the clustering performance. Unsupervised Representation Learning maps samples/images into semantically meaningful features without human annotations, which facilitates various down-stream tasks, such as object detection and classification. Previ- ously, various pretext tasks were heuristically designed for this purpose, such as colorization <ref type="bibr" target="#b40">[41]</ref>, rotation <ref type="bibr" target="#b11">[12]</ref>, jigsaw <ref type="bibr" target="#b28">[29]</ref>, etc. Recently, contrast learning methods combined with data augmentation strategies achieved great successes, such as SimCLR <ref type="bibr" target="#b6">[7]</ref>, MOCO <ref type="bibr" target="#b15">[16]</ref>, and BYOL <ref type="bibr" target="#b13">[14]</ref>, just to name a few. Semi-supervised Classification methods reduce the requirement of labeled data for training a classification model by providing a means of leveraging unlabeled data. In this category, remarkable results were obtained with consistency regularization <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref> that constrains the model to output the same prediction for different transformations of the same image, pseudo-labeling <ref type="bibr" target="#b23">[24]</ref> that uses confident predictions of the model as the labels to guide training processes, and entropy minimization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref> that steers the model to output high-confidence predictions. MixMatch <ref type="bibr" target="#b1">[2]</ref> algorithm combines these principles in a unified scheme and achieves an excellent performance, which is further improved by ReMixMatch <ref type="bibr" target="#b0">[1]</ref> along this direction. Recently, FixMatch <ref type="bibr" target="#b30">[31]</ref> proposed a simplified framework that uses the confident prediction of a weakly transformed image as the pseudo label when the model is fed a strong transformation of the same image, delivering superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework of SPICE</head><p>The proposed SPICE framework consists of three training stages. We first pretrain an unsupervised representa-tion learning model adapted from that used in SCAN <ref type="bibr" target="#b33">[34]</ref>, and then the CNN backbone of the pretrained model is frozen to extract embedding features in the following two stages: SPICE-Self and SPICE-Semi, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, SPICE-Self aims to train a classification model based on the embedding features in the unsupervised setting. SPICE-Self has three branches: the first branch takes original images as inputs and outputs the embedding features, the second branch takes the weakly transformed images as inputs and outputs the semantic labels, and the third branch takes the strongly transformed images as inputs and predicts the cluster labels. Given the outcomes of the first two branches, a pseudo-labeling algorithm based on the semantic-similarity generates the pseudo-labels to supervise the third branch. In practice, SPICE-Self only needs to train the light-weight classification head in the third branch, as analyzed in Subsection 4.5.2. To further improve the performance, SPICE-Semi first determines a set of reliably labeled images based on a local semantic consistency principle from the clustering results of SPICE-Self, and then the clustering task is reformulated as a semi-supervised learning problem to retrain the classification model. Finally, the trained classification model is able to predict the cluster labels of both the training images and the unseen images beyond training. In the following two sub-sections, we will introduce the details of SPICE-Self and SPICE-Semi respectively.</p><p>Algorithm 1: SPICE-Self Algorithm.</p><formula xml:id="formula_0">Input: Dataset X = {xi} N i=1 , CNN parameters θ B , k, M , m1, m2, r, E, α, A Output:</formula><p>Cluster label li of xi ∈ X 1 Set parameters of CNN to θ B , e = 0, and randomly initialize θ cls ;</p><formula xml:id="formula_1">2 while e &lt; E do 3 for b ∈ {1, 2, . . . , N M } do 4</formula><p>Step-1:</p><formula xml:id="formula_2">5 Select M samples as X b from X ; 6 for u ∈ {1, 2, . . . , M m 1 } do 7 Select m1 samples as Xu from X b ; 8 Compute embedding features fu = Φ(Xu; [θ B ]) ; 9</formula><p>Weakly transform samples Xw = α(Xu) ; Concatenate all embedding features as F ; <ref type="bibr" target="#b12">13</ref> Concatenate all predicted probabilities as P ; <ref type="bibr" target="#b13">14</ref> Step-2: <ref type="bibr" target="#b14">15</ref> Calculate cluster centers based on [P ; F ] and Eqs. <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula" target="#formula_4">(2)</ref> ; <ref type="bibr" target="#b15">16</ref> Select samples Xt from X b and assign labels Lt with Eq. (3) ; <ref type="bibr" target="#b16">17</ref> Step-3: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SPICE-Self</head><p>Given an image dataset X = {x i } N i=1 and the parameters θ B of the pre-trained CNN, SPICE-Self aims to group these images into the predefined k clusters by training a classification model; i.e., p i = Φ(x i ; [θ B , θ cls ]), where p i is the predicted probability over k clusters, Φ denotes the function of the neural network, and θ cls denotes the parameters of a multilayer perceptron (MLP), which is referred to as CLSHead. However, in the unsupervised setting we do not have the ground truth to train the model. Instead, we design a semantic-similarity based pseudo-labeling algorithm that dynamically estimates the pseudo labels for batch-wise samples in the training process, as in Algorithm 1.</p><p>Specifically, SPICE-Self consists of three steps in each training iteration. First, we compute the embedding features, F = [f i ] M ×D , of a large batch samples, X b , and the corresponding semantic predictions, P = [P ij ] M ×k , for the weakly transformed samples, X w = α(X b ), where M is batch size, D is the dimension of the feature vector, and α is the weak transformation. Due to the limited memory, we divide the large batch into mini-batches to compute these results, where each mini-batch has m 1 samples.</p><p>Second, we use the top confident samples to estimate the prototype for each cluster based on P and F , and then the indices of the cluster centers are assigned to their nearest neighbors as the pseudo labels. Formally, the top confident samples for each cluster are defined as</p><formula xml:id="formula_3">p sort c = sort({P ic |i ∈ [1, · · · , M ]}), p * = p sort c (n t ), n t = r × M/k id c = {i|P ic ≥ p * , i ∈ [1, · · · , M ]},<label>(1)</label></formula><p>where p sort c denotes the descendingly sorted probabilities of cluster c over M samples, n t is the number of top confident samples, r is the confident ratio, M/k means the balance assignment of M samples to k clusters, p * denotes the n tth largest probability, and id c denotes the indices of the top confident samples. Naturally, the cluster centers {γ c } k c=1 in the embedding space are computed as</p><formula xml:id="formula_4">γ c = 1 n t i∈idc f i .<label>(2)</label></formula><p>Based on the the cosine similarity between embedding features and the cluster center γ c , we select n nearest samples, denoted by N c , for each cluster, and assign the pseudo label of c to these samples, which can be formally described as</p><formula xml:id="formula_5">L = {c|x i ∈ N c , ∀c ∈ [0, k − 1], ∀x i ∈ X t },<label>(3)</label></formula><p>where L denotes the pseudo labels corresponding to the selected samples X t of all clusters. Without loss of generality, we assume that the training set has a balanced number of samples over clusters and set n = M/k.</p><p>A toy example of this pseudo-labeling process is shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, where there is a batch of 10 samples and 3 clusters, 2 confident samples for each cluster are selected to calculate the prototypes based on the semantic predictions and embedding features, and then 3 nearest samples to each cluster are selected and labeled. Note that there may exist overlapped samples between different clusters, so there are two options to handle these labels: one is the overlap assignment that one sample may have more than one cluster labels as indicated by the yellow and red circles in <ref type="figure" target="#fig_1">Fig.  2(c)</ref>, and the other is non-overlap assignment that all samples have only one cluster label as indicated by the dished red circle. We found that the overlap assignment is better as analyzed in Subsection 4.5.2.</p><p>Third, we update the parameters of CLSHead using the labeled samples. Specifically, the selected images X t are first transformed with a strong augmentation operator A, i.e., X a = A(X t ). Then, given the labeled dataset [X a ; L], a cross-entropy loss function with a double softmax activation function is used to optimize the classification model in terms of the loss function Eq. (4). The basic idea behind this loss function is to encourage the model to output confident predictions, which helps learn from unlabeled data <ref type="bibr" target="#b12">[13]</ref>.</p><formula xml:id="formula_6">L DSCE = − 1 m 2 m2 i=1 k−1 c=0 1(L i = c) log p ic , p i = e pi k−1 j=0 e pj ,<label>(4)</label></formula><p>where 1 denotes the indicator function, and m 2 is the minibatch size. Note that p i is already the output of the softmax function, and p i is the output of two cascaded softmax functions, and thus we name L DSCE as double softmax crossentropy (DS-CE). As the probability p i ∈ [0, 1], we have</p><formula xml:id="formula_7">p i ∈ [ 1 k−1+e , e k−1+e ] ⊂ [0, 1].</formula><p>Hence, the value of DS-CE is consistently larger than that of CE, e.g., even when the prediction p i is ideal one-hot encoding L DSCE is still not zero, which will enforce p i to be confident.</p><p>In this stage, we fix the parameters of CNN from the representation learning, and only optimize the light-weight CLSHead. Thus, the computational burden is significantly reduced so that we can train multiple CLSHeads simultaneously and independently. By doing so, the instability of clustering from the initialization can be effectively alleviated through selecting best CLSHeads. Specifically, the best head can be selected for the minimum loss value of L DSCE over the whole dataset; i.e, we set M = N and follow Steps 1, 2, and 3 in Algorithm 1 to compute the loss value. During testing, the trained model with the best CLSHead is used to classify the input images into different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SPICE-Semi</head><p>Given the clustering results and embedding features,</p><formula xml:id="formula_8">{(x i , l i , f i )} N i=1</formula><p>, from SPICE-Self, here we aim to further improve the clustering performance by retraining a classification model with a semi-supervised learning paradigm. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, we first select a set of reliably labeled images to alleviate the local semantic inconsistency. Specifically, for each sample (x i , l i , f i ), we can select its n s nearest samples according to the cosine similarity of embedding features, and the corresponding labels of these nearest samples are denoted by L ns . Then, the local consistency β i of the sample x i is defined as</p><formula xml:id="formula_9">β i = 1 n s lj ∈Ln s 1(l j = l i ).<label>(5)</label></formula><p>If β i &gt; τ c , then the sample (x i , l i ) is regarded as the reliably predicted and used as the labeled images in SPICE-Semi. Given the partially labeled images, we adopt a simple semi-supervised learning framework, i.e., FixMatch <ref type="bibr" target="#b30">[31]</ref>, to retrain the classification model, as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. During training, the reliable labels of local consistent samples keep fixed, and the pseudo labels of unlabeled images are adaptively generated by thresholding the confident predictions. Formally, the loss function of SPICE-Semi is</p><formula xml:id="formula_10">L semi = 1 B B b=1 H(l b , Φ (α(x b ); θ ))+ 1 µB B b=1 1(max(q b ) ≥ τ )H(q b , Φ (A(u b ); θ )),<label>(6)</label></formula><p>where B and µB denote the number of labeled and unlabeled images in each training batch, H is the cross-entropy loss function, Φ denotes the classification network with the parameter of θ, l b is the label of weakly-transformed im-</p><formula xml:id="formula_11">age α(x b ), Φ(A(u b ); θ ) is the prediction of the strongly- augmented image A(u b ), u b denotes an unlabeled image, q b = Φ(α(u b ); θ )</formula><p>is the prediction of a weakly-transformed image,q b = arg max(q b ) is the pseudo label, and τ is threshold confidence. As in FixMatch <ref type="bibr" target="#b30">[31]</ref>, all images including the labeled samples are used as unlabeled images. Thus, the first term in Eq. <ref type="formula" target="#formula_10">(6)</ref> is to learn cluster semantics from the semantic consistent predictions of SPICE-Self, and the second term constrains all samples to have the consistency predictions for different transformations. We evaluated the performance of SPICE on six commonly used image clustering datasets, including STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet. The key details on each dataset are summarized in <ref type="table" target="#tab_1">Table 1</ref>, where the datasets reflect a diversity of image sizes, the number of images, and the number of clusters. We used three popular metrics to evaluate clustering results, including Adjusted Rand Index (ARI) <ref type="bibr" target="#b18">[19]</ref>, Normalized Mutual Information (NMI) <ref type="bibr" target="#b31">[32]</ref> and clustering Accuracy (ACC) <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark datasets and evaluation metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>For fair comparison, we adopted the same backbone network used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17]</ref> for representation learning and SPICE-Self. The CLSHead in SPICE-Self consists of two fully-connected layers; i.e., D × D → D × k, where D and k are the feature dimension and the number of clusters respectively. For ImageNet-10 and ImageNet-Dog, we used <ref type="table">Table 2</ref>. Comparison with competing methods. '*' indicates that the methods were trained and evaluated on two split datasests (training and testing images are mutually exclusive), and the remaining methods were trained and evaluated on the whole dataset (training and testing datasets are identical). The best results are highlighted in bold. <ref type="table" target="#tab_1">ImageNet-10  ImageNet-Dog-15  Cifar10  Cifar100-20  Tiny-ImageNet-200  ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI</ref>  the pretrained model on ImageNet in MoCo-v2 <ref type="bibr" target="#b15">[16]</ref>. We directly used the original images without resizing for all datasets. In SPICE-Semi, we adopted the same networks used in FixMatch <ref type="bibr" target="#b30">[31]</ref>. Specifically, we used WideResNet-28-2 for Cifar10, WRN-28-8 for CIFAR-100, and WRN-37-2 for STL-10, ImageNet-10, ImageNet-Dog, and Tiny-ImageNet. For representation learning, we used MoCo-v2 <ref type="bibr" target="#b15">[16]</ref> in all our experiments, which was also used in SCAN <ref type="bibr" target="#b33">[34]</ref>. For weak augmentation, a standard flip-and-shift augmentation strategy was implemented as in FixMatch. For strong augmentation, we adopted the same strategies in SCAN <ref type="bibr" target="#b33">[34]</ref>. Specifically, the images were strongly augmented by composing Cutout <ref type="bibr" target="#b8">[9]</ref> and four randomly selected transformations from RandAugment <ref type="bibr" target="#b7">[8]</ref>. We empirically set M to 1,000 for STL10, Cifar10, and ImageNet-10 that contain 10 clusters, 1,500 for ImageNet-Dog with 15 clusters, 2,000 for Cifar100-20 with 20 clusters, and 5,000 for TinyImageNet with 200 clusters. We set m 2 to 256 for TinyImageNet, and 128 for the remaining datasets. The hyper-parameter m 1 can be any value according to the device memory and will not affect the performance. In SPICE-Self, the model consists of 10 CLSHeads, and the best head with the minimum loss was automatically selected as the final head in each trial. We set cofident ratio r to 0.5. To select the reliably labeled images in SPICE-Semi, we empirically set n s = 100 and τ c = 0.95. We set µ = 7 and τ = 0.95 that are the same as those in FixMatch <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method STL10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main results</head><p>We evaluated SPICE on six image clustering benchmarks on both the whole dataset and split datasets, and compared it with the most recent deep clustering methods shown in <ref type="table">Table 2</ref>. For clustering on the whole dataset, all training and testing images were combined to train and eval-uate models. In this task, SPICE improved ACC, NMI, and ARI by 13.8%, 12.8%, and 14.4% respectively relative to the best results recently reported by CC <ref type="bibr" target="#b26">[27]</ref>. Similarly, our proposed method also improved ACC, NMI, and ARI by 10+% on ImageNet-Dog-15, Cifar-10, Cifar100-20, Tiny-ImageNet-200, and by 7.6%, 6.8%, and 11.1% on ImageNet-10. Without the boosting stage, SPICE-Self is still better than the exiting deep clustering methods on all datasets. In addition, SPICE-Semi cannot boost the performance significantly for ImageNet-10 and ImageNet-Dog-15. The reason may be that the representation features are well pre-trained on the whole ImageNet, and thus hard to be further improved with sub-datasets.</p><p>For clustering on split datasets, the images in the training and testing datasets were used to train and test the models separately, i.e., the testing images were not used for training including representation learning. In this task, SPICE improved ACC, NMI, and ARI by 12.0%, 16.2%, and 20.7% on STL10, by 3.4%, 6.1%, and 6.4% on Cifar10, and by 7.7%, 9.7%, and 8.9% on Cifar100-20, respectively.</p><p>In clustering the images in Tiny-ImageNet-200, although our results are significantly better than the existing results, there were still some weaknesses. This is mainly due to the class hierarchies; i.e., some classes share the same supper class, as analyzed in <ref type="bibr" target="#b33">[34]</ref>. Due to the low performance, some clusters cannot be reliably labeled based on the local consistency principle so that SPICE-Semi cannot be applied for further boosting, which is a limitation of SPICE-Semi that needs to be addressed in the future.</p><p>Overall, our comparative results systematically demonstrate the superiority of the proposed SPICE method on both the whole and split datasets. In this subsection, we further compared SPICE with the recently proposed semi-supervised learning methods including MixMatch <ref type="bibr" target="#b1">[2]</ref>, UDA <ref type="bibr" target="#b36">[37]</ref>, ReMinMatch <ref type="bibr" target="#b0">[1]</ref>, and FixMatch <ref type="bibr" target="#b30">[31]</ref>, as shown in <ref type="table" target="#tab_2">Table 3</ref>. Here the semisupervised learning methods used 250 and 1,000 samples with ground truth labels on Cifar10 and STL10 respectively. It is found in our experiments that SPICE is comparable to and even better than these state-of-the-art semi-supervised learning methods. Actually, these results demonstrate that SPICE-Self with local consistency selection can reliably label a set of images with different semantics without human interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with semi-supervised learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Empirical analysis</head><p>In this subsection, we empirically analyze the effectiveness of different components and options in the proposed SPICE framework. We visualized the semantic clusters learned by SPICE-Self in terms of the prototype samples and the discriminative regions, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. Specifically, in each column the top three nearest samples to cluster centers represent the cluster prototypes, and in each image with an overlapped heat map the discriminative region is highlighted. The heat map is computed by computing the cosine similarity between the cluster center (with Eq. <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula" target="#formula_4">(2)</ref>) of the whole dataset and the convolutional feature map of the individual image, and then resized and normalized into [0, 1]. It shows that the top three samples exactly match the human annotations, and the discriminative regions focus on the semantic objects. For example, the cluster with label '3' captures the 'cat' class, and its most discriminative region covers the cat head. The visual results indicate that semantically meaningful clusters are learned, and the cluster center vectors can define the discriminative regions. We evaluated the effectiveness of different components of SPICE in an ablation study, as shown in <ref type="table">Table 5</ref>. In each experiment, we replaced one component of SPICE-Self with another option, and five trials were conducted to report the mean and standard deviation for each metric. Specifically, when we directly applied the k-means algorithm on the representation features, the clustering results are significantly degraded, indicating that the clustering performance depends on not only the representation features but also a better clustering algorithm. Then, we replaced DS-CE with CE or the CE with a temperature parameter (TCE), which was set to 0.2 for enforcing the model to output confident predictions. The results show that TCE is better than CE, which is consistent with the literature. On the other hand, the results of TCE are inferior to that with DS-CE. In addition, we do not need to set the hyper-parameter for DS-CE as defined in Eq. (4). The results also show that the overlap assignment is preferred over the non-overlap assignment, as introduced in Subsection 3.2, which may be explained by the fact that the non-overlap assignment may introduce extra local inconsistency when assigning the label to a sample far away from the cluster center, as shown in <ref type="figure" target="#fig_1">Fig. 2 (dashed red circle)</ref>. Next, we made the CNN backbone trainable and trained the model with a single head (Conv-SH) or multiple heads (Conv-MH) end-to-end, the corresponding results became significantly worse. Actually, the quality of pseudo labels not only depends on the similarity measurement but also the predictions of the classifier. When tuning all parameters during training, the model tends to output incorrect predictions in the initial stage, which will harm the pseudo-labeling quality and get trapped in a bad cycle. Finally, we added another entropy loss during training. The results were not changed, showing that our pseudo-labeling process alone is sufficient to prevent trivial solutions. However, when clustering a large number of clusters, e.g., 200 clusters in Tiny-ImageNet, the entropy loss was found necessary to avoid the empty clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Visualization of cluster semantics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Model selection</head><p>In unsupervised learning, the training process is hard to converge to the best state without the ground truth supervision. Thus, how to estimate the performance of models in the unsupervised training process is very important. As introduced in Subsection 3.2, we use the classification loss defined in Eq. (4) on the whole test dataset to approximate the model performance; i.e., the smaller loss, the better model. The model selection process is shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, it can be seen that the performance of the selected model is very close to that of the ground truth selection, proving the effectiveness of this loss metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Effect of local consistency</head><p>In Subsection 3.3, we introduced a local semantic consistency principle to select the reliably labeled images. Here we show the effectiveness of this principle in <ref type="figure" target="#fig_6">Fig. 5</ref>, where t-SNE was used to map the representation features of images in Cifar10 to 2D vectors for visualization. In <ref type="figure" target="#fig_6">Fig.  5(a)</ref>, some obvious inconsistent semantic samples are evident, and the predicted ACC of SPICE-Self on all samples is 83.8%. Using these samples for training SPICE-Semi, the ACC is not boosted. In <ref type="figure" target="#fig_6">Fig. 5(b)</ref>, the selected samples are shown, where the ratio of local inconsistency samples is significantly decreased, and the ACC is increased to 95.9% correspondingly. Using the selected samples for training, the ACC of SPICE-Semi is significantly boosted (92.6% v.s. 83.8%) compared with that of SPICE-Self. <ref type="table">Table 5</ref>. Ablation studies of SPICE-Self on STL10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Effect of data augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aug1</head><p>Aug2 ACC NMI ARI Weak Weak 0.905 ± 0.002 0.815 ± 0.003 0.808 ± 0.003 Strong Weak 0.883 ± 0.029 0.799 ± 0.019 0.781 ± 0.031 Strong Strong 0.902 ± 0.008 0.812 ± 0.009 0.803 ± 0.013 Weak strong 0.908 ± 0.001 0.817 ± 0.002 0.812 ± 0.002</p><p>We evaluated the effects of different data augmentations to SPICE-Self, as shown in <ref type="table" target="#tab_1">Table 1</ref>, where Aug1 and Aug2 correspond to data augmentations of the second and the third branches in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. The results show that when the labeling branch used the weak augmentation and the training branch used the strong augmentation, the model achieved the best performance. Moreover, the model had relatively worse performance when the labeling process used the strong augmentation, which is due to that the labeling branch aims to generate reliable pseudo labels that will be compromised by the strong augmentation. The model performed better when the training branch used the strong augmentation, as it will drive the model to output consistent predictions of different transformations. Overall, the data augmentation have a small impact on the results, because the pre-trained CNN had been already equipped with the transformation invariance ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a semantic framework for clustering, with the acronym "SPICE". SPICE synergizes the discrepancy between semantic clusters, the similarity among instance samples and the inconsistency of neighboring samples in the embedding space. To capitalize this synergy, SPICE consists of two components for (1) self-learningbased pseudo-labeling (SPICE-Self) and (2) semi-pseudolabel-based classification (SPICE-Semi). Extensive experiments have demonstrated the superiority of SPICE over the competing methods on six public datasets with an average performance boost of 10% in terms of adjusted rand index, normalized mutual information, and clustering accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Semantic relevance in the embedding space. (a) Global semantic similarity v.s. local instance similarity, where each point denotes a sample in the embedding space, stars denote the cluster centers, different colors denote different clusters, and black circles and lines indicate similar samples; (b) local semantic consistency to determine reliable and unreliable samples; and (c) neighboring samples of different semantics, where the first image is the query image and the other five images are nearest images with closest embedding features provided by SCAN [34].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the SPICE framework. (a) SPICE-Self trains a classification model via pseudo labeling, where CNN is fixed after pretraining through representation learning. (b) SPICE-Semi retrains the classification model via semi-pseudo-labeling, where reliable labels are selected from the results from SPICE-Self according to local consistency of neighboring samples. (c) A toy example of pseudo labeling, where red, green, and blue denote different clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10</head><label></label><figDesc>Predict probabilities pu = Φ(Xw; [θ B , θ cls ]) ; 11 end 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ARI JULE<ref type="bibr" target="#b38">[39]</ref> 0.277 0.182 0.164 0.300 0.175 0.138 0.138 0.054 0.028 0.272 0.192 0.138 0.137 0.103 0.033 0.033 0.102 0.006 DEC<ref type="bibr" target="#b35">[36]</ref> 0.359 0.276 0.186 0.381 0.282 0.203 0.195 0.122 0.079 0.301 0.257 0.161 0.185 0.136 0.050 0.037 0.115 0.007 DAC [5] 0.470 0.366 0.257 0.527 0.394 0.302 0.275 0.219 0.111 0.522 0.396 0.306 0.238 0.185 0.088 0.066 0.190 0.017 DeepCluster [3] 0.334 N/A N/A N/A N/A N/A N/A N/A N/A 0.374 N/A N/A 0.189 N/A N/A N/A N/A N/A DDC [4] 0.489 0.371 0.267 0.577 0.433 0.345 N/A N/A N/A 0.524 0.424 0.329 N/A N/A N/A N/A N/A N/A IIC [21] 0.610 N/A N/A N/A N/A N/A N/A N/A N/A 0.617 N/A N/A 0.257 N/A N/A N/A N/A N/A DCCM [35] 0.482 0.376 0.262 0.710 0.608 0.555 0.383 0.321 0.182 0.623 0.496 0.408 0.327 0.285 0.173 0.108 0.224 0.038 GATCluster [28] 0.583 0.446 0.363 0.762 0.609 0.572 0.333 0.322 0.200 0.610 0.475 0.402 0.281 0.215 0.116 N/A N/A N/A PICA [17] 0.713 0.611 0.531 0.870 0.802 0.761 0.352 0.352 0.201 0.696 0.591 0.512 0.337 0.310 0.171 0.098 0.277 0.040 CC [27] 0.850 0.746 0.726 0.893 0.859 0.822 0.429 0.445 0.274 0.790 0.705 0.637 0.429 0.431 0.266 0.140 0.340 0.071 SPICE-Self 0.908 0.817 0.812 0.969 0.927 0.933 0.546 0.498 0.362 0.838 0.734 0.705 0.468 0.448 0.294 0.305 0.449 0.161 SPICE 0.938 0.872 0.870 0.967 0.917 0.929 0.554 0.504 0.343 0.926 0.865 0.852 0.538 0.567 0.387 N/A N/A N/A ADC* [15] 0.530 N/A N/A N/A N/A N/A N/A N/A N/A 0.325 N/A N/A 0.160 N/A N/A N/A N/A N/A SCAN* [34] 0.755 0.654 0.590 N/A N/A N/A N/A N/A N/A 0.818 0.712 0.665 0.422 0.441 0.267 N/A N/A N/A SCAN-SL* [34] 0.809 0.698 0.646 N/A N/A N/A N/A N/A N/A 0.883 0.797 0.772 0.507 0.486 0.333 N/A N/A N/A SPICE-Self* 0.899 0.809 0.797 N/A N/A N/A N/A N/A N/A 0.820 0.694 0.666 0.467 0.442 0.290 0.288 0.523 0.142 SPICE* 0.929 0.860 0.853 N/A N/A N/A N/A N/A N/A 0.917 0.858 0.836 0.584 0.583 0.422 N/A N/A N/A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of learned semantic clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Model Selection. Each curve represents the changing process of ACC v.s. epoch of a specific CLS head. The blue squares mark the selected best CLS head for each epoch, and the red stars represent the corresponding best CLS head evaluated with the ground truth. The blue circle denotes the finally selected model. The red circle is the ground truth best model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Local consistency on Cifar10 dataset. Each point denotes a sample with various colors for different clusters. (a) All samples and (b) the selected semantic consistency samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Select the best CLSHead with minimum loss as θ * cls ; 26 foreach xi ∈ X do 27 pi := Φ(xi; [θ B , θ * cls ]) ;</figDesc><table><row><cell>18</cell><cell cols="2">Strongly transform samples Xa = A(Xt) ;</cell></row><row><cell>19</cell><cell>for v ∈ {1, 2, . . . , M m 2</cell><cell>} do</cell></row><row><cell>22</cell><cell>end</cell><cell></cell></row><row><cell>23</cell><cell>end</cell><cell></cell></row><row><cell>24 end</cell><cell></cell><cell></cell></row><row><cell>25</cell><cell></cell><cell></cell></row></table><note>20 Select m2 samples with pseudo labels from [Xa; Lt] ;21 Optimize θ cls by minimizing the DS-CE loss ;28 li := arg max h (p ih ); 29 end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Specifications and partitions of selected datasets.</figDesc><table><row><cell>Dataset</cell><cell>Image size</cell><cell cols="3"># Training # Testing # Classes</cell></row><row><cell>STL10</cell><cell>96 × 96</cell><cell>5000</cell><cell>8000</cell><cell>10</cell></row><row><cell>Cifar10</cell><cell>32 × 32</cell><cell>50000</cell><cell>10000</cell><cell>10</cell></row><row><cell>Cifar100-20</cell><cell>32 × 32</cell><cell>50000</cell><cell>10000</cell><cell>20</cell></row><row><cell>ImageNet-10</cell><cell>224 × 224</cell><cell>13000</cell><cell>N/A</cell><cell>10</cell></row><row><cell>ImageNet-Dog</cell><cell>224 × 224</cell><cell>19500</cell><cell>N/A</cell><cell>15</cell></row><row><cell>Tiny-ImageNet</cell><cell>64 × 64</cell><cell>100000</cell><cell>10000</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison results with semi-supervised learning on STL10 and Cifar10.</figDesc><table><row><cell cols="6">Method MixMatch UDA ReMixMatch FixMatch SCAN SSLabel</cell></row><row><cell>Cifar10</cell><cell>0.890</cell><cell>0.912</cell><cell>0.947</cell><cell>0.949</cell><cell>0.883 0.917</cell></row><row><cell>stl10</cell><cell>0.897</cell><cell>0.923</cell><cell>0.948</cell><cell>0.920</cell><cell>0.809 0.929</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies of SPICE on the whole STL10 dataset.</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>k-means</cell><cell>0.797 ± 0.046</cell><cell>0.768 ± 0.021</cell><cell>0.624 ± 0.041</cell></row><row><cell>CE</cell><cell>0.875 ± 0.031</cell><cell>0.784 ± 0.017</cell><cell>0.764 ± 0.033</cell></row><row><cell>TCE</cell><cell>0.895 ± 0.005</cell><cell>0.794 ± 0.010</cell><cell>0.787 ± 0.010</cell></row><row><cell>Non-overlap</cell><cell>0.885 ± 0.002</cell><cell>0.788 ± 0.003</cell><cell>0.771 ± 0.003</cell></row><row><cell>Conv-SH</cell><cell>0.622 ± 0.061</cell><cell>0.513 ± 0.037</cell><cell>0.437 ± 0.053</cell></row><row><cell>Conv-MH</cell><cell>0.687 ± 0.037</cell><cell>0.577 ± 0.029</cell><cell>0.512 ± 0.033</cell></row><row><cell>Entropy</cell><cell>0.907 ± 0.001</cell><cell>0.817 ± 0.003</cell><cell>0.810 ± 0.003</cell></row><row><cell>SPICE-Self</cell><cell>0.908 ± 0.001</cell><cell>0.817 ± 0.002</cell><cell>0.812 ± 0.002</cell></row><row><cell>SPICE</cell><cell>0.937 ± 0.001</cell><cell>0.871 ± 0.001</cell><cell>0.870 ± 0.001</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Shiming Xiang, and Chunhong Pan. Deep discriminative clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5880" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised multi-manifold clustering by learning deep representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshops, volume WS-17 of AAAI Workshops</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18613" to="18624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shanahan</surname></persName>
		</author>
		<idno>abs/1611.02648</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5747" to="5756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<editor>Thomas Brox, Andrés Bruhn, and Mario Fritz</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep semantic clustering by partition confidence maximisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep embedding network for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="1532" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, NIPS&apos;17</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1965" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminatively boosted image clustering with fully convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The relationships among various nonnegative matrix factorization methods for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">H Q</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<title level="m">Contrastive clustering. In AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gatcluster: Self-supervised gaussian-attention network for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="735" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cluster ensemblesa knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepcluster: A general clustering framework based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>Michelangelo Ceci, Jaakko Hollmén, Ljupčo Todorovski, Celine Vens, and Sašo Džeroski</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="809" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ICML&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
			<date type="published" when="2017-08" />
			<publisher>International Convention Centre</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selfsupervised convolutional subspace clustering network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep adversarial subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
