<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Aggregation then Local Distribution in Fully Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Torr Vision Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
							<email>youansheng@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DeepMotion AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maokeyang@deepmotion</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DeepMotion AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuanyang@deepmotion</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
							<email>yhtong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Global Aggregation then Local Distribution in Fully Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LI ET AL.: GLOBAL AGGREGATION THEN LOCAL DISTRIBUTION 1 Code is available at: https://github.com/lxtGH/GALD-Net</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It has been widely proven that modelling long-range dependencies in fully convolutional networks (FCNs) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection. However, global aggregation is often dominated by features of large patterns and tends to oversmooth regions that contain small patterns (e.g., boundaries and small objects). To resolve this problem, we propose to first use Global Aggregation and then Local Distribution, which is called GALD, where long-range dependencies are more confidently used inside large pattern regions and vice versa. The size of each pattern at each position is estimated in the network as a per-channel mask map. GALD is end-to-end trainable and can be easily plugged into existing FCNs with various global aggregation modules for a wide range of vision tasks, and consistently improves the performance of state-of-the-art object detection and instance segmentation approaches. In particular, GALD used in semantic segmentation achieves new state-of-the-art performance on Cityscapes test set with mIoU 83.3%. Code is available at: https://github.com/lxtGH/GALD-Net</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detection and segmentation tasks have made steady progress with more powerful representations learned from Fully Convolutional Networks (FCNs). Since stacking more convolutional layers is not an effective way to achieve large receptive fields for long-range dependency modeling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>, several Global Aggregation (GA) modules have been proposed to resolve this problem. c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  <ref type="figure">Figure 1</ref>: Our proposed GALD framework for semantic segmentation task. The imbalanced spread of information from small and large patterns in GA module is appropriately handled through LD module.</p><p>In contrast to a standard convolutional layer which aggregates features in a small local window, GA modules use long-range operators such as averaging pooling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref> and spatialwise feature propagation over the whole image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. FCNs coupled with GA modules have consistently improved basic FCNs especially for large objects.</p><p>Unfortunately, the advantage of GA modules for large objects is a disadvantage for small patterns such as object boundaries and small objects, where features from GA modules tends to oversmooth the predictions for these small patterns. Thus, a straightforwards idea is using GA features conditionally on the pattern size of each position. Accordingly, we propose a Local Distribution (LD) module after a GA module (together as GALD for short) to adaptively distribute GA features at each position as illustrated in <ref type="figure">Fig. 1</ref>. The adaptive process is controlled by a set of mask maps, where each mask map is estimated from a feature map that records activations of some latent pattern over the whole image.</p><p>LD is a simple and universal module, and can be combined with existing GA modules to form different GALD modules for various detection and segmentation tasks. In our experiment, LD is verified on GA modules such as PSP <ref type="bibr" target="#b41">[42]</ref>, ASPP <ref type="bibr" target="#b4">[5]</ref>, Non-Local <ref type="bibr" target="#b27">[28]</ref> and CGNL <ref type="bibr" target="#b35">[36]</ref>, and achieves consistent performance improvement. We also extensively verify GALD on three vision benchmarks, including Cityscapes for semantic segmentation, Pascal VOC 2007 for object detection, and MS COCO for both object detection and instance segmentation, and all achieve notable improvement. In particular, for semantic segmentation evaluated on Cityscapes test set, GALD achieves mIoU of 83.3% with single model and ResNet101 as our backbone network, which surpasses all previously best published singlemodel results using ResNet101 as backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To keep spatial information required by detection and segmentation tasks, convolutional networks designed for image classification are modified to FCNs by removing global information aggregation layers such as global average pooling layer and fully-connected layers <ref type="bibr" target="#b22">[23]</ref>. To quickly increase receptive field size while keeping the spatial resolution, filters in top convolutional layers are enlarged by dilation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>To further enlarge the receptive field to the whole image, several methods are proposed recently. Global average pooled features are concatenated into existing feature maps in <ref type="bibr" target="#b21">[22]</ref>. In PSPnet <ref type="bibr" target="#b41">[42]</ref>, average pooled features of multiple window sizes including global average pooling are upsampled to the same size and concatenated together to enrich global information. The DeepLab series of papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> propose atrous or dilated convolutions and atrous spatial pyramid pooling (ASPP) to increase the effective receptive field. DenseASPP <ref type="bibr" target="#b31">[32]</ref> improves on <ref type="bibr" target="#b4">[5]</ref> by densely connecting convolutional layers with different dilation rates to further increase the receptive field of network. In addition to concatenating global information into feature maps, multiplying global information into feature maps also shows better performance <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>.In particular, EncNet <ref type="bibr" target="#b36">[37]</ref> and DFN <ref type="bibr" target="#b33">[34]</ref> use attention along the channel dimension of the convolutional feature map to account for global context such as the co-occurrences of different classes in the scene. CBAM <ref type="bibr" target="#b28">[29]</ref> explores channel and spatial attention in cascade way to learn task specific representation.</p><p>Recently, advanced global information modeling approaches initiated from non-local network <ref type="bibr" target="#b27">[28]</ref> are showing promising results on scene understanding tasks. In contrast to convolutional operator where the information is aggregated locally defined by filters, the non-local operator aggregates information from the whole image based on an affinity matrix calculated among all positions around the image. Using non-local operator, impressive results are achieved in OCNet <ref type="bibr" target="#b34">[35]</ref>,CoCurNet <ref type="bibr" target="#b37">[38]</ref>, DANet <ref type="bibr" target="#b13">[14]</ref>, A2Net <ref type="bibr" target="#b6">[7]</ref>, CCnet <ref type="bibr" target="#b14">[15]</ref> and Compact Generalized Non-Local Net <ref type="bibr" target="#b35">[36]</ref>. OCNet <ref type="bibr" target="#b34">[35]</ref> uses non-local bolocks to learn pixel-wise relationship while CoCurNet <ref type="bibr" target="#b37">[38]</ref> adds extra global average pooling path to learn whole scene statistic. DANet <ref type="bibr" target="#b13">[14]</ref> explores orthogonal relationships in both channel and spatial dimension using non-local operator. CCnet <ref type="bibr" target="#b14">[15]</ref> models the long range dependencies by considering its surrounding pixels on the criss-cross path through a recurrent way to save both computation and memory cost. Compact Generalized non-local Net <ref type="bibr" target="#b35">[36]</ref> considers channel information into affinity matrix. Another similar work to model the pixel-wised relationship is PSANet <ref type="bibr" target="#b42">[43]</ref>. It captures pixel-to-pixel relations using an attention module that takes the relative location of each pixel into account.</p><p>Another way to get global representation is using graph convolutional networks, and do reasoning in a non-euclidean space <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> where messages are passing between each node before projection back to each position. Glore <ref type="bibr" target="#b7">[8]</ref> projects the feature map into interaction space using learned projection matrix and does graph convolution on projected fully connected graph. BeyondGrids <ref type="bibr" target="#b17">[18]</ref> learns to cluster different graph nodes and does graph convolution in parallel. DGCNet <ref type="bibr" target="#b38">[39]</ref> proposes to use graph convolution network in both channel and spatial space to harvest different global context information.</p><p>All previous work focus on global context modeling, our work also utilizes global information modeling but takes a further step to better distribute the global information to each position, and further improves GA modules on both detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>Our method, Global Aggregation (GA) then Local Distribution (LD), dubbed GALD, exploits the long-range contextual information of the feature F ∈ R H×W ×C from a fully-convolution network (FCN), and then adaptively distributed the global context to each spatial and channel position of the output feature, F GALD ∈ R H×W ×C . To be noted, one can choose any one of the methods discussed in Section 2 as GA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GALD</head><p>Global Aggregation. To calculate a feature vector for each position, GA module takes feature vectors of F in a large window even the whole feature map depending on different GA designs. Take the Compact Generalized Non-Local (CGNL) <ref type="bibr" target="#b35">[36]</ref> as an example, similar to non-local <ref type="bibr" target="#b27">[28]</ref>, it aggregates contextual information from all spatial and channel positions in  <ref type="figure" target="#fig_2">Figure 2</ref>: Schematic illustration of GALD, which contains two main components: Global Aggregation (GA) and Local Distribution (LD). GALD receives a feature map from the backbone network and outputs a feature map with same size with global information appropriately assigned to each local position.</p><p>the same group. Specifically, the global statistics are calculated for each group and multiplied back to the features in the same group, which forms F GA . In our implementation, we downsample F by a factor of 2 for saving memory and computation cost without observing performance degradation, which also demonstrates the coarse property of global aggregation.</p><p>Since GA modules calculate global statistics of features in large windows, they are easily biased towards features from large patterns as they contain more samples. Then the global information distributed to each position is also biased towards large patterns, which causes over-smoothing results for small patterns. One can refer to Section 4.3.1 for more detailed visualization results. Local Distribution. LD is proposed to adaptively use F GA considering patterns on each position. Without explicit supervision, the required patterns are latently described by C channels in F GA . For each pattern c ∈ {1, ...,C}, a spatial operator is learned to recalculate the spatial extent of the pattern in an image based on the activation map F GA [:, :, c] sliced from F GA . Intuitively, spatial operators for large patterns would shrink the spatial extent more while shrinking less even expand for small patterns.</p><p>The spatial operators for each pattern/channel is modeled as a set of depth-wise convolutional layers with F GA as input, i.e.,</p><formula xml:id="formula_0">M = σ (upsample(W d F GA )),<label>(1)</label></formula><p>where M ∈ [0, 1] H×W ×C contains the mask maps for each pattern and describes the recalculated spatial extents of each pattern, σ (·) is the sigmoid function, W d is the weights of those depth-wised convolutional filters with d as the downsampling rate by stride convolution. The output mask M is sensitive to both spatial and channel and it is upsampled using bilinear interpolation. With the mask maps M, F GA is refined into F GALD by</p><formula xml:id="formula_1">F GALD = M F GA + F GA ,<label>(2)</label></formula><p>where the element-wise multiplication, and elements in F GA are weighted according the estimated spatial extent of each pattern at each position. In summary, LD predicts local weights M for each position of GA features and avoids issues of coarse feature representation.</p><p>As a common practice <ref type="bibr" target="#b41">[42]</ref>, original feature F and global aggregated feature F GA are concatenated together for final task-specific head, i.e.,</p><formula xml:id="formula_2">F o = concat(F GALD , F) = concat(M F GA + F GA , F),<label>(3)</label></formula><p>where M adds point-wise trade-off between global information F GA and local detailed information F. Note that since the lack of details in GA, LD module only changes the proportion and distribution of coarse features in GA and leads to a fine-grained feature representation output F o . For object detection and instance segmentation task, GALD is added at the end of stage4 of a ResNet backbone, FPN <ref type="bibr" target="#b19">[20]</ref> is used to build a strong baseline with a feature pyramid for multi-scale object detection. F GALD sits on top of FPN and passes information from the top-down pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we verify GALD on three scene understanding tasks including semantic segmentation, object detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmarks</head><p>Cityscapes: Cityscapes <ref type="bibr" target="#b8">[9]</ref> is a benchmark that densely annotated for 19 categories in urban scenes, which contains 5000 fine annotated images in total and is divided into 2975, 500, and 1525 images for training, validation and testing, respectively. In addition, 20,000 coarse labeled images are also provided to enrich the training data. Images of this dataset are all with the same high resolution, i.e., 1024 × 2048. Following the standard protocol <ref type="bibr" target="#b8">[9]</ref>, mean Intersection over Union (mIoU) of all categories on validation set and test set is used for performance comparison. MS COCO: MS COCO <ref type="bibr" target="#b18">[19]</ref> is built for detecting and segmenting objects found in everyday life in their natural environment. The dataset for detection consists of three sets for 80 common object categories, i.e., the training set has 118,287 images, validation set has 5,000 images and test-dev set has more than 20,000 images. Pascal VOC: Pascal VOC <ref type="bibr" target="#b10">[11]</ref> is a widely used public benchmark for semantic segmentation and object detection covering 20 object categories including the background. We use VOC 2007 and VOC 2012 trainval set as training set and report results on VOC 2007 test set. </p><formula xml:id="formula_3">PSP ASPP NL CGNL LD LD LD LD (a) (b) (c) (d) GA LD LD LD GA GA (e) (f) (g) GA (h) LD (i)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Semantic Segmentation We employ Fully Convolutional Networks (FCNs) as baseline, where ResNet pretrained on ImageNet is chosen as the backbone following the same setting as PSPNet <ref type="bibr" target="#b41">[42]</ref>, the proposed GALD is appended to the backbone with random initialization. For optimization, we also keep the same setting as PSPNet, where mini-batch SGD with momentum 0.9 and initial learning rate 0.01 is used to train all models with 50K iterations, using mini-batch size of 8 and crop size of 769. During training, "poly" learning rate scheduling policy where power = 0.9 is used to adjust the learning rate. Synchronized batch normalization <ref type="bibr" target="#b36">[37]</ref> is used for better mean / variance estimation across GPUs. Object Detection and Instance Segmentation For object detection and instance segmentation, mmdetection <ref type="bibr" target="#b0">[1]</ref> is used as our baseline implementation for fair comparison. GALD is evaluated for object detection on Pascal VOC based on Faster R-CNN, and for both object detection and instance segmentation on MS COCO based on Mask R-CNN. FPN <ref type="bibr" target="#b19">[20]</ref> is used as default setting in all these experiments. For fair comparison, we report all the results that we re-implemented in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Cityscapes</head><p>Two groups of experiments are conducted on Cityscapes, the first group of experiments verifies the effectiveness of our GALD framework by ablation studies. The second group of experiments compares GALD to the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Studies</head><p>Comparison with baseline We explore our LD module with four different GA modules as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref> (a)-(d). (e) Ablation study on downsampling strategies for mask estimation in LD using ResNet50 as backbone, where the downsamping ratio is 8. Arrangements of LD and GA Considering LD module can also improve the baseline, we further study different arrangements of LD and GA as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref> (e)-(g). (f) and (g) represent LDGA and Parallel in <ref type="table">Table.</ref> 1(c) respectively. LDGA means first doing LD then doing GA while Parallel concatenates the output of LD and GA. <ref type="table">Table.</ref> 1(c) reports the results of the three different arrangements, where all improve the baseline and GALD achieves best result. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the mask maps learned in LDGA and GALD, where mask maps learned by GALD are more focused on regions inside large objects then weight global features more in these regions, while mask maps from LDGA have no obvious focus on large objects since the LD module has not accessed to global feature yet. Compared with stronger backbone To further prove the effectiveness of our method, we compare GALD using ResNet50 as backbone with a stronger backbone ResNet101 in Table 1(d). Our method achieves similar performance improvement comparing GA modules with stronger backbone which further prove the effectiveness of LD module. Comparison with different downsampling strategies We also explore three different downsampling strategies for LD, including average pooling, bilinear interpolation and depth-wise stride convolution. <ref type="table" target="#tab_0">Table 1</ref>(e) reports the comparison results, depth-wise stride convolution achieves the best result, while average pooling and bilinear interpolation even slightly degrades the performance, which shows that the learnable filters for each channel is important to refine the features from the GA module.</p><p>Visualization of GALD To further study the features at different stages, we add another two segmentation heads on features outputted from FCN and GA respectively, the model is fine   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with state-of-the-art</head><p>We further compare our results with other state-of-the-art methods in this section. We choose dilated ResNet50 and ResNet101 as backbone models. The results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. For fair comparison, we first compare methods trained with only fine annotation data in <ref type="table" target="#tab_3">Table 2</ref>(a), and then compare the results with other methods using extra training data in <ref type="table" target="#tab_3">Table 2</ref>(b). Following <ref type="bibr" target="#b41">[42]</ref>, multi-scale crop test is used for final test submission. As illustrated, our method surpasses all previous methods. In particular, our model based on a weak backbone ResNet50 can still achieve comparable performance, which is higher than most methods with stronger backbone. By using extra coarse annotation data for training, our method achieves 82.9% mIoU, which also surpasses the state-of-the-art methods. By further adding Mapillary <ref type="bibr" target="#b24">[25]</ref> as training data, the proposed method achieves 83.3% mIoU based on ResNet101. To the best of our knowledge, this is the first single model using ResNet101 as backbone that surpasses 83% mIoU on Cityscapes test server. More detailed per-class results, visualization results and training settings can be referred to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Pascal VOC and COCO dataset</head><p>Pascal VOC: We perform experiments on the PASCAL VOC 2007 data set to evaluate the effect of GALD for object detection. We train all the models on the union set of VOC 2007 trainval and VOC 2012 trainval (07+12) for 14 epochs with weight decay of 0.0001 and momentum of 0.9. For comparison, experiments of non-local block <ref type="bibr" target="#b27">[28]</ref> are also summarized and are denoted as NL. As results listed in <ref type="table" target="#tab_5">Table 3</ref>(a), GALD consistently improves detection accuracy over the strong baseline Faster-RCNN using both ResNet50 and ResNet101 as backbone, which demonstrates the effectiveness of GALD for object detection. COCO: To further verify the generality of GALD, we conduct the experiments on instance segmentation task on MS COCO based on the state-of-the-art method Mask R-CNN. Table 3(b) summarizes the AP of bounding box (AP-box) and AP of mask (AP-mask) evaluated on COCO minival. GALD improves the baseline by about 1% regardless the used backbone. <ref type="figure">Figure 6</ref>(b) compares the object detection and instance segmentation results of our method with baseline. With GALD, Mask R-CNN can find objects that are missed in baseline (e.g., the "light" in the third column), resolve ambiguity in region classification (e.g., the "bed" in the first column) and help to better estimate the spatial contents for objects (e.g., "bear" in last column).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask-RCNN</head><p>Mask-RCNN + GALD <ref type="figure">Figure 6</ref>: Comparison of object detection and instance segmentation results on MS COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose GALD to adaptively distribute global information to each position for scene understanding tasks. In contrast to existing methods that assign global information uniformly to each position and cause the problem of blurring, GALD learns a set of mask maps to distribute global information adaptively according pattern distributions over the image. GALD benefits from both the GA module for ambiguity resolving and LD module for detail refinement. Extensive experiments verify the universality of GALD in improving the performance of semantic segmentation, object detection and instance segmentation. In the future, we will study the effectiveness of GALD for more vision tasks where both global and local information are important such as depth estimation. -   Ctiyscapes provides about 20000 coarse labeled images for training. To verify the both capacity and generality, we further fine tune our model on coarse data set. Different from training on fine data set, we set batch size 16 and fix batch normalization layers in our model for about 50000 iterations using larger crop size. Then we fine tune the model back on the fine data set for 10000 iterations with lower learning rate. When we submit on test server, we use multi scale crop test with flip images input to get more accurate results. Finally we get better results and our single model can achieve 82.9% mIoU.</p><formula xml:id="formula_4">- - - - - - - - - - - - - - - - - -79.3 BiSeNet [33] - - - - - - - - - - - - - - - - - - -78.9 PSANet [43] - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Training with Mapillary Data</head><p>Mapillary <ref type="bibr" target="#b24">[25]</ref> is another city scene data containing 65 different labels. Here we only use 19 classes of 65 labels which are in cityscape category. Again we following the same steps in previous part, we get more accurate model and our single model with ResNet101 as backbone can achieve 83.3% mIoU ranked 3-rd in cityscapes leaderboard by the time of paper publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Detailed Results</head><p>Models trained with only fine-data set are shown in <ref type="table" target="#tab_8">Table 4</ref>. Models trained with extra data sets including COCO and Mapillary data sets are shown in <ref type="table" target="#tab_9">Table 5</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Detailed Results on VOC2007</head><p>Here we give the detailed detection results on VOC2007 shown in and compared the results with previous detection methods, ours model achieves considerable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">More Visible Results on Cityscapes and COCO</head><p>Here we show more results on Cityscapes and COCO dataset. COCO results are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. Cityscapes results are shown in Figure 8 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1909.07229v1 [cs.CV] 16 Sep 2019 GA LD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>illustrates the overall architecture with GALD. For semantic segmentation, GALD is added right after a FCN, features from Eq. 3 are used for final prediction. To further boost the performance, Online Hard Example Mining (OHEM) loss [30] is used for training, where only top-K ranked pixels according their losses are used during back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ablation studies on combinations of GA and LD. (a)-(d) shows the different GA modules with LD. (e)-(g) shows the different arrangements of GA and LD. (h)-(f) represents using GA and LD respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of mask maps learned in different arrangements of GA and LD. The mask maps are calculated by the mean of M along channel dimension. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of different parts output results in one model.(a), input images; (b),results after FCN's outputs; (c), results after GA module's outputs; (d), results after GALD module'ss outputs; (e), ground truth. Yellow boxes highlight regions that GA can handle global semantic consistency, while red boxes highlight regions that LD can recover more detailed information. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>More detection and Segmentation Results on COCO First row: Mask-RCNN; Second row: + GALD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>More cityscapes results: (a),input (b),FCN-res50 (c),+LD (d),+GA (e), +GALD (f),ground truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>(a) first reports the performances of adding four GA modules to the baseline FCN, where all methods are using the same backbone ResNet50 for fair comparison. Obviously, all GA modules significantly improves the baseline FCN</figDesc><table><row><cell>Method FCN (Baseline)</cell><cell>mIoU(%) 73.7</cell><cell>∆a -</cell><cell cols="2">Method FCN (Baseline)</cell><cell>mIoU(%) 73.7</cell><cell>∆a -</cell><cell>∆b -</cell></row><row><cell cols="3">+ASPP [3] +NL [28] +PSP [42] +CGNL [36] (a) Ablation study on different GA modules 77.2 3.5 ↑ 78.0 4.3 ↑ 76.2 2.5 ↑ 78.2 4.5 ↑ using ResNet50 as backbone.</cell><cell cols="5">+LD +PSP + LD +ASPP + LD +NL + LD +CGNL + LD (b) Ablation study on LD applied on different GA modules 77.5 3.8 ↑ -78.9 5.2 ↑ 2.7 ↑ 79.5 5.4 ↑ 2.3 ↑ 79.2 5.3 ↑ 1.2 ↑ 79.6 5.9 ↑ 1.4 ↑ using ResNet50 as backbone.</cell></row><row><cell>Method FCN (Baseline)</cell><cell>mIoU(%) 73.7</cell><cell>∆a -</cell><cell>Method FCN (Baseline) FCN (Baseline)</cell><cell cols="2">mIoU(%) 73.7 75.3</cell><cell>Backbone ResNet50 ResNet101</cell><cell>∆a --</cell></row><row><cell>+Parallel +LDGA +GALD</cell><cell>77.5 78.1 79.6</cell><cell>3.8 ↑ 4.4 ↑ 5.9 ↑</cell><cell>+CGNL +CGNL+LD +PSP</cell><cell></cell><cell>79.7 79.6 78.6</cell><cell>ResNet101 ResNet50 ResNet101</cell><cell>4.4 ↑ 5.9 ↑ 4.9 ↑</cell></row><row><cell cols="3">(c) Ablation study on different arrangements</cell><cell>+PSP+LD</cell><cell></cell><cell>78.9</cell><cell>ResNet50</cell><cell>5.2 ↑</cell></row><row><cell cols="3">of GA and LD using ResNet50 as backbone.</cell><cell cols="5">(d) Ablation study on different backbones.</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell cols="2">mIoU(%)</cell><cell>∆b</cell><cell></cell></row><row><cell></cell><cell cols="2">FCN (Baseline)</cell><cell></cell><cell>73.7</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>FCN + CGNL</cell><cell></cell><cell></cell><cell>78.2</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell cols="3">+CGNL+LD(depth-wise convolution)</cell><cell>79.6</cell><cell>1.4 ↑</cell><cell></cell></row><row><cell></cell><cell cols="3">+CGNL+LD(bilinear interpolation)</cell><cell>77.6</cell><cell>0.6 ↓</cell><cell></cell></row><row><cell></cell><cell cols="3">+CGNL+LD(average pooling)</cell><cell>76.5</cell><cell>1.7 ↓</cell><cell></cell></row></table><note>on semantic segmentation task, where CGNL performs better than other three GA modules. Table 1(b) reports the results by adding our proposed LD module. Directly using LD alone improves the baseline FCN by 3.8%, which demonstrates that features from FCN have the similar problem as features from GA modules. LD together with four different GA modules consistently improves the corresponding GA module. Comparing with baseline, the combi- nation of CGNL+LD achieves the best performance, and we mainly choose CGNL as our GA module in following experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison results on Cityscapes validation set, where ∆a denotes the performance difference comparing with baseline, and ∆b denotes performance difference between using GALD module and the corresponding GA module. All methods are evaluated with single- scale crop test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>State-of-the-art comparison experiments on Cityscapes test set. †means training with only the train-fine dataset. ‡means training with both the train-fine and coarse data tuned until converge to analyze segmentation ability of features from different stages. Figure 5 compares the segmentation results, segmentation based on GA resolves the ambiguities in FCN features but also tends to over smoothing regions of small patterns which are shown in red boxes. Segmentation of GALD keeps the global structure of GA while refines back the details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on Pascal VOC dataset (a) and MS COCO dataset (b).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>85.6 93.0 53.8 58.9 65.9 75.0 78.4 93.7 72.4 95.6 86.4 70.5 95.9 73.9 82.7 76.9 68.7 76.4 79.1 SegModel [12] 98.6 86.4 92.8 52.4 59.7 59.6 72.5 78.3 93.3 72.8 95.5 85.4 70.1 95.6 75.4 84.1 75.1 68.7 75.0 78.5 DFN [34]</figDesc><table><row><cell>Method</cell><cell>road swalk build wall fence pole tlight sign veg. terrain sky person rider car truck bus train mbike bike mIoU</cell></row><row><cell cols="2">ResNet38 [31] 98.5 85.7 93.0 55.5 59.1 67.1 74.8 78.7 93.7 72.6 95.5 86.6 69.2 95.7 64.5 78.8 74.1 69.0 76.7 78.4</cell></row><row><cell>PSPNet [42]</cell><cell>98.6 86.2 92.9 50.8 58.8 64.0 75.6 79.0 93.4 72.3 95.4 86.5 71.3 95.9 68.2 79.5 73.8 69.5 77.2 78.4</cell></row><row><cell>AAF [16]</cell><cell>98.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>80.1 DenseASPP [32] 98.7 87.1 93.4 60.7 62.7 65.6 74.6 78.5 93.6 72.5 95.4 86.2 71.9 96.0 78.0 90.3 80.7 69.7 76.8 80.6 Ours(ResNet50) 98.7 86.8 93.4 57.6 63.1 68.7 76.1 80.3 93.6 72.3 95.4 87.0 72.2 96.1 75.4 88.2 77.8 68.8 76.4 80.8 Ours(ResNet101) 98.7 87.2 93.8 59.3 61.9 71.4 79.2 82.0 93.9 72.8 95.6 88.4 74.8 96.3 74.1 90.6 81.1 73.4 79.8 81.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Per-category results on Cityscapes test set. Note that all the models are trained with only fine-data . ResNet101) 98.8 87.5 94.0 65.3 66.4 71.0 77.6 81.0 94.0 72.6 95.9 87.6 75.0 96.3 80.2 90.3 87.9 72.9 78.9 82.9 Ours+ Mapillary 98.8 87.7 94.2 65.0 66.7 73.1 79.3 82.4 94.2 72.9 96.0 88.4 76.2 96.5 79.8 89.6 87.7 74.0 80.0 83.3</figDesc><table><row><cell>Method</cell><cell>road swalk build wall fence pole tlight sign veg. terrain sky person rider car truck bus train mbike bike mIoU</cell></row><row><cell>PSPNet [42]</cell><cell>98.7 86.9 93.5 58.4 63.7 67.7 76.1 80.5 93.6 72.2 95.3 86.8 71.9 96.2 77.7 91.5 83.6 70.8 77.5 81.2</cell></row><row><cell>ResNet38 [31]</cell><cell>98.7 86.9 93.3 60.4 62.9 67.6 75.0 78.7 93.7 73.7 95.5 86.8 71.1 96.1 75.2 87.6 81.9 69.8 76.7 80.6</cell></row><row><cell cols="2">InPlaceABN [27] 98.4 85.0 93.6 61.7 63.9 67.7 77.4 80.8 93.7 71.9 95.6 86.7 72.8 95.7 79.9 93.1 89.7 72.6 78.2 82.0</cell></row><row><cell cols="2">DeepLabV3+ [6] 98.7 87.0 93.9 59.5 63.7 71.4 78.2 82.2 94.0 73.0 95.8 88.0 73.0 96.4 78.0 90.9 83.9 73.8 78.9 82.1</cell></row><row><cell cols="2">Auto-Deeplab [21] 98.8 87.6 93.8 61.4 64.4 71.2 77.6 80.9 94.1 72.7 96.0 87.8 72.8 96.5 78.2 90.9 88.4 69.0 77.6 82.1</cell></row><row><cell>DPC [4]</cell><cell>98.7 87.1 93.8 57.7 63.5 71.0 78.0 82.1 94.0 73.3 95.4 88.2 74.5 96.5 81.2 93.3 89.0 74.1 79.0 82.6</cell></row><row><cell>DRN-CRL [45]</cell><cell>98.8 87.7 94.0 65.1 64.2 70.1 77.4 81.6 93.9 73.5 95.8 88.0 74.9 96.5 80.8 92.1 88.5 72.1 78.8 82.8</cell></row><row><cell>Ours(</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Per-category results on Cityscapes test set trained with coarse data and Mapillary. Our model achieves the state of art results comparing with other methods using more stronger backbone.ãȂȂOur method achieves better results than those use stronger backbone<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>.</figDesc><table><row><cell>6 Appendix</cell></row><row><cell>6.1 Detailed Results on Cityscapes</cell></row><row><cell>6.1.1 Training with Coarse labeled Data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Method aero bike bird boat bottle bus car cat chair cow table dog house mbike person plant sheep sofa train tv mAP(%) R-FCN[10] 79.9 87.2 81.5 72.0 69.8 86.8 88.5 89.8 67.0 88.1 74.5 89.8 90.6 79.9 81.2 53.7 81.8 81.5 85.9 79.9 80.5 DSSD[13] 86.6 86.2 82.6 74.9 62.5 89.0 88.7 88.8 65.2 87.0 78.7 88.2 89.0 87.5 83.7 51.1 86.3 81.6 85.7 83.7 81.5 DFPR[17] 92.0 88.2 81.1 71.2 65.7 88.2 87.9 92.2 65.8 86.5 79.4 90.3 90.4 89.3 88.6 59.4 88.4 75.3 89.2 78.5 82.4 Faste-RCNN (base)[26] 86.5 85.9 82.9 70.4 70.4 83.3 88.1 88.6 66.0 82.5 74.6 89.1 87.1 83.4 85.8 58.5 84.8 79.2 85.9 77.5 80.7 Ours 86.7 87.7 85.3 74.5 74.4 86.1 89.0 89.5 71.2 87.5 77.5 89.0 87.9 85.4 86.5 59.8 86.1 80.8 87.6 83.2 83.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>PASCAL VOC 2007 test detection results. All models are trained with 07+12 (07 trainval + 12 trainval). All the models are using ResNet101 as backbone.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of DeepMotion AI Research for providing the computing resources in carrying out this research. LZ is supported by EPSRC Programme Grant Seebibyte EP/M013774/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmdetection" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aˆ2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12814</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Graph-based global reasoning networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic segmentation via structured patch prediction, context crf and guidance crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng Yan Falong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollãąr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02985</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dual graph convolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06959</idno>
		<title level="m">Dynamic graph message passing network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Scaleadaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dense relation network: Learning consistent and contextaware representation for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<editor>ICIP. IEEE</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
