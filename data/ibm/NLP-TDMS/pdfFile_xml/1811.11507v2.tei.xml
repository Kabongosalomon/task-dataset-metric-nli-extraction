<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
							<email>claudio.michaelis@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the problem of one-shot instance segmentation: Given an example image of a novel, previously unknown object category (the reference), find and segment all objects of this category within a complex scene (the query image). To address this challenging new task, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We demonstrate empirical results on MS-COCO highlighting challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories works very well, targeting the detection network towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research into more powerful and flexible scene analysis algorithms. Code is available at: https://github.com/bethgelab/siamese-mask-rcnn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query image</head><p>Output Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Reference</head><p>Reference Query images Old New <ref type="figure">Figure 1</ref>: Left: Classical one-shot learning tasks are phrased as multi-class discrimination on datasets such as Omniglot and miniImagenet. Right: We propose one-shot instance segmentation on MS-COCO. The bounding boxes and instance masks are outputs of our model.</p><p>Computer vision has made substantial progress in few-shot learning in the last years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b34">35]</ref>. However, the field has focused on image classification in a discriminative setting, using</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans do not only excel at acquiring novel concepts from a small number of training examples (few-shot learning), but can also readily point to such objects (object detection) and draw their outlines (instance segmentation). Conversely strong machine vision algorithms exist which can detect and segment a limited number of object categories in complex scenes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21]</ref>. However in contrast to humans they are unable to incorporate new object concepts for which only a small number of training examples are provided. Enabling these object detection and segmentation systems to perform few-shot learning would be extremely useful for many real-world applications for which no large-scale annotated datasets like MS-COCO <ref type="bibr" target="#b32">[33]</ref> or OpenImages <ref type="bibr" target="#b25">[26]</ref> exist. Examples include autonomous agents such as household, service or manufacturing robots, or detecting objects in images collected in scientific settings (e. g. medical imaging or satellite images in geosciences).</p><p>datasets such as Omniglot <ref type="bibr" target="#b26">[27]</ref> and MiniImagenet <ref type="bibr" target="#b61">[62]</ref> (see <ref type="bibr">Figure 1,</ref><ref type="bibr">left)</ref>. As a consequence, these approaches are limited to rather simple object-centered images and cannot trivially handle object detection.</p><p>In this paper, we combine few-shot learning and instance segmentation in one task: We learn to detect and segment arbitrary objects in complex real-world scenes based on a single visual example <ref type="figure">(Figure 1, right)</ref>. That is, we want our system to be able to find people and cars even though it has been provided with only one (or a few) labeled examples for each of those object categories.</p><p>To evaluate the success of such a system, we formulate the task of one-shot instance segmentation: Given a scene image and a previously unknown object category defined by a single reference instance, generate a bounding box and a segmentation mask for every instance of that category in the image. This task can be seen as an example-based version of the typical instance segmentation setup and is closely related to the everyday problem of visual search which has been studied extensively in human perception <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>We show that a new model, Siamese Mask R-CNN, which incorporates ideas from metric learning (Siamese networks <ref type="bibr" target="#b24">[25]</ref>) into Mask R-CNN <ref type="bibr" target="#b20">[21]</ref>, a state-of-the-art object detection and segmentation system <ref type="figure" target="#fig_0">(Figure 2</ref>), can learn this task and acquire a similarity metric that allows it to generalize to previously unknown object categories.</p><p>Our main contributions are:</p><p>• We introduce one-shot instance segmentation, a novel one-shot task, requiring object detection and instance segmentation based on a single visual example. • We present Siamese Mask R-CNN, a system capable of performing one-shot instance segmentation. • We establish an evaluation protocol for the task and evaluate our model on MS-COCO.</p><p>• We show that, for our model, targeting the detection towards the reference category is the main challenge, while segmenting the correctly identified objects works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Object detection and instance segmentation. In computer vision, object detection is the task of localizing and classifying individual objects in a scene <ref type="bibr" target="#b13">[14]</ref>. It is usually formalized as: Given an image (query image), localize all objects from a fixed set of categories and draw a bounding box around each of them. Current state-of-the-art models use a convolutional neural network (the backbone) to extract features from the query image and subsequently classify the detected objects into one of the n categories (or background). Most models either directly use the backbone features to predict object locations and categories (single stage) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b31">32]</ref> or first generate a set of class-agnostic object proposals which are subsequently classified (two stage) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Segmentation tasks require labeling all pixels belonging to a certain semantic category (semantic segmentation) or object instance (instance segmentation). While both tasks seem closely related, they in fact require quite different approaches: Semantic segmentation models perform pixel-wise classification and are usually implemented using fully convolutional architectures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b7">8]</ref>. In contrast, instance segmentation is more closely related to object detection, as it requires identifying individual object instances <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b20">21]</ref>. It therefore inherits the difficulties of object detection, which make it a significantly harder task than semantic segmentation. Consequently, the current state-of-the-art instance segmentation model (Mask R-CNN) <ref type="bibr" target="#b20">[21]</ref> is an extension of a successful object detection model (Faster R-CNN) <ref type="bibr" target="#b48">[49]</ref>.</p><p>Few-shot learning The goal of few-shot learning is to find models which can generalize to novel categories from few labeled examples <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>. This capability is usually evaluated through a number of episodes. Each episode consists of a few examples from novel categories (the support set) and a small test set of images from the same categories (the query set). When the support set contains k examples from n categories, the problem is usually referred to as an n-way, k-shot learning problem. In the extreme case when only a single example per category is given, this is referred to as one-shot learning.</p><p>There are two main approaches to solve this task: either train a model to learn a metric, based on which examples from novel categories can be classified (metric learning) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b62">63]</ref> or to learn a good learning strategy which can be applied in each episode (meta learning) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>To train these models, the categories in a dataset are usually split into training categories used to train the models and test categories used during the evaluation procedure. Therefore, the few-shot model will be trained and tested on different categories, forcing it to generalize to novel categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">One-shot object detection and instance segmentation on MS-COCO</head><p>The goal of one-shot object detection and instance segmentation is to develop models that can localize and segment objects from arbitrary categories when provided with a single visual example from that category. To this end, we 1) replace the widely used category-based object detection task by an example-based task setup and 2) split the available object categories into a training set and a non-overlapping test set, which is used to evaluate generalization to unknown categories. We use the popular MS-COCO dataset, which consists of a large variety of complex scenes with multiple objects from abroad range of categories and often challenging conditions like clutter.</p><p>Task setup: example-based instance segmentation. We define one-shot detection and segmentation as follows: Given a reference image showing a close-up example of a novel object category, find and segment all instances of objects belonging to this category in a separate query image, which shows an entire visual scene containing many objects <ref type="figure">(Figure 1</ref>, right). The main difference between this task and the usual object detection setup is the change from a category-based to an example-based setup. Instead of requiring to localize objects from a number of fixed categories, the example-based task requires to detect objects from a single category, which is defined through a reference image. The reference image shows a single object instance of the category that is to be detected, cropped to its bounding box (see <ref type="figure">Figure 1</ref> for two examples). It is provided without mask annotations.  <ref type="table" target="#tab_3">Table A1</ref> in the Appendix).</p><p>Because we use complex scenes which can contain objects from many categories, it is not feasible to ensure that the training images contain no instances of held-out categories. However, we do not provide any annotations for these categories during training and never use them as references. In other words, the model will see objects from the test categories during training, but is never provided with any information about them. This setup differs from the typical few-shot learning setup, in which the model never encounters any instance of the novel objects during training. However, in addition to being the only feasible solution, we consider this setup quite realistic for an autonomous agent, which may encounter unlabeled objects multiple times before they become relevant and label information is provided. Think of a household robot seeing, but not recognizing, a certain type of toy in various parts of the apartment multiple times before you instruct it to go pick it up for you.</p><p>Evaluation procedure. We propose to evaluate task performance using the following procedure: The same steps as above apply in the case of instance segmentation, with the difference that a segmentation mask instead of a bounding box is required for each predicted object.</p><p>Our evaluation procedure is simplified somewhat, because we ensure that the reference categories are actually present in each image used for evaluation. For a real-world application of such a system, this restriction would have to be removed. However, we found the task to be very challenging already with this simplification, so we believe it is justified for the time being.</p><p>Connection to few-shot learning and object detection. Our evaluation procedure lends from other few-shot setups that typically evaluate in episodes. Each episode consists of a support set (the training examples for the novel categories) and a query set (the images to be classified). In our case, an episode consists of the detection of objects of one novel category in one image. In this case, the support set is the set of examples from the category to be detected (the references) while the query set is a single image (the query image). Compared to object detection, the classifier is turned into a binary verification conditioned on the reference image(s). Compared to the typical few-shot learning setup, there are two key differences: First, as only one category is given, the task is not a discrimination task between the given categories, but a verification task between the given category and all other object categories. Second the query image may not only contain objects from the novel category given by the reference, but also other objects from known and unknown categories.</p><p>Connection to other related tasks. Our setup differs from a number of related paradigms. In contrast to recent work on few-shot object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b54">55]</ref>, we formulate our task as an example-based search task rather than learning an object detector from a small labeled dataset. This allows us to directly apply our model on novel categories without any retraining. We also extend all of these approaches by additionally asking the system to output segmentation masks for each instance and focus on the challenging MS-COCO dataset. Similarly our task shares similarities with zero-shot object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b68">69]</ref>, but with the crucial difference that in zero-shot detection the reference category is defined by a textual description instead of an image.</p><p>A range of one-shot segmentation tasks exist, including one-shot semantic segmentation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37]</ref>, texture segmentation <ref type="bibr" target="#b60">[61]</ref>, medical image segmentation <ref type="bibr" target="#b66">[67]</ref> and recent work on cosegmentation <ref type="bibr" target="#b27">[28]</ref> 2 . The key difference is that the models developed for these tasks output pixel-level semantic classifications rather than instance-level masks and, thus, cannot distinguish individual object instances. In co-segmentation very recent work <ref type="bibr" target="#b22">[23]</ref> explores instance co-segmentation, but not in a few-shot setting. Two studies segment instances in a few-shot setting, but with different task setups: (1) in one-shot video segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, object instances are tracked across a video sequence;</p><p>(2) in one-shot instance segmentation of homogeneous object clusters <ref type="bibr" target="#b64">[65]</ref> a model is proposed which segments, e. g., a pile of bricks into the individual instances based on a video pan of one of the bricks. Both of these setups are closer to particular object retrieval <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b18">19]</ref>, as they localize instances of a particular object rather than instances of the same object category, as is the focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Siamese Mask R-CNN</head><p>The key idea of one-shot instance segmentation is to detect and segment object instances based on a single visual example of some object category. Thus, our system has to deal with arbitrary, potentially previously unknown object categories which are defined only through a single reference image, rather than with a fixed set of categories for which extensive labeled data was provided during training. To solve this problem, we take a metric-learning approach: we learn a similarity metric between the reference and image regions in the scene. Based on this similarity metric, we then generate object proposals and classify them into matches and non-matches. The key advantage of this approach is that it can be directly applied to objects of novel categories without the need to retrain or fine-tune the learned model.</p><p>To compute the similarity metric we use Siamese networks, a classic metric learning approach <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>. We combine this form of similarity judgment with the domain knowledge built into current state-of-the-art object detection and instance segmentation systems by integrating it into Mask R-CNN <ref type="bibr" target="#b20">[21]</ref>. In the following paragraphs we provide a quick recap of Mask R-CNN before describing the changes we made to integrate the Siamese approach and how we compute the similarity metric. We build our implementation upon the Matterport Mask R-CNN library <ref type="bibr" target="#b1">[2]</ref>. The details can be found in Appendix A2 and in our code <ref type="bibr" target="#b2">3</ref> . Mask R-CNN. Mask R-CNN is a two-stage object detector that consists of a backbone feature extractor and multiple heads operating on these features (see <ref type="figure" target="#fig_0">Figure 2</ref>). The heads consist of two stages. First, the region proposal network (RPN) is applied convolutionally across the image to predict possible object locations in the scene. The most promising region proposals are then cropped from the backbone feature maps and used as inputs for the bounding box classification (CLS) and regression (BBOX) head as well as the instance masking head (MASK).</p><p>Siamese network backbone. To integrate the reference information into Mask R-CNN, the same backbone (ResNet50 <ref type="bibr" target="#b21">[22]</ref> with Feature Pyramid Networks (FPN) <ref type="bibr" target="#b30">[31]</ref>) is used with shared weights to extract features from both the reference and the scene. The resulting features are then used as a dropin replacement for the original Mask R-CNN features <ref type="bibr" target="#b3">4</ref> . The key difference is that they do not only encode the content of the scene image, but also its similarity to the reference image, which forms the basis for the subsequent heads to generate object proposals, classify matches vs. non-matches and generate instance masks.</p><p>Head architecture Because the computed features can be used as a drop-in replacement for the original features, we can use the same region proposal network and ROI pooling operations as Mask R-CNN. We can also use the same classification and bounding box regression head as Mask R-CNN, but change the classification from an 80-way category discrimination to a binary match/non-match discrimination and generate only a single, class-agnostic set of bounding box coordinates. Similarly, for the mask branch we predict only a single instance mask instead of one per potential category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success Cases</head><p>False Positives <ref type="figure">Figure 4</ref>: Examples of Siamese Mask R-CNN operating in the one-shot setting, i.e. segmenting novel objects which are not known from training (split S 2 ). The only information our model has about these categories is one reference image (shown in the lower-left corner of each example; the categories in the titles are just for the reader). The top two rows show success cases while the last row displays some results with a lot of false positives. Best viewed with zoom and color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We train Siamese Mask R-CNN jointly on object detection and instance segmentation in the examplebased setting using the training set of MS-COCO. We train one model on each of the four category splits defined in Section 3 and evaluate the trained models on both known (train) and unknown (test) categories using the MS-COCO validation set. In the following paragraphs, we highlight the most important changes between our training and evaluation protocol and that of Mask R-CNN. The full training and evaluation details are given in Appendix A3 and A4.</p><p>Training. We first pre-train the ResNet backbone on a reduced subset of ImageNet, which contains only images from the 687 ImageNet categories that have no correspondence in MS-COCO. We do this to avoid using any label information about the test categories during pre-training.</p><p>We then proceed by training episodically. For each image in a minibatch, we pick a random reference category among the training categories present in the image. We then crop a random instance of this category out of another random image in the training set. We keep only the annotations of this category; all other objects are treated as background.</p><p>Evaluation. We evaluate our model using the procedure described in Section 3. Each category split is evaluated separately. The final score is the mean of the scores from all four splits. This evaluation procedure is stochastic due to the random selection of references. We thus repeat the evaluation five times and report the average and 95% confidence intervals.</p><p>Baseline: random boxes. As a simple sanity check, we evaluate the performance of a model predicting random bounding boxes and segmentation masks. To do so, we take ground-truth bounding boxes and segmentation masks for the category of the reference image, and randomly shift the boxes around the image (assigning a random confidence value for each box between 0.8 and 1). We keep the ground-truth segmentation masks intact in the shifted boxes. This procedure allows us to get random predictions while keeping certain statistics of the ground-truth annotations (e.g. number of boxes per image, their sizes, etc.).  We run our models with one or five references per category and image (shots).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories used in training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Example-based detection and segmentation. We begin by applying the trained Siamese Mask R-CNN model to detect objects from the categories used for training. In this setting, all of the training examples are used to learn the metric, but the detection is based only on the similarity to one (or five) instance(s) from the reference category. IWith one reference, we achieve 37.6% and 34.9% mAP50 for object detection and instance segmentation, respectively. With five references, we achieve 41.3% and 38.4%, respectively ( <ref type="table" target="#tab_3">Table 1</ref>). We also report the 95% confidence interval estimated from five evaluation runs to quantify the variability introduced by to the random selection of reference images. The variation is below 0.2 percentage points in all cases, which suggests that evaluating five times is sufficient to handle the variability. We observe some additional variation between the splits, which seems to stem mostly from the over-representation of the person category (see <ref type="table" target="#tab_8">Appendix Table A2</ref> for results of each split).</p><p>One-shot instance segmentation. Next, we report the results of evaluating Siamese Mask R-CNN on novel categories not used for training, showcasingits ability to generalize to the 20 held-out categories that have not been annotated during training. With one reference (one-shot), the average detection mAP50 score for the test splits is 16.3%, while the segmentation performance is 14.5% <ref type="table" target="#tab_3">(Table 1)</ref>. While these values are significantly lower than those for the training categories, they still present a strong baseline and are far from chance (1.2%/0.5% for detection/segmentation) despite the difficulty of the one-shot setting. When using five references (five-shot), the performance improves to 18.5% and 16.7%, respectively. Taken together, these results suggest that the metric our model has learned allows some generalization outside of the training categories, but a substantial degree of overfitting on the those categories remains. Qualitative analysis. The first two rows of <ref type="figure">Figure 4</ref> show some examples of successful detection and segmentation of objects from novel categories. These examples allow us to get a feeling for the difficulty of the task: the reference inputs are quite different from the instances in the query image, often showing different perspectives, usually very different instances of the category and sometimes only parts of the reference object. Also note that the ground truth segmentation mask is not used to pre-segment the reference.</p><p>To generate bounding boxes and segmentation masks, the model can thus use only its general knowledge about objects. It has to rely on the metric learned on the categories annotated during training to decide whether the reference and the query instances belong to the same category. For instance, the bus and the horse in the second row of <ref type="figure">Figure 4</ref> are incomplete and the network has never been provided with ground truth bounding boxes or instance masks for either horses or buses. Nevertheless, it still finds the correct object in the query image and segments the entire object.</p><p>We also show examples of failure cases in the last row of <ref type="figure">Figure 4</ref>. The picture that emerges from both successful and failure cases is that the network produces overall good bounding boxes and segmentation masks, but often fails at targeting them towards the correct category. We elaborate more on the challenges of the task in the following paragraphs.</p><p>False positives when evaluating on novel categories. There is a marked drop in model performance between evaluating on the categories used during training and the novel categories, suggesting some degree of overfitting to the training categories. If this is indeed the case, we would expect false positives to be biased towards these categories and, in particular, towards those categories that are most frequent in the training set. Qualitatively, this bias seems indeed to exist <ref type="figure">(Figure 4</ref>). We verified this assumption quantitatively by computing a confusion matrix between categories (Appendix <ref type="figure">Figure A1</ref>). The confusion matrix shows that objects from the training categories are often falsely detected when searching for objects of the novel categories. Among the most commonly falsely detected categories are people, cars, airplanes and clocks which are overrepresented in the dataset.</p><p>Effect of image clutter. Previous work on synthetic data found that cluttered scenes are especially challenging in example based one-shot tasks <ref type="bibr" target="#b36">[37]</ref>. This effect is also present in the current context. Both detection and segmentation scores are substantially higher for images with a small number of total instances ( <ref type="figure" target="#fig_2">Figure 5</ref>), underscoring the importance of extending the model to robustly process cluttered scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>As outlined in section 3, our approach lies at the intersection of few-shot/metric learning, object detection/visual search, and instance segmentation. Each of these aspects has been investigated extensively. The novelty of our approach is the combination of all these aspects. A number of very recent and, to a large extent concurrent, works have started addressing few-shot detection. We review the most closely related work below. We are not aware of any previous work on category-based few-shot instance segmentation.</p><p>Dong et al. <ref type="bibr" target="#b12">[13]</ref> train a semi-supervised few-shot detector on the 20 categories of Pascal VOC using roughly 80 annotated images, supplemented by a large set of unlabeled images. They train a set of models, each of which generates training labels for the other models by using high-confidence detections in the unlabeled images. The low-shot transfer detector (LSTD) <ref type="bibr" target="#b6">[7]</ref> fine-tunes an object detector on a transfer task with new categories using two novel regularization terms: one for background depression and one for knowledge transfer from the source domain to the target domain. Kang et. al. <ref type="bibr" target="#b23">[24]</ref> extend a single-stage object detector -YOLOv2 [45] -by a category-specific feature reweighting that is predicted by a meta model, allowing them to incorporate novel classes with few examples. Schwartz et. al. <ref type="bibr" target="#b54">[55]</ref> replace the classification branch of Faster R-CNN with a metric learning module, which evaluates the similarity of each predicted box to a set of prototypes generated from the few provided examples. Very recent concurrent work <ref type="bibr" target="#b65">[66]</ref> evaluates the same task as we do for object detection on Pascal VOC using Faster R-CNN, although they employ separate feature fusions in the RPN and classifier head instead of the unified matching we employ. Recent works on zero-shot detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b68">69]</ref> use a similar approach to ours to target the detection towards a novel category, except that they learn a joint embedding for the query image and a textual description (instead of a visual description) of this novel category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We introduced the task of one-shot instance segmentation which requires models to generalize to object categories that have not been labeled during training in the challenging setting of instance segmentation. To address this task we proposed Siamese Mask R-CNN, a model combining a stateof-the-art instance segmentation model (Mask R-CNN) with a metric learning approach (Siamese networks). This model can detect and segment objects from novel categories based on a single reference image. While our approach is not as successful on novel categories as on those used for training, it performs far above chance, showcasing it's ability to generalize to categories outside of the training set. Generally, it is expected from any reasonable learning system that it should perform better on object categories for which it has been trained with thousands of examples than for those encountered in a few-shot setting. Considering the difficulty of this problem, the performance of our model should provide a strong baseline and we hope that our work provides a first step towards visual search algorithms with human like flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changes to previous version</head><p>Compared to the previous version (submitted to arxiv on 28 Nov 2018) this version additionally includes:</p><p>• a different evaluation procedure evaluating each split 5-times and reporting the mean and 95% confidence interval. • five-shot results using a prototypical approach to accomodate multiple reference images.</p><p>• a background section introducing information and notation of object detection and few-shot learning tasks. • discussion of concurrent work which was published on arxiv since the publication of the previous version <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b65">66]</ref>. • detailed description of the training and evaluation process in the Appendix.</p><p>• results for all metrics evaluated on the MS-COCO leaderboard to the Appendix.</p><p>Additionally to adding content we reworked large parts of the text to clarify the task setup the way we present related tasks and the corresponding solutions. We also update some of the figures, mainly combining the two figures for qualitative analysis into one figure which includes good and bad examples, adding a comparison with traditional few-shot learning tasks to the introduction figure and making the color coding in the model figure easier to understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Training and testing categories</head><p>This section contains the description of the category splits from Section 3 from the main paper as well as a table of those categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1 Splits S 1 -S 4</head><p>To be able to evaluate performance on novel categories we hold out some categories during training. We split the 80 object categories in MS-COCO into 60 training and 20 test categories. Following earlier work on Pascal VOC <ref type="bibr" target="#b55">[56]</ref>, we generate four such training/test splits by including every fourth category into the test split starting with the first, second, third or fourth category, respectively. These splits are shown in <ref type="table" target="#tab_3">Table A1</ref> below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2 Rationale</head><p>Providing four splits with equally distributed held-out categories has two main advantages: It allows to test on all categories in MS-COCO (albeit with different models) while sub sampling the super categories <ref type="bibr" target="#b32">[33]</ref> as evenly as possible. This approach assumes that we will know some objects from all broad object categories in the world and that we can infer the missing parts from this knowledge. This setup differs from tasks like tieredImageNet <ref type="bibr" target="#b47">[48]</ref> which require generalization to objects from vastly different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Implementation details A2.1 Backbone</head><p>We use the standard architecture of ResNet-50 <ref type="bibr" target="#b21">[22]</ref> without any modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2 Feature matching</head><p>• We use layers 5 res2c_relu (256 features), res3d_relu (512), res4f_relu (1024) and res5c_relu (2048) of the backbone as a feature representation of the inputs. For brevity, we refer to these layers as C 2 , C 3 , C 4 and C 5 .   • FPN generates multi-scale representations P i , i = {2, 3, 4, 5, 6} consisting of 256 features (for all i) as follows. P 5 is a result of applying a 1 × 1 conv layer to C 5 (to get 256 features). <ref type="figure" target="#fig_0">{2, 3, 4})</ref> is a sum of a 1 × 1 conv layer applied to C i and up-sampled (by a factor of two on each side) P i+1 . P 6 is a down-sampled P 5 (by a factor of two on each side). </p><formula xml:id="formula_0">P i (i =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3 Region Proposal Network (RPN)</head><p>• We use 3 anchor aspect ratios (0.5, 1, 2) at each pixel location for the 5 scales <ref type="bibr">(32, 64,</ref> 128, 256, 512) i = {2, . . . , 6} defined above, resulting in 3 × (32 2 + . . . + 512 2 ) ≈ 1M proposals in total. • The architecture is a 3 × 3 × 512 conv layer, followed by the 1 × 1 conv outputting k times number of anchors per location (three in our case) features (corresponding to proposal logits for k = 2 or to bounding box deltas for k = 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.4 Classification and bounding box regression head</head><p>The classification head produces same/different classifications for each proposal and performs bounding box regression.</p><p>• Inputs: the computed bounding boxes (outputs of the RPN) are cropped from P i , reshaped to 7 × 7, and concatenated for i = {2, . . . , 5}. Only 6000 top scoring anchors are processed for efficiency. • Architecture: two fc-layers (1024 units with ReLU) followed by a logistic regression into 2 classes (same as reference or not). • Bounding box regression is part of the classification branch, but uses a different output layer. This output layer produces fine adjustments (deltas) of the bounding box coordinates (instead of class probabilities). • Non-maximum suppression (NMS; threshold 0.7) is applied to the predicted bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.5 Segmentation head</head><p>• Inputs: the computed bounding boxes are cropped from P i , reshaped to 14 × 14, and concatenated for i = {2, . . . , 5}. • Architecture: four 3 × 3 conv layers (with ReLU and BN) followed by a transposed conv layer with 2 × 2 kernels and stride of 2, and a final 1 × 1 conv layer outputting two feature maps consisting of logits for foreground/background at each spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3 Training details</head><p>This section contains a detailed description of the training procedure. To make this section more readable and have all relevant information in one place it contains a few duplications with Section 5</p><p>Pre-training backbone. We pre-train the ResNet backbone on image classification on a reduced subset of ImageNet, which contains images from the 687 ImageNet categories without correspondence in MS-COCO -hence we refer to it as ImageNet-687. Pre-training on this reduced set ensures that we do not use any label information about the test categories at any training stage.</p><p>Training Siamese Mask R-CNN. We train the models using stochastic gradient descent with momentum for 160,000 steps with a batch size of 12 on 4 NVIDIA P100 GPUs in parallel. With this setup training takes roughly a week. We use an initial learning rate of 0.02 and a momentum of 0.9. We start our training with a warm-up phase of 1,000 steps during which we train only the heads. After that, we train the entire network, including the backbone and all heads, end-to-end. After 120,000 steps, we divide the learning rate by 10.</p><p>Construction of mini-batches. During training, a mini-batch contains 12 sets of reference and query images. We first draw the query images at random from the training set and pre-process them in the following way: (1) we resize an image so that the longer side is 1024 px, while keeping the aspect ratio, (2) we zero-pad the smaller side of the image to be square 1024 × 1024, (3) we subtract the mean ImageNet RGB value from each pixel. Next, for each image, we generate a reference image as follows: (1) draw a random category among all categories of the background set present in the image, (2) crop a random instance of the selected category out of any image in the training set (using the bounding box annotation), and (3) resize the reference image so that its longer side is 192 px and zero-pad the shorter side to get a square image of 192 × 192. To enable a quick look-up of reference instances, we created an index that contains a list of categories present in each image.</p><p>Labels. We use only the annotations of object instances in the query image that belong to the corresponding reference category. The annotations of all other objects are removed and subsequently they are treated as background.</p><p>Loss function. Siamese Mask R-CNN is trained on the same basic multi-task objective as Mask R-CNN: classification and bounding box loss for the RPN; classification, bounding box and mask loss for each RoI. There are a couple of differences as well. First, the classification losses consist of a binary cross-entropy of the match/non-match classification rather than an 80-way multinomial cross-entropy used for classification on MS-COCO. Second, we found that weighting the individual losses differently improved performance in the one-shot setting. Specifically, we apply the following weights to each component of the loss function: RPN classification loss: 2, RPN bounding box loss: 0.1, RoI classification loss: 2, RoI bounding box loss: 0.5 and mask loss: 1.</p><p>Exact hyper parameter details Complex systems like Mask R-CNN require a large set of hyper parameters to be set for optimal training performance. We mentioned all changes we made to the hyperparameter settings of the implementation we extended <ref type="bibr" target="#b1">[2]</ref>. For the full list of hyperparameter settings and exact details of our loss function implementation and data handling please refer to the code: https://github.com/bethgelab/siamese-mask-rcnn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Evaluation details</head><p>This section contains a detailed description and discussion of the evaluation procedure. As with the training section it contains a few duplications with the corresponding Section 3 from the main paper in order to have all information in one place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1 Category selection</head><p>The evaluation is performed on the MS Coco 2017 validation set (which corresponds to the 2014 minval set). The evaluation is performed for 4 subtasks, each using 60 categories for training and the remaining 20 categories for one-shot evaluation. Those 20 categories are selected by choosing every 4th category, therefore the ith split is constructed by: [i + 4 * k for k in range <ref type="bibr" target="#b19">(20)</ref>]. An explicit listing of all 4 splits can be found in <ref type="table" target="#tab_3">Table A1</ref> above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2 Evaluation procedure</head><p>Each of the subtasks is evaluated over the whole validation set using the corresponding set of categories. Therefore for each image the present categories from the current split are determined. Then for each present category a reference instance is randomly chosen from the whole evaluation set (those references are chosen individually for each image). The model is then evaluated for each of the references and the predictions of each of these runs is assigned to the corresponding category. If no category from the current split is present the image is skipped. After running this over all images the results contain predicted bounding boxes for each image but only for the categories of the selected split. These collected results can then be fed to a slightly modified version of the official MS-COCO analysis tools <ref type="bibr" target="#b0">[1]</ref> which can handle specific category subsets to get the final mAP50 scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.3 Noise induced by random reference sampling</head><p>Because only one reference is sampled per category and image the predictions can be rather noisy (especially in the one-shot case). For our model the std of the predicted results is ±1%. To get a good prediction of the actual mean we run the evaluation of each split 5 times thus reaching reaching a standard error of the mean of less than ±0.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.4 Comment on the evaluation procedure</head><p>We specifically chose to evaluate our model only on the categories present in each image. We think, that this scenario can realistically be assumed in real world tasks as a whole-image classification network can be used to pre-select if the reference category is present in an image before running the bounding box and instance segmentation prediction network. This choice, however, makes the task substantially easier than evaluating each image for all categories. It does not punish false positives as hard as the other task does. However, as visible in our results, false positives play an important role even in our simpler task, which leads us to the conclusion, that our task setup is still sufficiently difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.5 Note on non-maximum suppression</head><p>We use non-maximum suppression (NMS) on the predictions of each image/references combination individually and not on the combined output of an image after running the detection for all references because at test time the system needs to be able to detect and segment objects based on only a single reference example of each category separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.6 Choice of evaluation metric</head><p>We chose to use mAP50 instead of the so called "coco metric" mAP. mAP50 is evaluated at a single Intersection over Union (IoU) threshold of 50% between predicted and the ground truth bounding boxes (corresponding to around 70% overlap between two same-sized boxes/masks) while mAP is evaluated at IoU thresholds of [50%, 55%, ..., 95%] adding weight to exact bounding box/segmentation mask predictions.</p><p>We think, that mAP50 is the value most reflective of the result we are interested in: whether our model can find novel objects based on a single reference image. For instance segmentation the additional information about mask quality implicitly included in mAP might make sense. However we found, that correctly masking the sought objects was less of a problem for our model than correctly classifying them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 Confusion matrix</head><p>To quantify the errors of our model we compute a confusion matrix over the 80 categories in MS-COCO using a model trained on split S 2 ( <ref type="figure">Figure A1</ref>). The element (i, j) of this matrix corresponds to the AP50 value of detections obtained for reference images of category i, which are evaluated as if the reference images belonged to category j. If there were no false positives, the off-diagonal elements of the matrix would be zero. The sums of values in the columns show instances of categories that are most often falsely detected (the histogram of such sums is shown below the matrix). Among such commonly falsely predicted categories are people, cars, airplanes, clocks, and other categories that are common in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6 Additional results</head><p>In this section we discuss the noisiness of our evaluation approach and provide additional results including split-by-split values for the 95% confidence intervals we get from running the evaluation 5 times <ref type="table" target="#tab_8">(Table A2</ref>) and the full results on all metrics evaluated on the MS-COCO leaderboard (cocodataset.org/#detection-leaderboard) for object detection <ref type="table" target="#tab_9">(Tables A3 &amp; A5)</ref> and instance segmentation <ref type="table" target="#tab_10">(Tables A4 &amp; A6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.1 Noisiness of evaluation</head><p>The example based evaluation setting with a randomly drawn reference per category and image is naturally prone to be noisy. We therefore evaluate our models five times and take the mean of these 5 evaluations as our final result. We here want to discuss the amount of randomness generated by our evaluation procedure and the confidence of our mean.</p><p>We found the standard deviation of one-shot object detection and instance segmentation segmentation to be around 0.3% mAP50 while the standard deviation with five reference images is lower at 0.1% mAP50. The 95% confidence of the mean is around 0.1% (See <ref type="table" target="#tab_3">Table 1</ref>. The rather small deviations can be seen as a result of the evaluation procedure which considers every image and reference category as a single instance. This ensures that there are many samples per category over test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.2 Results for each split</head><p>We show the results for each split (S 1 -S 4 ) separately reporting mean and 95% confidence interval of five evaluation runs in <ref type="table" target="#tab_8">Table A2</ref>. We find slight difference in performance between these split with split S 1 showing the biggest gap between evaluating on the training and test categories. We assume, that this is due to the strong over representation of the person category in MS-COCO <ref type="bibr" target="#b32">[33]</ref>. With a lot of small instances and presence of persons in almost every image the removal of this category during training makes the dataset considerably easier, while requesting to detect them later is hard.</p><p>One-shot classes <ref type="figure">Figure A1</ref>: Confusion matrix for the Siamese Mask R-CNN model using split S 2 for one-shot evaluation. The element (i, j) shows the AP50 of using detections for category i and evaluating them as instances of category j. The histogram below the matrix shows the most commonly confused (or falsely predicted) categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.3 Full MS-COCO style results</head><p>In this section we report results on all metrics used at the MS-COCO leader board cocodataset. org/#detection-leaderboard. Beyond the mAP50 (AP 50 ) metric reported in the main paper these include the MS-COCO metric (AP) as well as other AP metrics at different thresholds (AP 75 ) and object sizes (AP S , AP M , AP L each as subsets of AP) as well as recall metrics (AR) with varying numbers of detections (AR 1 , AR 10 , AR 100 ) and object sizes (AR S , AR M , AR L each as parts of AR 100 ).     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>, IF = Image Features, L1 = Pointwise L1 Difference, RPN = Region Proposal Network CLS = Classifier, BBOX = Bounding Box Regressor SEGM Comparison of Mask R-CNN and Siamese Mask R-CNN. The main differences (marked in red) of our model are (1) the Siamese backbone which jointly encodes the image and reference, and (2) the matching of those embeddings to target the region proposal and classification heads towards the reference category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature matching. 3 : 1 . 3 Figure 3 :</head><label>3133</label><figDesc>To obtain a measure of similarity between the reference and different regions of the query image, we treat each (x,y) location of the encoded features of the query image as an embedding vector and compare it to the embedding of the reference image. This procedure can be viewed as a non-linear template matching in the embedding space instead of the pixel space. The matching procedure works as shown inFigure Averagepool the features of the reference image to an embedding vector. In the fewshot case (more than one reference) compute the average of the reference features as in prototypical networks<ref type="bibr" target="#b56">[57]</ref>. 2. Compute the absolute difference between the reference embedding and that of the scene at each (x,y) position. 3. Concatenate this difference to the scene representation. 4. Reduce the number of features with a 1 × 1 convolution. Sketch of the matching procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Results on split S 2 (in % mAP50) separated by the number of instances per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>S</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>The final similarity scores between the input scene and the reference at scale i are computed by obtaining P scene i and P ref i as described above, applying global average pooling to P ref i , and computing pixel-wise differences D i = abs(P scene i − pool(P ref i )). • The final feature representations containing information about similarities between the scene and the reference are computed by concatenating P scene i and D i , and applying a 1 × 1 conv layer, outputting 384 features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 0.2 41.3 ± 0.1 16.3 ± 0.1 18.5 ± 0.1 1.2 ± 0.1 Instance segmentation 34.9 ± 0.1 38.4 ± 0.1 14.5 ± 0.1 16.8 ± 0.1 0.5 ± 0.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Novel categories</cell><cell>Random</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Object detection</cell><cell>37.6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results on MS-COCO (in % mAP50 with 95% confidence intervals). Three settings are reported: Evaluating on training (train), novel (test) categories and randomly drawn boxes (random).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A1 :</head><label>A1</label><figDesc></figDesc><table /><note>Category splits (S 1 -S 4 , Section 3) of MS-COCO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>± 0.1 36.6 ± 0.1 37.5 ± 0.1 37.2 ± 0.2 37.6 ± 0.1 5 42.4 ± 0.1 40.5 ± 0.1 41.5 ± 0.1 40.9 ± 0.2 41.3 ± 0.1 Test 1 15.3 ± 0.2 16.8 ± 0.2 16.7 ± 0.2 16.4 ± 0.1 16.3 ± 0.1 5 16.8 ± 0.1 20.0 ± 0.1 18.2 ± 0.1 19.0 ± 0.1 18.5 ± 0.1 ± 0.1 33.5 ± 0.1 34.9 ± 0.1 34.5 ± 0.2 34.9 ± 0.1 5 39.7 ± 0.1 37.3 ± 0.1 38.7 ± 0.1 37.9 ± 0.2 38.4 ± 0.1 Test 1 13.5 ± 0.2 14.9 ± 0.1 15.5 ± 0.2 14.2 ± 0.1 14.5 ± 0.1 5 14.8 ± 0.1 18.0 ± 0.1 17.4 ± 0.1 16.9 ± 0.1 16.8 ± 0.1</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Object detection</cell><cell></cell><cell></cell></row><row><cell cols="2">Categories Shots</cell><cell>S 1</cell><cell>S 2</cell><cell>S 3</cell><cell>S 4</cell><cell>Ø</cell></row><row><cell>Train</cell><cell>1</cell><cell cols="2">39.1 Instance segmentation</cell><cell></cell><cell></cell></row><row><cell cols="2">Categories Shots</cell><cell>S 1</cell><cell>S 2</cell><cell>S 3</cell><cell>S 4</cell><cell>Ø</cell></row><row><cell>Train</cell><cell>1</cell><cell>36.6</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A2 :</head><label>A2</label><figDesc>Results on MS Coco (in % mAP50 with 95% confidence intervals). In split S i , every fourth category, starting at the i th , is placed into the test set.Model AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L</figDesc><table><row><cell>full</cell><cell>21.8</cell><cell>35.5</cell><cell>23.4</cell><cell cols="4">11.1 21.8 30.8 19.9</cell><cell>37.6</cell><cell>39.2</cell><cell>22.2</cell><cell>41.0</cell><cell>56.5</cell></row><row><cell cols="2">train S 1 23.6</cell><cell>39.1</cell><cell>25.0</cell><cell cols="4">11.4 23.3 33.8 20.9</cell><cell>38.9</cell><cell>40.7</cell><cell>22.9</cell><cell>43.1</cell><cell>57.5</cell></row><row><cell cols="2">train S 2 21.9</cell><cell>36.6</cell><cell>23.5</cell><cell cols="4">11.4 22.6 31.1 19.9</cell><cell>37.9</cell><cell>39.4</cell><cell>22.7</cell><cell>41.9</cell><cell>57.1</cell></row><row><cell cols="2">train S 3 23.3</cell><cell>37.5</cell><cell>25.2</cell><cell cols="4">11.1 22.5 33.4 20.9</cell><cell>39.3</cell><cell>41.0</cell><cell>21.8</cell><cell>43.1</cell><cell>59.7</cell></row><row><cell cols="2">train S 4 22.7</cell><cell>37.2</cell><cell>24.2</cell><cell cols="4">11.9 21.6 31.7 20.1</cell><cell>38.5</cell><cell>40.4</cell><cell>23.2</cell><cell>42.4</cell><cell>56.7</cell></row><row><cell>test S 1</cell><cell>8.6</cell><cell>15.3</cell><cell>8.8</cell><cell>5.0</cell><cell>8.6</cell><cell cols="2">13.5 10.3</cell><cell>26.4</cell><cell>27.7</cell><cell>14.4</cell><cell>29.9</cell><cell>43.2</cell></row><row><cell>test S 2</cell><cell>9.8</cell><cell>16.8</cell><cell>10.1</cell><cell>5.7</cell><cell>8.4</cell><cell cols="2">14.8 12.2</cell><cell>26.7</cell><cell>27.7</cell><cell>13.9</cell><cell>27.6</cell><cell>43.9</cell></row><row><cell>test S 3</cell><cell>8.9</cell><cell>16.7</cell><cell>8.8</cell><cell>5.6</cell><cell>8.2</cell><cell>16.6</cell><cell>9.4</cell><cell>23.6</cell><cell>24.6</cell><cell>15.3</cell><cell>25.1</cell><cell>40.0</cell></row><row><cell>test S 4</cell><cell>9.1</cell><cell>16.4</cell><cell>9.2</cell><cell>5.4</cell><cell>9.4</cell><cell cols="2">14.0 10.9</cell><cell>25.7</cell><cell>27.4</cell><cell>14.5</cell><cell>30.7</cell><cell>43.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A3 :</head><label>A3</label><figDesc>Full one-shot detection results on MS-COCO. train/test indicate evaluation on the training/test categories of split S i respectively. Each value is the mean of 5 evaluation runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A4 :</head><label>A4</label><figDesc>Full one-shot segmentation results on MS-COCO. train/test indicate evaluation on the training/test categories of split S i respectively. Each value is the mean of 5 evaluation runs. Model AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L</figDesc><table><row><cell>full</cell><cell>24.9</cell><cell>40.5</cell><cell>26.7</cell><cell cols="4">13.3 25.0 35.9 21.8</cell><cell>40.1</cell><cell>41.8</cell><cell>23.9</cell><cell>44.3</cell><cell>59.1</cell></row><row><cell cols="2">train S 1 25.7</cell><cell>42.4</cell><cell>27.1</cell><cell cols="4">12.6 25.6 36.2 22.1</cell><cell>40.6</cell><cell>42.4</cell><cell>24.3</cell><cell>45.1</cell><cell>59.3</cell></row><row><cell cols="2">train S 2 24.3</cell><cell>40.5</cell><cell>26.1</cell><cell cols="4">12.8 25.1 35.3 21.4</cell><cell>39.7</cell><cell>41.3</cell><cell>24.1</cell><cell>44.2</cell><cell>59.9</cell></row><row><cell cols="2">train S 3 25.8</cell><cell>41.5</cell><cell>28.0</cell><cell cols="4">12.7 25.2 38.2 22.4</cell><cell>41.0</cell><cell>42.7</cell><cell>23.5</cell><cell>45.1</cell><cell>61.5</cell></row><row><cell cols="2">train S 4 25.1</cell><cell>40.9</cell><cell>26.8</cell><cell cols="4">12.9 23.8 36.3 21.5</cell><cell>40.3</cell><cell>42.3</cell><cell>24.7</cell><cell>44.5</cell><cell>59.1</cell></row><row><cell>test S 1</cell><cell>9.4</cell><cell>16.8</cell><cell>9.7</cell><cell>5.6</cell><cell>9.3</cell><cell cols="2">14.6 11.0</cell><cell>28.1</cell><cell>29.4</cell><cell>15.8</cell><cell>31.9</cell><cell>45.8</cell></row><row><cell>test S 2</cell><cell>11.7</cell><cell>20.0</cell><cell>12.1</cell><cell>6.3</cell><cell>9.7</cell><cell cols="2">19.3 13.3</cell><cell>29.1</cell><cell>30.3</cell><cell>15.1</cell><cell>30.7</cell><cell>48.3</cell></row><row><cell>test S 3</cell><cell>9.8</cell><cell>18.2</cell><cell>9.5</cell><cell>6.7</cell><cell>9.2</cell><cell>17.5</cell><cell>9.6</cell><cell>25.0</cell><cell>26.0</cell><cell>16.3</cell><cell>26.4</cell><cell>42.4</cell></row><row><cell>test S 4</cell><cell>10.6</cell><cell>19.0</cell><cell>10.6</cell><cell>5.8</cell><cell cols="3">10.4 16.6 11.8</cell><cell>27.8</cell><cell>29.6</cell><cell>14.8</cell><cell>33.1</cell><cell>47.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A5 :</head><label>A5</label><figDesc>Full five-shot detection results on MS-COCO. train/test indicate evaluation on the training/test categories of split S i respectively. Each value is the mean of 5 evaluation runs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We chose to use mAP50 (mAP @ 50% Bounding Box IoU<ref type="bibr" target="#b13">[14]</ref>) instead of the COCO metric mAP (mean of mAP @ 50, 55, ..., 95% Bounding Box IoU<ref type="bibr" target="#b32">[33]</ref>), because we think it more directly reflects the result we are primarily interested in: whether our model can find novel objects based on a single reference image. For results using the MS-COCO metric see Appendix Section A6</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Most co-segmentation work (e.g.<ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b14">15]</ref>) uses the same object categories during training and test time and therefore does not operate in the few-shot setting 3 https://github.com/bethgelab/siamese-mask-rcnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As we use a backbone with feature pyramid networks (FPN) we get features at multiple resolutions. We therefore simply apply the described matching procedure at each resolution independently.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Using the notation from here: https://ethereon.github.io/netscope/#/gist/ db945b393d40bfa26006</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We want to thank Mengye Ren, Jake Snell, James Lucas, Marc Law, Richard Zemel and Eshed Ohn-Bar for helpful discussion. This work was supported by the Deutsche Forschungsgemeinschaft </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco Website</surname></persName>
		</author>
		<ptr target="http://cocodataset.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mask r-cnn for object detection and instance segmentation on keras and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/matterport/Mask_RCNN" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zero-Shot Object Detection. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature Verification Using A &quot;Siamese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Time Delay Neural Network. IJPRAI</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LSTD: A Low-Shot Transfer Detector for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional Feature Masking for Joint Object and Stuff Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. Zero-shot object detection by hybrid region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkan</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-Shot Semantic Segmentation with Prototype Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Few-Example Object Detection with Model Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-segmentation by Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end Learning of Deep Visual Representations for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepco 3 : Deep instance co-segmentation by co-peak search and co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01866</idno>
		<title level="m">Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Siamese Neural Networks for One-shot Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Object Co-Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><forename type="middle">Li</forename><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to Learn Quickly for Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08479</idno>
		<title level="m">Bernt Schiele, and Tat-Seng Chua. LCC: learning to customize and combine neural networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">One-Shot Segmentation in Clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05076</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Metalearning with Hebbian Fast Weights.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><forename type="middle">Yu Meta</forename><surname>Networks</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to Segment Object Candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07373</idno>
		<title level="m">Conditional Networks for Few-Shot Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An Incremental Improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-End Instance Segmentation with Recurrent Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-Learning for Semi-Supervised Few-Shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-Learning with Latent Embedding Optimization. In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Faster R-CNN Features for Instance Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR DeepVision workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Visual search. In Handbook of perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mieke</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="43" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">RepMet: Representative-based metric learning for classification and one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharathchandra</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Irfan Essa, and Byron Boots. One-Shot Learning for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
		<title level="m">Learning to Compare: Relation Network for Few-Shot Learning. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of CNN activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02654</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">One-shot Texture Segmentation</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02872</idno>
		<title level="m">Large Margin Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Visual search for arbitrary objects in real scenes. Attention, perception &amp; psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoana</forename><forename type="middle">I</forename><surname>Kuzmova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><forename type="middle">M</forename><surname>Sherman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-08" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1650" to="1671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Annotation-free and one-shot learning for instance segmentation of homogeneous object clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Comparison network for one-shot conditional object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02317</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transforms for one-shot medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Zero shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TCSVT, 2019. Model AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Full five-shot segmentation results on MS-COCO. train/test indicate evaluation on the training/test categories of split S i respectively</title>
	</analytic>
	<monogr>
		<title level="m">Table A6</title>
		<imprint/>
	</monogr>
	<note>Each value is the mean of 5 evaluation runs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
