<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Attention Network for Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Wang</surname></persName>
							<email>wangtw@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
							<email>z.yuanzhi@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
							<email>canjie.luo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Chen</surname></persName>
							<email>xxuechen@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Lenovo Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Lenovo Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxiang</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Lenovo Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupled Attention Network for Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text recognition has attracted considerable research interests because of its various applications. The cutting-edge text recognition methods are based on attention mechanisms. However, most of attention methods usually suffer from serious alignment problem due to its recurrency alignment operation, where the alignment relies on historical decoding results. To remedy this issue, we propose a decoupled attention network (DAN), which decouples the alignment operation from using historical decoding results. DAN is an effective, flexible and robust end-to-end text recognizer, which consists of three components: 1) a feature encoder that extracts visual features from the input image; 2) a convolutional alignment module that performs the alignment operation based on visual features from the encoder; and 3) a decoupled text decoder that makes final prediction by jointly using the feature map and attention maps. Experimental results show that DAN achieves state-of-the-art performance on multiple text recognition tasks, including offline handwritten text recognition and regular/irregular scene text recognition. Codes will be released. 1 * Corresponding author 1 https://github.com/Wang-Tianwei/Decoupled-attentionnetwork Figure 1: (a) Traditional attentional text recognizer, where the alignment operation is conducted using visual information and historical decoding information (red arrow). (b) Decoupled attention network, where the alignment operation is conducted using only visual information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text recognition has drawn much research interest in recent years. Benefiting from the development of deep learning and sequence-to-sequence learning, many text recognition methods have achieved notable success <ref type="bibr" target="#b25">(Long, He, and Yao 2018)</ref>. Connectionist temporal classification (CTC) <ref type="bibr" target="#b11">(Graves et al. 2006</ref>) and attention mechanism  are two most popular methods, among them attention mechanism shows significant better performance and has been studied frequently in recent years <ref type="bibr" target="#b25">(Long, He, and Yao 2018)</ref>.</p><p>The attention mechanism, proposed in  to tackle machine translation problem, was used to handle scene text recognition in <ref type="bibr" target="#b20">(Lee and Osindero 2016;</ref><ref type="bibr" target="#b36">Shi et al. 2016)</ref>, and since then it dominated text recognition with the following developments <ref type="bibr" target="#b43">(Yang et al. 2017;</ref><ref type="bibr" target="#b5">Cheng et al. 2017;</ref><ref type="bibr" target="#b1">Bai et al. 2018;</ref><ref type="bibr" target="#b28">Luo, Jin, and Sun 2019;</ref><ref type="bibr" target="#b21">Li et al. 2019)</ref>. The attention mechanism in text recognition is used to align and recognize characters, where the alignment operation has always been coupled with the decoding operation in previous work <ref type="bibr" target="#b36">(Shi et al. 2016;</ref><ref type="bibr" target="#b5">Cheng et al. 2017;</ref><ref type="bibr" target="#b1">Bai et al. 2018;</ref><ref type="bibr" target="#b21">Li et al. 2019)</ref>. As shown in <ref type="figure">Figure 1 (a)</ref>, the alignment operation of traditional attention mechanism is carried out using two types of information. The first is a feature map that can be regarded as visual information from the encoder, and the second is historical decoding information (in the form of a recurrent hidden state <ref type="bibr" target="#b29">Luong, Pham, and Manning 2015)</ref> or the embedding vector of previous decoding result <ref type="bibr" target="#b10">(Gehring et al. 2017;</ref><ref type="bibr" target="#b39">Vaswani et al. 2017)</ref>). The main idea underlying the attention mechanism is matching. Given a feature from the feature map, its attention score is computed by scoring how well it matches with the historical decoding information .</p><p>Traditional attention mechanism often encounters serious alignment problem <ref type="bibr" target="#b5">(Cheng et al. 2017;</ref><ref type="bibr" target="#b1">Bai et al. 2018;</ref><ref type="bibr" target="#b8">Chorowski et al. 2015;</ref><ref type="bibr" target="#b19">Kim, Hori, and Watanabe 2017)</ref>  <ref type="bibr" target="#b36">Shi et al. 2016)</ref> on long text. is because the coupling relationship inevitably leads to error accumulation and propagation. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the matching-based alignment is easily affected by decoding result. In the left image, the two consecutive "ly" confuses matching operation; in the right image, the misrecognized result "ing" confuses matching operation. <ref type="bibr" target="#b19">(Kim, Hori, and Watanabe 2017;</ref><ref type="bibr" target="#b8">Chorowski et al. 2015</ref>) also observed that attention mechanism struggles to align long sequence. Thus, it is intuitive to find a way to decouple the alignment operation from the historical decoding information, so that to reduce its negative impact.</p><p>To solve the aforementioned misalignment issue, in this paper we decouple the decoder of the traditional attention mechanism into an alignment module and a decoupled text decoder, and propose a new method called decoupled attention network (DAN) for text recognition. As shown in <ref type="bibr">Figure 1 (b)</ref>, compared with traditional attentional scene text recognizer, DAN needs no feedback from the decoding stage for alignment, thus avoiding the accumulation and propagation of decoding errors. The proposed DAN consists of three components including a feature encoder, a convolutional alignment module (CAM) and a decoupled text decoder. The feature encoder based on the convolutional neural network (CNN) extracts visual features from the input image. The CAM, substituting the traditional score-based recurrency alignment module, takes multi-scale visual features from the feature encoder as input, and generates attention maps with a fully convolutional network <ref type="bibr" target="#b26">(Long, Shelhamer, and Darrell 2014)</ref> (FCN) in channel-wise manner. The decoupled text decoder makes the final prediction by using the feature map and attention maps with a gated recurrent unit (GRU) <ref type="bibr" target="#b7">(Cho et al. 2014)</ref>.</p><p>In summary, our contributions are summarized as follows: • We propose a CAM to replace the recurrency alignment module in traditional attention decoders. The CAM conducts alignment operation from visual perspective, avoiding the use of historical decoding information, thus eliminating misalignment caused by decoding errors. • We propose DAN, which is a effective, flexible (can be easily switched to adapt to different scenarios) and robust (more robust to text length variation and subtle disturbances) attentional text recognizer. • DAN delivers state-of-the-art performance on several text recognition tasks, including handwritten text recognition and regular/irregular scene text recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Text recognition has attracted much research interest in the computer vision community. Early work of scene text recognition relied on low-level features, such as histogram of oriented gradients descriptors <ref type="bibr" target="#b40">(Wang, Babenko, and Belongie 2011)</ref>, connected components <ref type="bibr" target="#b32">(Neumann and Matas 2012)</ref>, etc. With the rapid development of deep learning, a large number of effective methods have been proposed. These methods can be mainly divided into two branches.</p><p>One branch is based on segmentation, it first detects characters then integrates characters into the output. <ref type="bibr" target="#b3">(Bissacco et al. 2013</ref>) proposed a five hidden layers for character recognition and a n-gram approach for language modeling. <ref type="bibr" target="#b41">(Wang et al. 2012</ref>) used a CNN to recognize characters and adopt a non-maximum suppression to obtain the final predictions.  proposed a weight-shared CNN for unconstrained text recognition. All of these methods require accurate individual detection of characters, which is very challenging.</p><p>The other branch is segmentation-free, it recognizes the text line as a whole and focuses on mapping the entire image directly to a word string. <ref type="bibr" target="#b15">(Jaderberg et al. 2016</ref>) regraded scene text recognition as a 90k-class classification task. <ref type="bibr" target="#b35">(Shi, Bai, and Yao 2017)</ref> modeled scene text recognition as a sequence problem by integrating the advantages of both deep convolutional neural network and recurrent neural network, and CTC was used to train the model end-to-end. <ref type="bibr" target="#b20">(Lee and Osindero 2016)</ref> and <ref type="bibr" target="#b36">(Shi et al. 2016)</ref> introduced attention mechanism to automatically align and translate words. From then on, more and more attention-based methods were proposed for text recognition. <ref type="bibr" target="#b5">(Cheng et al. 2017</ref>) observed the attention drift problem and proposed a focusing net to draw back the drifted attention, but character-level annotation was required. ) proposed a post-process, the edit probability to re-estimate the alignment; but they did not fundamentally solve misalignment. Focusing on recognition of irregular text, <ref type="bibr" target="#b36">(Shi et al. 2016)</ref>, <ref type="bibr" target="#b28">(Luo, Jin, and Sun 2019)</ref> and <ref type="bibr" target="#b45">(Zhan and Lu 2019)</ref> proposed to rectify text distortion and recognize the rectified text with an attention-based recognizer; <ref type="bibr" target="#b24">(Liu, Chen, and Wong 2018)</ref> proposed to rectify text at the character level; <ref type="bibr" target="#b43">(Yang et al. 2017)</ref> and <ref type="bibr" target="#b22">(Liao et al. 2019)</ref> proposed to recognize text in two-dimensional perspective but character-level annotation is required;  proposed to capture character feature in four directions. <ref type="bibr" target="#b9">(Fang et al. 2018)</ref> proposed an attention and language ensemble network, and multiple losses from attention and language are accumulated for training it. <ref type="bibr" target="#b21">(Li et al. 2019)</ref> proposed a simple and effective model using 2D attention mechanism.</p><p>Despite the notable success achieved by these attentionbased methods, all of them consider attention to be a coupled operation between historical decoding information and visual information, and no study to date has focused on applying attention mechanism in long text recognition to the best of our knowledge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAN</head><p>The proposed DAN aims at solving the misalignment issue of traditional attention mechanism through decoupling the alignment operation from using historical decoding results. To this end, we proposed a new convolutional alignment module (CAM) together with a decoupled text decoder to replace the traditional decoder. The overall architecture of DAN is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Details will be introduced in the followings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Encoder</head><p>We adopt a similar CNN-based feature encoder as previous study <ref type="bibr" target="#b37">(Shi et al. 2018</ref>). The feature encoder F encodes the input image x of size H × W into feature map F :</p><formula xml:id="formula_0">F = F(x), F ∈ R C×H/r h ×W/rw .<label>(1)</label></formula><p>where C , r h and r w denote the output channels, the height and the width downsampling ratio respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Alignment Module (CAM)</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the input of our proposed CAM is visual features of each scale from the feature encoder. These multi-scale features are first encoded by cascade downsampling convolutional layers then summarized as input. Inspired by the FCN that makes dense predictions per-pixel channel-wise (i.e., each channel denotes a heatmap of a class), we use a simple FCN architecture to conduct the attention operation channel-wise, which is quite different from current attention mechanism. The CAM has L layers; in the deconvolution stage, each output feature is added with the corresponding feature map from convolution stage. Sigmoid function with channel-wise normalization is finally adopted to generate attention maps A = {α 1 , α 2 , ..., α maxT }, where maxT denotes the maximum number of channels, i.e., the maximum number of decoding steps; and the size of each attention map is H/r h × W/r w . Compared with the FCN used for semantic segmentation, the CAM plays a completely different role to model a sequential problem. Although maxT is pre-defined and should be fixed during training and testing, we will experimentally show that the setting of maxT does not influence the final performance as long as it is reasonable.</p><p>By controlling the downsampling ratio r h and change the stride of CAM, DAN can be flexibly switched between 1D and 2D form. When H/r h = 1, DAN becomes a 1D recognizer and is suitable for long and regular text recognition; When H/r h &gt; 1(e.g., for input image with height of 32, r h = 4 results in a feature map with height of 4), DAN becomes a 2D recognizer and is suitable for irregular text recognition. Compared with previous 2D scene text recognizers, <ref type="bibr" target="#b43">(Yang et al. 2017;</ref><ref type="bibr" target="#b22">Liao et al. 2019)</ref> which need character-level annotation for supervision; <ref type="bibr" target="#b21">(Li et al. 2019)</ref> which uses a tailored 2D attention for 2D spatial relation- <ref type="figure">Figure 4</ref>: Detailed architecture of the decoupled text decoder. It consists of a GRU layer used to explore the contextual information and a linear layer to make predictions. 'EOS' denotes end-of-sequence symbol.</p><p>ships caption, result in more complex than 1D form and has a poor performance on regular text recognition, DAN is significantly simple and flexible, while achieves state-of-the-art or comparable performance both in 1D (handwritten text) and 2D (irregular scene text) recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoupled Text Decoder</head><p>Different from the traditional attentional decoder that conduct alignment and recognition concurrently, our decoupled text decoder takes encoded features and attention maps as input, and conducts recognition only. As shown in <ref type="figure">Figure 4</ref>, the decoupled text decoder computes context vector c t as:</p><formula xml:id="formula_1">c t = W/rw x=1 H/r h y=1 α t,x,y F x,y .<label>(2)</label></formula><p>At time step t, the classifier generates output y t :</p><formula xml:id="formula_2">y t = wh t + b,<label>(3)</label></formula><p>where h t is the hidden state of the GRU, computed as:</p><formula xml:id="formula_3">h t = GRU ((e t−1 , c t ), h t−1 ),<label>(4)</label></formula><p>e t is an embedding vector of the previous decoding result y t . The loss function of DAN is as follows:</p><formula xml:id="formula_4">Loss = − T t=1 logP (g t |I, θ),<label>(5)</label></formula><p>where θ and g t denote all trainable parameters in the DAN and groudtruth at step t, respectively. Just like other attentional text recognizers, DAN uses word-level annotation for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Evaluation</head><p>In our experiments, two tasks are employed to evaluate the effectiveness of DAN, including handwritten text recognition and scene text recognition. The detailed network configuration of feature encoder is given in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offline Handwritten Text Recognition</head><p>Owing to its long sentences (up to 90 characters), diverse writing styles, and character-touching problem, the offline handwritten text recognition problem is highly complicated and challenging to solve. Therefore, it is a favorable testbed to evaluate the robustness and effectiveness of DAN.</p><p>For exhaustive comparison, we also conduct experiments on two popular attentional decoders: Bahdanau's attention  and Luong's attention <ref type="bibr" target="#b29">(Luong, Pham, and Manning 2015)</ref>. These attentional decoders are widely adopted for text recognition <ref type="bibr" target="#b37">(Shi et al. 2018;</ref><ref type="bibr" target="#b6">Cheng et al. 2018;</ref><ref type="bibr" target="#b28">Luo, Jin, and Sun 2019;</ref><ref type="bibr" target="#b21">Li et al. 2019)</ref>. When comparing with these decoders, the CAM and decoupled text decoder are replaced by them for the sake of fairness.</p><p>Datasets Two public handwritten datasets are used to evaluate the effectiveness of DAN, including IAM <ref type="bibr" target="#b30">(Marti and Bunke 2002)</ref> and RIMES <ref type="bibr" target="#b12">(Grosicki et al. 2009</ref>). The IAM dataset is based on handwritten English text copied from the LOB corpus. It contains 747 documents (6,482 lines) in the training set, 116 documents (976 lines) in the validation set and 336 documents (2,915 lines) in the test set. The RIMES dataset consists of handwritten letters in French. There are 1,500 paragraphs (11,333 lines) in the training set, and 100 paragraphs (778 lines) in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>On both databases we use the original whole-line training set with an open-source dataaugmentation toolkit 2 to train the network. The height of the input image is normalized as 192 and the width is calculated with the original aspect ratio (up to 2048). To downsample the feature map into 1D, we add a convolution layer with kernel size 3×1 to the end of the feature encoder. maxT is set to 150 in order to cover the longest line. The measure of performance is the Character or Word Error Rate (CER% or WER%), corresponding to the edit distance between the recognition result and groundtruth, normalized by the number of groundtruth characters (or words). At test time on RIMES dataset, we crop the test image with six pre-defined strategies (e.g., {10,10} meant that the top 10 rows and the bottom 10 rows are cropped out), and then conduct recognition on them and the original image. A recognition score is calculated by averaging the output probabilities and the top scored one is chosen as the final result. All the layers of CAM except the last one are set as 128 channels in order to cover the longest text length. No language model or lexicon is used during experiments. Experimental Results As shown in <ref type="table" target="#tab_1">Table 2</ref>, DAN exhibits superior performance on both datasets. On IAM dataset, DAN outperforms previous state-of-the-art by 1.5% on CER. Note that although <ref type="bibr" target="#b2">(Bhunia et al. 2019)</ref> shows better performance on WER, their method needs cropped word images as input, while our method directly recognizes text lines. On RIMES, it is inferior to previous state-of-the-art by 0.2% on CER; but on WER, it has a great error reduction of 3.7% (relative error reduction of 29%). The great improvement in terms of WER indicates that DAN has a stronger capability of learning semantic information, which is helpful for long text recognition. Ablation Study In this subsection, we will evaluate the influence of different depth L and output length maxT of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAM.</head><p>Output length: As shown in <ref type="table" target="#tab_2">Table 3</ref>, different output lengths do not influence the performance, and the computation resource of additional channels is negligible, which indicates that DAN works well as long as the output length is reasonably set (longer than text length).</p><p>Depth: As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, the performance of DAN degrades seriously as we reduce L, which show that the CAM should be deep enough to reach good performance. To successfully align one character, the reception field of CAM must be big enough to cover the corresponding features of this character and its neighbor regions. <ref type="table" target="#tab_3">Table 4</ref>, compared with these two widely-used attentional decoders in the field of text recognition, DAN achieves significantly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Insight into Eliminating Misalignments As shown in</head><p>To fine-grained study the improvements brought by the better alignment of DAN, we quantitatively discuss the relationship between obtained improvements of DAN and corresponding eliminated alignment errors. We propose a simple misalignment measurement method, which is based on the priori knowledge that all texts are written from left to right. This method consists of two steps: 1) picking the region with maximum attention score as attention center; 2) if current attention center is on the left side of the previous one, recording one misalignment. We divide the test samples into five groups by the text length: [0, 30), <ref type="bibr">[30,</ref><ref type="bibr">40)</ref>, <ref type="bibr">[40, 50), [50, 60), [60, 70)</ref>; each group contains more than 100 samples. In each group, the misalignments are added up then averaged to produce meanmisalignments per image (MM/img).</p><p>The experimental results are shown in <ref type="figure" target="#fig_3">Figure 6</ref>; The changes of CER improvement and eliminated misalignments are almost the same trend, which validates the performance gain of DAN relative to traditional attention comes from eliminating misalignments. In <ref type="figure" target="#fig_4">Figure 7</ref>, we show some visualization results of eliminated misalignments by our DAN.</p><p>Error Analysis <ref type="figure" target="#fig_5">Figure 8</ref> shows some typical error samples of DAN. In <ref type="figure" target="#fig_5">Figure 8 (a)</ref>, the character 'e' is recognized as 'p' because of its confusing writing style. The misclassified 'p' is challenging for humans without contextual information. In <ref type="figure" target="#fig_5">Figure 8 (b)</ref>, a space symbol is missed by the recognizer, because the two relevant words are too close. In <ref type="figure" target="#fig_5">Figure 8</ref> (c), some noise texture is recognized as a word by DAN. However, DAN is still more robust than traditional attention on these samples. In <ref type="figure" target="#fig_5">Figure 8</ref> (c) the confusing noises disturb the alignment operation of traditional attention and lead to unpredictable errors, while DAN is robust in alignment even if extra results are generated. Considering that the noises have almost the same texture with normal text, this type of error is very difficult to avoid, especially for DAN which conduct alignment only based on visual features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Text Recognition</head><p>Scene text recognition often encounters problems owing to the large variations in the background, appearance, resolu- tion, text font, and so on. In this section, we will study the effectiveness and robustness of DAN on seven datasets including regular scene text datasets and irregular scene text datasets. We will validate the performance of DAN in 1D and 2D form (denote as DAN-1D and DAN-2D); the detailed configurations of feature encoder are shown in Table 1.</p><p>Datasets Two types of datasets are used for scene text recognition: regular scene text datasets, including IIIT5K-Words <ref type="bibr" target="#b31">(Mishra, Alahari, and Jawahar 2012)</ref>, Street View Text <ref type="bibr" target="#b40">(Wang, Babenko, and Belongie 2011</ref><ref type="bibr">), ICDAR 2003</ref><ref type="bibr" target="#b27">(Lucas et al. 2003</ref> and ICDAR 2013 <ref type="bibr" target="#b17">(Karatzas et al. 2013)</ref>; and irregular scene text datasets, including SVT-Perspective <ref type="bibr" target="#b32">(Neumann and Matas 2012)</ref>, CUTE80 <ref type="bibr" target="#b33">(Risnumawan et al. 2014</ref>) and ICDAR 2015 <ref type="bibr" target="#b18">(Karatzas et al. 2015)</ref>.</p><p>IIIT5k was collected from the Internet, and contained 3,000 cropped word images for testing.</p><p>Street View Text (SVT) was collected from the Google Street View, and contained 647 word images for testing.</p><p>ICDAR 2003 (IC03) contained 251 scene images that are labeled with text bounding boxes. The dataset contained 867 cropped images.</p><p>ICDAR 2013 (IC13) inherited most images from IC03 and extends it with some new images. It consisted of 1,015 cropped images without associated lexicon.  SVT-Perspective (SVT-P) was collected from the sideview angle snapshots in Google Street View, and contained 639 cropped images for testing .</p><p>CUTE80 focused on curved text, and consisted of 80 high-resolution images taken in natural scenes. This dataset contained 288 cropped natural images for testing.</p><p>ICDAR 2015 (IC15) contained 2,077 cropped images. A large proportion of images were blurred and multi-oriented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We train our model on synthetic samples released by  and <ref type="bibr" target="#b13">(Gupta, Vedaldi, and Zisserman 2016)</ref>. For better comparison, we compare DAN only with the methods that had also used these two synthetic datasets. The height of the input image is set to 32 and the width is calculated with the original aspect ratio (up to 128). maxT is set as 25; L is set as 8; and all the layers of CAM except the last one are set as 64. We use the bi-directional decoder proposed in <ref type="bibr" target="#b37">(Shi et al. 2018</ref>) for final prediction. channels. With ADADELTA (Zeiler 2012) optimization method, the learning rate is set as 1.0 and reduced to 0.1 after the third epoch. <ref type="table" target="#tab_4">Table 5</ref>, DAN achieves state-of-the-art or comparable performance on most datasets. For regular scene text recognition, DAN achieves state-of-the-art performance on IIIT5K and IC03, and is just a little behind the current state-of-the-art on SVT and IC13. DAN-1D performs a little better on IC03 and IC13, because images from these two datasets are usually clean and regular. For irregular scene text recognition, the most advanced methods can be divided into two types: rectification based and 2D based. DAN-2D achieves state-of-theart performance on SVT-P and CUTE80, and it exhibits the best performance among 2D recognizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results As shown in</head><p>Robustness Study Scene text is usually affected by environmental disturbances. To check whether DAN is sensitive to subtle disturbances, we also conduct robustness study on IIIT-5k and IC13 datasets, and compare DAN with the mostrecent 2D scene text recognizer, CA-FCN <ref type="bibr" target="#b22">(Liao et al. 2019)</ref>. We add some disturbances on these two datasets as follows:</p><p>IIIT-p: Padding the images in IIIT5k with extra 10% height vertically and 10% width horizontally by repeating the border pixels. IIIT-r-p: 1. Separately stretching the four vertexes of the images in IIIT5k with a random scale up to 20% of height and width respectively. 2. Repeating border pixels to fill the quadrilateral images. 3. Transforming the images back to axis-aligned rectangles. IC13-ex: Expanding the bounding boxes of the images in IC13 to expanded rectangles with extra 10% height and width before cropping. IC13-r-ex: 1. Expanding the bounding boxes of the images in IC13 randomly with a maximum 20% of width and height to form expanded quadrilaterals. 2. The pixels in axis-aligned circumscribed rectangles of those images are cropped.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 6</ref>. In most cases DAN exhibits to be more robust than CA-FCN, which again validates its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Advances of DAN: 1) Simple. DAN uses off-the-shelf components; all of them are easy to implement. 2) Effective. DAN achieves state-of-the-art performance on multiple text recognition tasks. 3) Flexible. The form of DAN can be easily switched between 1D and 2D. 4) Robust. DAN exhibits more reliable alignment performance when facing long text. It is also more robust facing subtle disturbances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations of DAN:</head><p>The CAM uses only visual information for alignment operation; thus when it comes text-like noises, it struggles to align the text. This kind of error is shown in <ref type="figure" target="#fig_5">Figure 8</ref> (c) and may be a common issue for most attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, an effective, flexible and robust decoupled attention network is proposed for text recognition. To address the misalignment issue, DAN decouples the decoder of the traditional attention mechanism into a convolutional alignment module and a decoupled text decoder. Compared with the traditional attention mechanism, DAN effectively eliminates the alignment errors and achieves the state-of-the-art performance. Experimental results on multiple text recognition tasks have shown its effectiveness and merit. Particularly, DAN shows significant superiority when dealing with long text recognition, such as handwritten text recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>, This arXiv:1912.10205v1 [cs.CV] 21 Dec 2019 Visualization of fractional alignment of traditional attention mechanism</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overall architecture of DAN, and detailed architectures of the feature encoder and the CAM. The input image has a normalized height of H and a scaled width of W , C 1 and C 2 are the numbers of channels of the feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison of different depth L on IAM dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>CER improvements of DAN on different text lengths and corresponding misalignments. 'Bah' and 'Luong' denote Bahdanau's attention and Luong's attention, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of attention maps and recognition results on IAM dataset. Top: original fractional images and corresponding groundtruth; middle: attention maps and recognition results of traditional attention; bottom: attention maps and recognition results of DAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of typical error samples of DAN. The order of images is same as Figure 7. (a) Substitute error where character 'p' is misrecognized as 'e'; (b) delete error where a space symbol is missed; (c) insert error where some textures are recognized as 'buck'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detailed configuration of the feature encoder. 'Num' and 'hw' mean number of blocks and handwritten text recognition experiments, respectively.</figDesc><table><row><cell>Name</cell><cell cols="2">Configuration Num</cell><cell cols="3">Downsampling Ratio hw scene-1D scene-2D</cell></row><row><cell>Res-block0</cell><cell>3 × 3 conv</cell><cell>1</cell><cell>2×1</cell><cell>1×1</cell><cell>1×1</cell></row><row><cell>Res-block1</cell><cell>1 × 1 conv, 32 3 × 3 conv, 32</cell><cell>3</cell><cell>2×2</cell><cell>2×2</cell><cell>2×2</cell></row><row><cell>Res-block2</cell><cell>1 × 1 conv, 64 3 × 3 conv, 64</cell><cell>4</cell><cell>2×2</cell><cell>2×2</cell><cell>1×1</cell></row><row><cell>Res-block3</cell><cell>1 × 1 conv, 128 3 × 3 conv, 128</cell><cell>6</cell><cell>2×1</cell><cell>2×1</cell><cell>2×2</cell></row><row><cell>Res-block4</cell><cell>1 × 1 conv, 256 3 × 3 conv, 256</cell><cell>6</cell><cell>2×2</cell><cell>2×1</cell><cell>1×1</cell></row><row><cell>Res-block5</cell><cell>1 × 1 conv, 512 3 × 3 conv, 512</cell><cell>3</cell><cell>2×2</cell><cell>2×1</cell><cell>1×1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on handwritten text datasets.</figDesc><table><row><cell>Methods</cell><cell cols="4">IAM WER CER WER CER RIMES</cell></row><row><cell>(Salvador et al. 2011)</cell><cell>22.4</cell><cell>9.8</cell><cell>-</cell><cell>-</cell></row><row><cell>(Pham et al. 2014)</cell><cell>35.1</cell><cell>10.8</cell><cell>28.5</cell><cell>6.8</cell></row><row><cell>(Bluche 2016)</cell><cell>24.6</cell><cell>7.9</cell><cell>12.6</cell><cell>2.9</cell></row><row><cell>(Sueiras et al. 2018)</cell><cell>23.8</cell><cell>8.8</cell><cell>15.9</cell><cell>4.8</cell></row><row><cell cols="2">(Bhunia et al. 2019) 1 17.2</cell><cell>8.4</cell><cell>10.5</cell><cell>6.4</cell></row><row><cell>(Zhang et al. 2019)</cell><cell>22.2</cell><cell>8.5</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN</cell><cell>19.6</cell><cell>6.4</cell><cell>8.9</cell><cell>2.7</cell></row></table><note>1 Word-level recognition, where the words in the original image are cropped out then recognized.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on different output lengths. The 'time/iter' means forward time per iteration on TITAN X GPU.</figDesc><table><row><cell>output length</cell><cell cols="2">IAM WER CER</cell><cell>time/iter</cell></row><row><cell>150</cell><cell>19.6</cell><cell>6.4</cell><cell>188.7 ms</cell></row><row><cell>200</cell><cell>19.5</cell><cell>6.3</cell><cell>189.5 ms</cell></row><row><cell>250</cell><cell>19.6</cell><cell>6.4</cell><cell>190.5 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Performance comparison of different decoders. FE</cell></row><row><cell cols="5">denotes the feature encoder of DAN. 'Bah' and 'Luong'</cell></row><row><cell cols="5">denote Bahdanau's attention and Luong's attention, respec-</cell></row><row><cell>tively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">IAM WER CER WER CER RIMES</cell></row><row><cell>FE + Bah</cell><cell>25.9</cell><cell>9.9</cell><cell>9.1</cell><cell>3.0</cell></row><row><cell>FE + Luong</cell><cell>25.7</cell><cell>10.3</cell><cell>9.3</cell><cell>3.3</cell></row><row><cell>DAN</cell><cell>19.6</cell><cell>6.4</cell><cell>8.9</cell><cell>2.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on regular and irregular scene text datasets. 'Rect' represents rectification-based methods; '2D' represents 2D-based methods.</figDesc><table><row><cell>Methods</cell><cell>Rect 2D</cell><cell cols="7">Regular IIIT5k SVT IC03 IC13 SVT-P CUTE80 IC15 Irregular</cell></row><row><cell>(Cheng et al. 2017) 1</cell><cell></cell><cell>87.4</cell><cell cols="2">85.9 94.2</cell><cell>93.3</cell><cell>-</cell><cell>-</cell><cell>70.6</cell></row><row><cell>(Cheng et al. 2018)</cell><cell></cell><cell>87.0</cell><cell cols="2">82.8 91.5</cell><cell>-</cell><cell>73.0</cell><cell>76.8</cell><cell>68.2</cell></row><row><cell>(Bai et al. 2018) 1</cell><cell></cell><cell>88.3</cell><cell cols="2">87.5 94.6</cell><cell>94.4</cell><cell>-</cell><cell>-</cell><cell>73.9</cell></row><row><cell>(Liu et al. 2018)</cell><cell></cell><cell>89.4</cell><cell cols="2">87.1 94.7</cell><cell>94.0</cell><cell>73.9</cell><cell>62.5</cell><cell>-</cell></row><row><cell>(Shi et al. 2018)</cell><cell></cell><cell>93.4</cell><cell cols="2">89.5 94.5</cell><cell>91.8</cell><cell>78.5</cell><cell>79.5</cell><cell>76.1</cell></row><row><cell>(Fang et al. 2018)</cell><cell></cell><cell>86.7</cell><cell cols="2">86.7 94.8</cell><cell>93.5</cell><cell>-</cell><cell>-</cell><cell>71.2</cell></row><row><cell>(Luo, Jin, and Sun 2019)</cell><cell></cell><cell>91.2</cell><cell cols="2">88.3 95.0</cell><cell>92.4</cell><cell>76.1</cell><cell>77.4</cell><cell>68.8</cell></row><row><cell>(Liao et al. 2019) 1</cell><cell></cell><cell>92.0</cell><cell>86.4</cell><cell>-</cell><cell>91.5 1</cell><cell>-</cell><cell>79.9</cell><cell>-</cell></row><row><cell>(Li et al. 2019)</cell><cell></cell><cell>91.5</cell><cell>84.5</cell><cell>-</cell><cell>91.0</cell><cell>76.4</cell><cell>83.3</cell><cell>69.2</cell></row><row><cell>(Xie et al. 2019)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.1</cell><cell>82.6</cell><cell>68.9</cell></row><row><cell>(Zhan and Lu 2019)</cell><cell></cell><cell>93.3</cell><cell>90.2</cell><cell>-</cell><cell>91.3</cell><cell>79.6</cell><cell>83.3</cell><cell>76.9</cell></row><row><cell>DAN-1D</cell><cell></cell><cell>93.3</cell><cell cols="2">88.4 95.2</cell><cell>94.2</cell><cell>76.8</cell><cell>80.6</cell><cell>71.8</cell></row><row><cell>DAN-2D</cell><cell></cell><cell>94.3</cell><cell cols="2">89.2 95.0</cell><cell>93.9</cell><cell>80.0</cell><cell>84.4</cell><cell>74.5</cell></row><row><cell cols="2">1 character-level annotation required.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Robustness study. 'ac': accuracy; 'gap': the gap between the original dataset; 'ratio': accuracy decreasing ratio.</figDesc><table><row><cell>Methods</cell><cell>IIIT ac</cell><cell>ac</cell><cell>IIIT-p gap ratio</cell><cell>ac</cell><cell>IIIT-r-p gap ratio</cell><cell>IC13 ac</cell><cell>ac</cell><cell>IC13-ex gap ratio</cell><cell>ac</cell><cell>IC13-r-ex gap ratio</cell></row><row><cell cols="11">CA-FCN 92.0 89.3 -2.7 2.9% 87.6 -4.4 4.8% 91.4 87.2 -3.7 4.1% 83.8 -6.9 7.6%</cell></row><row><cell cols="11">DAN-1D 93.3 91.5 -1.8 1.9% 88.2 -5.1 5.4% 94.2 91.2 -3.0 3.2% 86.9 -7.3 7.7%</cell></row><row><cell cols="11">DAN-2D 94.3 92.1 -2.2 2.3% 89.1 -5.2 5.5% 93.9 90.4 -3.5 3.7% 86.9 -7.0 7.5%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/Canjie-Luo/Scene-Text-Image-Transformer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Handwriting recognition in low-resource scripts using adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S R</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4767" to="4776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photoocr: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint line segmentation and transcription for end-to-end handwritten paragraph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5086" to="5094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AON: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention and language ensemble for scene text recognition with convolutional sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Results of the rimes evaluation campaign for handwritten mail processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Brodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="941" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>ICDAR 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scene text recognition from twodimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synthetically supervised feature learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="449" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7154" to="7161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scene text detection and recognition: The deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno>abs/1811.04256</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ICDAR 2003 robust reading competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MORAN: A multiobject rectified attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The iam-database: an english sentence database for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving offline handwritten text recognition with hybrid hmm/ann models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Francisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="79" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ASTER: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Offline continuous handwriting recognition using sequence to sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sueiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">289</biblScope>
			<biblScope unit="page" from="119" to="128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Endto-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregation cross-entropy for sequence recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6538" to="6547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3280" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence domain adaptation network for robust text image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2740" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
