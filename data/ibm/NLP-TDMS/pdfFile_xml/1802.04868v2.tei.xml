<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimplE Embedding for Link Prediction in Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><forename type="middle">Mehran</forename><surname>Kazemi</surname></persName>
							<email>smkazemi@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Poole</surname></persName>
							<email>poole@cs.ubc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SimplE Embedding for Link Prediction in Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at https://github.com/Mehran-k/SimplE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the past two decades, several knowledge graphs (KGs) containing (perhaps probabilistic) facts about the world have been constructed. These KGs have applications in several fields including search, question answering, natural language processing, recommendation systems, etc. Due to the enormous number of facts that could be asserted about our world and the difficulty in accessing and storing all these facts, KGs are incomplete. However, it is possible to predict new links in a KG based on the existing ones. Link prediction and several other related problems aiming at reasoning with entities and relationships are studied under the umbrella of statistical relational learning (SRL) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7]</ref>. The problem of link prediction for KGs is also known as knowledge graph completion. A KG can be represented as a set of (head , relation, tail ) triples 1 . The problem of KG completion can be viewed as predicting new triples based on the existing ones.</p><p>Tensor factorization approaches have proved to be an effective SRL approach for KG completion <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26]</ref>. These approaches consider embeddings for each entity and each relation. To predict whether a triple holds, they use a function which takes the embeddings for the head and tail entities and the relation as input and outputs a number indicating the predicted probability. Details and discussions of these approaches can be found in several recent surveys <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>One of the first tensor factorization approaches is the canonical Polyadic (CP) decomposition <ref type="bibr" target="#b14">[15]</ref>. This approach learns one embedding vector for each relation and two embedding vectors for each entity, one to be used when the entity is the head and one to be used when the entity is the tail. The head embedding of an entity is learned independently of (and is unrelated to) its tail embedding. This independence has caused CP to perform poorly for KG completion <ref type="bibr" target="#b39">[40]</ref>. In this paper, we develop a tensor factorization approach based on CP that addresses the independence among the two embedding vectors of the entities. Due to the simplicity of our model, we call it SimplE (Simple Embedding).</p><p>We show that SimplE: 1-can be considered a bilinear model, 2-is fully expressive, 3-is capable of encoding background knowledge into its embeddings through parameter sharing (aka weight tying), and 4-performs very well empirically despite (or maybe because of) its simplicity. We also discuss several disadvantages of other existing approaches. We prove that many existing translational approaches (see e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref>) are not fully expressive and we identify severe restrictions on what they can represent. We also show that the function used in ComplEx <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, a state-of-the-art approach for link prediction, involves redundant computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Notation</head><p>We represent vectors with lowercase letters and matrices with uppercase letters. Let v, w, x ∈ R d be vectors of length d. We define v, w,</p><formula xml:id="formula_0">x . = d j=1 v[j] * w[j] * x[j], where v[j], w[j]</formula><p>, and x[j] represent the jth element of v, w and x respectively. That is, v, w, x . = (v w) · x where represents element-wise (Hadamard) multiplication and · represents dot product. I d represents an identity matrix of size d.</p><p>[v 1 ; v 2 ; . . . ; v n ] represents the concatenation of n vectors v 1 , v 2 , . . . and v n .</p><p>Let E and R represent the set of entities and relations respectively. A triple is represented as (h, r , t), where h ∈ E is the head, r ∈ R is the relation, and t ∈ E is the tail of the triple. Let ζ represent the set of all triples that are true in a world (e.g., (paris, capitalOf , france)), and ζ represent the ones that are false (e.g., (paris, capitalOf , italy)). A knowledge graph KG is a subset of ζ. A relation r is reflexive on a set E of entities if (e, r , e) ∈ ζ for all entities e ∈ E. A relation r is symmetric on a set E of entities if (e 1 , r , e 2 ) ∈ ζ ⇐⇒ (e 2 , r , e 1 ) ∈ ζ for all pairs of entities e 1 , e 2 ∈ E, and is anti-symmetric if (e 1 , r , e 2 ) ∈ ζ ⇐⇒ (e 2 , r , e 1 ) ∈ ζ . A relation r is transitive on a set E of entities if (e 1 , r , e 2 ) ∈ ζ ∧ (e 2 , r , e 3 ) ∈ ζ ⇒ (e 1 , r , e 3 ) ∈ ζ for all e 1 , e 2 , e 3 ∈ E. The inverse of a relation r, denoted as r −1 , is a relation such that for any two entities e i and e j , (e i , r , e j ) ∈ ζ ⇐⇒ (e j , r −1 , e i ) ∈ ζ.</p><p>An embedding is a function from an entity or a relation to one or more vectors or matrices of numbers. A tensor factorization model defines two things: 1-the embedding functions for entities and relations, 2-a function f taking the embeddings for h, r and t as input and generating a prediction of whether (h, r , t) is in ζ or not. The values of the embeddings are learned using the triples in a KG. A tensor factorization model is fully expressive if given any ground truth (full assignment of truth values to all triples), there exists an assignment of values to the embeddings of the entities and relations that accurately separates the correct triples from incorrect ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Translational Approaches define additive functions over embeddings. In many translational approaches, the embedding for each entity e is a single vector v e ∈ R d and the embedding for each relation r is a vector v r ∈ R d and two matrices P r ∈ R d ×d and Q r ∈ R d ×d . The dissimilarity function for a triple (h, r , t) is defined as</p><formula xml:id="formula_1">||P r v h + v r − Q r v t || i (i.e. encouraging P r v h + v r ≈ Q r v t )</formula><p>where ||v|| i represents norm i of vector v. Translational approaches having this dissimilarity function usually differ on the restrictions they impose on P r and Q r . In TransE <ref type="bibr" target="#b3">[4]</ref>, d = d , P r = Q r = I d . In TransR <ref type="bibr" target="#b21">[22]</ref>, P r = Q r . In STransE <ref type="bibr" target="#b25">[26]</ref>, no restrictions are imposed on the matrices. FTransE <ref type="bibr" target="#b10">[11]</ref>, slightly changes the dissimilarity function defining it as ||P r v h + v r − αQ r v t || i for a value of α that minimizes the norm for each triple. In the rest of the paper, we let FSTransE represent the FTransE model where no restrictions are imposed over P r and Q r .</p><p>Multiplicative Approaches define product-based functions over embeddings. DistMult <ref type="bibr" target="#b45">[46]</ref>, one of the simplest multiplicative approaches, considers the embeddings for each entity and each relation to be v e ∈ R d and v r ∈ R d respectively and defines its similarity function for a triple (h, r , t)</p><p>as v h , v r , v t . Since DistMult does not distinguish between head and tail entities, it can only model symmetric relations. ComplEx <ref type="bibr" target="#b38">[39]</ref> extends DistMult by considering complex-valued instead of real-valued vectors for entities and relations. For each entity e, let re e ∈ R d and im e ∈ R d represent the real and imaginary parts of the embedding for e. For each relation r, let re r ∈ R d and im r ∈ R d represent the real and imaginary parts of the embedding for r. Then the similarity function of ComplEx for a triple (h, r , t) is defined as Real(</p><formula xml:id="formula_2">d j=1 (re h [j] + im h [j]i) * (re r [j] + im r [j]i) * (re t [j] − im t [j]i)),</formula><p>where Real(α + βi) = α and i 2 = −1. One can easily verify that the function used by ComplEx can be expanded and written as re h , re r , re t + re h , im r , im t + im h , re r , im t − im h , im r , re t . In RESCAL <ref type="bibr" target="#b27">[28]</ref>, the embedding vector for each entity e is v e ∈ R d and for each relation r is v r ∈ R d×d and the similarity function for a triple (h,</p><formula xml:id="formula_3">r , t) is v r · vec(v h ⊗ v t ),</formula><p>where ⊗ represents the outer product of two vectors and vec(.) vectorizes the input matrix. HolE <ref type="bibr" target="#b31">[32]</ref> is a multiplicative model that is isomorphic to ComplEx <ref type="bibr" target="#b13">[14]</ref>.</p><p>Deep Learning Approaches generally use a neural network that learns how the head, relation, and tail embeddings interact. E-MLP <ref type="bibr" target="#b36">[37]</ref> considers the embeddings for each entity e to be a vector v e ∈ R d , and for each relation r to be a matrix M r ∈ R 2k×m and a vector v r ∈ R m . To make a prediction about a triple (h, r , t), E-MLP feeds [v h ; v t ] ∈ R 2d into a two-layer neural network whose weights for the first layer are the matrix M r and for the second layer are v r . ER-MLP <ref type="bibr" target="#b9">[10]</ref>, considers the embeddings for both entities and relations to be single vectors and feeds [v h ; v r ; v t ] ∈ R 3d into a two layer neural network. In <ref type="bibr" target="#b34">[35]</ref>, once the entity vectors are provided by the convolutional neural network and the relation vector is provided by the long-short time memory network, for each triple the vectors are concatenated similar to ER-MLP and are fed into a four-layer neural network. Neural tensor network (NTN) <ref type="bibr" target="#b36">[37]</ref> combines E-MLP with several bilinear parts (see Subsection 5.4 for a definition of bilinear models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SimplE: A Simple Yet Fully Expressive Model</head><p>In canonical Polyadic (CP) decomposition <ref type="bibr" target="#b14">[15]</ref>, the embedding for each entity e has two vectors h e , t e ∈ R d , and for each relation r has a single vector v r ∈ R d . h e captures e's behaviour as the head of a relation and t e captures e's behaviour as the tail of a relation. The similarity function for a triple (e 1 , r , e 2 ) is h e1 , v r , t e2 . In CP, the two embedding vectors for entities are learned independently of each other: observing (e 1 , r , e 2 ) ∈ ζ only updates h e1 and t e2 , not t e1 and h e2 . SimplE takes advantage of the inverse of relations to address the independence of the two vectors for each entity in CP. While inverse of relations has been used for other purposes (see e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref>), using them to address the independence of the entity vectors in CP is a novel contribution.</p><p>Model Definition: SimplE considers two vectors h e , t e ∈ R d as the embedding of each entity e (similar to CP), and two vectors v r , v r −1 ∈ R d for each relation r. The similarity function of SimplE for a triple (e i , r , e j ) is defined as</p><formula xml:id="formula_4">1 2 ( h ei , v r , t ej + h ej , v r −1 , t ei ),</formula><p>i.e. the average of the CP scores for (e i , r , e j ) and (e j , r −1 , e i ). In our experiments, we also consider a different variant, which we call SimplE-ignr. During training, for each correct (incorrect) triple (e i , r , e j ), SimplE-ignr updates the embeddings such that each of the two scores h ei , v r , t ej and h ej , v r −1 , t ei become larger (smaller). During testing, SimplE-ignr ignores r −1 s and defines the similarity function to be h ei , v r , t ej .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning SimplE Models:</head><p>To learn a SimplE model, we use stochastic gradient descent with minibatches. In each learning iteration, we iteratively take in a batch of positive triples from the KG, then for each positive triple in the batch we generate n negative triples by corrupting the positive triple. We use Bordes et al. <ref type="bibr" target="#b3">[4]</ref>'s procedure to corrupt positive triples. The procedure is as follows. For a positive triple (h, r , t), we randomly decide to corrupt the head or tail. If the head is selected, we replace h in the triple with an entity h randomly selected from E − {h} and generate the corrupted triple (h , r , t). If the tail is selected, we replace t in the triple with an entity t randomly selected from E − {t} and generate the corrupted triple (h, r , t ). We generate a labelled batch LB by labelling positive triples as </p><formula xml:id="formula_5">h(e0) 1 0 0 . . . 0 1 0 0 . . . 0 . . . 1 0 0 . . . 0 h(e1) 0 1 0 . . . 0 0 1 0 . . . 0 . . . 0 1 0 . . . 0 h(e2) 0 0 1 . . . 0 0 0 1 . . . 0 . . . 0 0 1 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . h(e |E|−1 ) 0 0 0 . . . 1 0 0 0 . . . 1 . . . 0 0 0 . . . 1 v(r0) 1 1 1 . . . 1 0 0 0 . . . 0 . . . 0 0 0 . . . 0 v(r1) 0 0 0 . . . 0 1 1 1 . . . 1 . . . 0 0 0 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v(r |R|−1 ) 0 0 0 . . . 0 0 0 0 . . . 0 . . . 1 1 1 . . . 1</formula><p>+1 and negatives as −1. Once we have a labelled batch, following <ref type="bibr" target="#b38">[39]</ref> we optimize the L2 regularized negative log-likelihood of the batch: min θ ((h,r ,t),l)∈LB sof tplus(−l · φ(h, r , t)) + λ||θ|| <ref type="bibr">2 2</ref> , where θ represents the parameters of the model (the parameters in the embeddings), l represents the label of a triple, φ(h, r , t) represents the similarity score for triple (h, r , t), λ is the regularization hyperparameter, and sof tplus(x) = log(1 + exp(x)). While several previous works (e.g., TransE, TransR, STransE, etc.) consider a margin-based loss function, Trouillon and Nickel <ref type="bibr" target="#b37">[38]</ref> show that the margin-based loss function is more prone to overfitting compared to log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical Analyses</head><p>In this section, we provide some theoretical analyses of SimplE and other existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fully Expressiveness</head><p>The following proposition establishes the full expressivity of SimplE. Proposition 1. For any ground truth over entities E and relations R containing γ true facts, there exists a SimplE model with embedding vectors of size min(|E| · |R|, γ + 1) that represents that ground truth.</p><p>Proof. First, we prove the |E| · |R| bound. With embedding vectors of size |E| * |R|, for each entity e i we let the n-th element of h ei = 1 if (n mod |E|) = i and 0 otherwise, and for each relation r j we let the n-th element of v rj = 1 if (n div |E|) = j and 0 otherwise (see <ref type="figure" target="#fig_0">Fig 1)</ref>. Then for each e i and r j , the product of h ei and v rj is 0 everywhere except for the (j * |E| + i)-th element. So for each entity e k , we set the (j * |E| + i)-th element of t e k to be 1 if (e i , r j , e k ) holds and −1 otherwise. Now we prove the γ + 1 bound. Let γ be zero (base of the induction). We can have embedding vectors of size 1 for each entity and relation, setting the value for entities to 1 and for relations to −1. Then h ei , v rj , t e k is negative for every entities e i and e k and relation r j . So there exists embedding vectors of size γ + 1 that represents this ground truth. Let us assume for any ground truth where γ = n − 1 (1 ≤ n ≤ |R||E| 2 ), there exists an assignment of values to embedding vectors of size n that represents that ground truth (assumption of the induction). We must prove for any ground truth where γ = n, there exists an assignment of values to embedding vectors of size n + 1 that represents this ground truth. Let (e i , r j , e k ) be one of the n true facts. Consider a modified ground truth which is identical to the ground truth with n true facts, except that (e i , r j , e k ) is assigned false. The modified ground truth has n − 1 true facts and based on the assumption of the induction, we can represent it using some embedding vectors of size n. Let q = h ei , v rj , t e k where h ei , v rj and t e k are the embedding vectors that represent the modified ground truth. We add an element to the end of all embedding vectors and set it to 0. This increases the vector sizes to n + 1 but does not change any scores. Then we set the last element of h ei to 1, v rj to 1, and t e k to q + 1. This ensures that h ei , v rj , t e k &gt; 0 for the new vectors, and no other score is affected.</p><p>DistMult is not fully expressive as it forces relations to be symmetric. It has been shown in <ref type="bibr" target="#b39">[40]</ref> that ComplEx is fully expressive with embeddings of length at most |E| · |R|. According to the universal approximation theorem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, under certain conditions, neural networks are universal approximators of continuous functions over compact sets. Therefore, we would expect there to be a representation based on neural networks that can approximate any ground truth, but the number of hidden units might have to grow with the number of triples. Wang et al. <ref type="bibr" target="#b43">[44]</ref> prove that TransE is not fully expressive. Proposition 2 proves that not only TransE but also many other translational approaches are not fully expressive. The proposition also identifies severe restrictions on what relations these approaches can represent. Proposition 2. FSTransE is not fully expressive and has the following restrictions. R1 : If a relation r is reflexive on ∆ ⊂ E, r must also be symmetric on ∆, R2 : If r is reflexive on ∆ ⊂ E, r must also be transitive on ∆, and R3 : If entity e 1 has relation r with every entity in ∆ ⊂ E and entity e 2 has relation r with one of the entities in ∆, then e 2 must have the relation r with every entity in ∆.</p><p>Proof. For any entity e and relation r, let p re = P r v e and q re = Q r v e . For a triple (h, r , t) to hold, we should ideally have p rh + v r = αq rt for some α. We assume s 1 , s 2 , s 3 and s 4 are entities in ∆.</p><p>R1 : A relation r being reflexive on ∆ implies p rs1 + v r = α 1 q rs1 and p rs2 + v r = α 2 q rs2 . Suppose (s 1 , r , s 2 ) holds as well. Then we know p rs1</p><formula xml:id="formula_6">+ v r = α 3 q rs2 . Therefore, p rs2 + v r = α 2 q rs2 = α2 α3 (p rs1 + v r ) = α2 α3 α 1 q rs1 = α 4 q rs1 , where α 4 = α2α1 α3 . Therefore, (s 2 , r , s 1 ) must holds. R2 : A relation r being reflexive implies p rs1 + v r = α 1 q rs1 , p rs2 + v r = α 2 q rs2 , and p rs3 + v r = α 3 q rs3 . Suppose (s 1 , r , s 2 ) and (s 2 , r , s 3 ) hold. Then we know p rs1 + v r = α 4 q rs2 and p rs2 + v r = α 5 q rs3 . We can conclude p rs1 + v r = α 4 q rs2 = α4 α2 (p rs2 + v r ) = α4 α2 α 5 q rs3 = α 6 q rs3 , where α 6 = α4α5</formula><p>α2 . The above equality proves (s 1 , r , s 3 ) must hold. R3 : Let e 2 have relation r with s 1 . We know p re1</p><formula xml:id="formula_7">+ v r = α 1 q rs1 , p re1 + v r = α 2 q rs2 , and p re2 + v r = α 3 q rs1 . We can conclude p re2 + v r = α 3 q rs1 = α3 α1 (p re1 + v r ) = α3 α1 α 2 q rs2 = α 4 q rs2 , where α 4 = α3α2</formula><p>α1 . Therefore, (e 2 , r , s 2 ) must hold. Corollary 1. Other variants of translational approaches such as TransE, FTransE, STransE, TransH <ref type="bibr" target="#b40">[41]</ref>, and TransR <ref type="bibr" target="#b21">[22]</ref> also have the restrictions mentioned in Proposition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Incorporating Background Knowledge into the Embeddings</head><p>In SimplE, each element of the embedding vector of the entities can be considered as a feature of the entity and the corresponding element of a relation can be considered as a measure of how important that feature is to the relation. Such interpretability allows the embeddings learned through SimplE for an entity (or relation) to be potentially transferred to other domains. It also allows for incorporating observed features of entities into the embeddings by fixing one of the elements of the embedding vector of the observed value. Nickel et al. <ref type="bibr" target="#b29">[30]</ref> show that incorporating such features helps reduce the size of the embeddings.</p><p>Recently, incorporating background knowledge into tensor factorization approaches has been the focus of several studies. Towards this goal, many existing approaches rely on post-processing steps or add additional terms to the loss function to penalize predictions that violate the background knowledge <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref>. Minervini et al. <ref type="bibr" target="#b24">[25]</ref> show how background knowledge in terms of equivalence and inversion can be incorporated into several tensor factorization models through parameter tying 2 . Incorporating background knowledge by parameter tying has the advantage of guaranteeing the predictions follow the background knowledge for all embeddings. In this section, we show how three types of background knowledge, namely symmetry, anti-symmetry, and inversion, can be incorporated into the embeddings of SimplE by tying the parameters 3 (we ignore the equivalence between two relations as it is trivial). Proposition 3. Let r be a relation such that for any two entities e i and e j we have (e i , r , e j ) ∈ ζ ⇐⇒ (e j , r , e i ) ∈ ζ (i.e. r is symmetric). This property of r can be encoded into SimplE by tying the parameters v r −1 to v r .</p><p>Proof. If (e i , r , e j ) ∈ ζ, then a SimplE model makes h ei , v r , t ej and h ej , v r −1 , t ei positive. By tying the parameters v r −1 to v r , we can conclude that h ej , v r , t ei and h ei , v r −1 , t ej also become positive. Therefore, the SimplE model predicts (e j , r , e i ) ∈ ζ. Proposition 4. Let r be a relation such that for any two entities e i and e j we have (e i , r , e j ) ∈ ζ ⇐⇒ (e j , r , e i ) ∈ ζ (i.e. r is anti-symmetric). This property of r can be encoded into SimplE by tying the parameters v r −1 to the negative of v r .</p><p>Proof. If (e i , r , e j ) ∈ ζ, then a SimplE model makes h ei , v r , t ej and h ej , v r −1 , t ei positive. By tying the parameters v r −1 to the negative of v r , we can conclude that h ej , v r , t ei and h ei , v r −1 , t ej become negative. Therefore, the SimplE model predicts (e j , r , e i ) ∈ ζ .</p><p>Proposition 5. Let r 1 and r 2 be two relations such that for any two entities e i and e j we have (e i , r 1 , e j ) ∈ ζ ⇐⇒ (e j , r 2 , e i ) ∈ ζ (i.e. r 2 is the inverse of r 1 ). This property of r 1 and r 2 can be encoded into SimplE by tying the parameters v to v r1 and v r2 to v r −1 1 , we can conclude that h ei , v r −1 2 , t ej and h ej , v r2 , t ei also become positive. Therefore, the SimplE model predicts (e j , r 2 , e i ) ∈ ζ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Time Complexity and Parameter Growth</head><p>As described in <ref type="bibr" target="#b2">[3]</ref>, to scale to the size of the current KGs and keep up with their growth, a relational model must have a linear time and memory complexity. Furthermore, one of the important challenges in designing tensor factorization models is the trade-off between expressivity and model complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Family of Bilinear Models</head><p>Bilinear models correspond to the family of models where the embedding for each entity e is v e ∈ R d , for each relation r is M r ∈ R d×d (with certain restrictions), and the similarity function for a triple (h, r , t) is defined as v T h M r v t . These models have shown remarkable performance for link prediction in knowledge graphs <ref type="bibr" target="#b30">[31]</ref>. DistMult, ComplEx, and RESCAL are known to belong to the family of bilinear models. We show that SimplE (and CP) also belong to this family.</p><p>DistMult can be considered a bilinear model which restricts the M r matrices to be diagonal as in <ref type="figure" target="#fig_4">Fig. 2(a)</ref>. For ComplEx, if we consider the embedding for each entity e to be a single vector [re e ; im e ] ∈ R 2d , then it can be considered a bilinear model with its M r matrices constrained according to <ref type="figure" target="#fig_4">Fig. 2(b)</ref>. RESCAL can be considered a bilinear model which imposes no constraints on the M r matrices. Considering the embedding for each entity e to be a single vector [h e ; t e ] ∈ R 2d , CP can be viewed as a bilinear model with its M r matrices constrained as in <ref type="figure" target="#fig_4">Fig 2(c)</ref>. For a triple (e 1 , r , e 2 ), multiplying [h e1 ; t e1 ] to M r results in a vector v e1r whose first half is zero and whose second half corresponds to an element-wise product of h e1 to the parameters in M r . Multiplying v e1r to [h e2 ; t e2 ] corresponds to ignoring h e2 (since the first half of v e1r is zeros) and taking the dot-product of the second half of v e1r with t e2 . SimplE can be viewed as a bilinear model similar to CP except that the M r matrices are constrained as in <ref type="figure" target="#fig_4">Fig 2(d)</ref>. The extra parameters added to the matrix compared to CP correspond to the parameters in the inverse of the relations. on the M r matrices are equivalent to SimplE, e.g., restricting M r matrices to be zero everywhere except on the counterdiagonal. Viewing SimplE as a single-vector-per-entity model makes it easily integrable (or compatible) with other embedding models (in knowledge graph completion, computer vision and natural language processing) such as <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b35">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Redundancy in ComplEx</head><p>As argued earlier, with the same number of parameters, the number of computations in ComplEx are 4x and 2x more than SimplE-ignr and SimplE. Here we show that a portion of the computations performed by ComplEx to make predictions is redundant. Consider a ComplEx model with embedding vectors of size 1 (for ease of exposition). Suppose the embedding vectors for h, r and t are [α 1 + β 1 i], [α 2 + β 2 i], and [α 3 + β 3 i] respectively. Then the probability of (h, r , t) being correct according to ComplEx is proportional to the sum of the following four terms: 1) α 1 α 2 α 3 , 2) α 1 β 2 β 3 , 3) β 1 α 2 β 3 , and 4) −β 1 β 2 α 3 . It can be verified that for any assignment of (non-zero) values to α i s and β i s, at least one of the above terms is negative. This means for a correct triple, ComplEx uses three terms to overestimate its score and then uses a term to cancel the overestimation. is positive suggesting e 1 probably has relation r with e 3 . However the score for triple (e 2 , r , e 3 ) is negative suggesting e 2 probably does not have relation r with e 3 . Since the only difference between e 1 and e 2 is that the imaginary part changes from 4 to 6, it is difficult to associate a meaning to these numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>Datasets:</p><p>We conducted experiments on two standard benchmarks: WN18 a subset of Wordnet <ref type="bibr" target="#b23">[24]</ref>, and FB15k a subset of Freebase <ref type="bibr" target="#b1">[2]</ref>. We used the same train/valid/test sets as in <ref type="bibr" target="#b3">[4]</ref>.  <ref type="bibr" target="#b38">[39]</ref>. We report the results of TransR and NTN from <ref type="bibr" target="#b26">[27]</ref>, and ER-MLP from <ref type="bibr" target="#b31">[32]</ref> for further comparison.</p><p>Evaluation Metrics: To measure and compare the performances of different models, for each test triple (h, r , t) we compute the score of (h , r , t) triples for all h ∈ E and calculate the ranking rank h of the triple having h, and we compute the score of (h, r , t ) triples for all t ∈ E and calculate the ranking rank t of the triple having t. Then we compute the mean reciprocal rank (MRR) of these rankings as the mean of the inverse of the rankings: M RR = 1 Bordes et al. <ref type="bibr" target="#b3">[4]</ref> identified an issue with the above procedure for calculating the MRR (hereafter referred to as raw MRR). For a test triple (h, r , t), since there can be several entities h ∈ E for which (h , r , t) holds, measuring the quality of a model based on its ranking for (h, r , t) may be flawed.</p><p>That is because two models may rank the test triple (h, r , t) to be second, when the first model ranks a correct triple (e.g., from train or validation set) (h , r , t) to be first and the second model ranks an incorrect triple (h , r , t) to be first. Both these models will get the same score for this test triple when the first model should get a higher score. To address this issue, <ref type="bibr" target="#b3">[4]</ref> proposed a modification to raw MRR. For each test triple (h, r , t), instead of finding the rank of this triple among triples (h , r , t) for all h ∈ E (or (h, r , t ) for all t ∈ E), they proposed to calculate the rank among triples (h , r , t) only for h ∈ E such that (h , r , t) ∈ train ∪ valid ∪ test. Following <ref type="bibr" target="#b3">[4]</ref>, we call this measure filtered MRR. We also report hit@k measures. The hit@k for a model is computed as the percentage of test triples whose ranking (computed as described earlier) is less than or equal k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation:</head><p>We implemented SimplE in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. We tuned our hyper-parameters over the validation set. We used the same search grid on embedding size and λ as <ref type="bibr" target="#b38">[39]</ref> to make our results directly comparable to their results. We fixed the maximum number of iterations to 1000 and the batch size to 100. We set the learning rate for WN18 to 0.1 and for FB15k to 0.05 and used adagrad to update the learning rate after each batch. Following <ref type="bibr" target="#b38">[39]</ref>, we generated one negative example per positive example for WN18 and 10 negative examples per positive example in FB15k. We computed the filtered MRR of our model over the validation set every 50 iterations for WN18 and every 100 iterations for F B15k and selected the iteration that resulted in the best validation filtered MRR. The best embedding size and λ values on WN18 for SimplE-ignr were 200 and 0.001 respectively, and for SimplE were 200 and 0.03. The best embedding size and λ values on FB15k for SimplE-ignr were 200 and 0.03 respectively, and for SimplE were 200 and 0.1. The table shows that models with many parameters (e.g., NTN and STransE) do not perform well on these datasets, as they probably overfit. Translational approaches generally have an inferior performance compared to other approaches partly due to their representation restrictions mentioned in Proposition 2. As an example for the friendship relation in FB15k, if an entity e 1 is friends with 20 other entities and another entity e 2 is friends with only one of those 20, then according to Proposition 2 translational approaches force e 2 to be friends with the other 19 entities as well (same goes for, e.g., netflix genre in FB15k and has part in WN18). The table also shows that bilinear approaches tend to have better performances compared to translational and deep learning approaches. Even DistMult, the simplest bilinear approach, outperforms many translational and deep learning approaches despite not being fully expressive. We believe the simplicity of embeddings and the scoring function is a key property for the success of SimplE. (ei , hasPart, ej ) ∈ ζ ⇔ (ej , partOf , ei ) ∈ ζ 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Entity Prediction Results</head><p>(ei , memberOfDomainTopic, ej ) ∈ ζ ⇔ (ej , synsetDomainTopicOf , ei ) ∈ ζ 6 (ei , memberOfDomainUsage, ej ) ∈ ζ ⇔ (ej , synsetDomainUsageOf , ei ) ∈ ζ 7</p><p>(ei , memberOfDomainRegion, ej ) ∈ ζ ⇔ (ej , synsetDomainRegionOf , ei ) ∈ ζ 8</p><p>(ei , similarTo, ej ) ∈ ζ ⇔ (ej , similarTo, ei ) ∈ ζ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Incorporating background knowledge</head><p>When background knowledge is available, we might expect that a knowledge graph might not include redundant information because it is implied by background knowledge and so the methods that do not include the background knowledge can never learn it. In section 5.2, we showed how background knowledge that can be formulated in terms of three types of rules can be incorporated into SimplE embeddings. To test this empirically, we conducted an experiment on WN18 in which we incorporated several such rules into the embeddings as outlined in Propositions 3, 4, and 5. The rules can be found in <ref type="table" target="#tab_4">Table 2</ref>. As can be viewed in <ref type="table" target="#tab_4">Table 2</ref>, most of the rules are of the form ∀e i , e j ∈ E : (e i , r 1 , e j ) ∈ ζ ⇔ (e j , r 2 , e i ) ∈ ζ. For (possibly identical) relations such as r 1 and r 2 participating in such a rule, if both (e i , r 1 , e j ) and (e j , r 2 , e i ) are in the training set, one of them is redundant because one can be inferred from the other. We removed redundant triples from the training set by randomly removing one of the two triples in the training set that could be inferred from the other one based on the background rules. Removing redundant triples reduced the number of triples in the training set from (approximately) 141K to (approximately) 90K, almost 36% reduction in size. Note that this experiment provides an upper bound on how much background knowledge can improve the performance of a SimplE model.</p><p>We trained SimplE-ignr and SimplE (with tied parameters according to the rules) on this new training dataset with the best hyper-parameters found in the previous experiment. We refer to these two models as SimplE-ignr-bk and SimplE-bk. We also trained another SimplE-ignr and SimplE models on this dataset, but without incorporating the rules into the embeddings. For sanity check, we also trained a ComplEx model over this new dataset. We found that the filtered MRR for SimplE-ignr, SimplE, and ComplEx were respectively 0.221, 0.384, and 0.275. For SimplE-ignr-bk and SimplE-bk, the filtered MRRs were 0.772 and 0.776 respectively, substantially higher than the case without background knowledge. In terms of hit@k measures, SimplE-ignr gave 0.219, 0.220, and 0.224 for hit@1, hit@3 and hit@10 respectively. These numbers were 0.334, 0.404, and 0.482 for SimplE, and 0.254, 0.280 and 0.313 for ComplEx. For SimplE-ignr-bk, these numbers were 0.715, 0.809 and 0.877 and for SimplE-bk they were 0.715, 0.818 and 0.883, also substantially higher than the models without background knowledge. The obtained results validate that background knowledge can be effectively incorporated into SimplE embeddings to improve its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 1 .</head><label>1</label><figDesc>Let likes(p, m) represent if a person p likes a movie m and acted(m, a) represent who acted in which movie. Which actors play in a movie is expected to affect who likes the movie. In CP, observations about likes only update the t vector of movies and observations about acted only update the h vector. Therefore, what is being learned about movies through observations about acted does not affect the predictions about likes and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>h e s and v r s in the proof of Proposition 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>−1 1 to v r2 and v r − 1 2 1 ,</head><label>111</label><figDesc>to v r1 . Proof. If (e i , r 1 , e j ) ∈ ζ, then a SimplE model makes h ei , v r1 , t ej and h ej , v r −1 t ei positive. By tying the parameters v r −1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Models with many parameters usually overfit and give poor performance. While the time complexity for TransE is O(d) where d is the size of the embedding vectors, adding the projections as in STransE (through the two relation matrices) increases the time complexity to O(d 2 ). Besides time complexity, the number of parameters to be learned from data grows quadratically with d. A quadratic time complexity and parameter growth may arise two issues: 1-scalability problems, 2-overfitting. Same issues exist for models such as RESCAL and NTNs that have quadratic or higher time complexities and parameter growths. DistMult and ComplEx have linear time complexities and the number of their parameters grow linearly with d. The time complexity of both SimplE-ignr and SimplE is O(d), i.e. linear in the size of vector embeddings. SimplE-ignr requires one multiplication between three vectors for each triple. This number is 2 for SimplE and 4 for ComplEx. Thus, with the same number of parameters, SimplE-ignr and SimplE reduce the computations by a factor of 4 and 2 respectively compared to ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>The constraint over M r matrices in SimplE is very similar to the constraint in DistMult. v T h M r in both SimplE and DistMult can be considered as an element-wise product of the parameters, except that the M r s in SimplE swap the first and second halves of the resulting vector. Compared to ComplEx, SimplE removes the parameters on the main diagonal of M r s. Note that several other restrictions The constraints over M r matrices for bilinear models (a) DistMult, (b) ComplEx, (c) CP, and (d) SimplE. The lines represent where the parameters are; other elements of the matrices are constrained to be zero. In ComplEx, the parameters represented by the dashed line is tied to the parameters represented by the solid line and the parameters represented by the dotted line is tied to the negative of the dotted-and-dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The following example shows how this redundancy in ComplEx may affect its interpretability:Example 2. Consider a ComplEx model with embeddings of size 1. Consider entities e 1 , e 2 and e 3 with embedding vectors [1 + 4i], [1 + 6i], and [3 + 2i] respectively, and a relation r with embedding vector [1 + i]. According to ComplEx, the score for triple (e 1 , r , e 3 )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>WN18 contains 40, 943 entities, 18 relations, 141, 442 train, 5, 000 validation and 5, 000 test triples. FB15k contains 14, 951 entities, 1, 345 relations, 483, 142 train, 50, 000 validation, and 59, 071 test triples. Baselines: We compare SimplE with several existing tensor factorization approaches. Our baselines include canonical Polyadic (CP) decomposition, TransE, TransR, DistMult, NTN, STransE, ER-MLP, and ComplEx. Given that we use the same data splits and objective function as ComplEx, we report the results of CP, TransE, DistMult, and ComplEx from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on WN18 and FB15k. Best results are in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB15k</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MRR</cell><cell></cell><cell>Hit@</cell><cell></cell><cell cols="2">MRR</cell><cell></cell><cell>Hit@</cell><cell></cell></row><row><cell>Model</cell><cell>Filter</cell><cell>Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>Filter</cell><cell>Raw</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>CP</cell><cell>0.075</cell><cell>0.058</cell><cell>0.049</cell><cell>0.080</cell><cell>0.125</cell><cell>0.326</cell><cell>0.152</cell><cell>0.219</cell><cell>0.376</cell><cell>0.532</cell></row><row><cell>TransE</cell><cell>0.454</cell><cell>0.335</cell><cell>0.089</cell><cell>0.823</cell><cell>0.934</cell><cell>0.380</cell><cell>0.221</cell><cell>0.231</cell><cell>0.472</cell><cell>0.641</cell></row><row><cell>TransR</cell><cell>0.605</cell><cell>0.427</cell><cell>0.335</cell><cell>0.876</cell><cell>0.940</cell><cell>0.346</cell><cell>0.198</cell><cell>0.218</cell><cell>0.404</cell><cell>0.582</cell></row><row><cell>DistMult</cell><cell>0.822</cell><cell>0.532</cell><cell>0.728</cell><cell>0.914</cell><cell>0.936</cell><cell>0.654</cell><cell>0.242</cell><cell>0.546</cell><cell>0.733</cell><cell>0.824</cell></row><row><cell>NTN</cell><cell>0.530</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>0.661</cell><cell>0.250</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>0.414</cell></row><row><cell>STransE</cell><cell>0.657</cell><cell>0.469</cell><cell>−</cell><cell>−</cell><cell>0.934</cell><cell>0.543</cell><cell>0.252</cell><cell>−</cell><cell>−</cell><cell>0.797</cell></row><row><cell>ER-MLP</cell><cell>0.712</cell><cell>0.528</cell><cell>0.626</cell><cell>0.775</cell><cell>0.863</cell><cell>0.288</cell><cell>0.155</cell><cell>0.173</cell><cell>0.317</cell><cell>0.501</cell></row><row><cell>ComplEx</cell><cell>0.941</cell><cell>0.587</cell><cell>0.936</cell><cell>0.945</cell><cell>0.947</cell><cell>0.692</cell><cell>0.242</cell><cell>0.599</cell><cell>0.759</cell><cell>0.840</cell></row><row><cell>SimplE-ignr</cell><cell>0.939</cell><cell>0.576</cell><cell>0.938</cell><cell>0.940</cell><cell>0.941</cell><cell>0.700</cell><cell>0.237</cell><cell>0.625</cell><cell>0.754</cell><cell>0.821</cell></row><row><cell>SimplE</cell><cell>0.942</cell><cell>0.588</cell><cell>0.939</cell><cell>0.944</cell><cell>0.947</cell><cell>0.727</cell><cell>0.239</cell><cell>0.660</cell><cell>0.773</cell><cell>0.838</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>shows the results of our experiments. It can be viewed that both SimplE-ignr and SimplE do a good job compared to the existing baselines on both datasets. On WN18, SimplE-ignr and SimplE perform as good as ComplEx, a state-of-the-art tensor factorization model. On FB15k, SimplE outperforms the existing baselines and gives state-of-the-art results among tensor factorization approaches. SimplE (and SimplE-ignr) work especially well on this dataset in terms of filtered MRR and hit@1, so SimplE tends to do well at having its first prediction being correct.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Background Knowledge Used in Section 6.2. (ei , hyponym, ej ) ∈ ζ ⇔ (ej , hypernym, ei ) ∈ ζ 2 (ei , memberMeronym, ej ) ∈ ζ ⇔ (ej , memberHolonym, ei ) ∈ ζ 3 (ei , instanceHyponym, ej ) ∈ ζ ⇔ (ej , instanceHypernym, ei ) ∈ ζ 4</figDesc><table><row><cell>Rule Number</cell><cell>Rule</cell></row><row><cell>1</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Triples are complete for relations. They are sometimes written as (subject, verb, object) or (individual , property, value).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although their incorporation of inversion into DistMult is not correct as it has side effects.<ref type="bibr" target="#b2">3</ref> Note that such background knowledge can be exerted on some relations selectively and not on the others. This is different than, e.g., DistMult which enforces symmetry on all relations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">* |tt| (h,r ,t)∈tt 1 rank h + 1 rankt , where tt represents the test triples. MRR is a more robust measure than mean rank, since a single bad ranking can largely influence mean rank.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a simple interpretable fully expressive bilinear model for knowledge graph completion. We showed that our model, called SimplE, performs very well empirically and has several interesting properties. For instance, three types of background knowledge can be incorporated into SimplE by tying the embeddings. In future, SimplE could be improved or may help improve relational learning in several ways including: 1-building ensembles of SimplE models as <ref type="bibr" target="#b17">[18]</ref> do it for DistMult, 2-adding SimplE to the relation-level ensembles of <ref type="bibr" target="#b43">[44]</ref>, 3-explicitly modelling the analogical structures of relations as in <ref type="bibr" target="#b22">[23]</ref>, 4-using [8]'s 1-N scoring approach to generate many negative triples for a positive triple (Trouillon et al. <ref type="bibr" target="#b38">[39]</ref> show that generating more negative triples improves accuracy), 5combining SimplE with logic-based approaches (e.g., with <ref type="bibr" target="#b18">[19]</ref>) to improve property prediction, 6combining SimplE with (or use SimplE as a sub-component in) techniques from other categories of relational learning as <ref type="bibr" target="#b32">[33]</ref> do with ComplEx, 7-incorporating other types of background knowledge (e.g., entailment) into SimplE embeddings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.7158</idno>
		<title level="m">Irreflexive and hierarchical relations as translations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximations by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on AKBC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical relational artificial intelligence: Logic, probability, and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Luc De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriraam</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="189" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embedding using simple constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by flexible translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="557" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05563</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10744</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relnn: A deep neural model for relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vít Nováček, and Pierre-Yves Vandenbussche. Regularizing knowledge graph embeddings via equivalence and inversion axioms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emir</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An overview of embedding models of entities and relationships for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Wide Web</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end differentiable proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3791" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Semantic Parsing</title>
		<meeting>the ACL 2014 Workshop on Semantic Parsing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Complex and holographic embeddings of knowledge graphs: a comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Christopher R Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bouchard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06879</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1859" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On multi-relational link prediction with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large-scale knowledge base completion: Inferring via grounding network sampling over selected instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengya</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
