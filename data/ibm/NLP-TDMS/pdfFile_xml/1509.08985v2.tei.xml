<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCSD ECE UCSD Cognitive Science UCSD Cognitive Science</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
							<email>patrick.w.gallagher@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UCSD ECE UCSD Cognitive Science UCSD Cognitive Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCSD ECE UCSD Cognitive Science UCSD Cognitive Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.</p><p>Both the mixed strategy and the gated strategy involve combinations of fixed pooling operations; a complementary</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent resurgence of neurally-inspired systems such as deep belief nets (DBN) <ref type="bibr" target="#b9">[10]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b17">[18]</ref>, and the sum-and-max infrastructure <ref type="bibr" target="#b31">[32]</ref> has derived significant benefit from building more sophisticated network structures <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b32">33]</ref> and from bringing learning to non-linear activations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. The pooling operation has also played a central role, contributing to invariance to data variation and perturbation. However, pooling operations have been little revised beyond the current primary options of average, max, and stochastic pooling Patent disclosure, UCSD Docket No. SD2015-184, "Forest Convolutional Neural Network", filed on March 4, 2015. UCSD Docket No. SD2016-053, "Generalizing Pooling Functions in Convolutional Neural Network", filed on Sept 23, 2015 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40]</ref>; this despite indications that e.g. choosing from more than just one type of pooling operation can benefit performance <ref type="bibr" target="#b30">[31]</ref>.</p><p>In this paper, we desire to bring learning and "responsiveness" (i.e., to characteristics of the region being pooled) into the pooling operation. Various approaches are possible, but here we pursue two in particular. In the first approach, we consider combining typical pooling operations (specifically, max pooling and average pooling); within this approach we further investigate two strategies by which to combine these operations. One of the strategies is "unresponsive"; for reasons discussed later, we call this strategy mixed max-average pooling. The other strategy is "responsive"; we call this strategy gated max-average pooling, where the ability to be responsive is provided by a "gate" in analogy to the usage of gates elsewhere in deep learning.</p><p>Another natural generalization of pooling operations is to allow the pooling operations that are being combined to themselves be learned. Hence in the second approach, we learn to combine pooling filters that are themselves learned. Specifically, the learning is performed within a binary tree (with number of levels that is pre-specified rather than "grown" as in traditional decision trees) in which each leaf is associated with a learned pooling filter. As we consider internal nodes of the tree, each parent node is associated with an output value that is the mixture of the child node output values, until we finally reach the root node. The root node corresponds to the overall output produced by the tree. We refer to this strategy as tree pooling. Tree pooling is intended (1) to learn pooling filters directly from the data; <ref type="bibr" target="#b1">(2)</ref> to learn how to combine leaf node pooling filters in a differentiable fashion; <ref type="bibr" target="#b2">(3)</ref> to bring together these other characteristics within a hierarchical tree structure.</p><p>When the mixing of the node outputs is allowed to be "responsive", the resulting tree pooling operation becomes an integrated method for learning pooling filters and combinations of those filters that are able to display a range of different behaviors depending on the characteristics of the region being pooled.</p><p>We pursue experimental validation and find that: In the ar-arXiv:1509.08985v2 [stat.ML] 10 Oct 2015 chitectures we investigate, replacing standard pooling operations with any of our proposed generalized pooling methods boosts performance on each of the standard benchmark datasets, as well as on the larger and more complex ImageNet dataset. We attain state-of-the-art results on MNIST, CIFAR10 (with and without data augmentation), and SVHN. Our proposed pooling operations can be used as drop-in replacements for standard pooling operations in various current architectures and can be used in tandem with other performance-boosting approaches such as learning activation functions, training with data augmentation, or modifying other aspects of network architecture -we confirm improvements when used in a DSN-style architecture, as well as in AlexNet and GoogLeNet. Our proposed pooling operations are also simple to implement, computationally undemanding (ranging from 5% to 15% additional overhead in timing experiments), differentiable, and use only a modest number of additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the current deep learning literature, popular pooling functions include max, average, and stochastic pooling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">40]</ref>. A recent effort using more complex pooling operations, spatial pyramid pooling <ref type="bibr" target="#b8">[9]</ref>, is mainly designed to deal with images of varying size, rather than delving in to different pooling functions or incorporating learning. Learning pooling functions is analogous to receptive field learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>. However methods like <ref type="bibr" target="#b14">[15]</ref> lead to a more difficult learning procedure that in turn leads to a less competitive result, e.g. an error rate of 16.89% on unaugmented CIFAR10.</p><p>Since our tree pooling approach involves a tree structure in its learning, we observe an analogy to "logic-type" approaches such as decision trees <ref type="bibr" target="#b26">[27]</ref> or "logical operators" <ref type="bibr" target="#b24">[25]</ref>. Such approaches have played a central role in artificial intelligence for applications that require "discrete" reasoning, and are often intuitively appealing. Unfortunately, despite the appeal of such logic-type approaches, there is a disconnect between the functioning of decision trees and the functioning of CNNs -the output of a standard decision tree is non-continuous with respect to its input (and thus nondifferentiable). This means that a standard decision tree is not able to be used in CNNs, whose learning process is performed by back propagation using gradients of differentiable functions. Part of what allows us to pursue our approaches is that we ensure the resulting pooling operation is differentiable and thus usable within network backpropagation.</p><p>A recent work, referred to as auto-encoder trees <ref type="bibr" target="#b12">[13]</ref>, also pays attention to a differentiable use of tree structures in deep learning, but is distinct from our method as it focuses on learning encoding and decoding methods (rather than pooling methods) using a "soft" decision tree for a generative model. In the supervised setting, <ref type="bibr" target="#b3">[4]</ref> incorporates multilayer perceptrons within decision trees, but simply uses trained perceptrons as splitting nodes in a decision forest; not only does this result in training processes that are separate (and thus more difficult to train than an integrated training process), this training process does not involve the learning of any pooling filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalizing Pooling Operations</head><p>A typical convolutional neural network is structured as a series of convolutional layers and pooling layers. Each convolutional layer is intended to produce representations (in the form of activation values) that reflect aspects of local spatial structures, and to consider multiple channels when doing so. More specifically, a convolution layer computes "feature response maps" that involve multiple channels within some localized spatial region. On the other hand, a pooling layer is restricted to act within just one channel at a time, "condensing" the activation values in each spatiallylocal region in the currently considered channel. An early reference related to pooling operations (although not explicitly using the term "pooling") can be found in <ref type="bibr" target="#b10">[11]</ref>. In modern visual recognition systems, pooling operations play a role in producing "downstream" representations that are more robust to the effects of variations in data while still preserving important motifs. The specific choices of average pooling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and max pooling <ref type="bibr" target="#b27">[28]</ref> have been widely used in many CNN-like architectures; <ref type="bibr" target="#b2">[3]</ref> includes a theoretical analysis (albeit one based on assumptions that do not hold here).</p><p>Our goal is to bring learning and "responsiveness" into the pooling operation. We focus on two approaches in particular. In the first approach, we begin with the (conventional, non-learned) pooling operations of max pooling and average pooling and learn to combine them. Within this approach, we further consider two strategies by which to combine these fixed pooling operations. One of these strategies is "unresponsive" to the characteristics of the region being pooled; the learning process in this strategy will result in an effective pooling operation that is some specific, unchanging "mixture" of max and average. To emphasize this unchanging mixture, we refer to this strategy as mixed max-average pooling.</p><p>The other strategy is "responsive" to the characteristics of the region being pooled; the learning process in this strategy results in a "gating mask". This learned gating mask is then used to determine a "responsive" mix of max pooling and average pooling; specifically, the value of the inner product between the gating mask and the current region being pooled is fed through a sigmoid, the output of which is used as the mixing proportion between max and average. To emphasize the role of the gating mask in determining the "responsive" mixing proportion, we refer to this strategy as gated max-average pooling.</p><p>generalization to these strategies is to learn the pooling operations themselves. From this, we are in turn led to consider learning pooling operations and also learning to combine those pooling operations. Since these combinations can be considered within the context of a binary tree structure, we refer to this approach as tree pooling. We pursue further details in the following sections. At present, max pooling is often used as the default in CNNs. We touch on the relative performance of max pooling and, e.g., average pooling as part of a collection of exploratory experiments to test the invariance properties of pooling functions under common image transformations (including rotation, translation, and scaling); see <ref type="figure">Figure 2</ref>.</p><p>The results indicate that, on the evaluation dataset, there are regimes in which either max pooling or average pooling demonstrates better performance than the other (although we observe that both of these choices are outperformed by our proposed pooling operations). In the light of observation that neither max pooling nor average pooling dominates the other, a first natural generalization is the strategy we call "mixed" max-average pooling, in which we learn specific mixing proportion parameters from the data. When learning such mixing proportion parameters one has several options (listed in order of increasing number of parameters): learning one mixing proportion parameter (a) per net, (b) per layer, (c) per layer/region being pooled (but used for all channels across that region), (d) per layer/channel (but used for all regions in each channel) (e) per layer/region/channel combination.</p><p>The form for each "mixed" pooling operation (written here for the "one per layer" option; the expression for other options differs only in the subscript of the mixing proportion a) is:</p><formula xml:id="formula_0">fmix(x) = a · fmax(x) + (1 − a ) · favg(x)<label>(1)</label></formula><p>where a ∈ [0, 1] is a scalar mixing proportion specifying the specific combination of max and average; the subscript is used to indicate that this equation is for the "one per layer" option. Once the output loss function E is defined, we can automatically learn each mixing proportion a (where we now suppress any subscript specifying which of the options we choose). Vanilla backpropagation for this learning is given by</p><formula xml:id="formula_1">∂E ∂a = ∂E ∂fmix(x) ∂fmix(x) ∂a = δ (max i xi − 1 N N i=1 xi),<label>(2)</label></formula><p>where δ = ∂E/∂f mix (x) is the error backpropagated from the following layer. Since pooling operations are typically placed in the midst of a deep neural network, we also need to compute the error signal to be propagated back to the previous layer:</p><formula xml:id="formula_2">∂E ∂xi = ∂E ∂fmix(xi) ∂fmix(xi) ∂xi (3) = δ a · 1[xi = max i xi] + (1 − a) · 1 N ,<label>(4)</label></formula><p>where 1[·] denotes the 0/1 indicator function. In the experiment section, we report results for the "one parameter per pooling layer" option; the network for this experiment has 2 pooling layers and so has 2 more parameters than a network using standard pooling operations. We found that even this simple option yielded a surprisingly large performance boost. We also obtain results for a simple 50/50 mix of max and average, as well as for the option with the largest number of parameters: one parameter for each combination of layer/channel/region, or pc × ph × pw parameters for each "mixed" pooling layer using this option (where pc is the number of channels being pooled by the pooling layer, and the number of spatial regions being pooled in each channel is ph × pw). We observe that the increase in the number of parameters is not met with a corresponding boost in performance, and so we pursue the "one per layer" option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">"Gated" max-average pooling</head><p>In the previous section we considered a strategy that we referred to as "mixed" max-average pooling; in that strat-egy we learned a mixing proportion to be used in combining max pooling and average pooling. As mentioned earlier, once learned, each mixing proportion a remains fixed -it is "nonresponsive" insofar as it remains the same no matter what characteristics are present in the region being pooled. We now consider a "responsive" strategy that we call "gated" max-average pooling. In this strategy, rather than directly learning a mixing proportion that will be fixed after learning, we instead learn a "gating mask" (with spatial dimensions matching that of the regions being pooled). The scalar result of the inner product between the gating mask and the region being pooled is fed through a sigmoid to produce the value that we use as the mixing proportion. This strategy means that the actual mixing proportion can vary during use depending on characteristics present in the region being pooled. To be more specific, suppose we use x to denote the values in the region being pooled and ω to denote the values in a "gating mask". The "responsive" mixing proportion is then given by</p><formula xml:id="formula_3">σ(ω x), where σ(ω x) = 1/(1+exp{−ω x}) ∈ [0, 1] is a sigmoid func- tion.</formula><p>Analogously to the strategy of learning mixing proportion parameter, when learning gating masks one has several options (listed in order of increasing number of parameters): learning one gating mask (a) per net, (b) per layer, (c) per layer/region being pooled (but used for all channels across that region), (d) per layer/channel (but used for all regions in each channel) (e) per layer/region/channel combination. We suppress the subscript denoting the specific option, since the equations are otherwise identical for each option.</p><p>The resulting pooling operation for this "gated" maxaverage pooling is:</p><formula xml:id="formula_4">fgate(x) = σ(ω x)fmax(x) + (1 − σ(ω x))favg(x) (5)</formula><p>We can compute the gradient with respect to the internal "gating mask" ω using the same procedure considered previously, yielding</p><formula xml:id="formula_5">∂E ∂ω = ∂E ∂fgate(x) ∂fgate(x) ∂ω (6) = δ σ(ω x)(1 − σ(ω x)) x (max i xi − 1 N N i=1 xi),<label>(7)</label></formula><p>and</p><formula xml:id="formula_6">∂E ∂xi = ∂E ∂fgate(xi) ∂fgate(xi) ∂xi (8) = δ σ(ω x)(1 − σ(ω x)) ωi (max i xi − 1 N N i=1 xi) (9) + σ(ω x) · 1[xi = max i xi] + (1 − σ(ω x)) 1 N .</formula><p>In a head-to-head parameter count, every single mixing proportion parameter a in the "mixed" max-average pooling strategy corresponds to a gating mask ω in the "gated" strategy (assuming they use the same parameter count option). To take a specific example, suppose that we consider a network with 2 pooling layers and pooling regions that are 3 × 3. If we use the "mixed" strategy and the per-layer option, we would have a total of 2 = 2 × 1 extra parameters relative to standard pooling. If we use the "gated" strategy and the per-layer option, we would have a total of 18 = 2 × 9 extra parameters, where 9 is the number of parameters in each gating mask. The "mixed" strategy detailed immediately above uses fewer parameters and is "nonresponsive"; the "gated" strategy involves more parameters and is "responsive". In our experiments, we find that "mixed" (with one mix per pooling layer) is outperformed by "gated" with one gate per pooling layer. Interestingly, an 18 parameter "gated" network with only one gate per pooling layer also outperforms a "mixed" option with far more parameters (40,960 with one mix per layer/channel/region) -except on the relatively large SVHN dataset. We touch on this below; Section 5 contains details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Quick comparison: mixed and gated pooling</head><p>The results in <ref type="table" target="#tab_0">Table 1</ref> indicate the benefit of learning pooling operations over not learning. Within learned pooling operations, we see that when the number of parameters in the mixed strategy is increased, performance improves; however, parameter count is not the entire story. We see that the "responsive" gated max-avg strategy consistently yields better performance (using 18 extra parameters) than is achieved with the &gt;40k extra parameters in the 1 per layer/rg/ch "non-responsive" mixed max-avg strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tree pooling</head><p>The strategies described above each involve combinations of fixed pooling operations; another natural generalization of pooling operations is to allow the pooling operations that are being combined to themselves be learned. These pooling layers remain distinct from convolution layers since pooling is performed separately within each channel; this channel isolation also means that even the option that introduces the largest number of parameters still introduces far fewer parameters than a convolution layer would introduce. The most basic version of this approach would not involve combining learned pooling operations, but simply learning pooling operations in the form of the values in pooling filters. One step further brings us to what we refer to as tree pooling, in which we learn pooling filters and also learn to responsively combine those learned filters.</p><p>Both aspects of this learning are performed within a binary tree (with number of levels that is pre-specified rather than "grown" as in traditional decision trees) in which each leaf is associated with a pooling filter learned during training. As we consider internal nodes of the tree, each parent node is associated with an output value that is the mixture of the child node output values, until we finally reach the root node. The root node corresponds to the overall output produced by the tree and each of the mixtures (by which child outputs are "fused" into a parent output) is responsively learned. Tree pooling is intended (1) to learn pooling filters directly from the data; (2) to learn how to "mix" leaf node pooling filters in a differentiable fashion; (3) to bring together these other characteristics within a hierarchical tree structure. Each leaf node in our tree is associated with a "pooling filter" that will be learned; for a node with index m, we denote the pooling filter by v m ∈ R N . If we had a "degenerate tree" consisting of only a single (leaf) node, pooling a region x ∈ R N would result in the scalar value v m x. For (internal) nodes (at which two child values are combined into a single parent value), we proceed in a fashion analogous to the case of gated max-average pooling, with learned "gating masks" denoted (for an internal node m) by ω m ∈ R N . The "pooling result" at any arbitrary node m is thus</p><formula xml:id="formula_7">fm(x) = v m x if leaf node σ(ω m x)f m,left (x) + (1 − σ(ω m x))f m,right (x) if internal node<label>(10)</label></formula><p>The overall pooling operation would thus be the result of evaluating f root node (x). The appeal of this tree pooling approach would be limited if one could not train the proposed layer in a fashion that was integrated within the network as a whole. This would be the case if we attempted to directly use a traditional decision tree, since its output presents points of discontinuity with respect to its inputs. The reason for the discontinuity (with respect to input) of traditional decision tree output is that a decision tree makes "hard" decisions; in the terminology we have used above, a "hard" decision node corresponds to a mixing proportion that can only take on the value 0 or 1. The consequence is that this type of "hard" function is not differentiable (nor even continuous with respect to its inputs), and this in turn interferes with any ability to use it in iterative parameter updates during backpropagation. This motivates us to instead use the internal node sigmoid "gate" function σ(ω m x) ∈ [0, 1] so that the tree pooling function as a whole will be differentiable with respect to its parameters and its inputs.</p><p>For the specific case of a "2 level" tree (with leaf nodes "1" and "2" and internal node "3") pooling function f tree (x) = σ(ω 3 x)v 1 x+(1−σ(ω 3 x))v 2 x, we can use the chain rule to compute the gradients with respect to the leaf node pooling filters v 1 , v 2 and the internal node gating mask ω 3 :</p><formula xml:id="formula_8">∂E ∂v 1 = ∂E ∂ftree(x) ∂ftree(x) ∂v 1 = δ σ(ω 3 x)x (11) ∂E ∂v 2 = ∂E ∂ftree(x) ∂ftree(x) ∂v 2 = δ (1 − σ(ω 3 x))x (12) ∂E ∂ω 3 = ∂E ∂ftree(x) ∂ftree(x) ∂ω 3 = δ σ(ω 3 x)(1 − σ(ω 3 x)) x (v 1 − v 2 )x (13)</formula><p>The error signal to be propagated back to the previous layer is <ref type="table" target="#tab_1">Table 2</ref> collects results related to tree pooling. We observe that on all datasets but the comparatively simple MNIST, adding a level to the tree pooling operation improves performance. However, even further benefit is obtained from the use of tree pooling in the first pooling layer and gated max-avg in the second. Comparison with making the network deeper using conv layers To further investigate whether simply adding depth to our baseline network gives a performance boost comparable to that observed for our proposed pooling operations, we report in <ref type="table" target="#tab_3">Table 3</ref> below some additional experiments on CIFAR10 (error rate in percent; no data augmentation). If we count depth by counting any layer with learned parameters as an extra layer of depth (even if there is only 1 parameter), the number of parameter layers in a baseline network with 2 additional standard convolution layers matches the number of parameter layers in our best performing net (although the convolution layers contain many more parameters).  <ref type="figure">Figure 2</ref>: Controlled experiment on CIFAR10 investigating the relative benefit of selected pooling operations in terms of robustness to three types of data variation. The three kinds of variations we choose to investigate are rotation, translation, and scale. With each kind of variation, we modify the CIFAR10 test images according to the listed amount. We observe that, across all types and amounts of variation (except extreme down-scaling) the proposed pooling operations investigated here (gated max-avg and 2 level tree pooling) provide improved robustness to these transformations, relative to the standard choices of maxpool or avgpool.</p><formula xml:id="formula_9">∂E ∂x = ∂E ∂ftree(x) ∂ftree(x) ∂x (14) = δ [σ(ω 3 x)(1 − σ(ω 3 x))ω3(v 1 − v 2 )x (15) + σ(ω 3 x)v1 + (1 − σ(ω 3 x))v2]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Quick comparison: tree pooling</head><p>Our method requires only 72 extra parameters and obtains state-of-the-art 7.62% error. On the other hand, making networks deeper with conv layers adds many more parameters but yields test error that does not drop below 9.08% in the configuration explored. Since we follow each additional conv layer with a ReLU, these networks correspond to increasing nonlinearity as well as adding depth and adding (many) parameters. These experiments indicate that the performance of our proposed pooling is not accounted for as a simple effect of the addition of depth/parameters/nonlinearity. Comparison with alternative pooling layers To see whether we might find similar performance boosts by replacing the max pooling in the baseline network configuration with alternative pooling operations such as stochastic pooling, "pooling" using a stride 2 convolution layer as pooling (cf All-CNN), or a simple fixed 50/50 proportion in max-avg pooling, we performed another set of experiments on unaugmented CIFAR10. From the baseline error rate of 9.10%, replacing each of the 2 max pooling layers with stacked stride 2 conv:ReLU (as in <ref type="bibr" target="#b33">[34]</ref>) lowers the error to 8.77%, but adds 0.5M extra parameters. Using stochastic pooling <ref type="bibr" target="#b39">[40]</ref> adds computational overhead but no parameters and results in 8.50% error. A simple 50/50 mix of max and average is computationally light and yields 8.07% error with no additional parameters. Finally, our tree+gated max-avg configuration adds 72 parameters and achieves a state-of-the-art 7.62% error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Quick Performance Overview</head><p>For ease of discussion, we collect here observations from subsequent experiments with a view to highlighting aspects that shed light on the performance characteristics of our proposed pooling functions.</p><p>First, as seen in the experiment shown in <ref type="figure">Figure 2</ref> replacing standard pooling operations with either gated max-avg or (2 level) tree pooling (each using the "one per layer" option) yielded a boost (relative to max or avg pooling) in CIFAR10 test accuracy as the test images underwent three different kinds of transformations. This boost was observed across the entire range of transformation amounts for each of the transformations (with the exception of extreme downscaling). We already observe improved robustness in this initial experiment and intend to investigate more instances of our proposed pooling operations as time permits.</p><p>Second, the performance that we attain in the experiments reported in <ref type="figure">Figure 2</ref>, <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table" target="#tab_1">Table 2</ref>, <ref type="table" target="#tab_5">Table 4</ref>, and <ref type="table" target="#tab_8">Table 5</ref> is achieved with very modest additional numbers of parameters -e.g. on CIFAR10, our best performance (obtained with the tree+gated max-avg configuration) only uses an additional 72 parameters (above the 1.8M of our baseline network) and yet reduces test error from 9.10% to 7.62%; see the CIFAR10 Section for details. In our AlexNet experiment, replacing the maxpool layers with our proposed pooling operations gave a 6% relative reduction in test error (top-5, single-view) with only 45 additional parameters (above the &gt;50M of standard AlexNet); see the Im-ageNet 2012 Section for details. We also investigate the additional time incurred when using our proposed pooling operations; in the experiments reported in the Timing section, this overhead ranges from 5% to 15%.</p><p>Testing invariance properties Before going to the overall classification results, we investigate the invariance properties of networks utilizing either standard pooling operations (max and average) or two instances of our proposed pooling operations (gated max-avg and 2 level tree, each using the "1 per pool layer" option) that we find to yield best performance (see Sec. 5 for architecture details used across each network). We begin by training four different networks on the CIFAR10 training set, one for each of the four pooling operations selected for consideration; training details are found in Sec. 5. We seek to determine the respective invariance properties of these networks by evaluating their accuracy on various transformed versions of the CIFAR10 test set. <ref type="figure">Figure 2</ref> illustrates the test accuracy attained in the presence of image rotation, (vertical) translation, and scaling of the CIFAR10 test set.</p><p>Timing In order to evaluate how much additional time is incurred by the use of our proposed learned pooling operations, we measured the average forward+backward time per CIFAR10 image. In each case, the one per layer option is used. We find that the additional computation time incurred ranges from 5% to 15%. More specifically, the baseline network took 3.90 ms; baseline with mixed maxavg took 4.10 ms; baseline with gated max-avg took 4.16 ms; baseline with 2 level tree pooling took 4.25 ms; finally, baseline with tree+gated max-avg took 4.46 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed max-average pooling and tree pooling approaches on five standard benchmark datasets: MNIST <ref type="bibr" target="#b19">[20]</ref>, CIFAR10 <ref type="bibr" target="#b15">[16]</ref>, CIFAR100 <ref type="bibr" target="#b15">[16]</ref>, SVHN <ref type="bibr" target="#b25">[26]</ref> and ImageNet <ref type="bibr" target="#b29">[30]</ref>. To control for the effect of differences in data or data preparation, we match our data and data preparation to that used in <ref type="bibr" target="#b20">[21]</ref>. Please refer to <ref type="bibr" target="#b20">[21]</ref> for the detailed description.</p><p>We now describe the basic network architecture and then will specify the various hyperparameter choices. The basic experiment architecture contains six 3 × 3 standard convolutional layers (named conv1 to conv6) and three mlpconv layers (named mlpconv1 to mlpconv3) <ref type="bibr" target="#b23">[24]</ref>, placed after conv2, conv4, and conv6, respectively. We chose the number of channels at each layer to be analogous to the choices in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>; the specific numbers are provided in the sections for each dataset. We follow every one of these conv-type layers with ReLU activation functions. One final mlpconv layer (mlpconv4) is used to reduce the dimension of the last layer to match the total number of classes for each different dataset, as in <ref type="bibr" target="#b23">[24]</ref>. The overall model has parameter count analogous to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>. The proposed maxaverage pooling and tree pooling layers with 3 × 3 pooling regions are used after mlpconv1 and mlpconv2 layers 1 . We provide a detailed listing of the network configurations in <ref type="table" target="#tab_0">Table A1</ref> in the Supplementary Materials.</p><p>Moving on to the hyperparameter settings, dropout with rate 0.5 is used after each pooling layer. We also use hidden layer supervision to ease the training process as in <ref type="bibr" target="#b20">[21]</ref>. The learning rate is decreased whenever the validation error stops decreasing; we use the schedule {0.025, 0.0125, 0.0001} for all experiments. The momentum of 0.9 and weight decay of 0.0005 are fixed for all datasets as another regularizer besides dropout. All the initial pooling filters and pooling masks have values sampled from a Gaussian distribution with zero mean and standard deviation 0.5. We use these hyperparameter settings for all experiments reported in Tables 1, 2, and 3. No model averaging is done at test time. <ref type="table" target="#tab_0">Tables 1 and 2</ref> show our overall experimental results. Our baseline is a network trained with conventional max pooling. Mixed refers to the same network but with a max-avg pooling strategy in both the first and second pooling layers (both using the mixed strategy); Gated has a corresponding meaning. Tree (with specific number of levels noted below) refers to the same again, but with our tree pooling in the first pooling layer only; we do not see further improvement when tree pooling is used for both pooling layers. This observation motivated us to consider following a tree pooling layer with a gated max-avg pooling layer: Tree+Max-Average refers to a network configuration with (2 level) tree pooling for the first pooling layer and gated max-average pooling for the second pooling layer. All results are produced from the same network structure and hyperparameter settings -the only difference is in the choice of pooling function. See <ref type="table" target="#tab_0">Table A1</ref>  channels for mlpconv1 to mlpconv3, respectively. We also performed an experiment in which we learned a single pooling filter without the tree structure (i.e., a singleton leaf node containing 9 parameters; one such singleton leaf node per pooling layer) and obtained 0.3% improvement over the baseline model. Our results indicate that performance improves when the pooling filter is learned, and further improves when we also learn how to combine learned pooling filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification results</head><p>The All-CNN method in <ref type="bibr" target="#b33">[34]</ref> uses convolutional layers in place of pooling layers in a CNN-type network architecture. However, a standard convolutional layer requires many more parameters than a gated max-average pooling layer (only 9 parameters for a 3 × 3 pooling region kernel size in the 1 per pooling layer option) or a tree-pooling layer (27 parameters for a 2 level tree and 3 × 3 pooling region kernel size, again in the 1 per pooling layer option). The pooling operations in our tree+max-avg network con-  <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. A "-" indicates that the cited work did not report results for that dataset. A fixed network configuration using the proposed tree+max-avg pooling (1 per pool layer option) yields state-of-the-art performance on all datasets (with the exception of CIFAR100).</p><formula xml:id="formula_10">Method MNIST CIFAR10 CIFAR10 + CIFAR100 SVHN</formula><p>CNN <ref type="bibr" target="#b13">[14]</ref> 0.53 ----Stoch. Pooling <ref type="bibr" target="#b39">[40]</ref> 0 figuration use 7 × 9 = 63 parameters for the (first, 3 level) tree-pooling layer -4 leaf nodes and 3 internal nodesand 9 parameters in the gating mask used for the (second) gated max-average pooling layer, while the best result in <ref type="bibr" target="#b33">[34]</ref> contains a total of nearly 500, 000 parameters in layers performing "pooling like" operations; the relative CI-FAR10 accuracies are 7.62% (ours) and 9.08% (All-CNN).</p><p>For the data augmentation experiment, we followed the standard data augmentation procedure <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. When training with augmented data, we observe the same trends seen in the "no data augmentation" experiments. We note that <ref type="bibr" target="#b6">[7]</ref> reports a 4.5% error rate with extensive data augmentation (including translations, rotations, reflections, stretching, and shearing operations) in a much wider and deeper 50 million parameter network -28 times more than are in our networks. ImageNet 2012 In this experiment we do not directly compete with the best performing result in the challenge (since the winning methods <ref type="bibr" target="#b37">[38]</ref> involve many additional aspects beyond pooling operations), but rather to provide an illustrative comparison of the relative benefit of the proposed pooling methods versus conventional max pooling on this dataset. We use the same network structure and parameter setup as in <ref type="bibr" target="#b16">[17]</ref> (no hidden layer supervision) but simply replace the first max pooling with the (proposed 2 level) tree pooling (2 leaf nodes and 1 internal node for 27 = 3 × 9 parameters) and replace the second and third max pooling with gated max-average pooling (2 gating masks for 18 = 2 × 9 parameters). Relative to the original AlexNet, this adds 45 more parameters (over the &gt;50M in the original) and achieves relative error reduction of 6% (for top-5, single-view) and 5% (for top-5, multiview). Our GoogLeNet configuration uses 4 gated max-avg pooling layers, for a total of 36 extra parameters over the 6.8 million in standard GoogLeNet. <ref type="table" target="#tab_8">Table 5</ref> shows a direct comparison (in each case we use single net predictions rather than ensemble). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Supplementary Materials</head><p>Visualization of network internal representations To gain additional qualitative understanding of the pooling methods we are considering, we use the popular t-SNE <ref type="bibr" target="#b38">[39]</ref> algorithm to visualize embeddings of some internal feature responses from pooling operations. Specifically, we again use four networks (one utilizing each of the selected types of pooling) trained on the CIFAR10 training set (see Sec. 5 for architecture details used across each network). We extract feature responses for a randomly chosen 800-image subset of the CIFAR10 test set at the first (i.e., earliest) and second pooling layers of each network. These feature response vectors are then embedded into 2-d using t-SNE; see <ref type="figure" target="#fig_1">Figure A1</ref>.</p><p>The first row shows the embeddings of the internal activations immediately after the first pooling operation; the second row shows embeddings of activations immediately after the second pooling operation. From left to right we plot the t-SNE embeddings of the pooling activations within networks that are trained with average, max, gated maxavg, and (2 level) tree pooling. We can see that certain classes such as "0" (airplane), "2" (bird), and "9" (truck) are more separated with the proposed methods than they are with the conventional average and max pooling functions. We can also see that the embeddings of the secondpooling-layer activations are generally more separable than the embeddings of first-pooling-layer activations. Tree pooling <ref type="figure" target="#fig_1">Figure A1</ref>: t-SNE embeddings of the output responses from different pooling operations on the CIFAR10 test set (with classes indicated). From left to right: average, max, gated max-avg, and (2 level) tree pooling. The first and the second rows show the first and the second pooling layers, respectively. Best viewed in color. <ref type="table" target="#tab_0">Table A1</ref>: Here we provide explicit statement of the experimental conditions (specifically, network layer configurations) explored in <ref type="table" target="#tab_0">Tables 1, 2</ref>, and 4. We list all conv-like layers and pool-like layers, but ReLUs are suppressed to lighten the amount of text; these follow each standard conv layer. Also, all network configurations incorporate deep supervision after each standard convolution layer; this is also suppressed for clarity. We bold the changes made to the baseline DSN layer configuration. We now describe the meaning of entries in the table. Each column in the table lists the sequence of layer types used in that network configuration. When a row cell spans multiple columns (i.e. configurations), this indicates that the layer type listed in that cell is kept the same across the corresponding network configurations. Thus, every network in our experiments begins with a stacked pair of 3x3 (standard) conv layers followed by a 1x1 mlpconv layer. For a specific example, let us consider the network configuration in the column headed "mixed max-avg" -the sequence of layers in this configuration is: 3x3 (standard) conv, 3x3 (standard) conv, 1x1 mlpconv, 3x3 mixed max-avg pool, 3x3 (standard) conv, 3x3 (standard) conv, 1x1 mlpconv, 3x3 mixed max-avg pool, 3x3 (standard) conv, 3x3 (standard) conv, 1x1 mlpconv, 1x1 mlpconv, 8x8 global vote (cf. <ref type="bibr" target="#b23">[24]</ref>) (we again omit mention of ReLUs and deep supervision). CIFAR100 uses (2 level) tree+max-avg; CIFAR10 uses (3 level) tree+max-avg. As a final note: for the MNIST experiments only, the second pooling operation uses 2x2 regions instead of the 3x3 regions used on the other datasets.</p><p>Network layer configurations reported in <ref type="table" target="#tab_0">Tables 1, 2</ref>, and 4 of the main paper. DSN (baseline) mixed max-avg gated max-avg 2 level tree pool 3 level tree pool tree+gated max-avg pool 3x3 (standard) conv 3x3 (standard) conv 1x1 mlpconv 3x3 maxpool 3x3 mixed max-avg 3x3 gated max-avg 3x3 2 level tree pool 3x3 3 level tree pool 3x3 2/3 level tree pool 3x3 (standard) conv 3x3 (standard) conv 1x1 mlpconv 3x3 maxpool 3x3 mixed max-avg 3x3 gated max-avg 3x3 maxpool 3x3 maxpool 3x3 gated max-avg 3x3 (standard) conv 3x3 (standard) conv 1x1 mlpconv 1x1 mlpconv 8x8 global vote</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 . 1</head><label>31</label><figDesc>Combining max and average pooling functions 3.1.1 "Mixed" max-average pooling The conventional pooling operation is fixed to be either a simple average f ave (x) = 1 N N i=1 x i or a maximum operation f max (x) = max i x i , where the vector x contains the activation values from a local pooling region of N pixels (typical pooling region dimensions are 2 × 2 or 3 × 3) in an image or a channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of proposed pooling operations: (a) mixed max-average pooling, (b) gated max-average pooling, and (c) Tree pooling (3 levels in this figure). We indicate the region being pooled by x, gating masks by ω, and pooling filters by v (subscripted as appropriate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification error (in %) comparison between baseline model (trained with conventional max pooling) and corresponding networks in which max pooling is replaced by the pooling operation listed. A superscripted + indicates the standard data augmentation as in<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. We report means and standard deviations over 3 separate trials without model averaging.</figDesc><table><row><cell>The</cell></row></table><note>± 0.16</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">Classification error (in %) comparison between</cell></row><row><cell cols="6">our baseline model (trained with conventional max pool-</cell></row><row><cell cols="6">ing) and proposed methods involving tree pooling. A su-</cell></row><row><cell cols="6">perscripted + indicates the standard data augmentation as</cell></row><row><cell>in [24, 21, 34].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>CIFAR10 +</cell><cell>CIFAR100</cell><cell>SVHN</cell></row><row><cell>Our baseline</cell><cell>0.39</cell><cell>9.10</cell><cell>7.32</cell><cell>34.21</cell><cell>1.91</cell></row><row><cell>Tree 2 level; 1 per pool layer</cell><cell>0.35</cell><cell>8.25</cell><cell>6.88</cell><cell>33.53</cell><cell>1.80</cell></row><row><cell>Tree 3 level; 1 per pool layer</cell><cell>0.37</cell><cell>8.22</cell><cell>6.67</cell><cell>33.13</cell><cell>1.70</cell></row><row><cell>Tree+Max-Avg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 per pool layer</cell><cell>0.31</cell><cell>7.62</cell><cell>6.05</cell><cell>32.37</cell><cell>1.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Classification error (%) on CIFAR10 (without</cell></row><row><cell cols="3">data augmentation) comparison between networks made</cell></row><row><cell cols="3">deeper with standard convolution layers and proposed</cell></row><row><cell>Tree+(gated) Max-Avg pooling.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Extra</cell></row><row><cell>Method</cell><cell>% Error</cell><cell>parameters</cell></row><row><cell>Baseline</cell><cell>9.10</cell><cell>0</cell></row><row><cell>w/ 1 extra conv layer (+ReLU)</cell><cell>9.08</cell><cell>0.6M</cell></row><row><cell>w/ 2 extra conv layers (+ReLU)</cell><cell>9.17</cell><cell>1.2M</cell></row><row><cell>w/ Tree+(gated) Max-Avg</cell><cell>7.62</cell><cell>72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>for details.</figDesc><table><row><cell>MNIST Our MNIST model has {128, 128, 192, 192, 256,</cell></row><row><cell>256} channels for conv1 to conv6 and {128, 192, 256}</cell></row><row><cell>channels for mlpconv1 to mlpconv3, respectively. Our only</cell></row><row><cell>preprocessing is mean subtraction. Tables 4,1, and 2 show</cell></row><row><cell>previous best results and those for our proposed pooling</cell></row><row><cell>methods.</cell></row><row><cell>CIFAR10 Our CIFAR10 model has {128, 128, 192, 192,</cell></row><row><cell>256, 256} channels for conv1 to conv6 and {128, 192, 256}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Classification error (in %) reported by recent comparable publications on four benchmark datasets with a single model and no data augmentation, unless otherwise indicated. A superscripted + indicates the standard data augmentation as in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>CIFAR100 Our CIFAR100 model has 192 channels for all convolutional layers and {96, 192, 192} channels for mlp-conv1 to mlpconv3, respectively.Street view house numbersOur SVHN model has {128, 128, 320, 320, 384, 384} channels for conv1 to conv6 and {96, 256, 256} channels for mlpconv1 to mlpconv3, respectively. In terms of amount of data, SVHN has a larger training data set (&gt;600k versus the ≈50k of most of the other benchmark datasets). The much larger amount of training data motivated us to explore what performance we might observe if we pursued the one per layer/channel/region option, which even for the simple mixed max-avg strategy results in a huge increase in total the number of parameters to learn in our proposed pooling layers: specifically, from a total of 2 in the mixed max-avg strategy, 1 parameter per pooling layer option, we increase to40,960.    Using this one per layer/channel/region option for the mixed max-avg strategy, we observe test error (in %) of 0.30 on MNIST, 8.02 on CIFAR10, 6.61 on CIFAR10 + , 33.27 on CIFAR100, and 1.64 on SVHN.</figDesc><table><row><cell>Interestingly, for</cell></row><row><cell>MNIST, CIFAR10 + , and CIFAR100 this mixed max-avg</cell></row><row><cell>(1 per layer/channel/region) performance is between mixed</cell></row><row><cell>max-avg (1 per layer) and gated max-avg (1 per layer);</cell></row><row><cell>on CIFAR10 mixed max-avg (1 per layer/channel/region)</cell></row><row><cell>is worse than either of the 1 per layer max-avg strate-</cell></row><row><cell>gies. The SVHN result using mixed max-avg (1 per</cell></row><row><cell>layer/channel/region) sets a new state of the art.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>ImageNet 2012 test error (in %). BN denotes Batch Normalization<ref type="bibr" target="#b11">[12]</ref>.</figDesc><table><row><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>Method</cell><cell>s-view</cell><cell>s-view</cell><cell>m-view</cell><cell>m-view</cell></row><row><cell>AlexNet [17]</cell><cell>43.1</cell><cell>19.9</cell><cell>40.7</cell><cell>18.2</cell></row><row><cell>AlexNet w/ ours</cell><cell>41.4</cell><cell>18.7</cell><cell>39.3</cell><cell>17.3</cell></row><row><cell>GoogLeNet [38]</cell><cell>-</cell><cell>10.07</cell><cell>-</cell><cell>9.15</cell></row><row><cell>GoogLeNet w/ BN</cell><cell>28.68</cell><cell>9.53</cell><cell>27.81</cell><cell>9.09</cell></row><row><cell>GoogLeNet w/ BN + ours</cell><cell>28.02</cell><cell>9.16</cell><cell>27.60</cell><cell>8.93</cell></row><row><cell cols="4">6 Observations from Experiments</cell><cell></cell></row><row><cell cols="5">In each experiment, using any of our proposed pooling</cell></row><row><cell cols="5">operations boosted performance. A fixed network con-</cell></row><row><cell cols="5">figuration using the proposed tree+max-avg pooling (1</cell></row><row><cell cols="5">per pool layer option) yields state-of-the-art performance</cell></row><row><cell cols="5">on MNIST, CIFAR10 (with and without data augmenta-</cell></row><row><cell cols="5">tion), and SVHN. We observed boosts in tandem with</cell></row><row><cell cols="5">data augmentation, multi-view predictions, batch normal-</cell></row><row><cell cols="5">ization, and several different architectures -NiN-style,</cell></row><row><cell cols="5">DSN-style, the &gt;50M parameter AlexNet, and the 22-layer</cell></row><row><cell>GoogLeNet.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">There is one exception: on the very small images of the MNIST dataset, the second pooling layer uses 2 × 2 pooling regions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work is supported by NSF awards IIS-1216528 (IIS-1360566) and IIS-0844566(IIS-1360568).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning activation functions to improve deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ask the locals: multi-way local pooling for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Theoretical Analysis of Feature Pooling in Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Decision Forests for Semantic Image Labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout Networks. In ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Fractional Max-Pooling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learnednorm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Autoencoder Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS Dept., U Toronto, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply-Supervised Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization of NNs using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent CNNs for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Logic-Based Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Science &amp; Business Media</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">C4.5: Programming for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse Feature Learning for Deep Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Riedmiller. Striving for Simplicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving deep neural networks with probabilistic maxout units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Networks with Internal Selective Attention through Feedback Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going Deeper with Convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<title level="m">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
