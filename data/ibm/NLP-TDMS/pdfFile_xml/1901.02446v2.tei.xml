<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Panoptic Feature Pyramid Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Panoptic Feature Pyramid Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-ofthe-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, topperforming method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our community has witnessed rapid progress in semantic segmentation, where the task is to assign each pixel a class label (e.g. for stuff classes), and more recently in instance segmentation, where the task is to detect and segment each object instance (e.g. for thing classes). These advances have been aided by simple yet powerful baseline methods, including Fully Convolutional Networks (FCN) <ref type="bibr" target="#b40">[41]</ref> and Mask R-CNN <ref type="bibr" target="#b23">[24]</ref> for semantic and instance segmentation, respectively. These methods are conceptually simple, fast, and flexible, serving as a foundation for much of the subsequent progress in these areas. In this work our goal is to propose a similarly simple, single-network baseline for the joint task of panoptic segmentation <ref type="bibr" target="#b29">[30]</ref>, a task which encompasses both semantic and instance segmentation.</p><p>While conceptually straightforward, designing a single network that achieves high accuracy for both tasks is   <ref type="bibr" target="#b35">[36]</ref>, widely used in object detection, for extracting rich multi-scale features. (b) As in Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>, we use a region-based branch on top of FPN for instance segmentation. (c) In parallel, we add a lightweight denseprediction branch on top of the same FPN features for semantic segmentation. This simple extension of Mask R-CNN with FPN is a fast and accurate baseline for both tasks.</p><p>challenging as top-performing methods for the two tasks have many differences. For semantic segmentation, FCNs with specialized backbones enhanced by dilated convolutions <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b9">10]</ref> dominate popular leaderboards <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>. For instance segmentation, the region-based Mask R-CNN <ref type="bibr" target="#b23">[24]</ref> with a Feature Pyramid Network (FPN) <ref type="bibr" target="#b35">[36]</ref> backbone has been used as a foundation for all top entries in recent recognition challenges <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b42">43]</ref>. While there have been attempts to unify semantic and instance segmentation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>, the specialization currently necessary to achieve top performance in each was perhaps inevitable given their parallel development and separate benchmarks.</p><p>Given the architectural differences in these top methods, one might expect compromising accuracy on either instance or semantic segmentation is necessary when designing a single network for both tasks. Instead, we show a simple, flexible, and effective architecture that can match accuracy for both tasks using a single network that simultaneously generates region-based outputs (for instance segmentation) and dense-pixel outputs (for semantic segmentation). Our approach starts with the FPN <ref type="bibr" target="#b35">[36]</ref> backbone popular for instance-level recognition <ref type="bibr" target="#b23">[24]</ref> and adds a branch for performing semantic segmentation in parallel with the existing region-based branch for instance segmentation, see <ref type="figure" target="#fig_1">Figure 1</ref>. We make no changes to the FPN backbone when adding the dense-prediction branch, making it compatible with existing instance segmentation methods. Our method, which we call Panoptic FPN for its ability to generate both instance and semantic segmentations via FPN, is easy to implement given the Mask R-CNN framework <ref type="bibr" target="#b22">[23]</ref>.</p><p>While Panoptic FPN is an intuitive extension of Mask R-CNN with FPN, properly training the two branches for simultaneous region-based and dense-pixel prediction is important for good results. We perform careful studies in the joint setting for how to balance the losses for the two branches, construct minibatches effectively, adjust learning rate schedules, and perform data augmentation. We also explore various designs for the semantic segmentation branch (all other network components follow Mask R-CNN). Overall, while our approach is robust to exact design choices, properly addressing these issues is key for good results. When trained for each task independently, our method achieves excellent results for both instance and semantic segmentation on both COCO <ref type="bibr" target="#b36">[37]</ref> and Cityscapes <ref type="bibr" target="#b13">[14]</ref>. For instance segmentation, this is expected as our method in this case is equivalent to Mask R-CNN. For semantic segmentation, our simple dense-prediction branch attached to FPN yields accuracy on par with the latest dilation-based methods, such as the recent DeepLabV3+ <ref type="bibr" target="#b11">[12]</ref>.</p><p>For panoptic segmentation <ref type="bibr" target="#b29">[30]</ref>, we demonstrate that with proper training, using a single FPN for solving both tasks simultaneously yields accuracy equivalent to training two separate FPNs, with roughly half the compute. With the same compute, a joint network for the two tasks outperforms two independent networks by a healthy margin. Example panoptic segmentation results are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Panoptic FPN is memory and computationally efficient, incurring only a slight overhead over Mask R-CNN. By avoiding the use of dilation, which has high overhead, our method can use any standard top-performing backbone (e.g. a large ResNeXt <ref type="bibr" target="#b54">[55]</ref>). We believe this flexibility, together with the fast training and inference speeds of our method, will benefit future research on panoptic segmentation.</p><p>We used a preliminary version of our model (semantic segmentation branch only) as the foundation of the firstplace winning entry in the COCO Stuff Segmentation <ref type="bibr" target="#b5">[6]</ref> track in 2017. This single-branch model has since been adopted and generalized by several entries in the 2018 COCO and Mapillary Challenges 1 , showing its flexibility and effectiveness. We hope our proposed joint panoptic segmentation baseline is similarly impactful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Panoptic segmentation: The joint task of thing and stuff segmentation has a rich history, including early work on scene parsing <ref type="bibr" target="#b50">[51]</ref>, image parsing <ref type="bibr" target="#b51">[52]</ref>, and holistic scene understanding <ref type="bibr" target="#b55">[56]</ref>. With the recent introduction of the joint panoptic segmentation task <ref type="bibr" target="#b29">[30]</ref>, which includes a simple task specification and carefully designed task metrics, there has been a renewed interest in the joint task.</p><p>This year's COCO and Mapillary Recognition Challenge <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref> featured panoptic segmentation tracks that proved popular. However, every competitive entry in the panoptic challenges used separate networks for instance and semantic segmentation, with no shared computation. <ref type="bibr" target="#b0">1</ref> Our goal is to design a single network effective for both tasks that can serve as a baseline for future work. <ref type="bibr" target="#b0">1</ref> For details of not yet published winning entries in the 2018 COCO and Mapillary Recognition Challenge please see: http://cocodataset. org/workshop/coco-mapillary-eccv-2018.html. TRI-ML used separate networks for the challenge but a joint network in their recent updated tech report <ref type="bibr" target="#b32">[33]</ref> (which cites a preliminary version of our work).</p><p>Instance segmentation: Region-based approaches to object detection, including the Slow/Fast/Faster/Mask R-CNN family <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b23">24]</ref>, which apply deep networks on candidate object regions, have proven highly successful. All recent winners of the COCO detection challenges have built on Mask R-CNN <ref type="bibr" target="#b23">[24]</ref> with FPN <ref type="bibr" target="#b35">[36]</ref>, including in 2017 <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45]</ref> and 2018. <ref type="bibr" target="#b0">1</ref> Recent innovations include Cascade R-CNN <ref type="bibr" target="#b6">[7]</ref>, deformable convolution <ref type="bibr" target="#b14">[15]</ref>, and sync batch norm <ref type="bibr" target="#b44">[45]</ref>. In this work, the original Mask R-CNN with FPN serves as the starting point for our baseline, giving us excellent instance segmentation performance, and making our method fully compatible with these recent advances.</p><p>An alternative to region-based instance segmentation is to start with a pixel-wise semantic segmentation and then perform grouping to extract instances <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b0">1]</ref>. This direction is innovative and promising. However, these methods tend to use separate networks to predict the instancelevel information (e.g., <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38]</ref> use a separate network to predict instance edges, bounding boxes, and object breakpoints, respectively). Our goal is to design a single network for the joint task. Another interesting direction is to use position-sensitive pixel labeling <ref type="bibr" target="#b34">[35]</ref> to encode instance information fully convolutionally; <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b8">9]</ref> build on this.</p><p>Nevertheless, region-based approaches remain dominant on detection leaderboards <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b42">43]</ref>. While this motivates us to start with a region-based approach to instance segmentation, our approach would be fully compatible with a dense-prediction branch for instance segmentation.</p><p>Semantic segmentation: FCNs <ref type="bibr" target="#b40">[41]</ref> serve as the foundation of modern semantic segmentation methods. To increase feature resolution, which is necessary for generating highquality results, recent top methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b58">59]</ref> rely heavily on the use of dilated convolution <ref type="bibr" target="#b56">[57]</ref> (also known as atrous convolution <ref type="bibr" target="#b9">[10]</ref>). While effective, such an approach can substantially increase compute and memory, limiting the type of backbone network that can be used. To keep this flexibility, and more importantly to maintain compatibility with Mask R-CNN, we opt for a different approach.</p><p>As an alternative to dilation, an encoder-decoder <ref type="bibr" target="#b1">[2]</ref> or 'U-Net' <ref type="bibr" target="#b48">[49]</ref> architecture can be used to increase feature resolution <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>. Encoder-decoders progressively upsample and combine high-level features from a feedforward network with features from lower-levels, ultimately generating semantically meaningful, high-resolution features (see <ref type="figure" target="#fig_5">Figure 5</ref>). While dilated networks are currently more popular and dominate leaderboards, encoder-decoders have also been used for semantic segmentation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In our work we adopt an encoder-decoder framework, namely FPN <ref type="bibr" target="#b35">[36]</ref>. In contrast to 'symmetric' decoders <ref type="bibr" target="#b48">[49]</ref>, FPN uses a lightweight decoder (see <ref type="figure" target="#fig_5">Fig. 5</ref>). FPN was designed for instance segmentation, and it serves as the default backbone for Mask R-CNN. We show that without changes, FPN can also be highly effective for semantic segmentation.</p><p>Multi-task learning: Our approach is related to multi-task learning. In general, using a single network to solve multiple diverse tasks degrades performance <ref type="bibr" target="#b31">[32]</ref>, but various strategies can mitigate this <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42]</ref>. For related tasks, there can be gains from multi-task learning, e.g. the box branch in Mask R-CNN benefits from the mask branch <ref type="bibr" target="#b23">[24]</ref>, and joint detection and semantic segmentation of thing classes also shows gains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46]</ref>. Our work studies the benefits of multi-task training for stuff and thing segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Panoptic Feature Pyramid Network</head><p>Our approach, Panoptic FPN, is a simple, single-network baseline whose goal is to achieve top performance on both instance and semantic segmentation, and their joint task: panoptic segmentation <ref type="bibr" target="#b29">[30]</ref>. Our design principle is to start from Mask R-CNN with FPN, a strong instance segmentation baseline, and make minimal changes to also generate a semantic segmentation dense-pixel output (see <ref type="figure" target="#fig_1">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>Feature Pyramid Network: We begin by briefly reviewing FPN <ref type="bibr" target="#b35">[36]</ref>. FPN takes a standard network with features at multiple spatial resolutions (e.g., ResNet <ref type="bibr" target="#b24">[25]</ref>), and adds a light top-down pathway with lateral connections, see Instance segmentation branch: The design of FPN, and in particular the use of the same channel dimension for all pyramid levels, makes it easy to attach a region-based object detector like Faster R-CNN <ref type="bibr" target="#b47">[48]</ref>. Faster R-CNN performs region of interest (RoI) pooling on different pyramid levels and applies a shared network branch to predict a refined box and class label for each region. To output instance segmentations, we use Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>, which extends Faster R-CNN by adding an FCN branch to predict a binary segmentation mask for each candidate region, see <ref type="figure" target="#fig_1">Figure 1b</ref>.</p><p>Panoptic FPN: As discussed, our approach is to modify Mask R-CNN with FPN to enable pixel-wise semantic segmentation prediction. However, to achieve accurate predictions, the features used for this task should: (1) be of suitably high resolution to capture fine structures, (2) encode sufficiently rich semantics to accurately predict class labels, and (3) capture multi-scale information to predict stuff regions at multiple resolutions. Although FPN was designed for object detection, these requirements -high-resolution, rich, multi-scale features -identify exactly the characteristics of FPN. We thus propose to attach to FPN a simple and fast semantic segmentation branch, described next. Semantic segmentation branch: To generate the semantic segmentation output from the FPN features, we propose a simple design to merge the information from all levels of the FPN pyramid into a single output. It is illustrated in detail in <ref type="figure" target="#fig_4">Figure 3</ref>. Starting from the deepest FPN level (at 1/32 scale), we perform three upsampling stages to yield a feature map at 1/4 scale, where each upsampling stage consists of 3√ó3 convolution, group norm <ref type="bibr" target="#b53">[54]</ref>, ReLU, and 2√ó bilinear upsampling. This strategy is repeated for FPN scales 1/16, 1/8, and 1/4 (with progressively fewer upsampling stages). The result is a set of feature maps at the same 1/4 scale, which are then element-wise summed. A final 1√ó1 convolution, 4√ó bilinear upsampling, and softmax are used to generate the per-pixel class labels at the original image resolution. In addition to stuff classes, this branch also outputs a special 'other' class for all pixels belonging to objects (to avoid predicting stuff classes for such pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>We use a standard FPN configuration with 256 output channels per scale, and our semantic segmentation branch reduces this to 128 channels. For the (pre-FPN) backbone, we use ResNet/ResNeXt <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b54">55]</ref> models pre-trained on ImageNet <ref type="bibr" target="#b49">[50]</ref> using batch norm (BN) <ref type="bibr" target="#b27">[28]</ref>. When used in fine-tuning, we replace BN with a fixed channel-wise affine transformation, as is typical <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference and Training</head><p>Panoptic inference: The panoptic output format <ref type="bibr" target="#b29">[30]</ref> requires each output pixel to be assigned a single class label (or void) and instance id (the instance id is ignored for stuff classes). As the instance and semantic segmentation outputs from Panoptic FPN may overlap; we apply the simple postprocessing proposed in <ref type="bibr" target="#b29">[30]</ref> to resolve all overlaps. This post-processing is similar in spirit to non-maximum suppression and operates by: (1) resolving overlaps between different instances based on their confidence scores, (2) resolving overlaps between instance and semantic segmentation outputs in favor of instances, and (3) removing any stuff regions labeled 'other' or under a given area threshold.  <ref type="figure">Figure 4</ref>: Backbone architecture efficiency. We compare methods for increasing feature resolution for semantic segmentation, including dilated networks, symmetric decoders, and FPN, see <ref type="figure" target="#fig_5">Figure 5</ref>. We count multiply-adds and memory used when applying ResNet-101 to a 2 megapixel image. FPN at output scale 1/4 is similar computationally to dilation-16 (1/16 resolution output), but produces a 4√ó higher resolution output. Increasing resolution to 1/8 via dilation uses a further ‚àº3√ó more compute and memory.</p><p>Joint training: During training the instance segmentation branch has three losses <ref type="bibr" target="#b23">[24]</ref>: L c (classification loss), L b (bounding-box loss), and L m (mask loss). The total instance segmentation loss is the sum of these losses, where L c and L b are normalized by the number of sampled RoIs and L m is normalized by the number of foreground RoIs. The semantic segmentation loss, L s , is computed as a per-pixel cross entropy loss between the predicted and the ground-truth labels, normalized by the number of labeled image pixels.</p><p>We have observed that the losses from these two branches have different scales and normalization policies. Simply adding them degrades the final performance for one of the tasks. This can be corrected by a simple loss re-weighting between the total instance segmentation loss and the semantic segmentation loss. Our final loss is thus:</p><formula xml:id="formula_0">L = Œª i (L c + L b + L m ) + Œª s L s .</formula><p>By tuning Œª i and Œª s it is possible to train a single model that is comparable to two separate task-specific models, but at about half the compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>Our motivation for predicting semantic segmentation using FPN is to create a simple, single-network baseline that can perform both instance and semantic segmentation. However, it is also interesting to consider the memory and computational footprint of our approach relative to model architectures popular for semantic segmentation. The most common designs that produce high-resolution outputs are dilated convolution <ref type="figure" target="#fig_5">(Figure 5b</ref>) and symmetric encoderdecoder models that have a mirror image decoder with lateral connections <ref type="figure" target="#fig_5">(Figure 5c</ref>). While our primary motivation is compatibility with Mask R-CNN, we note that FPN is much lighter than a typically used dilation-8 network, ‚àº2√ó more efficient than the symmetric encoder-decoder, and roughly equivalent to a dilation-16 network (while producing a 4√ó higher resolution output). See <ref type="figure">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our goal is to demonstrate that our approach, Panoptic FPN, can serve as a simple and effective single-network baseline for instance segmentation, semantic segmentation, and their joint task of panoptic segmentation <ref type="bibr" target="#b29">[30]</ref>. For instance segmentation, this is expected, since our approach extends Mask R-CNN with FPN. For semantic segmentation, as we simply attach a lightweight dense-pixel prediction branch <ref type="figure" target="#fig_4">(Figure 3)</ref> to FPN, we need to demonstrate it can be competitive with recent methods. Finally, we must show that Panoptic FPN can be trained in a multi-task setting without loss in accuracy on the individual tasks.</p><p>We therefore begin our analysis by testing our approach for semantic segmentation (we refer to this single-task variant as <ref type="figure">Semantic FPN)</ref>. Surprisingly, this simple model achieves competitive semantic segmentation results on the COCO <ref type="bibr" target="#b36">[37]</ref> and Cityscapes <ref type="bibr" target="#b13">[14]</ref> datasets. Next, we analyze the integration of the semantic segmentation branch with Mask R-CNN, and the effects of joint training. Lastly, we show results for panoptic segmentation, again on COCO and Cityscapes. Qualitative results are shown in Figures 2 and 6. We describe the experimental setup next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>COCO: The COCO dataset <ref type="bibr" target="#b36">[37]</ref> was developed with a focus on instance segmentation, but more recently stuff annotations were added <ref type="bibr" target="#b5">[6]</ref>. For instance segmentation, we use the 2017 data splits with 118k/5k/20k train/val/test images and 80 thing classes. For semantic segmentation, we use the 2017 stuff data with 40k/5k/5k splits and 92 stuff classes. Finally, panoptic segmentation <ref type="bibr" target="#b29">[30]</ref> uses all 2017 COCO images with 80 thing and 53 stuff classes annotated.</p><p>Cityscapes: Cityscapes <ref type="bibr" target="#b13">[14]</ref> is an ego-centric street-scene dataset. It has 5k high-resolution images (1024√ó2048 pixels) with fine pixel-accurate annotations: 2975 train, 500 val, and 1525 test. An additional 20k images with coarse annotations are available, we do not use these in our experiments. There are 19 classes, 8 with instance-level masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-task metrics:</head><p>We report standard semantic and instance segmentation metrics for the individual tasks using evaluation code provided by each dataset. For semantic segmentation, the mIoU (mean Intersection-over-Union) <ref type="bibr" target="#b17">[18]</ref> is the primary metric on both COCO and Cityscapes. We also report fIoU (frequency weighted IoU) on COCO <ref type="bibr" target="#b5">[6]</ref> and iIoU (instance-level IoU) on Cityscapes <ref type="bibr" target="#b13">[14]</ref>. For instance segmentation, AP (average precision averaged over categories and IoU thresholds) <ref type="bibr" target="#b36">[37]</ref> is the primary metric and AP 50 and AP 75 are selected supplementary metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Panoptic segmentation metrics:</head><p>We use PQ (panoptic quality) as the default metric to measure Panoptic FPN performance, for details see <ref type="bibr" target="#b29">[30]</ref>. PQ captures both recognition and segmentation quality, and treats both stuff and thing categories in a unified manner. This single, unified metric allows us to directly compare methods. Additionally, we use PQ St and PQ Th to report stuff and thing performance separately. Note that PQ is used to evaluate Panoptic FPN predictions after the post-processing merging procedure is applied to the outputs of the semantic and instance branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO training:</head><p>We use the default Mask R-CNN 1√ó training setting <ref type="bibr" target="#b22">[23]</ref> with scale jitter (shorter image side in [640, 800]). For semantic segmentation, we predict 53 stuff classes plus a single 'other' class for all 80 thing classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cityscapes training:</head><p>We construct each minibatch from 32 random 512√ó1024 image crops (4 crops per GPU) after randomly scaling each image by 0.5 to 2.0√ó. We train for 65k iterations starting with a learning rate of 0.01 and dropping it by a factor of 10 at 40k and 55k iterations. This differs from the original Mask R-CNN setup <ref type="bibr" target="#b23">[24]</ref> but is effective for both instance and semantic segmentation. For the largest backbones for semantic segmentation, we perform color augmentation <ref type="bibr" target="#b39">[40]</ref> and crop bootstrapping <ref type="bibr" target="#b4">[5]</ref>. For semantic segmentation, predicting all thing classes, rather than a single 'other' label, performs better (for panoptic inference we discard these predictions). Due to the high variance of the mIoU (up to 0.4), we report the median performance of 5 trials of each experiment on Cityscapes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">FPN for Semantic Segmentation</head><p>Cityscapes: We start by comparing our baseline Semantic FPN to existing methods on the Cityscapes val split in <ref type="table" target="#tab_3">Table 1a</ref>. We compare to recent top-performing methods, but not to competition entires which typically use ensembling, COCO pre-training, test-time augmentation, etc.</p><p>Our approach, which is a minimal extension to FPN, is able to achieve strong results compared to systems like DeepLabV3+ <ref type="bibr" target="#b11">[12]</ref>, which have undergone many design iterations. In terms of compute and memory, Semantic FPN is lighter than typical dilation models, while yielding higher resolution features (see <ref type="figure">Fig. 4</ref>). We note that adding dilation into FPN could potentially yield further improvement but is outside the scope of this work. Moreover, in our baseline we deliberately avoid orthogonal architecture improvements like Non-local <ref type="bibr" target="#b52">[53]</ref> or SE <ref type="bibr" target="#b26">[27]</ref>, which would likely yield further gains. Overall, these results demonstrate that our approach is a strong baseline for semantic segmentation.</p><p>COCO: An earlier version of our approach won the 2017 COCO-Stuff challenge. Results are reported in <ref type="table" target="#tab_3">Table 1b</ref>. As this was an early design, the the semantic branch differed slightly (each upsampling module had two 3√ó3 conv layers and ReLU before bilinear upscaling to the final resolution, and features were concatenated instead of summed, please compare with <ref type="figure" target="#fig_4">Figure 3</ref>). As we will show in the ablations shortly, results are fairly robust to the exact branch design. Our competition entry was trained with color augmentation <ref type="bibr" target="#b39">[40]</ref> and at test time balanced the class distribution and used multi-scale inference. Finally, we note that at the time we used a training schedule specific to semantic segmentation similar to our Cityscapes schedule (but with double learning rate and halved batch size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations:</head><p>We perform a few ablations to analyze our proposed semantic segmentation branch (shown in <ref type="figure" target="#fig_4">Figure 3</ref>). For consistency with further experiments in our paper, we use stuff annotations from the COCO Panoptic dataset (which as discussed differ from those used for the COCO Stuff competition). <ref type="table" target="#tab_3">Table 1c</ref> shows ResNet-50 Semantic FPN with varying number of channels in the semantic branch. We found that 128 strikes a good balance between accuracy and efficiency. In <ref type="table" target="#tab_3">Table 1d</ref> we compare elementwise sum and concatenation for aggregating feature maps from different FPN levels. While accuracy for both is comparable, summation is more efficient. Overall we observe that the simple architecture of the new dense-pixel labelling branch is robust to exact design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Task Training</head><p>Single-task performance of our approach is quite effective; for semantic segmentation the results in the previous section demonstrate this, for instance segmentation this is known as we start from Mask R-CNN. However, can we jointly train for both tasks in a multi-task setting?</p><p>To combine our semantic segmentation branch with the instance segmentation branch in Mask R-CNN, we need to determine how to train a single, unified network. Previous work demonstrates that multi-task training is often challenging and can lead to degraded results <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>. We likewise observe that for semantic or instance segmentation, adding the secondary task can degrade the accuracy in comparison with the single-task baseline.</p><p>In <ref type="table" target="#tab_5">Table 2</ref> we show that with ResNet-50-FPN, using a simple loss scaling weight on the semantic segmentation loss, Œª s , or instance segmentation loss, Œª i , we can obtain a re-weighting that improves results over single-task baselines. Specifically, adding a semantic segmentation branch with the proper Œª s improves instance segmentation, and vice-versa. This can be exploited to improve single-task results. However, our main goal is to solve both tasks simultaneously, which we explore in the next section.  Adding a semantic segmentation branch can slightly improve instance segmentation results over a single-task baseline with properly tuned Œª s (results bolded). Note that Œª s indicates the weight assigned to the semantic segmentation loss and Œª s = 0.0 serves as the single-task baseline. (c,d) Adding an instance segmentation branch can provide even stronger benefits for semantic segmentation over a single-task baseline with properly tuned Œª i (results bolded). As before, Œª i indicates the weight assigned to the instance segmentation loss and Œª i = 0.0 serves as the single-task baseline. While promising, we are more interested in the joint task, for which results are shown in <ref type="table" target="#tab_7">Table 3</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Panoptic FPN</head><p>We now turn to our main result: testing Panoptic FPN for the joint task of panoptic segmentation <ref type="bibr" target="#b29">[30]</ref>, where the network must jointly and accurately output stuff and thing segmentations. For the following experiments, for each setting we select the optimal Œª s and Œª i from {0.5, 0.75, 1.0}, ensuring that results are not skewed by fixed choice of Œª's. <ref type="table" target="#tab_7">Table 3a</ref> we compare two networks trained separately to Panoptic FPN with a single backbone. Panoptic FPN yields comparable accuracy but with roughly half the compute (the backbone dominates compute, so the reduction is almost 50%). We also balance computational budgets by comparing two separate networks with ResNet-50 backbones each and Panoptic FPN with ResNet-101, see <ref type="table" target="#tab_7">Table 3b</ref>. Using roughly equal computational budget, Panoptic FPN significantly outperforms two separate networks. Taken together, these results demonstrate that the joint approach is strictly beneficial, and that our Panoptic FPN can serve as a solid baseline for the joint task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main results: In</head><p>Ablations: We perform additional ablations on Panoptic FPN with ResNet-50. First, by default, we combine the instance and semantic losses together during each gradient update. A different strategy is to alternate the losses on each iteration (this may be useful as different augmentation strategies can be used for the two tasks). We compare these two options in <ref type="table" target="#tab_7">Table 3c</ref>; the combined loss demonstrates better performance. Next, in <ref type="table" target="#tab_7">Table 3d</ref> we compare with an architecture where FPN channels are grouped into two sets, and each task uses one of the two features sets as its input. While the results are mixed, we expect more sophisticated multi-task approaches could give stronger gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons:</head><p>We conclude by comparing Panoptic FPN with existing methods. For these experiments, we use Panoptic FPN with a ResNet-101 backbone and without bells-and-whistles. In <ref type="table" target="#tab_8">Table 4a</ref> we show that Panoptic FPN substantially outperforms all single-model entries in the recent COCO Panoptic Segmentation Challenge. This establishes a new baseline for the panoptic segmentation task. On Cityscapes, we compare Panoptic FPN with an approach for panoptic segmentation recently proposed in <ref type="bibr" target="#b0">[1]</ref> in <ref type="table" target="#tab_8">Table 4b</ref>. Panoptic FPN outperforms [1] by a 4.3 point PQ margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a conceptually simple yet effective baseline for panoptic segmentation. The method starts with Mask R-CNN with FPN and adds to it a lightweight semantic segmentation branch for dense-pixel prediction. We hope it can serve as a strong foundation for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Instance Segmentation Branch (c) Semantic Segmentation Branch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Panoptic FPN: (a) We start with an FPN backbone</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Panoptic FPN results on COCO (top) and Cityscapes (bottom) using a single ResNet-101-FPN network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 1a. The top-down pathway starts from the deepest layer of the network and progressively upsamples it while adding in transformed versions of higher-resolution features from the bottom-up pathway. FPN generates a pyramid, typically with scales from 1/32 to 1/4 resolution, where each pyramid level has the same channel dimension (256 by default).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Semantic segmentation branch. Each FPN level (left) is upsampled by convolutions and bilinear upsampling until it reaches 1/4 scale (right), theses outputs are then summed and finally transformed into a pixel-wise output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Backbone architectures for increasing feature resolution. (a) A standard convolutional network (dimensions are denoted as #blocks√ó#channels√óresolution). (b) A common approach is to reduce the stride of select convolutions and use dilated convolutions after to compensate. (c) A U-Net<ref type="bibr" target="#b48">[49]</ref> style network uses a symmetric decoder that mirrors the bottom-up pathway, but in reverse. (d) FPN can be seen as an asymmetric, lightweight decoder whose top-down pathway has only one block per stage and uses a shared channel dimension. For a comparison of the efficiency of these models, please seeFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Cityscapes Semantic FPN. Performance is reported on the val set and all methods use only fine Cityscapes annotations for training. The backbone notation includes the dilated resolution 'D' (note that<ref type="bibr" target="#b11">[12]</ref> uses both dilation and an encoder-decoder backbone). All top-performing methods other than ours use dilation. FLOPs (multiply-adds √ó10 12 ) and memory (# activations √ó10 9 ) are approximate but informative. For these larger FPN models we train with color and crop augmentation. Our baseline is comparable to state-of-the-art methods in accuracy and efficiency.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">backbone</cell><cell cols="3">mIoU FLOPs memory</cell></row><row><cell cols="2">DeeplabV3 [11]</cell><cell cols="2">ResNet-101-D8</cell><cell></cell><cell>77.8</cell><cell>1.9</cell><cell>1.9</cell></row><row><cell cols="2">PSANet101 [59]</cell><cell cols="2">ResNet-101-D8</cell><cell></cell><cell>77.9</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell>Mapillary [5]</cell><cell></cell><cell cols="3">WideResNet-38-D8</cell><cell>79.4</cell><cell>4.3</cell><cell>1.7</cell></row><row><cell cols="2">DeeplabV3+ [12]</cell><cell cols="2">X-71-D16</cell><cell></cell><cell>79.6</cell><cell>0.5</cell><cell>1.9</cell></row><row><cell cols="2">Semantic FPN</cell><cell cols="3">ResNet-101-FPN</cell><cell>77.7</cell><cell>0.5</cell><cell>0.8</cell></row><row><cell cols="2">Semantic FPN</cell><cell cols="3">ResNeXt-101-FPN</cell><cell>79.1</cell><cell>0.8</cell><cell>1.4</cell></row><row><cell cols="5">(a) backbone</cell><cell></cell><cell>mIoU</cell><cell>fIoU</cell></row><row><cell>Vllab [13]</cell><cell></cell><cell cols="4">Stacked Hourglass</cell><cell>12.4</cell><cell>38.8</cell></row><row><cell cols="3">DeepLab VGG16 [10]</cell><cell cols="2">VGG-16</cell><cell></cell><cell>20.2</cell><cell>47.5</cell></row><row><cell>Oxford [4]</cell><cell></cell><cell cols="3">ResNeXt-101</cell><cell></cell><cell>24.1</cell><cell>50.6</cell></row><row><cell>G-RMI [19]</cell><cell></cell><cell cols="4">Inception ResNet v2</cell><cell>26.6</cell><cell>51.9</cell></row><row><cell cols="2">Semantic FPN</cell><cell cols="4">ResNeXt-152-FPN</cell><cell>28.8</cell><cell>55.7</cell></row><row><cell cols="4">Width Cityscapes COCO</cell><cell cols="3">Aggr. Cityscapes COCO</cell></row><row><cell>64</cell><cell>74.1</cell><cell>39.6</cell><cell></cell><cell cols="2">Sum</cell><cell>74.5</cell><cell>40.2</cell></row><row><cell>128</cell><cell>74.5</cell><cell>40.2</cell><cell></cell><cell cols="2">Concat</cell><cell>74.4</cell><cell>39.9</cell></row><row><cell>256</cell><cell>74.6</cell><cell>40.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(c) Ablation (mIoU): Channel width</cell><cell cols="3">(d) Ablation (mIoU): Sum aggre-</cell></row><row><cell cols="4">of 128 for the features in the seman-</cell><cell cols="3">gation of the feature maps in the</cell></row><row><cell cols="4">tic branch strikes a good balance be-</cell><cell cols="3">semantic branch is marginally bet-</cell></row><row><cell cols="3">tween accuracy and efficiency.</cell><cell></cell><cell cols="3">ter and is more efficient.</cell></row></table><note>(b) COCO-Stuff 2017 Challenge results. We submitted an early version of Semantic FPN to the 2017 COCO Stuff Segmentation Challenge held at ECCV (http://cocodataset.org/#stuff-2017). Our en- try won first place without ensembling, and we outperformed competing methods by at least a 2 point margin on all reported metrics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Semantic Segmentation using FPN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Multi-Task Training: (a,b)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Panoptic Segmentation: Panoptic R50-FPN vs. R50-FPN√ó2. Using a single FPN network for solving both tasks simultaneously yields comparable accuracy to two independent FPN networks for instance and semantic segmentation, but with roughly half the compute. Panoptic Segmentation: Panoptic R101-FPN vs. R50-FPN√ó2.Given a roughly equal computational budget, a single FPN network for the panoptic task outperforms two independent FPN networks for instance and semantic segmentation by a healthy margin. Training Panoptic FPN. During training, for each minibatch we can either combine the semantic and instances loss or we can alternate which loss we compute (in the latter case we train for twice as long). We find that combining the losses in each minibatch performs much better. Grouped FPN. We test a variant of Panoptic FPN where we group the 256 FPN channels into two sets and apply the instance and semantic branch to its own dedicated group of 128. While this gives mixed gains, we expect better multi-task strategies can improve results.</figDesc><table><row><cell>backbone R50-FPN√ó2 33.9 46.6 AP PQ Th mIoU PQ St PQ 40.2 27.9 39.2 R50-FPN 33.3 45.9 41.0 28.7 39.0 -0.6 -0.7 +0.8 +0.8 -0.2 R50-FPN√ó2 32.2 51.3 74.5 62.4 57.7 R50-FPN 32.0 51.6 75.0 62.2 57.7 -0.2 +0.3 +0.5 -0.2 +0.0 (a) backbone COCO Cityscapes COCO R50-FPN√ó2 33.9 46.6 AP PQ Th mIoU PQ St PQ 40.2 27.9 39.2 R101-FPN 35.2 47.5 42.1 29.5 40.3 +1.3 +0.9 +1.9 +1.6 +1.1 Cityscapes R50-FPN√ó2 32.2 51.3 74.5 62.4 57.7 R101-FPN 33.0 52.0 75.7 62.5 58.1 +0.8 +0.7 +1.3 +0.1 +0.4 (b) loss AP PQ Th mIoU PQ St PQ COCO alternate 31.7 43.9 40.2 28.0 37.5 combine 33.3 45.9 41.0 28.7 39.0 +1.6 +2.0 +0.8 +0.7 +1.5 Cityscapes alternate 32.0 51.4 74.3 61.3 57.4 combine 32.0 51.6 75.0 62.2 57.7 +0.0 +0.2 +0.7 +0.9 +0.3 AP PQ Th mIoU PQ St PQ COCO original 33.3 45.9 41.0 28.7 39.0 grouped 33.1 45.7 41.2 28.4 38.8 -0.2 -0.2 +0.2 -0.3 -0.2 Cityscapes original 32.0 51.6 75.0 62.2 57.7 grouped 32.0 51.8 75.3 61.7 57.5 +0.0 +0.2 +0.3 -0.5 -0.2 (c) FPN (d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Panoptic FPN Results. Panoptic Segmentation on COCO test-dev. We submit Panoptic FPN to the COCO test-dev leaderboard (for details on competing entries, please seehttp://cocodataset.org/#panoptic-leaderboard). We only compare to entires that use a single network for the joint task. We do not compare to competition-level entires that utilize ensembling (including methods that ensemble separate networks for semantic and instance segmentation). For methods that use one network for panoptic segmentation, our approach improves PQ by an ‚àº9 point margin. Panoptic Segmentation on Cityscapes. For Cityscapes, there is no public leaderboard for panoptic segmentation at this time. Instead, we compare on val to the recent work of Arnab and Torr<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> who develop a novel approach for panoptic segmentation, named DIN. DIN is representative of alternatives to region-based instance segmentation that start with a pixelwise semantic segmentation and then perform grouping to extract instances (see the related work). Panoptic FPN, without extra coarse training data or any bells and whistles, outperforms DIN by a 4.3 point PQ margin.</figDesc><table><row><cell cols="2">Figure 6: More Panoptic FPN results on COCO (top) and Cityscapes (bottom) using a single ResNet-101-FPN network.</cell></row><row><cell>PQ 16.9 26.2 27.2 32.1 40.9 (a) coarse PQ Th PQ St Artemis 16.8 17.0 LeChen 31.0 18.9 MPS-TU Eindhoven [16] 29.6 23.4 MMAP-seg 38.9 22.0 Panoptic FPN 48.3 29.7 DIN [1, 34] Panoptic FPN (b)</cell><cell>PQ PQ Th PQ St mIoU AP 53.8 42.5 62.1 80.1 28.6 58.1 52.0 62.5 75.7 33.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of ResNet-101 Panoptic FPN to the state of the art.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">COCO-Stuff 2017 Challenge: Oxford Active Vision Lab team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul√≤</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Triply supervised decoder networks for joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09299</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MaskLab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">COCO-Stuff 2017 Challenge: Vllab team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BlitzNet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">COCO-Stuff 2017 Challenge: G-RMI team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">InstanceCut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">UberNet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01192</idno>
		<title level="m">Learning to fuse things and stuff</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SGN: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bul√≤</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">BiSeg: Simultaneous instance segmentation and semantic segmentation with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
