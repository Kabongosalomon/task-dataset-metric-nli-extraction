<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive Neural Network for Named Entity Recognition in Vietnamese</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">FPT Technology Research Institute</orgName>
								<orgName type="department" key="dep2">FPT Technology Research Institute</orgName>
								<orgName type="institution">FPT University Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">Anh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">FPT University Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attentive Neural Network for Named Entity Recognition in Vietnamese</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-named entity recognition</term>
					<term>neural network</term>
					<term>con- ditional random fields</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We propose an attentive neural network for the task of named entity recognition in Vietnamese. The proposed attentive neural model makes use of character-based language models and word embeddings to encode words as vector representations. A neural network architecture of encoder, attention, and decoder layers is then utilized to encode knowledge of input sentences and to label entity tags. The experimental results show that the proposed attentive neural network achieves the state-of-the-art results on the benchmark named entity recognition datasets in Vietnamese in comparison to both hand-crafted features based models and neural models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Named entity recognition (NER) is one of fundamental sequence labeling tasks as well as other tasks such as word segmentation, part-of-speech (POS) tagging, or noun phrase chunking. The NER task aims to identify named entities in the given texts and then to assign named entities to particular entity types such as location, organization or person name. NER task plays a crucial role in natural language understanding and downstream applications such as relation extraction, entity linking, question answering, or machine translation.</p><p>In the previous studies, NER approaches make use of linear statistical models to label entity tags such as hidden Markov models (HMM), maximum entropy models (ME), or conditional random fields (CRF) ( <ref type="bibr" target="#b0">[1]</ref>). However, most those kinds of models rely heavily on hand-crafted features and taskspecific resources, leading that those models are difficult to adapt to new tasks or to shift to new domains. For example, in English, orthographic features and external resources of gazetteers are commonly used in NER task. For Vietnamese, the approach in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> used the information of word, word shapes, part-of-speech tags, chunking tags as hand-crafted features for CRF to label entity tags.</p><p>In the past few years, neural networks for NER have been proposed to deal with drawbacks of statistical-based NER models by extracting automatically features instead creating heavily hand-crafted features. Neural architectures for NER often make use of the combination of either recurrent neural network (RNN) and CRF or convolution neural network (CNN) and CRF to extract automatically information from the inputs and detect NER labels. Reference <ref type="bibr" target="#b3">[4]</ref>, among others, proposed a neural architecture by using recurrent neural network with long short-term memory units (LSTM) ( <ref type="bibr" target="#b4">[5]</ref>) and CRF to label NER tags. Moreover, the combination of bidirectional LSTM, CNN, and CRF is introduced to obtain benefits from both word-and character-level representations automatically for detecting NER labels as in <ref type="bibr" target="#b5">[6]</ref>. Recently, as in <ref type="bibr" target="#b6">[7]</ref>, a combination of language model (LM), LSTM, and CRF is used to extract knowledge from raw texts and empower the sequence labeling task including NER task. For Vietnamese, a non-hand-crafted feature based model which is combination of LSTM, CNN, and CRF is applied to solve the task of Vietnamese NER as in <ref type="bibr" target="#b7">[8]</ref>. Moreover, ZA-NER model ( <ref type="bibr" target="#b8">[9]</ref>) which is based on a combination of bidirectional LSTM and CRF is proposed to extract named entities.</p><p>In this paper, we introduce an attentive neural network (VNER) for Vietnamese NER task without using any handcrafted features or task-specific resources. In the proposed neural network, we incorporate a neural language model to encode the character-based words. Similar to <ref type="bibr" target="#b6">[7]</ref>, the prediction of the next character in the language model is adapted to predict the next word. Moreover, the pre-trained word embeddings are also utilized to extract knowledge from word level. The concatenation of character-based word and pre-trained word embedding is then used as the vector representation of a wordor token-layer. A bidirectional LSTM is then applied as an encoder layer to encode the knowledge of the input sentence. We then make use of a LSTM as a decoder together with an attention mechanism to decode the outputs of encoder layer. Finally, a CRF layer is used to model context dependencies and entity labels.</p><p>For the experiment, we evaluate the VNER model on two benchmark datasets of Vietnamese NER task which are VLSP-2016 ( <ref type="bibr" target="#b9">[10]</ref>) and VLSP-2018 1 NER datasets. The experimental results show that the VNER model achieves the state-of-theart results compared to both hand-crafted based models and neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Named entity recognition is a fundamental NLP research problem that has been studied for years. In the literature, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kate</head><p>Moss proposed approaches for NER task can be divided into two types: the first type is based on linear statistical approaches and the second type is based on neural models. In the first approach, based-statistical NER systems have been dominated for years. These systems rely on a pre-defined set of hand-crafted features such as lemma, word embeddings, semantic dictionary, word-shape, POS tags, or chunking tags. Each sentence is represented as a set of features and then fed into a linear model such as HMM, ME, or CRF to label entity tags for each word or token. In comparison with neural models, this type of NER systems is straightforward and requires less resources. In addition, these system are also proved to work well for low-resource languages such as Vietnamese. However, these kinds of NER systems are relied heavily on the feature set use, and on hand-crafted features that are expensive to construct and are difficultly reusable.</p><formula xml:id="formula_0">v√† Naomi Campbell B-PER I-PER O B-PER I-PER</formula><p>For the second approach, thanks to the recent advancements in computing technology, neural-based models have emerged as a powerful tools for a number of research problems, including sequence labeling tasks. Neural-based NER systems are end-to-end systems that require no exclusively pre-defined features. Those proposed models are based on complex deep learning architecture such as RNN, LSTM, or CNN ( <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>). Word embeddings and/or character embeddings are often used to represent the semantic relations of words or characters. Other information such as POS tags or chunking tags are also used to provide additional syntactic information. Sentences are represented as vector representations and fed into variety architectures of deep neural networks to encode knowledge from them. A CRF layer then can be used on top to infer entity tags for words or tokens. Consequently, neural models are easy to adapt to new domains and can achieve state-ofthe-art results on many sequence labeling problems. However, due to this type of models that is quite complex, these models require large training data and take time for training.</p><p>There have been considerable work proposed by Vietnamese researchers in solving the NER problem such as dynamic feature induction model ( <ref type="bibr" target="#b10">[11]</ref>), CRF model ( <ref type="bibr" target="#b1">[2]</ref>), or LSTM ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>). CRF-based model achieves state-of-the-art results on the VLSP 2016 and VLSP 2018 competitions; however, it still suffers from the linear statistical model drawbacks as mentioned above. Our proposed attentive neural network has similar architecture as the ones mentioned in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> with additional highway layers for enhancing word embeddings and character embeddings at run time. Moreover, an attention mechanism is used to further improve the system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VNER: AN ATTENTIVE NEURAL NETWORK FOR VIETNAMESE NAMED ENTITY RECOGNITION</head><p>In this section, we describe the components (layers) in the architecture of VNER model. The neural architecture of the proposed model is visualized in <ref type="figure" target="#fig_1">Figure 1</ref>. The VNER model includes the layers of word, encoder, attention, and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Word Layer</head><p>As mentioned in Section I, a word is the concatenation of both character-and word-based vector representations. The visualization of word layer is illustrated in <ref type="figure">Figure 2</ref>. Specifically, for the character-based word, we adopt characterlevel layer in <ref type="bibr" target="#b6">[7]</ref> to represent the vector representation of character-based word by using the neural language model. The character-level language models are trained on unannotated sequence of the input sentence. Furthermore, we make use of two LSTM networks to model the sequence of characters in both forward and backward directions. For each word in the input sentence, the prediction of all characters in each word is the next word, helping to capture better lexical information of the next word rather than its spelling. In addition, we employ four highway units ( <ref type="bibr" target="#b11">[12]</ref>) to enhance the output's performance of language model. Specifically, the highway units compute non-linear transformation as follows:</p><formula xml:id="formula_1">O = H(x) = t Œ¶(W H x + b H )<label>(1)</label></formula><p>where Œ¶ is non-linear activation function, is the elementwise product, and </p><formula xml:id="formula_2">t = œÉ(W T x + b T ) is</formula><formula xml:id="formula_3">w i = [ E i ; O N fi ; O N bi‚àí1 ]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder Layer</head><p>In order to capture the information of whole input sequence, we make use of bidirectional LSTM networks to encode an input sequence. Precisely, for a given input sentence x = (x 1 , x 2 , ..., x n ) containing n words. A forward LSTM network aims to encode the input sequence x from the start to the end of the input sequence and to generate a hidden representation h f t at every word t; and a backward LSTM network encodes the input sequence from the end to start of the input sequence and computes a hidden representation h b t at every word t.</p><p>The representation of a word w t in the input sentence x using the bidirectional LSTM encoder is then obtained by concatenating its forward hidden representation and backward hidden representation as follows:</p><formula xml:id="formula_4">h t = [ h f t ; h b t ]<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Layer</head><p>The attention mechanism has gained popularity in recent years in training neural networks. Reference <ref type="bibr" target="#b12">[13]</ref> proposed and successfully applied attention mechanism to jointly translate and align words for the task of neural machine translation. Ideally, attention mechanism is often applied in between the layers of encoder and decoder, aiming to selectively focus on parts of encoder layer's outputs corresponding to time step in the decoder layer. Specifically, the attention layer first takes the outputs of encoder layer as inputs to compute probability distribution of encoder's outputs for each word w t at the time step t of decoder layer as follows:</p><formula xml:id="formula_5">score(h t , ¬Ø h e ) = h t ¬∑ ¬Ø h T e (4) Œ± t (e) = exp(score(h t , ¬Ø h e )) e exp(score(h t , ¬Ø h e ))<label>(5)</label></formula><formula xml:id="formula_6">c t = e Œ± t (e) ¬Ø h e<label>(6)</label></formula><p>in which h t is hidden state of decoder layer at time step t; Œ± t (e) is the attention weights according to the hidden states of encoder layer ¬Ø h e and h t ; and c t is the context vector of attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Decoder Layer</head><p>This layer aims to decode and label entity tags which are dependency tags. Therefore, it is beneficial to observe the relationships between entity tags in neighborhoods and jointly decode the highest probability of entity tags for a given input sentence. For example, it is meaningless to label I-PER after I-ORG in the NER task with BIO annotation. To do so, firstly, we make use of a LSTM network which is considered as a decoder to process the outputs of encoder and attention layers. Given the outputs of encoder layer h e = [h e1 , h e2 , ..., h en ], the input of decoder layer at each time step t is the concatenation of three elements which are the hidden state h et of encoder layer, the previous state of decoder layer h dt‚àí1 , and the context vector of attention layer c t as follows:</p><formula xml:id="formula_7">h dt = [h et ; h dt‚àí1 ; c t ]<label>(7)</label></formula><p>Secondly, we apply CRF model to the outputs of the decoder to model dependency tags. Formally, as in <ref type="bibr" target="#b5">[6]</ref>, we use z = (z 1 , z 2 , ..., z n ) to represent the output of the decoder in which z i is the vector representation of ith word in the input sentence; Y(z) stands for the set of all possible sequences for z; and y = (y 1 , y 2 , ..., y n ) represents the sequence of labels for z. The probabilities of possible label sequences y given z are defined as follows:</p><formula xml:id="formula_8">p(y|z; W, b) = n i=1 œà i (y i‚àí1 , y i , z) y ‚ààY(z) n i=1 œà i (y i‚àí1 , y i , z)<label>(8)</label></formula><p>where œà i (y , y, z) = exp(W T y ,y z i + b y ,y ) are potential functions; and W T y ,y and b y ,y are weight matrix and bias corresponding to the label pair (y , y), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Joint Training</head><p>As mentioned in Section III-A, in the VNER model, we make use of both forward and backward neural language models to jointly learn character-based word embeddings. Specifically, due to both neural language models which consider the predictions to words and utilize the character sequence as the inputs, the probabilities for the models of both forward p f and backward p b to generate words are defined as follows:</p><formula xml:id="formula_9">p f (x 1 , ..., x n ) = n i=1 p f (x i |c 0, , ..., c i‚àí1, )<label>(9)</label></formula><formula xml:id="formula_10">p b (x 1 , ..., x n ) = n i=1 p b (x i |c i+1, , ..., c n, )<label>(10)</label></formula><p>where x i is the ith word; and c i,j is the jth character of the ith word. By combining the equations of 8, 9, and 10, the objective function of joint model used to label NER tags can be defined as the following equation:</p><formula xml:id="formula_11">J = ‚àí i p(y i |z i ) + Œª(log p f (x i ) + log p b (x i ))<label>(11)</label></formula><p>in which Œª is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NER datasets</head><p>In our experiments, we evaluate the VNER model on two benchmark datasets for Vietnamese NER which are VLSP-2016 NER task and VLSP-2018 NER task. Specifically, VLSP-2016 dataset consists of four entity types including location (LOC), organization (ORG), person (PER), and miscellaneous (MISC). VLSP-2016 dataset additionally provides the information about word segmentation, part-of-speech, and chunking tags. Similarly, VLSP-2018 dataset also contains four types of entity which are LOC, ORG, PER, and MISC. However, unlike VLSP-2016 dataset, VLSP-2018 dataset is only annotated with entity tags without having any additional information such as word segmentation, POS, or chunking tags. In addition, VLSP-2018 dataset contains nested entities which contain other entities inside them.</p><p>In this paper, we only consider single-layer entities without nested entities. Hence, for VLSP-2018, we retain only the entities tags of the outer-most layer. <ref type="table" target="#tab_2">Table I</ref> shows the statistic of our two experimental NER datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>Due to VLSP-2016 dataset that does not have development set, hence we create a development set by sampling randomly 2000 samples of train set as in <ref type="bibr" target="#b10">[11]</ref>; and the rest of train set is used for training VNER model. We then train VNER model on both VLSP-2016 and VLSP-2018 datasets with the train set and further tune up the model with the development set. The parameters used to train VNER model are summarized in <ref type="table" target="#tab_2">Table II.</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>For VLSP-2016 dataset, we compare VNER model with CRF model based on hand-crafted features ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>) (henceforth, feature-based CRF) which is the state-of-the-art model on this dataset. To do so, we evaluate VNER model on three setups as reported in <ref type="bibr" target="#b1">[2]</ref>. In the first setup, we use only word information to train VNER model without using the information of annotated POS and chunking tags. For the second setup, we train VNER model by using all annotated information including word, POS tags, and chunking tags. For the last setup, we rely on the Underthesea toolkit to generate POS and chunking tags for the sentences in VLSP-2016 dataset. We then make use of the information of word, those generated POS, and chunking tags to train VNER model. Besides, we also evaluate VNER model on VLSP-2016 dataset with another setup as in the experiment of VnCoreNLP ( <ref type="bibr" target="#b10">[11]</ref>) in which the contiguous syllable constituting a PER tag is merged to form a word. In comparison to other neural network for Vietnamese NER task, we compare the performance of VNER model with two neural models: NNVLP model ( <ref type="bibr" target="#b7">[8]</ref>) that makes use of the combination of bidirectional LSTM, CNN, and CRF models; and vie-ner-lstm model ( <ref type="bibr" target="#b13">[14]</ref>) that incorporates automatic syntactic features with word embeddings as input for bidirectional LSTM network. <ref type="table" target="#tab_2">Table III</ref> shows the performance of VNER model and baseline models in terms of F1 score. Note that all results of baseline models are reported from their original experiments. Overall, VNER model outperforms all baseline models across four setups of training data. In the first setup, VNER model obtains a comparable result compared to feature-based CRF model with 90.37 F1 score for VNER model in comparison to 90.03 F1 score for feature-based CRF model. For the second setup, four models used all information about word and the annotation of POS, and chunking tags to train those models. VNER model shows a large improvement with 95.33 F1 score compared to three baseline models including feature-based CRF, VnCoreNLP, and vie-ner-lstm with the F1 scores of 93.93, 92.91, and 92.05, respectively. Similar to three above setups, in the VnCoreNLP setup, VNER model also outperforms VnCoreNLP model where PER entity was reconstructed by merging contiguous syllables to a word form. Concretely, VNER model achieves 89.58 F1 score in comparison with 88.55 F1 score of VnCoreNLP. The motivation behind is to make the dataset to be more realistic. Because the annotated POS tags are not available in the real-world application.</p><p>In the VLSP-2018 dataset, we experiment the performance of VNER model in comparison with the performance of two baseline models which are feature-based CRF and ZA-NER models. While feature-based CRF model relied on variety of hand-crafted features, both VNER and ZA-NER models are trained on the original data of VLSP-2018 NER dataset. <ref type="table" target="#tab_2">Table  IV</ref> shows the performance of three models on recognizing named entities of VLSP-2018 NER dataset. Concretely, VNER model outperforms both feature-based CRF and ZA-NER models with 77.52 F1 score of VNER model compared to 76.63 F1 score and 74.00 F1 score for feature-based CRF model and ZA-NER model, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposed an attentive neural model, namely VNER to recognize named entities in Vietnamese. The VNER model is constructed by four main layers including word layer, encoder layer, attention layer, and decoder layer. A series of experiments on benchmark datasets for Vietnamese NER task showed that VNER model outperform both hand-crafted feature and neural network models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of VNER model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>TABLE I THE SIZE OF VLSP-2016 AND VLSP-2018 DATASETS</head><label>I</label><figDesc></figDesc><table>Type 
VLSP-2016 
VLSP-2018 
Train Test 
Train Test 
Dev 
LOC 
6,245 1,379 
8,831 2,525 3,043 
ORG 
1,213 
274 
3,471 1,616 1,203 
PER 
7,480 1,294 
6,427 3,518 2,168 
MISC 
282 
49 
805 
296 
179 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>TABLE II THE MODEL PARAMETERS</head><label>II</label><figDesc></figDesc><table>Param 
Value 
character hidden dim. 
300 
word hidden dim. 
300 
char dim. 
30 
word dim. 
100 
dropout rate 
0.6 
No. of word layers 
1 
No. of char layers 
1 
update function 
Adam 
learning rate 
0.001 
batch size 
128 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>TABLE III THE PERFORMANCE OF VNER MODEL COMPARED TO BASELINE MODELS ON VLSP-2016 DATASET</head><label>III</label><figDesc></figDesc><table>Setting 
VNER Feature-based CRF VnCoreNLP NNVLP vie-ner-lstm 
Without POS, chunking tags (1st setup) 
90.37 
90.03 
-
-
-
Annotated POS, chunking tags (2nd setup) 
95.33 
93.93 
-
92.91 
92.05 
Underthesea-based POS, chunking tags (3rd setup) 
90.17 
89.30 
-
-
-
VnCoreNLP setup 
89.58 
-
88.55 
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>TABLE IV THE</head><label>IV</label><figDesc></figDesc><table>PERFORMANCE OF VNER AND FEATURE-BASED MODELS ON 
VLSP-2018 DATASET 

Model 
Precision Recall 
F1 
ZA-NER 
76.00 
72.00 
74.00 
Feature-based CRF 
73.46 
80.08 
76.63 
VNER 
75.70 
79.43 
77.52 

</table></figure>

			<note place="foot" n="1"> http://vlsp.org.vn/vlsp2018/eval/ner</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning (ICML)</title>
		<meeting>the Eighteenth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pham Quang Nhat</forename><surname>Minh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04375</idno>
		<title level="m">A Feature-Rich Vietnamese Named-Entity Recognition Model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Feature-Based Model for Nested NamedEntity Recognition at VLSP-2018 NER Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pham Quang Nhat</forename><surname>Minh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vietnamese Speech and Language Processing (VLSP)</title>
		<meeting>Vietnamese Speech and Language Processing (VLSP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HTL)</title>
		<meeting>The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HTL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Fangzheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NNVLP: A Neural Network-Based Vietnamese Language Processing Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai-Hoang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Khoai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuong</forename><surname>Le-Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ZA-NER: Vietnamese Named Entity Recognition at VLSP 2018 Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Thang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><forename type="middle">Kim</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vietnamese Speech and Language Processing (VLSP)</title>
		<meeting>Vietnamese Speech and Language Processing (VLSP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vlsp 2016 shared task: Named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen Thi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vietnamese Speech and Language Processing (VLSP)</title>
		<meeting>Vietnamese Speech and Language Processing (VLSP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VnCoreNLP: A Vietnamese Natural Language Processing Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2018: Demonstrations</title>
		<meeting>NAACL-HLT 2018: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="56" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<title level="m">Highway Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Importance of Automatic Syntactic Features in Vietnamese Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai-Hoang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuong</forename><surname>Le-Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 31st Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>31st Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="97" to="103" />
		</imprint>
	</monogr>
	<note>PACLIC 31)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
