<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Concrete Autoencoders for Differentiable Feature Selection and Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Fatih Balin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
						</author>
						<title level="a" type="main">Concrete Autoencoders for Differentiable Feature Selection and Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the concrete autoencoder, an endto-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned. During test time, the selected features can be used with the decoder network to reconstruct the remaining input features. We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-theart methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be use to impute the expression levels of the remaining genes. In doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20%. The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head> <ref type="bibr" target="#b25">Wang et al., 2013)</ref><p>. These methods provide insight into the relationship between features in complex data and can simplify the process of training downstream models. Feature selection is particularly useful if the data with the full set of features is expensive or difficult to collect, as it can eliminate the need to measure irrelevant or redundant features.</p><p>As a motivating example, consider a dataset that consists of the expression of various genes across tissue samples. Such "omics" measurements are increasingly carried out to fully characterize biological samples at the individual and singlecell level <ref type="bibr" target="#b2">(Bock et al., 2016;</ref><ref type="bibr" target="#b15">Huang et al., 2017)</ref>. Yet it remains expensive to conduct all of the biological assays that are needed to characterize such samples. It is natural to ask: what are the most important features in this dataset? Are there redundant features that do not need to be measured? The idea of only measuring a subset of biological features and then reconstructing the remaining features is not new; in fact, this line of thinking has motivated the identification of the landmark genes, also known as the L1000 genes, which are a small subset of the over 20,000 human genes. The expression levels of the L1000 are strongly correlated with the expression levels of other genes, and thus this subset can be measured cheaply and then used to impute the remaining gene expression levels .</p><p>The problem of feature selection is different from the more general problem of dimensionality reduction. Standard techniques for dimensionality reduction, such as principal components analysis <ref type="bibr" target="#b14">(Hotelling, 1933)</ref> and autoencoders <ref type="bibr" target="#b13">(Hinton &amp; Salakhutdinov, 2006)</ref>, are be able to represent data with fewer dimensions while preserving maximal variance or minimizing reconstruction loss. However, such methods do not select a set of features present in the original dataset, and thus cannot be directly used to eliminate redundant features and reduce experimental costs. We emphasize the unsupervised nature of this problem: specific prediction tasks may not be known ahead of time, and thus it is important to develop methods that can identify a subset of features while allowing imputation of the remaining features with minimal distortion for arbitrary downstream tasks.</p><p>In this paper, we propose a new end-to-end method to perform feature subset selection and imputation that leverages the power of deep autoencoders for discrete feature selection. Our method, the concrete autoencoder, uses a relaxation of the discrete distribution, the Concrete distribution <ref type="bibr">(Maddi-arXiv:1901.09346v2 [cs.</ref>LG] 31 Jan 2019</p><p>Concrete Autoencoders  <ref type="bibr">(d)</ref>. The reconstructed versions of the images, using only the 20 selected pixels, shows that generally the digit is identified correctly and some stylistic features, such as the swirl in the digit "2", are captured. (cf. figures in Appendix A which show the results of applying concrete autoencoder to individual classes of digits.) <ref type="bibr">son et al., 2016)</ref>, and the reparametrization trick <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2013)</ref> to differentiate through an arbitrary (e.g. reconstruction) loss and select input features to minimize this loss. A visual example of results from our method is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the concrete autoencoder selects the 20 most informative pixels (out of a total of 784) on the MNIST dataset, and reconstructs the original images with high accuracy.</p><p>We test concrete autoencoders on a variety of datasets, and find that they generally outperform state-of-the-art methods for feature selection and data reconstruction. We have made the code for our algorithm and experiments available on a public repository 1 .</p><p>Related Works Feature selection methods are generally divided into three classes: filter, wrapper, and embedded methods. Filter methods rank the features of the dataset using statistical tests, such as the variance, in order to select features that individually maximize the desired criteria <ref type="bibr" target="#b1">(Battiti, 1994;</ref><ref type="bibr" target="#b7">Duda et al., 2012)</ref>. Filter methods generally do not consider interactions between features and do not provide a way to impute the remaining features; one must train a separate algorithm to do so. Wrapper methods select subsets of features that maximize a objective function, which is optimized over the choice of input features using a black-box optimization method, such as sequential search or genetic algorithms <ref type="bibr" target="#b18">(Kohavi &amp; John, 1997;</ref><ref type="bibr" target="#b9">Goldberg &amp; Holland, 1988)</ref>. Since wrapper methods evaluate subsets of features, they are able to detect potential relationships between features, but usually at the expense 1 Code available at: https://github.com/mfbalin/ Concrete-Autoencoders of increased computation time. Embedded methods also consider relationships between features but generally do so more efficiently as they incorporate feature selection into the learning phase of another algorithm. A well-known example is the Lasso <ref type="bibr" target="#b24">(Tibshirani, 1996)</ref>, which can be used to select features for regression by varying the strength of 1 regularization.</p><p>Many embedded unsupervised feature selection algorithms use regularization as the means to select discrete features. The popular UDFS algorithm Yang et al. (2011) incorporates 2,1 regularization on a set of weights applied to the input to select features most useful for local discriminative analysis. Similarly, the MCFS algorithm <ref type="bibr" target="#b3">(Cai et al., 2010)</ref> uses regularization to solve for the features which preserve the clustering structure in the data. The recently-proposed AEFS <ref type="bibr" target="#b11">(Han et al., 2017)</ref> algorithm also uses 2,1 regularization on the weights of the encoder that maps the input data to a latent space and optimizes these weights for their ability to reconstruct the original input.</p><p>In this paper, we select discrete features using an embedded method but without resorting to regularization. Rather, we use a relaxation of the discrete random variables, the Concrete distribution <ref type="bibr" target="#b22">(Maddison et al., 2016)</ref>, which allows a low-variance estimate of the gradient through discrete stochastic nodes. By using Concrete random variables, we can directly parametrize the selection of input features, and differentiate through the parameters. As we show through experiments in Section 4, this leads to lower reconstruction errors on real-world datasets compared to the aforementioned regularization-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>We now describe the problem of global feature selection. Although global feature selection is relevant for both unsupervised and supervised settings, we describe here the unsupervised case, which is the primary focus of this paper, and defer discussion of the supervised case to Appendix F.</p><p>Consider a data-generating probability distribution p(x) over a d-dimensional space. The goal is to learn a subset S ⊆ {1, 2 . . . d} of features of specified size |S| = k and also learn a reconstruction function f θ (·) : R k → R d , such that the expected loss between the reconstructed sample f θ (x S ) and the original sample x is minimized, where x S ∈ R k consists of those elements x i such that i ∈ S. In other words, we would like to optimize arg min</p><formula xml:id="formula_0">S,θ E p(x) [ f θ (x S ) − x 2 ]<label>(1)</label></formula><p>In practice, we do not know p(x); rather we have n samples, generally assumed to be drawn i.i.d. from p(x). These samples can be represented in a data matrix X ∈ R n×d , and so the goal becomes choosing k columns of X such that sub-matrix X S ∈ R n×k , defined analogously to x S , can be used to reconstruct the original matrix X. Let us overload f θ (X S ) to mean the matrix that results from applying f θ (·) to each of the rows of X S and stacking the resulting outputs. We seek to minimize the empirical reconstruction error:</p><formula xml:id="formula_1">arg min S,θ f θ (X S ) − X F ,<label>(2)</label></formula><p>where · F denotes the Frobenius norm of the matrix. The principal difficulty in solving (2) is the optimization over the discrete set of features S, whose choices grow exponentially in d. Thus, even for simple choices of f θ (·), such as linear regression, the optimization problem in (2) is NP-hard to solve <ref type="bibr" target="#b0">(Amaldi &amp; Kann, 1998)</ref>.</p><p>Furthermore, the complexity of f θ (·) can significantly affect reconstruction error and even the choice of S. More expressive and non-linear choices for f θ (·), such as neural networks, will naturally allow for lower reconstruction error, potentially at the expense of a more difficult optimization problem. We seek to develop a method that can approximate the solution for any given class of functions f θ (·), from linear regression to deep fully-connected neural networks.</p><p>Finally, we note that the choice of mean-squared reconstruction error as the metric for optimization in (1) and (2) stems from the fact that it is a smooth differentiable function that serves as a proxy for many downstream analyses, such as clustering performance and classification accuracy. However, other differentiable metrics, such as a variational approximation of the mutual information between f θ (X S ) and X, may be considered as well <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The concrete autoencoder is an adaption of the standard autoencoder <ref type="bibr" target="#b13">(Hinton &amp; Salakhutdinov, 2006)</ref> for discrete feature selection. Instead of using series of fully-connected layers for the encoder, we propose a concrete selector layer with a user-specified number of nodes, k. This layer selects stochastic linear combinations of input features during training, which converge to a discrete set of k features by the end of training and during test time.</p><p>The way in which input features are combined depends on the temperature of this layer, which we modulate using a simple annealing schedule. As the temperature of the layer approaches zero, the layer selects k individual input features. The decoder of a concrete autoencoder, which serves as the reconstruction function, is the same as that of a standard autoencoder: a neural network whose architecture can be set by the user based on dataset size and complexity. In effect, then, the concrete autoencoder is a method for selecting a discrete set of features that are optimized for an arbitrarily-complex reconstruction function. We describe the ingredients for our method in more detail in the next two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concrete Selector Layer</head><p>The concrete selector layer is based on Concrete random variables <ref type="bibr" target="#b22">(Maddison et al., 2016;</ref><ref type="bibr" target="#b16">Jang et al., 2016)</ref>. A Concrete random variable can be sampled to produce a continuous relaxation of the one-hot vector. The extent to which the one-hot vector is relaxed is controlled by a temperature parameter T ∈ (0, ∞). To sample a Concrete random variable in d dimensions with parameters α ∈ R d &gt;0 and T , one first samples a d-dimensional vector of i.i.d. samples from a Gumbel distribution <ref type="bibr" target="#b10">(Gumbel, 1954)</ref>, g.</p><p>Then each element of the sample m from the Concrete distribution is defined as:</p><formula xml:id="formula_2">m j = exp((log α j + g j )/T ) d k=1 exp((log α k + g k )/T ) ,<label>(3)</label></formula><p>where m j refers to the j th element in a particular sample vector. In the limit T → 0, the concrete random variable smoothly approaches the discrete distribution, outputting one hot vectors with m j = 1 with probability α j / p α p . The desirable aspect of the Concrete random variable is that it allows for differentiation with respect to its parameters α via the reparametrization trick <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2013)</ref>.</p><p>We use Concrete random variables to select input features in the following way. For each of the k nodes in the concrete selector layer, we sample a d-dimensional Concrete random variable m (i) , i ∈ {1 . . . k} (note that the superscript here indexes the node in the selector layer, whereas the subscript earlier referred to the element in the vector). The i th node in the selector layer u (i) outputs x · m (i) . This is, in general, a . . . . . . . . . . . .</p><p>x 1</p><p>x 2</p><p>x 3</p><formula xml:id="formula_3">x d u (1) u (k)x 1 x 2 x 3 x d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concrete selector layer</head><p>Decoder layers </p><formula xml:id="formula_4">Arbitrary f θ (·) (a) (b)</formula><formula xml:id="formula_5">(i) , m (i) ∼ Concrete(α (i) , T).</formula><p>During test time, these weights are fixed and the element with the highest value in α (i) is selected by the corresponding i th hidden neuron. The architecture of the decoder remains the same during train and test time, namely thatx = f θ (u), where u is the vector consisting of each u (i) . (b) Here, we show pseudocode for the concrete autoencoder algorithm, see Appendix B for more details.</p><p>weighted linear combination of the input features, but notice that when T → 0, each node in the concrete selector layer outputs exactly one of the input features. After the network is trained, during test time, we thus replace the concrete selector layer with a discrete arg max layer in which the output of the i th neuron is x arg maxj α (i) j .</p><p>We randomly initialize α i to small positive values, to encourage the selector layer to stochastically explore different linear combinations of input features. However, as the network is trained, the values of α i become more sparse, as the network becomes more confident in particular choices of input features, reducing the stochasticity in selected features. The concrete autoencoder architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(a) and the pseudocode for training in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Annealing Schedule</head><p>The temperature of Concrete random variables in the concrete selector layer has a significant affect on the output of the nodes. If the temperature is held high, the concrete selector layer continuously outputs a linear combination of features. On the contrary, if the temperature is held low, the concrete selector layer is not able to explore different combinations of features and converges to a poor local minimum. Neither fixed temperature allows the concrete selector layer to converge to informative features.</p><p>Instead, we propose a simple annealing schedule that sets the temperature for all of the concrete variables, initially beginning with a high temperature T 0 and gradually decaying the temperature until a final temperate T B at each epoch according to a first-order exponential decay:</p><formula xml:id="formula_6">T (b) = T 0 (T B /T 0 ) b/B where T (b)</formula><p>is the temperature at epoch number b, and B is the total number of epochs. We compare various methods for setting the temperature of the concrete selector nodes in <ref type="figure" target="#fig_2">Fig. 3</ref>. We find that this annealing schedule allows the concrete selector layer to effectively stochastically explore combinations of features in the initial phases of training, while in the later stages of training, the lowered temperature allows the the network to converge to informative individual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we carry out experiments to compare the performance of concrete autoencoders to other feature subset selections on standard public datasets. For all of the experiments, we use Adam optimizer with a learning rate of 10 −3 . The initial temperature of the concrete autoencoder T 0 was set to 10 and the final temperature T B to 0.01. We trained the concrete autoencoder until until the mean of the concrete samples exceeded 0.99. For UDFS and AEFS, we swept values of each regularization hyperparameter and report the results with optimal hyperparameters according to mean squared error for reconstruction for each method.</p><p>Furthermore, since the reconstruction f θ (·) can overfit to patterns particular to the training set, we divide each dataset randomly into train, validation, and test datasets according to a 72-8-20 split 2 . We use the training set to learn the parameters of the concrete autoencoders, the validation set to select optimal hyperparameters, and the test set to evaluate generalization performance, which we report below.</p><p>We compare concrete autoencoders to many of the unsupervised feature selection methods mentioned in Related Works including UDFS, MCFS, and AEFS. We also include principal feature analysis (PFA), proposed by <ref type="bibr" target="#b21">Lu et al. (2007)</ref>, which is a popular method for selecting discrete features based on PCA, as well as a spectral method, the Laplacian score <ref type="bibr" target="#b12">(He et al., 2006)</ref>. Where available, we made use of scikit-feature implementation of each method . In our experiments, we also include, as upper bounds on performance, dimensionality reduction methods that are not restricted to choosing individual features. In experiments with linear decoders, we use PCA and in experiments with non-linear decoders, we use equivalent autoencoders. The methods, because they allow k combinations of features, bound the performance of any feature selection techniqe.</p><p>We evaluate these methods on a number of datasets (the sizes of the datasets can be found in <ref type="table" target="#tab_1">Table 1)</ref>:</p><p>MNIST and MNIST-Fashion consist of 28-by-28 grayscale images of hand-written digits and clothing items, respectively. We choose these datasets because they are widely known in the machine learning community. Although these are image datasets, the objects in each image are centered, which means we can meaningfully treat each 784 pixels in the image as a separate feature.</p><p>ISOLET consists of preprocessed speech data of people speaking the names of the letters in the English alphabet. This dataset is widely used as a benchmark in the feature selection literature. Each feature is one of the 617 quantities produced as a result of preprocessing, including spectral coefficients and sonorant features.</p><p>COIL-20 consists of centered grayscale images of 20 objects. Images of the objects were taken at pose intervals of 5 degrees amounting to 72 images for each object. During preprocessing, the images were resized to produce 20-by-20 images, with each feature being one of the 400 pixels.</p><p>Smartphone Dataset for Human Activity Recognition consists of sensor data collected from a smartphone mounted on subjects while they performed several activities such as walking upstairs, standing and laying. Each feature represents one of the 561 raw or processed quantities from the sensors on the phone.</p><p>Mice Protein Dataset consists of protein expression levels measured in the cortex of normal and trisomic mice who had been exposed to different experimental conditions. Each feature is the expression level of one protein.</p><p>GEO Dataset consists of gene expression profiles measured from a large number of samples through a microarray-based platform. Each of the 10,463 features represents the expression level of one gene.</p><p>To evaluate the various feature selection methods, we examine two metrics, both reported on a hold-out test set:</p><p>Reconstruction error: We extract the k selected features. We pass the resulting matrix X S through the reconstruction function f θ that we have trained. We measure the Frobenius norm between the original and reconstructed test matrices f θ (X S ) − X F , normalized by the number of features d.  <ref type="figure" target="#fig_5">Figure 4</ref>. Results on the ISOLET dataset using non-linear decoders. Here, we compare the concrete autoencoder (CAE) to other feature selection methods using a 1-hidden layer neural network as the reconstructor. (a) We find that across all values of k tested, concrete autoencoders have lowest reconstruction errors (b) We find that the features learned by the concrete autoencoder also tend result in higher classification accuracies.</p><p>by the method. We then pass the resulting matrix X S to an extremely randomized trees classifier <ref type="bibr" target="#b8">(Geurts et al., 2006)</ref>, a variant of random forests that has been used with feature selection methods in prior literature <ref type="bibr" target="#b6">(Drotár et al., 2015)</ref>. We measure the accuracy between the predicted labels and true labels, which are available for each of the datasets. Note that the labels are only used for training the classifier and not for the feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concrete Autoencoders (Non-Linear Decoder)</head><p>First, we constructed a concrete autoencoder with a nonlinear decoder architecture consisting of one hidden layer with 3k/2 neurons, with k being the number of selected features. We performed a series of experiments with the ISO-LET dataset, which is widely used as a benchmark in prior feature selection literature. We benchmarked each feature selection method (besides UDFS whose run-time was prohibitive) with varying numbers of of features (from k = 10 to k = 85), measuring the reconstruction error using a 1-hidden-layer neural network as well as classification accuracy. The number of neurons in the hidden layer of the reconstruction network was varied within [4k/9, 2k/3, k, 3k/2], and the network with the highest validation accuracy was selected and measured on the test set.</p><p>To control for the performance of the reconstruction network, we trained each reconstruction network for the same number of epochs, 200. For the concrete autoencoder, we did not use the decoder that was learned during training, but re-trained the reconstruction networks from scratch. Our resulting classification accuracies and reconstruction error on each dataset are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. We find that the concrete autoencoder consistently outperformed other feature selection methods on the ISOLET dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concrete Autoencoders (Linear Decoder)</head><p>Next, we carried out a series of experiments in which we compared concrete autoencoders with linear decoders to the other methods using linear regression as the reconstruction function. Since linear regression can be trained easily to convergence, this allowed us to isolate the effect of using the concrete selector layer for feature selection, and allowed us to train on a wider variety of datasets with less risk of overfitting. We selected a fixed number k = 50 of features with each method, with the exception of the Mice Protein Dataset, for which we used k = 10 due to its small size.</p><p>After selecting the features using concrete autoencoder and the other feature selection methods, we trained a standard linear regressor with no regularization to impute the original features. The resulting reconstruction errors on a hold-out test set are shown in <ref type="table" target="#tab_1">Table 1</ref>. We also used the selected features to measure classification accuracies, which are shown in <ref type="table" target="#tab_4">Table 2</ref> in Appendix C. On almost all datasets, we found that the concrete autoencoder continued to have the lowest reconstruction error and a high classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interpreting Related Features</head><p>An added benefit of using the concrete selector layer is that it allows the user to not only identify the most informative features for reconstruction, but also identify sets of related features through examination of the learned Concrete parameters α (i) . Because the concrete selector layer samples the input features stochastically based on α (i) , any of the features with the large values in the vector α (i) may be selected, and are thus likely to be correlated to one another.</p><p>In <ref type="figure" target="#fig_3">Fig. 5</ref>, we show how this can reveal related features by visualizing the top 3 pixels with the highest values in the α (i) vector for each of the 20 concrete selector nodes on the MNIST digits. We notice that the pixels that are selected by each node are spatially close to one another, which agrees with intuitive notions of related features, as neighboring pixel values are likely to be correlated in handwritten digits. These patterns are even more striking when generated for individual classes of digits; in that case, the set of correlated pixels may even suggest the direction of the stroke when the digit was written (see Appendix D for more details). Such analysis may be carried out more generally to find sets of related features, such as sets of related genes in a gene expression dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Case Study: L1000 Gene Expression</head><p>We now turn to a large-scale test of the concrete autoencoder: examining whether we can improve gene expression inference. Gene expression inference arises from an im-  Here, we illustrate the top 3 pixels selected by each of the 20 nodes in the concrete layer when trained on the MNIST dataset. We color each group of 3 pixels with the same color (note that some colors are repeated because of the limited color palette). cf. Appendix D, which shows pixel group for classes of digits.</p><p>portant problem in molecular biology: characterizing the state of cells in different biological conditions. In particular, the response of cells to diseases, mutations, and drugs is often characterized by the measurement of gene expression patterns .</p><p>However, measuring all of the genes expressed in a human cell can be expensive, and thus researchers have looked to computational methods to reduce the cost and time required for gene expression profiling. In particular, researchers from the LINCS Project found that, because gene expression is correlated in different conditions, a set of roughly a thousand carefully-chosen genes can capture most of the gene expression information in the entire human transcriptome . It is thus possible to use a linear regression model trained on genome-wide gene expression to infer the gene expression values of the remaining genes. More recently, <ref type="bibr" target="#b5">Chen et al. (2016)</ref> showed that it is possible to leverage the representation power of neural networks to improve the accuracy of gene expression inference in an approach they referred to as D-GEX.</p><p>Here, we ask whether it is possible to use concrete autoencoders to determine a good subset of genes, perhaps as an alternative to the landmark genes, without utilizing any prior biological knowledge of gene networks or gene function. We relied only on a large dataset of gene expression data, from which we aim to select the most informative features.</p><p>We used the version of the GEO dataset used in the D-GEX paper, and followed the same preprocessing scheme to obtain a dataset of sample size 112,171 and dimensionality 10,463 genes. We then randomly partitioned the dataset in a manner similar to that performed by <ref type="bibr" target="#b5">Chen et al. (2016)</ref>: as a result, the training set had 88,807 samples, the validation set had 11,101, and the test set had 12,263. We then considered 3 kinds of reconstruction functions: in the simplest case, we considered multitarget linear regression, and we also implemented neural networks with 1 and 2 hidden layers. See Appendix E for the architecture of the networks.</p><p>First, we trained a concrete autoencoder to select 943 using only a linear regression decoder. An analysis of the selected genes showed very little overlap with the landmark genes: only 90 of the CAE-selected 943 genes were among the landmark genes. For consistency with the D-GEX results, we used this same set of 943 genes, selected by a concrete autoencoder with a linear decoder, with all of our reconstruction networks.</p><p>We trained each reconstruction networks to impute all of the original 10,463 genes. We measured the reconstruction error on a hold-out test set that was used neither to train the concrete autoencoder nor the reconstruction functions. Our results are summarized in <ref type="figure">Fig. 6(a)</ref>, where we plot the mean-squared error of imputation. We show that not only is it possible to use concrete autoencoders to perform gene selection gene expression inference in a differentiable, endto-end manner on large-scale datasets, doing so improves the performance of the gene expression imputation on a holdout test set of gene expression by around 3% for each architecture, which is significant as the L1000 landmark genes were expert curated using a combination of computational prediction with domain knowledge, and is a very strong benchmark and is widely used in genomics.  <ref type="figure">Figure 6</ref>. Imputation errors of concrete autoencoders and landmark genes. Here, we show the mean-squared error of the imputation task using both the 943 landmark genes (red) and the 943 genes selected by the concrete autoencoder (blue) on the test set. The task is to impute the expression of all 10,463 genes. We observe about a 3% reduction (note that y-axis begins at 0.20) of the reconstruction error when using the genes selected by the concrete autoencoders (CAE) across all architectures. These are results averaged over three trials. Standard deviation bars are shown but were very low, as the final imputations were very similar across all trials. (b) We train the CAE with different numbers of selected features, and calculate the MSE using linear regression on the test set. We find that we can achieve a similar MSE to the landmark genes using only around 750 genes, a 20% reduction in the number of genes measured.</p><p>Next, we investigated whether it would be possible to obtain similar accuracies as to the landmark genes while using a smaller set of CAE-selected genes. We trained concrete autoencoders from scratch using k = 750, 800, 850, 900, 943, using the same architecture described in Appendix E and using a linear regression decoder. We found that using linear regression as the reconstruction function, we could obtain reconstruction MSEs about as low as the landmark genes, using only 750 genes, which represents roughly a 20% reduction in the number of genes measured, potentially saving substantial experimental costs. These results are illustrated in <ref type="figure">Fig. 6(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we have proposed a new method for differentiable, end-to-end feature selection via backpropagation. At its core, the concrete autoencoder uses Concrete random variables and the reparametrization trick to allow gradients to flow through a layer that stochastically selects discrete input features. The stochasticity of the concrete autoencoder allows it to efficiently explore and converge to a subset of input features of specified size that minimizes a particular loss, as described in Section 3. The learned parameters can be further probed to allow the anlayst to interpret related features, as demonstrated in Section 4.3. This makes con-crete autoencoders different from many competing methods, which rely on regularization to encourage sparse feature selection.</p><p>We show via experiments on a variety of public datasets that concrete autoencoders effectively minimize the reconstruction error and maximize classification accuracy using selected features. In the six public datasets that we tested concrete autoencoders, we found that concrete autoencoders outperformed many different complex feature selection methods. This remains the case even when we reduced the decoder layer to be a single linear layer, showing that the concrete selector node is useful even when selecting input features that minimize the loss when using a linear regression as the reconstruction function.</p><p>Because the concrete autoencoder is an adaptation of the standard autoencoder, it scales easily to datasets with many samples or high dimensionality. We demonstrated this in section 4.4 using a gene expression dataset with more than 100,000 samples and 10,000 features, where the features selected by the concrete autoencoder outperformed the stateof-the-art gene subset. Furthermore, because of its general formulation, the concrete autoencoder can be easily extended in many ways. For example, it possible to use concrete autoencoders in a supervised manner -to select a set of features that minimize a cross-entropy loss, for example, rather than a reconstruction loss. More details and examples of this approach are provided in Appendix F. Another possible extension is to attach different costs to selecting different features, for example if certain features represent tests or assays that are much more expensive than others. Such as cost may be incorporated into the loss function and allow the analyst to trade off cost for accuracy.</p><p>Advantages of the concrete autoencoder include its generality and ease of use. Implementing the architecture in popular machine learning frameworks requires only modifying a few lines of code from a standard autoencoder. Furthermore, the runtime of the concrete autoencoder is similar to that of the standard autoencoder and improves with hardware acceleration and parallelization techniques commonplace in deep learning. The only additional hyperparameters of the concrete autoencoder are the initial and final temperatures used in the annealing schedule. We find that the default values used in this paper work well for a variety of datasets.</p><p>Concrete autoencoders, like the other feature selection methods we compared with in this paper, do not provide p-values or statistical significance quantification. Features discovered through concrete autoencoders should be validated through hypothesis testing or additional analysis using relevant domain knowledge. We believe that the concrete autoencoder can be of particular use in simplifying assays and experiments that measure a large number of related quantities, such as medical lab tests and genotype sequencing.</p><p>Yang, Y., Shen, H. T., Ma, Z., Huang, Z., and Zhou, X. l2, 1-norm regularized discriminative feature selection for unsupervised learning. 2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Selected Features for Single Classes in MNIST</head><p>Here, we show additional examples of using the concrete autoencoder on subsets of the MNIST data that consist of a single digit. Here, we select k = 10 features for each subset of data. <ref type="figure">Figure 7</ref>. Here, we show the results of using concrete autoencoders to select the k = 10 most informative pixels of images of the digit 0 in the MNIST dataset. Compare with <ref type="figure" target="#fig_0">Fig. 1</ref> in the main paper for more information.</p><formula xml:id="formula_7">(a) (b) (c) (d)</formula><p>(a) (b) (c) (d) <ref type="figure">Figure 8</ref>. Here, we show the results of using concrete autoencoders to select the k = 10 most informative pixels of images of the digit 0 in the MNIST dataset. Compare with <ref type="figure" target="#fig_0">Fig. 1</ref> in the main paper for more information.</p><p>(a) (b) (c) (d) <ref type="figure">Figure 9</ref>. Here, we show the results of using concrete autoencoders to select the k = 10 most informative pixels of images of the digit 0 in the MNIST dataset. Compare with <ref type="figure" target="#fig_0">Fig. 1</ref> in the main paper for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudocode for a Concrete Autoencoder</head><p>Here, we have the pseudocode for training the concrete autoencoder in more detail. We also describe how to use a trained concrete autoencoder for feature selection on new data, as well as how to use the concrete autoencoder for imputation. </p><formula xml:id="formula_8">for b ∈ {1 . . . B} do Let T = T 0 (T B /T 0 ) b/B for i ∈ {1 . . . k} do Sample m (i) ∼ Concrete(α (i) , T ) Let X (i) S = X · m (i) end for</formula><p>Define X S to be the matrix ∈ R n×k that results from horizontally concatenating the X </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Accuracies for Feature Selection Methods with Linear Reconstruction</head><p>We carried out a series of experiments in which we compared concrete autoencoders with linear decoders to the other feature selection methods using linear regression as the reconstruction function. We selected k = 50 of features with each method.</p><p>After selecting the features using concrete autoencoder and the other feature selection methods, we trained a standard linear regressor with no regularization to impute the original features. The resulting reconstruction errors on a hold-out test set are shown in <ref type="table" target="#tab_1">Table 1</ref> in the main text. We also used the selected features to measure classification accuracies, which are shown in <ref type="table" target="#tab_4">Table 2</ref> here. Generally, we find that the concrete autoencoder continues to have the lowest reconstruction error and a high (but not always the highest) classification accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Examples of Feature Groups in MNIST Digits</head><p>Here, we show examples of feature groups that were selected by the concrete autoencoder on single classes of digits in the MNIST dataset (see Section 4.3 in the main text for more details). The patterns in the case of single classes of digits are even more striking; here, the set of correlated pixels can be used to infer the direction of the stroke when the digit was written, as correlated pixels are more likely to be part of the same stroke. For example, consider <ref type="figure" target="#fig_0">Fig. 10(b)</ref>, in which the pixel groups shown for the digit '1'. We note that the pixel groups tend to form vertical subsets, as the writing stroke connected those sets of pixels together.</p><p>(a) (b) (c) (d) <ref type="figure" target="#fig_0">Figure 10</ref>. Here, we show the results of using concrete autoencoders to select the k = 10 most informative pixels groups of images of the digit 0 in the MNIST dataset. Compare with <ref type="figure" target="#fig_3">Fig. 5</ref> in the main paper. Each panel is a different digit: (a) the digit 0, (a) the digit 1, (a) the digit 2, (a) the digit 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architecture for GEO Dataset Experiments</head><p>For the GEO dataset, we trained three reconstruction networks to perform the imputation from both the landmark and CAE-selected genes. In each case, the hidden layers consisted of 9000 neurons and dropout rate was set to 0.1. The initial learning rate was 0.001 and decayed after each epoch by multiplication with 0.95. The number of epochs was set to 100 for training the reconstruction networks. We used batch sizes of 256.</p><p>The concrete autoencoder was trained with a linear decoder network for 5000 epochs. For this dataset, we set the initial temperature to be 10, and the final temperature to be 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Supervised Concrete Autoencoders</head><p>Concrete autoencoders can be easily adapted to the supervised setting by replacing the reconstruction neural network in the decoder with a neural network classifier. The pseudocode, shown below, is quite similar to training the standard concrete autoencoder. Let L be the cross entropy loss between the true labels y and the logits f θ (X S ) Compute the gradient of the loss w.r.t. θ using backpropagation and w.r.t each α (i) using the reparametrization trick. Update the parameters θ ← θ − λ∇ θ L, and α (i) ← α (i) − λ∇ α (i) L end for Return: trained reconstruction function f θ (·) and trained Concrete parameters α (i)</p><p>We trained a concrete autoencoder in this supervised manner on the MNIST digits, some representative images are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. Generally, we found the imputation quality to be not as good as when the objective function is directly reconstruction error. The reconstructed versions of the images, using only the 20 selected pixels, shows that generally the digit is identified correctly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Demonstrating concrete autoencoders on the MNIST dataset. Here, we show the results of using concrete autoencoders to select in an unsupervised manner the k = 20 most informative pixels of images in the MNIST dataset. (a) The 20 selected features (out of the 784 pixels) on the MNIST dataset are shown in white. (b) A sample of input images in MNIST dataset with the top 2 rows being training images and the bottom 3 rows being test images. (c) The same input images with only the selected features shown as colored dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Concrete autoencoder architecture and pseudocode. (a) The architecture of a concrete autoencoder consists of a single encoding layer, shown in brown, and arbitrary decoding layers (e.g. a deep feedforward neural network), shown in teal. The encoder has one neuron for each feature to be selected. During the training phase, the i th neuron u (i) takes the value x m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Annealing schedules for the concrete autoencoder.Here, we show the effect of different annealing schedules on a concrete autoencoder trained on the MNIST dataset with k = 20 selected features. At each epoch, we plot the temperature in red, average of the largest value in each concrete sample m (i) in black, as well the reconstruction error (using linear regression with the top k = 20 features on validation data), shown in blue. If the temperature is kept high, the concrete samples do not converge to individual features, and the reconstruction error remains large (top left). If the temperature is kept low, the samples immediately converge to poor features, and the error remains large (top right). If the temperature is exponentially decayed (the annealing schedule we use), the samples converge to informative features, and the reconstruction error reaches a suitable minimum (bottom left). Finally, if the temperature is dropped abruptly, the samples converge, but the error is suboptimal (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Pixel groups selected by concrete selector nodes on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the loss L be defined as f θ (X S ) − X F Compute the gradient of the loss w.r.t. θ using backpropagation and w.r.t each α (i) using the reparametrization trick. Update the parameters θ ← θ − λ∇ θ L, andα (i) ← α (i) − λ∇ α (i) L endfor Return: trained reconstruction function f θ (·) and trained Concrete parameters α (i) Algorithm 2. Using a Trained Concrete Autoencoder for Feature Selection Input: test sample x ∈ R d , trained Concrete parameters α (i) for i ∈ {1 . . . k} do Let m (i) = arg max j (α (i) j ), where j indexes the elements of the sample vector Let x (i) S = x m (i) end for Return: x S Algorithm 3. Using a Trained Concrete Autoencoder for Imputation Input: test sample with subset of featuresx ∈ R k , trained reconstruction function f θ (·) Return: f θ (x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 4 .</head><label>4</label><figDesc>Training a Concrete AutoencoderInput: training features X ∈ R n×d , training labels y, number of features to select k, classifier function f θ (·), number of epochs B, learning rate λ, initial temperature T 0 , final temperature T B .for i ∈ {1 . . . k} doInitialize a d-dimensional vector of parameters α (i) with small positive values. end for Initialize the parameters θ of the reconstruction function in a standard way for neural networks.for b ∈ {1 . . . B} do Let T = T 0 (T B /T 0 ) b/B for i ∈ {1 . . . k} do Sample m (i) ∼ Concrete(α (i) , T ) Let X (i) S = X · m (i) end forDefine X S to be the matrix ∈ R n×k that results from horizontally concatenating the X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Demonstrating concrete autoencoders on the MNIST dataset. Here, we show the results of using concrete autoencoders to select in an supervised manner the k = 20 most informative pixels of images in the MNIST dataset. (a) The 20 selected features (out of the 784 pixels) on the MNIST dataset are shown in white. (b) A sample of input images in MNIST dataset with the top 2 rows being training images and the bottom 3 rows being test images. (c) The same input images with only the selected features shown as white dots. (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Reconstruction errors of feature selection methods using linear regression reconstruction. Here, we show the mean-squared errors of the various feature methods on six publicly available datasets. Here Lap refers to the Laplacian score and CAE refers to the concrete autoencoder. For each method, we select k = 50 features (except for mice protein, where we use k = 10 because the original data is lower dimensional) and use a linear regressor for reconstruction. All reported values are on a hold-out test set. (Lower is better.)</figDesc><table><row><cell>Dataset</cell><cell>(n, d)</cell><cell>PCA</cell><cell>Lap</cell><cell>AEFS</cell><cell>UDFS</cell><cell>MCFS</cell><cell>PFA</cell><cell>CAE</cell></row><row><cell>MNIST</cell><cell>(10000, 784)</cell><cell>0.012</cell><cell>0.070</cell><cell>0.033</cell><cell>0.035</cell><cell>0.064</cell><cell>0.051</cell><cell>0.026</cell></row><row><cell cols="2">MNIST-Fashion (10000, 784)</cell><cell>0.012</cell><cell>0.128</cell><cell>0.047</cell><cell>0.133</cell><cell>0.096</cell><cell>0.043</cell><cell>0.041</cell></row><row><cell>COIL-20</cell><cell>(1440, 400)</cell><cell>0.008</cell><cell>0.126</cell><cell>0.061</cell><cell>0.116</cell><cell>0.085</cell><cell>0.061</cell><cell>0.093</cell></row><row><cell>Mice Protein</cell><cell>(1080, 77)</cell><cell>0.003</cell><cell>0.603</cell><cell>0.783</cell><cell>0.867</cell><cell>0.695</cell><cell>0.871</cell><cell>0.372</cell></row><row><cell>ISOLET</cell><cell>(7797, 617)</cell><cell>0.009</cell><cell>0.344</cell><cell>0.301</cell><cell>0.375</cell><cell>0.471</cell><cell>0.316</cell><cell>0.299</cell></row><row><cell>Activity</cell><cell>(5744, 561)</cell><cell>0.002</cell><cell>0.139</cell><cell>0.112</cell><cell>0.173</cell><cell>0.170</cell><cell>0.197</cell><cell>0.108</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1. Training a Concrete Autoencoder Input: training dataset X ∈ R n×d , number of features to select k, decoder network f θ (·), number of epochs B, learning rate λ, initial temp T 0 , final temp T B . for i ∈ {1 . . . k} do Initialize a d-dimensional vector of parameters α (i) with small positive values. end for Initialize the parameters θ of the reconstruction function in a standard way for neural networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracies of feature selection methods. Here, we show the classification accuracies of the various feature methods on six publicly available datasets. Here CAE refers to the concrete autoencoder. For each method, we select k = 50 features (except for mice protein dataset, for which we use k = 10) and use a neural network with 1 hidden layer for reconstruction. All reported values are on the test set. The classifier used here was a Extremely Randomized Trees classifier (a variant of Random Forests) with the number of trees being 50. (Higher is better.)</figDesc><table><row><cell>Dataset</cell><cell>(n, d)</cell><cell>PCA</cell><cell>Lap</cell><cell>AEFS</cell><cell>UDFS</cell><cell>MCFS</cell><cell>PFA</cell><cell>CAE</cell></row><row><cell>MNIST</cell><cell>(10000, 784)</cell><cell>0.925</cell><cell>0.646</cell><cell>0.690</cell><cell>0.892</cell><cell>0.807</cell><cell>0.852</cell><cell>0.906</cell></row><row><cell cols="2">MNIST-Fashion (10000, 784)</cell><cell>0.825</cell><cell>0.517</cell><cell>0.580</cell><cell>0.547</cell><cell>0.513</cell><cell>0.683</cell><cell>0.677</cell></row><row><cell>COIL-20</cell><cell>(1440, 400)</cell><cell>0.996</cell><cell>0.389</cell><cell>0.580</cell><cell>0.556</cell><cell>0.635</cell><cell>0.642</cell><cell>0.586</cell></row><row><cell>Mice Protein</cell><cell>(1080, 77)</cell><cell>0.721</cell><cell>0.134</cell><cell>0.125</cell><cell>0.139</cell><cell>0.139</cell><cell>0.130</cell><cell>0.134</cell></row><row><cell>ISOLET</cell><cell>(7797, 617)</cell><cell>0.895</cell><cell>0.407</cell><cell>0.576</cell><cell>0.455</cell><cell>0.522</cell><cell>0.622</cell><cell>0.685</cell></row><row><cell>Activity</cell><cell>(5744, 561)</cell><cell>0.796</cell><cell>0.280</cell><cell>0.240</cell><cell>0.287</cell><cell>0.295</cell><cell>0.364</cell><cell>0.420</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">High-dimensional datasets often pose a challenge for machine learning algorithms. Feature selection methods aim to reduce dimensionality of data by identifying the subset of relevant features in a dataset. A large number of algorithms have been proposed for feature selection in both supervised and unsupervised settings<ref type="bibr" target="#b18">(Kohavi &amp; John, 1997;</ref>   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For the MNIST, MNIST-Fashion, and Epileptic datasets, we only used 6000, 6000 and 8000 samples respectively to train and validate the model (using a 90-10 split), because of long runtime of the UDFS algorithm. The remaining samples were used for the test set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful for helpful comments by Amirata Ghorbani in the development of this technique, as well to Ali Abid and Aneeqa Abid for feedback regarding figures. The authors also thank Manway Liu, Brielin Brown, Matt Edwards, and Thomas Snyder for discussions that inspired this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="237" to="260" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using mutual information for selecting features in supervised neural net learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Battiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="550" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-omics of single cells: strategies and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Sheffield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in biotechnology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="605" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multi-cluster data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to explain: An information-theoretic perspective on model interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gene expression inference with deep learning. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1832" to="1839" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An experimental comparison of feature selection methods on two-class biomedical datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drotár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gazda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Smékal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Genetic algorithms and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="99" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures. Number 33</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Govt. Print. Office</title>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Autoencoder feature selector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reducing the dimensionality of data with neural networks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">More is better: recent progress in multi-omics data integration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">X</forename><surname>Garmire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The connectivity map: using geneexpression signatures to connect small molecules, genes, and disease. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Modell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Blat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="1929" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature selection: A data perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Trevino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature selection using principal feature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia</title>
		<meeting>the 15th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A method for high-throughput gene expression signature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lamb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A feature subset selection algorithm automatic recommendation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
