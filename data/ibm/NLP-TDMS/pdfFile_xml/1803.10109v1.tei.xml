<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Aswin</roleName><forename type="first">Szu-Jui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmugam</forename><surname>Subramanian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<email>shinjiw@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Speech recognition</term>
					<term>noise robustness</term>
					<term>mask- based beamforming</term>
					<term>lattice-free MMI</term>
					<term>LSTM language mod- eling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a new baseline system for automatic speech recognition (ASR) in the CHiME-4 challenge to promote the development of noisy ASR in speech processing communities by providing 1) state-of-the-art system with a simplified single system comparable to the complicated top systems in the challenge, 2) publicly available and reproducible recipe through the main repository in the Kaldi speech recognition toolkit. The proposed system adopts generalized eigenvalue beamforming with bidirectional long short-term memory (LSTM) mask estimation. We also propose to use a time delay neural network (TDNN) based on the lattice-free version of the maximum mutual information (LF-MMI) trained with augmented all six microphones plus the enhanced data after beamforming. Finally, we use a LSTM language model for lattice and n-best re-scoring. The final system achieved 2.74% WER for the real test set in the 6-channel track, which corresponds to the 2nd place in the challenge. In addition, the proposed baseline recipe includes four different speech enhancement measures, short-time objective intelligibility measure (STOI), extended STOI (eSTOI), perceptual evaluation of speech quality (PESQ) and speech distortion ratio (SDR) for the simulation test set. Thus, the recipe also provides an experimental platform for speech enhancement studies with these performance measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, multi-channel speech recognition has been applied on devices used in daily life, such as Amazon Echo and Google Home. The recognition accuracy is greatly improved by exploiting microphone arrays when compared to single channel microphone devices <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. However, satisfactory performance is still not achieved in noisy everyday environments. Hence, the CHiME-4 challenge is designed to conquer this scenario by recognizing speech in challenging noisy environments <ref type="bibr" target="#b3">[4]</ref>. Through the series of the challenge activities, several speech enhancement and recognition techniques are established as an effective method for this scenario including mask-based beamforming, multichannel data augmentation, and system combination with various front-end techniques <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">[9]</ref>.</p><p>Although many submitted systems in the CHiME-4 challenge have yielded a lot of outcomes in this multi-channel Automatic Speech Recognition (ASR) scenario <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, one of the drawbacks is that all top systems are highly complicated due to multiple systems and fusion techniques, and it is not easy for the other research groups to follow these outcomes. This paper aims to deal with the above drawback by building a new baseline to promote the development of noisy ASR in speech enhancement, separation, and recognition communities.</p><p>We propose a single ASR system to further push the border of this challenge. Most important of all, our system is reproducible since it is implemented in the Kaldi ASR toolkit and other opensource toolkits. All the scripts in our experiments can be downloaded from the official GitHub website 1 . The original CHiME-4 baseline is described in <ref type="bibr" target="#b3">[4]</ref>, which uses a delayand-sum beamformer (BeamformIt) <ref type="bibr" target="#b10">[10]</ref>, a deep neural network with state-level minimum Bayes Risk (DNN+sMBR) criterion <ref type="bibr" target="#b11">[11]</ref>, and recurrent neural network-based language model (RNNLM) <ref type="bibr" target="#b12">[12]</ref>. On the contrary, our proposed system is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We adopt to use Bidirectional long short-term memory (BLSTM) mask based beamformer (Section 3.2), which has been shown to be more effective <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> than BeamformIt. For an acoustic model, the DNN used in baseline is limited to represent long-term dependencies between acoustic characteristics. Hence, a sub-sampled time delay neural network (TDNN) <ref type="bibr" target="#b15">[15]</ref> with the lattice-free version of the maximum mutual information (LF-MMI) is used for our acoustic model <ref type="bibr" target="#b16">[16]</ref> (Section 3.3). This paper also shows the great improvement on the word error rate (WER) when we combine it with data augmentation in a multichannel scenario using all six microphones plus the enhanced data after beamforming. Then, we further use a LSTM language model (LSTMLM), which uses a new training criterion and importance sampling, and has been shown to be more efficient and better in performance <ref type="bibr" target="#b17">[17]</ref>, to re-score hypotheses.</p><p>We also incorporate computation of four different speech enhancement measures in our recipe -perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b18">[18]</ref>, short-time objective intelligibility measure (STOI) <ref type="bibr" target="#b19">[19]</ref>, extended STOI (eSTOI) <ref type="bibr" target="#b20">[20]</ref> and speech distortion ratio (SDR) <ref type="bibr" target="#b21">[21]</ref>. We include these measurements as part of the recipe for two reasons. First, the ASR performance shows only one aspect of the speech enhancement algorithm. Objective enhancement metrics can give an indication on how well the enhancement is with different aspects (e.g., intelligibility, signal distortions). Second, testing an enhancement algorithm with ASR takes a significant amount of computational time, whereas obtaining these scores is quite fast. Hence, it can give an initial indication of how good the enhancement is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In [6], a fusion system in the DNN posterior domain is proposed to get the best result in the competition. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">[9]</ref> also use fusion systems in the decoding hypothesis domain with multiple systems mainly using different front-end techniques. Unlike these highly complicated systems, our proposed system is based on a single system without the above fusion systems, yet achieves comparable performance to these top systems in the challenge task. One of the unique technical aspects of our proposed system is to fully utilize the effectiveness of TDNN with LF-MMI by combining it with multichannel data augmentation techniques, which achieves significant improvement. Our new LSTMLM also contributes to boost the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed system</head><p>Our system starts from BLSTM mask based beamformer and followed by feature extraction. Phoneme to audio alignments are then generated by GMM acoustic model and are fed into TDNN acoustic model for training. Finally, the lattices after first pass decoding in TDNN is re-scored by a 5-gram LM and further re-scored by LSTMLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data augmentation</head><p>Training with multichannel data has been shown to be effective for ASR systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">22]</ref>. This augmentation can increase the variety in the training data and help the generalization to test set. In our work, we not only use data from all 6 channels but also add the enhanced data generated by beamformer to training set.</p><p>Let</p><formula xml:id="formula_0">O = (o(t) ∈ R D |t = 1, .</formula><p>. . , T ) be a sequence of D-dimensional feature vectors with length T , which is a single channel speech recognition case. In our case, we deal with an M -channel input (M = 6), which is represented as O = (om(t) ∈ R D |t = 1, . . . , T, m = 1, . . . , M ). Then, the original training method only uses a particular channel input (e.g., m-th input) as training data to obtain acoustic model parameters Θ, as follows:</p><formula xml:id="formula_1">Θ = arg max Θ L(Om),<label>(1)</label></formula><p>where L is an objective function (log likelihood for the GMM case and negative cross entropy for the DNN case), with reference labels as supervisions. Data augmentation approach tries to use training data of all channels, as follows:</p><formula xml:id="formula_2">Θ = arg max Θ L(O = {Om} M m=1 )<label>(2)</label></formula><p>Further, we extend to include an enhanced data O enh = (o enh(t) ∈ R D |t = 1, . . . , T ) with the above multichannel data, that isΘ = arg max</p><formula xml:id="formula_3">Θ L({O, O enh }),<label>(3)</label></formula><p>where the enhancement data O enh is obtained by a singlechannel masking or beamformer method, which is described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BLSTM mask based beamformer</head><p>We use the BLSTM mask based Generalized Eigenvalue (GEV) beamformer described in <ref type="bibr" target="#b14">[14]</ref>. The GEV beamforming procedure requires an estimate of the Cross-Power Spectral Density (PSD) matrix of the noise and the target speech. The BLSTM model estimates two masks: the first mask indicates the time frequency bin that are probably dominated by speech and the other indicates which are dominated by noise. With the combined speech and noise masks, we can estimate the PSD matrices of speech components Φspeech(b) ∈ C M ×M at frequency bin b, and that of noise components Φnoise(b) ∈ C M ×M , as follows: where</p><formula xml:id="formula_4">Φv(b) = T t=1 wv(t, b)y(t, b)y(t, b) H where v ∈ {speech, noise},<label>(4)</label></formula><formula xml:id="formula_5">y(t, b) ∈ C M is an M -dimensional complex spectrum at time (frame) t in frequency bin b. y H denotes the conjugate transpose. wv(t, b) ∈ [0, 1] is the mask value.</formula><p>The goal of GEV beamformer <ref type="bibr" target="#b23">[23]</ref> is to estimate the beamforming filter f (b), which maximizes the expected SNR for each frequency bin b as given by the equation below:</p><formula xml:id="formula_6">fGEV(b) = argmax f (b) f H (b)Φspeech(b)f (b) f H (b)Φnoise(b)f (b) .<label>(5)</label></formula><p>Eq. <ref type="formula" target="#formula_6">(5)</ref> is equivalent to solve the following eigenvalue problem:</p><formula xml:id="formula_7">(Φnoise(b)) −1 Φspeech(b)f (b) = λf (b),<label>(6)</label></formula><p>where f (b) ∈ C M at each frequency bin b is the M -dimensional complex eigenvector and λ is the eigenvalue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Time delayed neural network with lattice-free MMI</head><p>For acoustic model, we use TDNN with LF-MMI training <ref type="bibr" target="#b16">[16]</ref> instead of DNN+sMBR <ref type="bibr" target="#b11">[11]</ref>. The architecture is similar to those described in <ref type="bibr" target="#b24">[24]</ref>. The LF-MMI objective function is shown below, which is different from usual MMI training <ref type="bibr" target="#b25">[25]</ref> in a sense that we use phoneme sequence L instead of a word sequence to narrow down a search space in the denominator:</p><formula xml:id="formula_8">LMMI = N n=1 log p(O n |S n ) C P (L n ) L p(O n |S L ) C P (L)<label>(7)</label></formula><p>where p(O n |S L ) is the likelihood function of a speech feature sequence O n given the state sequence S L at n'th utterance. P (L) is the phoneme language model probability and C is the probability scale. Note that when combined with the data augmentation technique (described in Section 3.1), TDNN is more effective than DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">LSTM language modeling</head><p>The LSTM based language model (LSTMLM) has been shown to be effective on language modeling <ref type="bibr" target="#b26">[26]</ref>. It is better in finding a longer period of contextual information than conventional RNN. With this property, LSTMLM can predict the next word in a more accurate way than RNNLM. Hence, instead of using a vanilla RNNLM <ref type="bibr" target="#b12">[12]</ref>, we train an LSTMLM on WSJ data, which combines the use of subword features and one-hot encoding. An importance sampling method is used to speed up training. Most important of all, a new objective function LLM is used for LM training, which behaves like cross-entropy objective but trains the output to auto-normalize in order to speed up test time computation: where z is a pre-activation vector in the layer of neural network before the final softmax operation and j is an index for the correct word. More detail can be found in <ref type="bibr" target="#b17">[17]</ref>. </p><formula xml:id="formula_9">LLM = zj + 1 − i exp(zi)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Speech Enhancement Experiments</head><p>First experiments describe the speech enhancement performance of BLSTM-based speech enhancement. For the single channel track, we used the BLSTM masking technique <ref type="bibr" target="#b27">[27]</ref> trained on the 6 channel data and took only the speech mask after the forward propagation. We took a Hadamard product of the single channel spectrogram with the speech mask and used it as the enhanced signal to compare it with the original signal without any enhancement. For the 2 channel and 6 channel tracks, we used the BLSTM based GEV beamformer described in Section 3.2 and compare it with BeamformIt. Four different scores as described in Section 1 -PESQ, STOI, eSTOI and SDR are computed. The BLSTM architecture used in the experiments is listed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The enhancement scores are shown in <ref type="table" target="#tab_0">Table 1</ref>. The 5th channel clean signal from the 6ch data convolved with room impulse response was used as the reference signal for computing all the four metrics. For the 1 channel track, the BLSTM mask gives significantly better scores in all four metrics compared to using the noisy data without any enhancement. However, this is contrary to the ASR results, which will be discussed in the next section. BeamformIt has better SDR scores compared to BLSTM GEV in both the multi-channel tracks. Also, for both the multi-channel track data, eSTOI is slightly better for BLSTM GEV. In the 6ch track experiments, BLSTM GEV has a significantly better PESQ score. Overall, BLSTM-based speech enhancement shows improvement in most of conditions except for the case of the multichannel SDR metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Speech Recognition Experiments</head><p>Our system is trained on the speech recognition toolkit Kaldi <ref type="bibr" target="#b28">[28]</ref>. For TDNN acoustic model training, backstitch optimization method <ref type="bibr" target="#b29">[29]</ref> is used. The decoding is based on 3-gram language models with explicit pronunciation and silence probability modeling as described in <ref type="bibr" target="#b30">[30]</ref>. The model is re-scored by a 5-gram language model first. Then the Kaldi-RNNLM <ref type="bibr" target="#b17">[17]</ref> is used for training the LSTMLM, and n-best re-scoring is used to improve performance. We got our best result in 6 channel experiments by averaging forward and backward LSTMLM. The RNN re-score weight is set to be 1.0, which means the results of 5-gram LM is completely discarded. All the results in this section are reported in terms of word error rate (WER). We also provide the parameters used in our system in <ref type="table" target="#tab_1">Table 2</ref>.  <ref type="table" target="#tab_2">Table 3</ref> shows the effectiveness of the data augmentation for the system using TDNN with BeamformIt and RNNLM, which are described in Section 2, in the 6 channel track experiment. We confirmed the improvement by adding enhanced data in almost all cases except for the simulation test data. This is also found in 2 channels experiment when using TDNN (i.e. row 3 and row 4 in <ref type="table" target="#tab_4">table 5)</ref>. <ref type="table" target="#tab_3">Tables 4 and 5</ref> show the WER of 6 channel and 2 channel experiments. We change our experimental condition incrementally to compare the effectiveness of each method described in Section 2. In most of the situations, every method improved the WER steadily. We observed that the performance was degraded   if we applied enhanced data on the system using DNN+sMBR (i.e. row 2 and row 3 in table 4), while TDNN with LF-MMI could make use of the enhanced data, as discussed above. In addition, comparing with the speech enhancement results in Table 1, it shows that better speech enhancement scores do not necessarily gives lower WER. Especially, there always seems to be a negative correlation between the ASR performance and the SDR scores. <ref type="table" target="#tab_5">Table 6</ref> illustrates the results of the 1 channel track experiment. We found that BLSTM masking was not effective if we only used one microphone although it scores better in terms of all four speech enhancement metrics in <ref type="table" target="#tab_0">Table 1</ref>. From row 3 and row 5 of 6, the WER with BLSTM masking was degraded more than twice when compared to the system without BLSTM masking. However, we also discovered that after adding the enhanced data into the system with BLSTM masking, the WER became closer to the best setup without masking, which can be seen in row 4 and row 6 of 6. Thus, adding the enhanced data seems to be a good strategy to mitigate the degradation of speech enhancement.</p><p>Finally, <ref type="table" target="#tab_6">Table 7</ref> presents the comparison with the official baseline and top systems in the CHiME-4 challenge. We can see that all of these systems use a fusion technique to get their best WER. On the other hand, our proposed single system achieved 76% relative improvement from the official baseline, and achieved the 2nd best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper describes our single ASR system for CHiME-4 speech separation and recognition challenge. The system consists of BLSTM masked GEV beamformer (Section3.2), TDNN with LF-MMI as acoustic model (Section3.3) and re-scoring using LSTMLM (Section3.4), which trained on all 6 channels data plus enhanced data generated by beamformer (Section3.1). The system finally achieved 2.74% WER, which outperforms the 2nd place result in the challenge. The system is publicly available through the Kaldi speech recognition toolkit. Our future work will explore different architectures for TDNN and LSTM networks to further improvement. Furthermore, this system can be applied to other multichannel tasks such as AMI <ref type="bibr" target="#b31">[31]</ref>, and the CHiME-5 challenge <ref type="bibr" target="#b32">[32]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of speech recognition system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Speech Enhancement Scores</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Dev (Simu)</cell><cell></cell><cell></cell><cell cols="2">Test (Simu)</cell><cell></cell></row><row><cell cols="10">Track Enhancement Method PESQ STOI eSTOI SDR PESQ STOI eSTOI SDR</cell></row><row><cell>1ch</cell><cell>No Enhancement</cell><cell>2.01</cell><cell>0.82</cell><cell>0.61</cell><cell>3.92</cell><cell>1.98</cell><cell>0.81</cell><cell>0.60</cell><cell>4.95</cell></row><row><cell>1ch</cell><cell>BLSTM Mask</cell><cell>2.52</cell><cell>0.88</cell><cell>0.73</cell><cell>9.26</cell><cell>2.46</cell><cell>0.87</cell><cell>0.71</cell><cell>10.76</cell></row><row><cell>2ch</cell><cell>BeamformIt</cell><cell>2.15</cell><cell>0.85</cell><cell>0.65</cell><cell>4.61</cell><cell>2.07</cell><cell>0.83</cell><cell>0.62</cell><cell>5.60</cell></row><row><cell>2ch</cell><cell>BLSTM Gev</cell><cell>2.13</cell><cell>0.87</cell><cell>0.69</cell><cell>2.86</cell><cell>2.12</cell><cell>0.87</cell><cell>0.69</cell><cell>3.10</cell></row><row><cell>6ch</cell><cell>BeamformIt</cell><cell>2.31</cell><cell>0.88</cell><cell>0.70</cell><cell>5.52</cell><cell>2.20</cell><cell>0.86</cell><cell>0.65</cell><cell>6.30</cell></row><row><cell>6ch</cell><cell>BLSTM Gev</cell><cell>2.45</cell><cell>0.88</cell><cell>0.75</cell><cell>3.57</cell><cell>2.46</cell><cell>0.87</cell><cell>0.73</cell><cell>2.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental configurations</figDesc><table><row><cell>BLSTM mask estimation</cell><cell></cell></row><row><cell>input layer dimension</cell><cell>513</cell></row><row><cell>L1 -BLSTM layer dimension</cell><cell>256</cell></row><row><cell>L2 -FF layer 1 (ReLU) dimension</cell><cell>513</cell></row><row><cell>L3 -FF layer 2 (clipped ReLU) dimension</cell><cell>513</cell></row><row><cell>L4 -FF layer (Sigmoid) dimension</cell><cell>1026</cell></row><row><cell>p dropout for L1, L2 and L3</cell><cell>0.5</cell></row><row><cell>TDNN acoustic model</cell><cell></cell></row><row><cell>input layer dimension</cell><cell>40</cell></row><row><cell>hidden layer dimension</cell><cell>750</cell></row><row><cell>output layer dimension</cell><cell>2800</cell></row><row><cell>l2-regularize</cell><cell>0.00005</cell></row><row><cell>num-epochs</cell><cell>6</cell></row><row><cell>initial-effective-lrate</cell><cell>0.003</cell></row><row><cell>final-effective-lrate</cell><cell>0.0003</cell></row><row><cell>shrink-value</cell><cell>1.0</cell></row><row><cell>num-chunk-per-minibatch</cell><cell>128,64</cell></row><row><cell>LSTM language model</cell><cell></cell></row><row><cell>layers dimension</cell><cell>2048</cell></row><row><cell>recurrent-projection-dim</cell><cell>512</cell></row><row><cell>N-best list size</cell><cell>100</cell></row><row><cell>RNN re-score weight</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>WER of adding enhanced data when using TDNN with BeamformIt and RNNLM in the 6 channel track experiment</figDesc><table><row><cell>Data Augmentation</cell><cell>Dev (%)</cell><cell>Test (%)</cell></row><row><cell></cell><cell cols="2">real simu real simu</cell></row><row><cell>all 6ch data</cell><cell cols="2">3.97 4.33 7.04 7.39</cell></row><row><cell cols="3">all 6ch and enhanced data 3.74 4.31 6.84 7.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>WER of 6 channel track experiments</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>Dev (%)</cell><cell cols="2">Test (%)</cell></row><row><cell>Data Augmentation</cell><cell>Acoustic Model</cell><cell cols="3">Beamforming Language Model real simu</cell><cell>real</cell><cell>simu</cell></row><row><cell>only 5th channel</cell><cell>DNN+sMBR</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell cols="3">5.79 6.73 11.50 10.92</cell></row><row><cell>all 6ch data</cell><cell>DNN+sMBR</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell>5.05 5.82</cell><cell>9.50</cell><cell>9.24</cell></row><row><cell>all 6ch and enhanced data</cell><cell>DNN+sMBR</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell cols="2">5.62 6.46 10.27</cell><cell>9.41</cell></row><row><cell cols="2">all 6ch and enhanced data TDNN with LF-MMI</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell>3.74 4.31</cell><cell>6.84</cell><cell>7.49</cell></row><row><cell cols="3">all 6ch and enhanced data TDNN with LF-MMI BLSTM Gev</cell><cell>RNNLM</cell><cell>2.83 2.94</cell><cell>4.01</cell><cell>3.80</cell></row><row><cell cols="3">all 6ch and enhanced data TDNN with LF-MMI BLSTM Gev</cell><cell>LSTMLM</cell><cell>1.90 2.10</cell><cell>2.74</cell><cell>2.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>WER of 2 channel track experiments</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>Dev (%)</cell><cell cols="2">Test (%)</cell></row><row><cell>Data Augmentation</cell><cell>Acoustic Model</cell><cell cols="3">Beamforming Language Model real simu</cell><cell>real</cell><cell>simu</cell></row><row><cell>only 5th channel</cell><cell>DNN+sMBR</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell cols="3">8.23 9.50 16.58 15.33</cell></row><row><cell>all 6ch data</cell><cell>DNN+sMBR</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell cols="3">6.87 8.06 13.33 12.57</cell></row><row><cell>all 6ch data</cell><cell>TDNN with LF-MMI</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell cols="2">5.57 6.08 10.53</cell><cell>9.90</cell></row><row><cell cols="2">all 6ch and enhanced data TDNN with LF-MMI</cell><cell>BeamformIt</cell><cell>RNNLM</cell><cell cols="3">5.03 6.02 10.20 10.35</cell></row><row><cell cols="3">all 6ch and enhanced data TDNN with LF-MMI BLSTM Gev</cell><cell>RNNLM</cell><cell>3.79 5.03</cell><cell>6.93</cell><cell>6.07</cell></row><row><cell cols="3">all 6ch and enhanced data TDNN with LF-MMI BLSTM Gev</cell><cell>LSTMLM</cell><cell>2.85 3.94</cell><cell>5.40</cell><cell>5.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>WER of 1 channel track experiments</figDesc><table><row><cell>Dev (%)</cell><cell>Test (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Final WER comparison for the real test set.</figDesc><table><row><cell>System</cell><cell cols="2"># systems WER (%)</cell></row><row><cell>CHiME-4 baseline [4]</cell><cell>1</cell><cell>11.51</cell></row><row><cell>Proposed system</cell><cell>1</cell><cell>2.74</cell></row><row><cell>USTC-iFlytek [6]</cell><cell>5</cell><cell>2.24</cell></row><row><cell>RWTH/UPB/FORTH [7]</cell><cell>5</cell><cell>2.91</cell></row><row><cell>MERL [8]</cell><cell>6</cell><cell>2.98</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/kaldi-asr/kaldi/pull/2142 arXiv:1803.10109v1 [cs.SD] 27 Mar 2018</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The third CHiMEspeech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A summary of the REVERB challenge: state-ofthe-art and remaining challenges in reverberant speech processing research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leutnant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Acoustic modeling for google home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caroselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="399" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of environment, microphone and data simulation mismatches in robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="535" to="557" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Espi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="436" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The USTC-iFlytek system for CHiME-4 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHiME</title>
		<meeting>CHiME</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The RWTH/UPB/FORTH system combination for the 4th CHiME challenge evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Menne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexandridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in CHiME-4 workshop</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multichannel speech recognition: LSTMs all the way through</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHiME-4 workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised network adaptation and phonetically-oriented system combination for the chime-4 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Homma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHiME</title>
		<meeting>CHiME</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acoustic beamforming for speaker diarization of meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wooters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2011" to="2022" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequencediscriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model,&quot; in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved MVDR beamforming using single-channel mask prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1981" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Phoneme recognition using time-delay neural networks,&quot; in Readings in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural network language modeling with letterbased features and importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual Evaluation of Speech Quality (PESQ)-a New Method for Speech Quality Assessment of Telephone Networks and Codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
		<idno>ser. ICASSP &apos;01</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Acoustics, Speech, and Signal Processing</title>
		<meeting>the Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-microphone speech recognition integrating beamforming, robust feature extraction, and advanced DNN/RNN backend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="401" to="418" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind acoustic beamforming based on generalized eigenvalue decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Warsitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1529" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discriminative training for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Felix And Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename></persName>
		</author>
		<title level="m">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note>booktitle=International Conference on Latent Variable Analysis and Signal Separation. organiza-tion=Springer</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Backstitch: Counteracting finite-sample bias via negative steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pronunciation and silence probability modeling for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The AMI meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bourban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karaiskos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research</title>
		<meeting>the 5th International Conference on Methods and Techniques in Behavioral Research</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The fifth &apos;CHiME speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>submitting</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
