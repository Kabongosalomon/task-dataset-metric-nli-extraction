<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TLSAN: Time-aware Long-and Short-term Attention Network for Next-item Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202116-03-17">March 17, 2021 16 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjing</forename><surname>Wang</surname></persName>
							<email>dongjing.wang@hdu.edu.cndongjingwang</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TLSAN: Time-aware Long-and Short-term Attention Network for Next-item Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202116-03-17">March 17, 2021 16 Mar 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Neurocomputing</note>
					<note>(Dongjin Yu) sensitive next-item recommendation.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>personalized recommendation</term>
					<term>next-item recommendation</term>
					<term>time-aware</term>
					<term>long-and short-term</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep neural networks are widely applied in recommender systems for their effectiveness in capturing/modeling users' preferences. Especially, the attention mechanism in deep learning enables recommender systems to incorporate various features in an adaptive way. Specifically, as for the next item recommendation task, we have the following three observations: 1) users' sequential behavior records aggregate at time positions ("time-aggregation"), 2) users have personalized taste that is related to the "time-aggregation" phenomenon ("personalized time-aggregation"), and 3) users' short-term interests play an important role in the next item prediction/recommendation. In this paper, we propose a new Time-aware Long-and Short-term Attention Network (TLSAN) to address those observations mentioned above. Specifically, TLSAN consists of two main components. Firstly, TLSAN models "personalized time-aggregation" and learn user-specific temporal taste via trainable personalized time position embeddings with category-aware correlations in long-term behaviors. Secondly, long-and short-term feature-wise attention layers are proposed to effectively capture users' long-and short-term preferences for accurate recommendation. Especially, the attention mechanism enables TLSAN to utilize users' preferences in an adaptive way, and its usage in long-and short-term layers enhances TLSAN's ability of dealing with sparse interaction data. Extensive experiments are conducted on Amazon datasets from different fields (also with different size), and the results show that TLSAN outperforms state-ofthe-art baselines in both capturing users' preferences and performing time- * Corresponding author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As people enjoy the convenience of the Internet every day, such as shopping online, massive amounts of data have been generated and recorded. Especially, those data may indicate important information, such as users' preferences and behavior patterns, which can be utilized by recommender systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to provide personalized services or contents for users and promote their experience.</p><p>Existing recommender systems are based on various methods and strategies, such as collaborative filtering (CF) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, Markov chains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, matrix factorization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>, Bayesian probabilistic models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> and deep neural networks (DNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In recent years, attention mechanism <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> begins to be used in many tasks, including recommendation, to incorporate various features adaptively and increase model training speed. Especially, attention can be combined with various methods without adding too much model complexity <ref type="bibr" target="#b12">[13]</ref>. Furthermore, next-item recommender systems model users' records and corresponding temporal context as behavior sequences to utilize the context in a better way. Specifically, users' preferences can be captured from the entire sequence, and it can help make accurate predictions to provide personalized services.</p><p>However, existing approaches still face five main issues: (1) Categoryaware correlations modeling: Most existing approaches implicitly group users, such as user-based CF <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and social circle <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, which can not explicitly model the correlation between item category. Besides, some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> explicitly incorporate user category information as users' feature at the cost of high time complexity. Furthermore, users' categories may change dynamically over time and it is not accurate to model user as a particular category at each time position. (2) Feature-level representation learning: As is shown in <ref type="figure" target="#fig_1">Figure 1</ref> (Embeddings), each dimension in the item's embedding have specific effects, which may make different contribution to the tasks of prediction/recommendation. For example, the user in <ref type="figure" target="#fig_1">Figure 1</ref> is more interested in books (Book 1 and Book 2 ) than game (Game 1 ), so the weight of the feature that represents book is 9. Furthermore, the user  <ref type="figure" target="#fig_1">Figure 1</ref>: The user's long-term preferences can be inferred from her/his long-term behaviors, and the user's short-term interests are reflected in her/his short-term behaviors, both of which change over time and can be captured by the user's embedding. Specifically, Book 1 and Book 2 are in long-term behaviors while Game 1 is in short-term behaviors. (1) Long-term: If the prominent item category in user's behavior records is book, her/his category will be book. (2) Embeddings: We concatenate ID embedding and category embedding as user embedding. Each dimension of the embedding is manually labeled for convenience of description. (3) Long-and Short-term: Considering only long-term preferences of users without strengthening the impact of recent interests can lead to inaccurate recommendations. (4) Time-aggregation: Behavior records generally aggregate at time positions, which may be different for each user who has personalized behavior patterns.</p><p>prefers Book 1 to Book 2 , which should be incorporated when we calculate her/his preference score. (3) Personalized time-aggregation: Each user's behavior records generally aggregate at time positions ("time-aggregation"), as shown in <ref type="figure" target="#fig_1">Figure 1</ref> (Time-aggregation layer). Furthermore, each person has her/his own personalized behavior patterns, so different people react differently to similar contexts, such as interval of "time-aggregation". We call this phenomenon as "personalized time-aggregation", which is also consistent with our daily behavior patterns. For example, some people who are not active on normal days may generate a lot of shopping records on Amazon on some certain days like Black Friday. Besides, some active users do not enjoy the fun of Black Friday because of the complex coupons. Therefore, the recommendation mechanism for two kinds of users are different since they have personalized behavior patterns and "time-aggregation" phenomenon. (4) Long-and short-term preferences integration: The next recommendation task is time-sensitive, and users' recent behaviors (shortterm) generally play a more important role than users' actions/records that happened long time ago (long-term) . As shown in <ref type="figure" target="#fig_1">Figure 1</ref> (long-and shortterm), the user like books, and he/she become interested in games recently. Therefore, both users' long-and short-term preferences are important for accurate prediction and recommendation, although they may have different influences. (5) Data sparsity problem: The data in many real-world scenarios is quite sparse, since the amount of users or items may reach millions easily. Especially, it is difficult to capture users' current preferences accurately only from sparse behaviors data.</p><p>Some recommendation methods are proposed to solve one or several of the issues mentioned above. For example, ATRank <ref type="bibr" target="#b19">[20]</ref> captures users' long-term preferences by self-attention and models the correlation between target item and the historical items by vanilla attention, but it ignores users' individual information, such as their category and temporal preferences. CSAN <ref type="bibr" target="#b20">[21]</ref> considers different features by feature-wise self-attention and describes "timeaggregation" with position matrix. However, the position matrix is untrainable, which may fail to capture personalized position preferences and cause an order of magnitude difference between position matrix and embeddings. SHAN <ref type="bibr" target="#b21">[22]</ref> adopt a hierarchical attention network to capture users' long-and short-term preferences separately, but the user embedding in SHAN does not incorporate users' information, such as category. Besides, SHAN ignores time decay in long-term layer. In PACA <ref type="bibr" target="#b11">[12]</ref>, the trainable global position embedding for modeling the effect of records in each position is the same for all the users, which may influence its ability of generalize personalized results. However, to the best of our knowledge, there is no attempt to address all those four issues.</p><p>In this paper, we propose a new recommendation model named Timeaware Long-and Short-term Attention Network (TLSAN) to address those issues mentioned above. Especially, TLSAN can effectively incorporate features of users and items as well as their correlations. Besides, the devised attention mechanism enables TLSAN to capture users' long-and short-term preferences from their behavior sequences and leverage both preferences adaptively for accurate prediction/recommendation. Specifically, the proposed approach TLSAN consists of two main parts: (1) Personalized time position embedding module is designed to model users' "personalized time-aggregation" and capture user-specific temporal preferences. Especially, behavior records at different time positions may have different contributions to the prediction/recommendation tasks, which depends on users' personalized behavior patterns. (2) Long-and short-term feature-wise attention layers use long-term layer to capture users' long-term preferences and adopt a short-term layer to emphasize users' short-term interests. In sparse dataset, short-term interests can be the main determinants of predicting the next item and alleviating the data sparsity problem due to its small data demands. Specifically, feature-wise attention (a more finegrained attention) can help to effectively capture users' long and short-term preferences in each dimension and leverage both preferences for better recommendation. Besides, we model the category-aware interactions between features of users or items in user-user or user-item pairs by dynamic user category extraction before model training, which does not increase model complexity. What is more, multi-heads integration is adopt to model the information from different semantic sub-spaces and integrate them as well as their correlations in a unified and parallel way to further improve the performance of recommendation.</p><p>Previous models utilized deep neural networks to incorporate a lot of factors at the cost of more pre-training time and higher model complexity. TLSAN utilizes attention mechanism comprehensively and adds more parallelism to itself, so that TLSAN can converge quickly and achieve good results. Due to the lightness of attention mechanism as well as its concise principle, TLSAN can be easily adjusted to various datasets and scenarios in an efficient way.</p><p>The main contributions in this paper are as follows:</p><p>• We propose a Time-aware Long-and Short-term Attention Network (TLSAN) to capture users' temporal preferences from their historical behavior sequences for accurate next item recommendation.</p><p>• To strengthen time-sensitive next-item recommendation ability and deal with sparsity, we firstly capture weakened users' long-term preferences by feature-wise attention in long-term layer with the personalized time position embeddings dynamically weakening long-term behaviors. Then we emphasize the effect of users' short-term interests by featurewise attention in short-term layer.</p><p>• In the above two layers, we incorporate category-aware user-user and user-item correlations without increasing model complexity by dynamic user category extraction. Besides, multi-head strategy is introduced to improve parallelism and find correlations between attention blocks.</p><p>• Experiments on several kinds of Amazon public datasets with different size show that TLSAN outperforms state-of-the-art baselines and can maintain effectiveness on different recommendation scenarios.</p><p>The remaining of this paper is organized as follows. Section 2 describes existing works related to our approach TLSAN. Section 3 introduces TLSAN in details, including the framework and the main components. In Section 4, we evaluate TLSAN on several kinds of datasets with different size, and also explore the impact of hyper-parameters as well as TLSAN's key components. Finally, the conclusion and future works are provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Earlier next-item and sequence recommendation utilized traditional methods, such as collaborative filtering and Markov Chain. Later, the emergence of neural networks further enhanced recommender systems' ability of extracting users' preferences. However, most deep neural networks based methods are generally more complex, which influences their parallelism and interpretability, especially when many factors are incorporated. The attention mechanism that is widely applied in natural language processing (NLP) and Computer vision (CV), has shown its effectiveness in recommender systems. The attention mechanism has more parallelism than convolutional neural networks (CNN) and recurrent neural networks (RNN), and provides more possibilities of constructing powerful networks. Besides, researchers have proposed various methods and strategies, such as position embedding, to capture time series information for better sequential recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sequential Recommendation</head><p>The methods used in existing sequential recommender systems can be classified into three main categories: collaborative filtering (CF), Markov chain (MC) and neural networks.</p><p>CF based methods: Earlier, with the assumption that users who have made similar behaviors may behave similarly next time, researchers utilize CF <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref> to perform next item recommendation. However, this kind of recommender systems are mainly based on statistical method <ref type="bibr" target="#b23">[24]</ref>, which cannot accurately distinguish similar users.</p><p>MC based methods: To capture the sequential information in the sequence, researchers also introduce MC to the recommendation models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. These methods can well model sequential patterns (short-term interests) with context information as Fossil <ref type="bibr" target="#b5">[6]</ref>, and are also good at predicting the next item. However, MC only considers the correlations between adjacent items <ref type="bibr" target="#b4">[5]</ref>, so the models based on MC may have difficulty in capturing the users' long-term preferences. Besides, such methods cannot effectively model the dynamic changes of original context over time.</p><p>Neural networks based methods: Multi-layer perceptions (MLPs) can introduce non-linear user-item correlations <ref type="bibr" target="#b24">[25]</ref>, but it is not easy to determine hyper-parameters, such as how many layers are sufficient for the tasks. CNN can be utilized to extract features from texts, audios and pictures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. For example, Realtime-MF <ref type="bibr" target="#b26">[27]</ref> employs a CNN to obtain event embedding from the related words in reviews and the researchers find that the frequent words can describe the event well, but it is not easy to capture semantics without enough texts. RNN can capture dynamic time series information <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref>. As important variants of RNN, LSTM <ref type="bibr" target="#b28">[29]</ref> and GRU <ref type="bibr" target="#b29">[30]</ref> use the gating mechanism to weaken the impact of long-term preferences, but they need much time to train the effective models. To build representations of both past and future contexts, recent approaches also apply bi-directional encoders <ref type="bibr" target="#b30">[31]</ref>. Recent recurrent recommendation model CDHRM devises a cross-domain user-level RNN to capture global users' dynamic preferences and two domain-specific session-level RNNs to separately capture users' specific preferences in different domains, and fuse these two kinds of RNN to obtain comprehensive users' preferences <ref type="bibr" target="#b27">[28]</ref>. However, the concatenation and product fusion method in CDHRM is still not adaptive enough to fuse domain-specific RNNs together from different domains with different preferences, behavior distribution, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanism</head><p>Attention mechanism is similar to human's visual attention: we always focus on the most important part of what we see. It has been widely utilized in many other fields, such as NLP <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> and CV <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Specifically, attention mechanism makes it easy to memorize various remote dependencies or focus on important parts of the input. In addition, attention-based methods are often more interpretable <ref type="bibr" target="#b35">[36]</ref> than traditional deep learning models.</p><p>Parallelism of attention mechanism and its preferences that can be fused with other models are widely applied in various kinds of task. For example, Transformer <ref type="bibr" target="#b10">[11]</ref> has been widely utilized in NLP, CV, recommender systems and other fields since proposed <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. It fully takes advantage of attention mechanism, and further improves parallelism with multi-heads <ref type="bibr" target="#b38">[39]</ref>. ATRank <ref type="bibr" target="#b19">[20]</ref> applies multi-head self-attention components in Transformer to recommendation tasks, speeding up training and improving prediction ability, but it does not introduce users' individual information. CSAN <ref type="bibr" target="#b20">[21]</ref> introduces a feature-wise self-attention mechanism with bi-directional position embedding on input sequence to better discover internal correlations within sequences, which need lots of trainable parameters increasing model complexity. SHAN <ref type="bibr" target="#b21">[22]</ref> utilizes original attention mechanism to capture users' long-and short-term preferences and introduce context information by user embedding, but it ignores time decay in the long-term behaviors and the user-item correlations in each feature dimensions. As we can see, existing attention mechanisms still have much room to improve in modeling dynamic and diverse preferences of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Position Embedding (PE)</head><p>In the field of recommender systems, how to deal with sequence information has become a topic widely considered. For the next-item recommendation, user preferences are not fixed but change with time. The temporal recommendation of the Netflix Prize utilized time series information on many datasets, which greatly optimized the effect of models <ref type="bibr" target="#b39">[40]</ref>. Recent studies have also pointed out that PE can make improvements on models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>CNNs implicitly capture time series information by adjusting the size of the kernel and sequentially moving the kernel during training. If explicit PE is utilized at the same time, it can make some improvements to the model <ref type="bibr" target="#b41">[42]</ref>. One approach of explicit PE is to take time information (such as timestamps) from datasets, and then concatenate its embedding or add its value directly to the input embedding like ATRank, but it assumes that users all have the same taste to the same time interval. Second approach of explicit PE can be a deterministic function that calculates the value at each position by sine and cosine, such as Transformer, with the same assumption mentioned above. Another approach is to generate a position matrix and add its values to input embeddings at corresponding position like CSAN, but untrainable position matrix cannot capture personalized position preferences for users and can cause an order of magnitude difference between position matrix and embeddings. In addition, PACA <ref type="bibr" target="#b11">[12]</ref> treats each position as a position embedding, and then multiply position embedding and session embedding to capture time information of each session. But PACA's trainable embedding is only able to learn the effect of each position in the session for all the users, it cannot recognize the different effects of the same position to two users.</p><p>The PE methods mentioned above, except PACA, all generate the fixed values obtained from original datasets. However, they ignore the fact that difference between two persons even when their behavior records contain the same position information such as intervals. We call the fact as the "personalized position information", combine it with the "time-aggregation" phenomenon and name the combination as "personalized time-aggregation" phenomenon. Then we describe this phenomenon by personalized time position embedding and set a global trainable parameter that can automatically tune the order of magnitude of the position embedding while training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Before going into details of our proposed model, we first define the problem and basic concepts. Formally, let U = {u 1 , u 2 , ..., u, ...} and I = {j 1 , j 2 , ..., j, ...} denote the user set and item set, respectively. C = {c 1 , c 2 , ..., c, ...} is the set of item categories. For each user u ∈ U, her/his sequential behaviors are represented as</p><formula xml:id="formula_0">L u t = {S u 1 , S u 2 , ..., S u t }, where t is the current time, and S u i ⊆ I(i ∈ [1, t])</formula><p>represents the session of user u at time i. Specifically, we divide users' behavior sequences into sessions by data, and each session represents a user's behaviors within a day. Obviously, the session S u t contains user u's recently purchased items which reflect her/his short-term interests at time t. Besides, u's long-term behavior sequence at</p><formula xml:id="formula_1">time 1 ∼ (t−1) (before time t), denoted by L u t−1 = {S u 1 , S u 2 , ..., S u t−1 }, indicate u's long-term preferences.</formula><p>We call L u t−1 and S u t long-term and short-term behavior sequences w.r.t time t, respectively. In summary, our task is to predict the target users' next behaviors and recommend appropriate items to them.</p><p>The key notations used in this paper are presented in <ref type="table" target="#tab_1">Table 1</ref>. User embedding at time t for user u u t−1 , u t User long-term and current preferences representation for user u y j Label of item j λ L2-loss weight</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework</head><p>The architecture diagram of our new model is shown in <ref type="figure">Figure 2</ref>. Specifically, TLSAN performs next item recommendation according to the following four steps: (1) Dynamic user category extraction: We obtain dynamic user category ID from L u t and C, denoted by c u t . Then c u t is utilized in lookups from C to get dynamic user category embedding at time t. User ID embeddings U and other initial embeddings are shown in <ref type="table" target="#tab_1">Table 1</ref>. Especially, we limit the long-term sequence length to L s , so L u t−1 = {l n−Ls , ..., l j , ..., l n } where n is the index of the last item in L u t−1 . Note that l j is the concatenation of item ID and category embedding, so as s j in S u t .</p><p>(2) Personalized time position embedding: We propose trainable personalized time position embeddings, denoted by P u = {p n−Ls , ..., p j , ..., p n }, to model "personalized time-aggregation" phenomenon in long-term sequences and capture user-specific temporal preferences. Then, we can get time-aware history  <ref type="figure">Figure 2</ref>: The framework of TLSAN. Specifically, we divide user's behavior records into long-term and short-term behavior records by time, and randomly select an item in shortterm records as the next item to be predicted. We model user's "personalized timeaggregation" and capture user-specific temporal preferences by personalized time position embeddings on long-term records and get time-aware history representation. Then we can obtain user's current preferences at time t by the long-and short-term feature-wise attention layers. To make the picture more concise, we have omitted some superscript u.</p><p>representation H u t−1 = {h n−Ls , ..., h j , ..., h n }.</p><p>(3) Long-and short-term feature-wise attention layers: We propose long-term feature-wise attention layer to capture the long-term preferences of user u, denoted by u t−1 . Then we propose short-term feature-wise attention layer to combine longterm preferences u t−1 and short-term interests from S u t to obtain u's current preferences, denoted by u t . Besides, multi-heads integration is adopt to model the information from different semantic sub-spaces in a parallel way. (4) Finally, we utilize TLSAN to recommend the next item that matches the target user's current preferences u t . Next, we will introduce each step in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic User Category Extraction</head><p>Most existing approaches implicitly group users, such as user-based CF <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and social circle <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, which cannot explicitly model the correlations between item category. Besides, some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> explicitly incorporate user category information as users' feature at the cost of high time complexity. Furthermore, users' categories may change dynamically over time and it is not accurate to model user as a particular category at each time position. Especially, the item category that the user is mostly interested in at time t can be considered as her/his current category to incorporate both strong user-user and user-item correlations in terms of category. Therefore, we choose the most frequent item category from L u t of user u as the category of user u at time t to obtain dynamic user category, denoted by c u t , which can be done before model training. Then we utilize lookups to extract user category embeddings from C and concatenate it vertically with user ID embedding to get u e,t . The specific implementation is as follows:</p><formula xml:id="formula_2">u e,t = Conc(U (u), C(c u t )),<label>(1)</label></formula><p>where u e,t ∈ R 2d f , U (.) and C(.) represent user ID lookups and category embedding lookups. Conc(.) is the concatenation function. By replacing U (u) with I(j) and c u t with c j we can also obtain the item embeddings, where I(.) and c j represent item ID lookups and item category, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Personalized time position embedding</head><p>Historical events such as promotions occurred to user (e.g. Black Friday) all happened near a certain time, which results in dense and sparse gaps in user's behavior records. We call this phenomenon "time-aggregation" which can implicitly reflect related environmental information, preferences information, etc. Obviously, each user has her/his own personalized behavior patterns, so different people react differently to similar context, such as the same interval of "time-aggregation", which is called "personalized timeaggregation" phenomenon. To model the "personalized time-aggregation" phenomenon in long-term sequences and capture user-specific temporal preferences, we propose personalized time position embedding method. During experiments, we find that the order of magnitude of position embedding is smaller than item embedding, which can weaken the influence of position embedding. Therefore, we multiply a global trainable parameter γ (we take the initial value as 1.0) to the position embedding. Because we use the reciprocal of the absolute value of the bucketized time difference between the past and the current time as the value of time position, the importance of the items in the current sequence starts to decrease from the last one in the time decreasing direction. At the same time, if the behaviors were generated at the same day, the values in the original time-aggregation embedding will be the same. The values in the original time-aggregation embedding can be figuratively compared to a stair, and each rung is a time position. More specifically, the personalized time position embedding method is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Formally, we can obtain the time-aware historical representations as follows:</p><formula xml:id="formula_3">h j = γ p j l j ,<label>(2)</label></formula><p>where denotes the element-wise product, h j and l j are the j-th item in the time-aware historical representations and the corresponding long-term embeddings, respectively. p j is the j-th personalized time position embedding for user u. Then user u's historical behavior records are represented as H u t−1 = {h n−Ls , ..., h j , ..., h n } ∈ R 4d f ×Ls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Long-and Short-term Feature-wise Attention Layers</head><p>The long-and short-term feature-wise attention layers consist of two parts: long-term feature-wise attention layer and short-term feature-wise attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Long-term Feature-wise Attention Layer</head><p>In fact, user's behavior records always contain a lot of timestamps, and earlier behavior records are generally less important than recent behavior records. Feature-wise attention is utilized on time-aware history representation H t−1 to capture the changes of user's preferences in each dimension by the following formulas: </p><formula xml:id="formula_4">att long j = Att 2d f (W 1 , W 2 , b 1 , b 2 , h j ) = W T 1 σ(W 2 h j + b 2 ) + b 1 ,<label>(3)</label></formula><formula xml:id="formula_5">u t−1 = Ls j=1 a long j h j ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">W 1 , W 2 ∈ R 2d f ×2d f , and b 1 , b 2 ∈ R 2d f are trainable parameters.</formula><p>The superscript 2d f means the dimensions of the embeddings in Att(.). Especially, we choose ReLU for activation function σ(.) to enhance non-linear capability. Now we have captured the long-term preferences of user u, denoted by u t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Short-term Feature-wise Attention Layer</head><p>Users' long-term preferences can be inferred from their long-term behavior records, which cannot represent their recent interests well. For the next-item recommendation, short-term interests are generally more important than users' long-term preferences, especially in sparse dataset. Therefore, we separate short-term behaviors out to emphasize the role of recent user's behavior records. However, it still requires careful consideration how to select the number of items in the short-term session. According to the "Peak-End Rule" <ref type="bibr" target="#b42">[43]</ref> in behavior economics, the most impressive and last items generally have the most significant impact on the current decision. The previous most concentrated item has been taken into account in the long-term layer, and the behaviors happened in the latest day become the focus in the short-term layer.</p><p>Besides, short-term layer combines user's short-term interests and longterm preferences together. Because the behaviors in the short-term session only happened within a day, we do not utilize time information and position embedding here to emphasize short-term interests and reduce model complexity. Formally, the short-term layer is defined as:</p><formula xml:id="formula_8">att short j = Att 2d f (W 3 , W 4 , b 3 , b 4 , s j ) = W T 3 σ(W 4 (s j ) + b 4 ) + b 3 ,<label>(6)</label></formula><formula xml:id="formula_9">[a short j ] k = e [att short j ] k |S u t |+1 j=0 e [att short j ] k ,<label>(7)</label></formula><formula xml:id="formula_10">u t = u e,t ⊕ |S u t |+1 j=1 a short j s j ,<label>(8)</label></formula><p>where ⊕ is an element-wise addition, W 3 , W 4 ∈ R 2d f ×2d f and b 3 , b 4 ∈ R 2d f are trainable parameters, s j ∈ R 2d f , s j ∈ S u t when j &gt; 0 and s j = u t−1 when j = 0. We apply u e,t to add context information. Similarly, we keep activation function the same as long-term attention layer.</p><p>Finally, user's current preferences are captured and represented as u t , which leverages long-term preferences and short-term interests. Especially, we can obtain all the user's current preferences in the same way. Now with these user profiles, we can recommend the next item that is appropriate for the target user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Multi-heads integration</head><p>Multi-heads method can improve parallelism and find correlations between different semantic sub-spaces <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>. Considering those advantages, we implement multi-heads on the attention mechanism mentioned above to model the information from different semantic sub-spaces and integrate them as well as their correlations in a unified and parallel way to further improve the performance of recommendation. Formally, with m-heads method, the attention function is defined as (superscript * means "modified"):</p><p>* Att 2d f = Conc(Ahead 1 , Ahead 2 , ..., Ahead m ),</p><p>Ahead k = Att 2d f /m k (parameters).</p><p>We equally separate the features of the embeddings into Ahead k (k = 1, 2, ..., m), and then concatenate them in original order to avoid extra computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Network Training</head><p>We train TLSAN with the all users' behavior records in the training set, and then predict the labels(items) in the test set. Specifically, the closer the predicted label is to the truth, the more effective the model is. In this paper, we only need to predict whether the next item will be purchased or not. Therefore, we choose the unified sigmoid cross entropy loss <ref type="bibr" target="#b19">[20]</ref> for model optimization:</p><formula xml:id="formula_13">Loss = − u,j y j log(σ(f (u t , s j )))+(1−y j ) log(1−σ(f (u t , s j )))+λ||Θ|| 2 ,<label>(11)</label></formula><p>where f (.) denotes a ranking function, which can be either a dot-product function or a more complex deep neural network, Θ = {U, I, W * , b * } and λ is the l2-loss weight. Labels are denoted by y ∈ {0, 1}, and σ(.) represents the sigmoid function. The detailed learning algorithm is presented in Algorithm 1. shuffle the set of observations {(u, L u t−1 , S u t )} for all users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for each observation (u, L u t−1 , S u t ) do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>obtain user's current preferences u t according to Equation (1)-(10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>compute Loss according to Equation <ref type="formula" target="#formula_2">(11)</ref> 6:</p><p>update Θ with gradient descent <ref type="bibr">7:</ref> end for 8: until convergence 9: return Θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate whether the proposed approach TLSAN performs well in scenarios from various fields, we utilize the datasets which cover most fields in our daily life. Specifically, the purpose of the experiments is to answer the following three questions:</p><p>RQ1 : Does TLSAN has better performance than other state-of-the-art models?</p><p>RQ2 : How does the parameter setting influence TLSAN? RQ3 : How does each component of TLSAN contribute to its performance on recommendation tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Designs 4.1.1. Datasets</head><p>Amazon is the world's largest e-commerce platform, which has the largest and most extensive behavior data volume. Especially, its products cover most fields in life and have good diversity. Amazon also exposes the official datasets 1 which have filtered out users and items with less than 5 reviews and removed a large amount of invalid data. In the following experiments, only users, items, interactions, and category information are utilized. Then we perform the preprocessing according to the following two steps and the statistics for the datasets is shown in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Firstly, the users whose interactions less than 10 and the items with interactions less than 8 are removed to ensure the effectiveness of each user and item. Secondly, we choose the users whose number of transactions is more than 4 but less than 90. This step guarantees the existence of longand short-term behavior records and all behavior records occurred within recent three months.</p><p>We divide all users' behavior records into ordered sessions by day and randomly choose the item in the newest session at time t as the next item while training. If the session at time t contains only one item, the first item in the session at time (t+1) is chosen. For the models that explicitly consider long-and short-term (SHAN, LSPM <ref type="bibr" target="#b44">[45]</ref>, TLSAN), we consider the newest session without the chosen item as short-term session, and 1 ∼ (t − 1)-th sessions as long-term sessions to generate the training set. As for the other models, all sessions before time t are regarded as historical sessions to get  <ref type="bibr" target="#b0">[1]</ref> trains user's positive and negative behavior pairs to minimize the posterior probability of the difference for each user. Each item embedding is the concatenation of item ID embedding and category embedding.</p><p>CNN+Pooling (deep neural networks; multi-head): Max pooling operation is applied over the feature map which has kernel size 32 in the CNN structure <ref type="bibr" target="#b43">[44]</ref>. We utilize this method to encode users' historical behaviors and pass all pooled features to a fully connected layer to generate user behavior embeddings.</p><p>Bi-LSTM (deep neural networks): LSTM <ref type="bibr" target="#b28">[29]</ref> is renown by its capability of capturing implicit long-and short-term sequential data. In order to capture both forward and backward correlations among sequences, bidirectional long-and short-term memory network, called as Bi-LSTM, comes into being. In this paper, we implement this method on recommendation tasks.</p><p>ATRank (attention; multi-head): It considers heterogeneous users' behaviors by projecting all types of behaviors into latent semantic spaces. ATRank then utilizes self-attention layer and vanilla attention layer with DNN to obtain users' preferences <ref type="bibr" target="#b19">[20]</ref>. PACA (attention; position embedding): Position-aware context attention <ref type="bibr" target="#b11">[12]</ref> considers each time position as a trainable position vector. Then PACA captures the context of each item and the corresponding session as a session-specific feature vector by multi-layer perceptron (MLP). Attention values are generated by these two vectors. CSAN− (attention; feature-wise; position embedding; multihead): Feature-wise self-attention is introduced after the embedding layer in CSAN. This model then <ref type="bibr" target="#b20">[21]</ref> simply adds the untrainable position encoding matrices to feature-wise self-attention. Finally, it generates user behavior embeddings through vanilla self-attention network. Since CSAN also utilizes texts, audios and images, and we do not utilize them, we represent this incomplete model in our experiments as CSAN−. LSPM (long-and short-term behaviors): Long-and short-term preference model (LSPM) <ref type="bibr" target="#b44">[45]</ref> captures each user's long-term preferences by the vector in a trainable matrix. Then it combines each user's most recent k items' embeddings together to model her/his short-term preferences. Finally, LSPM combines these two preferences to obtain users' profile. SHAN (attention; long-and short-term behaviors): Long-and short-term behavior records are both important, so SHAN <ref type="bibr" target="#b21">[22]</ref> proposes nonlinear hierarchical attention networks to capture long-and short-term users' preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Evaluation Metrics</head><p>We utilize three kinds of evaluation methods, area under the curve (AUC), precision and recall to evaluate models' abilities of capturing users' preferences and performing time-sensitive next-item recommendations.</p><p>AUC <ref type="bibr" target="#b19">[20]</ref> is the area enclosed by the coordinate axis under Receiver Operating Characteristic (ROC) curve. The points on ROC curve represent the value of the true tax rate (TPR) at a certain false positive rate (FPR). If the model works well, the FPR should decrease and the TPR should increase during the process of training. In other words, the better the model works, the higher the AUC value is. Formally, AUC is defined as:</p><formula xml:id="formula_14">AU C = 1 |U | u∈U 1 |S||S | s∈S s ∈S δ(p u,s &gt; p u,s ),<label>(12)</label></formula><p>where S and S denote positive and negative sample sets, respectively. p u,s means the predicted probability that user u may choose positive item s in the test set, and s means the negative item. δ(.) is an indicator function. If p u,s &gt; p u,s δ(p u,s &gt; p u,s ) returns 1 and 0 otherwise. Precision@K and Recall@K <ref type="bibr" target="#b20">[21]</ref>: Precision rate refers to the ratio of the number of positive samples classified by the model to the total number of positive samples. Recall rate is the ratio of the number of positive samples classified by the model to the total number of the considered positive samples, and K means only considering the top-K items. Formally, their definitions are shown below:</p><formula xml:id="formula_15">P recision@K = 1 |U | u∈U K s=1 f p(s, pos(u)) K ,<label>(13)</label></formula><formula xml:id="formula_16">Recall@K = 1 |U | u∈U K s=1 f p(s, pos(u)) N K(u) ,<label>(14)</label></formula><p>where pos(u) denotes the set of the ground-truth items related to user u, and N K(u) represents the number of positive items in the top-K predicted items of user u. f p(s, pos(u)) is an indicator which returns 1 if item s is in pos(u), and 0 otherwise. AUC can measure classification ability of a classifier. For a binary classification problem, it is the ability of classifying the target values as true or false. However, AUC does not set fixed thresholds when measuring classification capabilities but considers all thresholds <ref type="bibr" target="#b46">[46]</ref>. In this way, when positive and negative sample sizes are very different, the AUC value will be abnormally high, thus losing the reference value. As for next item recommendation tasks, high AUC means that the model is able to capture users' preferences, but the meaning of "the most likely next item" for each user is not reflected, especially in time-sensitive next-item recommendations. On the other hand, Recall@K and Precision@K can address this problem, so we utilize those three kinds of evaluation metrics together to make the experiments more comprehensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Implementation Detail</head><p>To ensure the fairness and comparability of the experiments, we keep the common parameters in each model the same and the unique parameters the optimal. For all models, we set the embedding size to 32, training batch size to 32 and testing batch size to 128. As for the specific parameters, we set the number of heads to 8 for ATRank, CNN+Pooling, CSAN− and TLSAN, L2-loss weight to 0.01 for LSPM and to 0.00005 for other models using L2loss. Besides, we set position kernel size to 10 for PACA, the length of recent sessions to 5 and 90 for LSPM and PACA, the length of recent longterm sessions to 10 for TLSAN. Moreover, the weight of short preferences is set to 1.0 for LSPM. The detailed process and analysis of choosing the critical parameters for our model TLSAN are shown in Section 4.3. Note that the parameters for other baselines are already tuned by us, but we will not show the details for them since the limited space in this paper. During the experiments, we uniformly utilize stochastic gradient descent (SGD) for training, and dynamically adjust learning rate. The initial learning rate is set to 1.0 for fast training. When each model reaches about 80% of total training steps, learning rate is set to 0.1. All the experiments in this paper are implemented with Python 3.5 and Tensorflow 1.8.0, and run on a server with two 2.1 GHz Intel Xeon E5-2620V4 CPU, 128 GB 2133 MHz DDR4 RAM, Nvidia Tesla K40 GPU with 12 GB memory, running Ubuntu 16.04 LST. The source code of our model and the baselines with the processed datasets are publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">RQ1: Performance Analysis</head><p>We have trained each model to converge during the experiments. The performances of them are shown in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>. The main observations, which are classified into traditional method, deep neural networks, attention methods, considering position embedding and considering long-and short-term behaviors, are as follows:</p><p>(1) Traditional method: BPR-MF performs not as good as other methods on AUC, because it performs prediction based on Bayesian posterior probability, which cannot utilize temporal sequential information well. Therefore, temporal sequential information is important for recommendation. The proposed model TLSAN is time-aware, which inherently captures this information. (2) Deep neural networks: On average CNN is about 16.5% better than BPR-MF for the ability of discovering users' preferences with time series information. However, its convolution kernel cannot exploit much data at one time. In the kernel, the importance of each position is the <ref type="table">Table 3</ref>: AUC on the Amazon public datasets. Bold font and underlined value indicate the optimal result and the suboptimal result, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>ATRank  <ref type="figure" target="#fig_4">Figure 5</ref> for using deep neural networks. Due to the comprehensive utilization of attention mechanism, although TLSAN takes so many factors into consideration, it is not too complicated and can be trained faster than baselines shown as <ref type="figure" target="#fig_4">Figure 5</ref> because of parallelism. (3) Attention methods: As shown in <ref type="table">Table 3</ref>, ATRank performs best on the Electronics dataset (second largest) except for TLSAN, which indicates that ATRank has better prediction ability on large datasets with attention mechanism. Specifically, ATRank performs worse than CSAN− at Precision@1, and one reason is that it fails to explicitly consider "time-aggregation". As one of state-of-the-art models, PACA achieves good performance for attention mechanism. However, TLSAN performs the best among them for it explores correlations on each feature through feature-wise attention, as for attention mechanism. (4) Considering position embedding: Although PACA introduces position embedding, the embeddings in PACA are global and they are the same for all the users, which loses personalization to some extent and causes its unstable performance as shown in <ref type="table">Table 3</ref>. CSAN− still performs well on large datasets for the position matrix describes "time-aggregation" phenomenon. However, it does not perform as good as the proposed approach TLSAN.  The reason is that its consistent position matrix may have different order of magnitude with historical embedding and CSAN− does not incorporate user-specific temporal taste to the position embeddings, while TLSAN does. <ref type="bibr" target="#b4">(5)</ref> Considering long-and short-term behaviors: LSPM and its simple version achieves good results on most datasets even on sparse dataset. The reason is that they distinguish long-and short-term preferences. However, it is not good as the proposed approach TLSAN. The reason is that LSPM only models short-term preferences from behavior sequences, and it regards users' embedding as the long-term preferences, which may lose both time series information and personalized position preferences. For example, LSPM gets a relatively good score on Recall@1 and Precision@1, but it becomes powerless when K is larger for the reason mentioned above. The Clothing-Shoes dataset is the smallest datasets with many categories but few records (sparser) as shown in <ref type="table" target="#tab_4">Table 2</ref>, which influences most baselines' ability of capturing users' preferences. Especially, PACA cannot deal with sparse or small interaction data effectively since it does not fully exploit short-term preferences. SHAN captures long-and short-term users' preference, so it performs well on this sparse dataset as well as most remaining datasets in <ref type="table">Table 3</ref>. TLSAN performs better than these baselines, since TLSAN explicitly considers "personalized time-aggregation" and captures users' long-term pref-erences and short-term interests when generating users' current preferences. Besides, it incorporate category-aware user-user and user-item correlations through dynamic user categories to capture more information.</p><p>Similarly, SHAN considers both long-term and short-term behavior records, but it does not emphasize short-term interests. Moreover, both PACA and SHAN ignore time decay, which also influence their performance on time-sensitive next-item recommendation. Especially, Recall@K and Pre-cision@K can evaluate the ability of time-sensitive next-item recommendation, while AUC is used to evaluate the ability of capturing users' preferences, as mentioned in Section 4.1.3. So, PACA and SHAN both perform well on AUC but poorly on Recall@K and Precision@K. When it lacks long-term behaviors such as sparse dataset (e.g. Clothing-Shoes dataset), short-term layer in SHAN dominates and helps the model get nice performance as shown in <ref type="table">Table 3</ref>. Whether on AUC or Recall@K and Precision@K, TLSAN has improved a lot compared to other baselines especially on Recall@K and Pre-cision@K, which means its time-sensitive next-item recommendation ability is excellent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">RQ2: Parameters Analysis</head><p>Results of experiments for parameters analysis on the Amazon Electronics dataset are shown in <ref type="table" target="#tab_6">Table 4</ref>-6 and <ref type="figure">Figure 6</ref>. Specifically, we analyze the critical parameters (embedding size (ES), heads (H) and long-term sequence length (L s )) as follows (Note that we only change the parameter currently being considered and keep others the same as TLSAN.):    <ref type="figure">Figure 6 (a)</ref>. The reason is that as the embedding size increases, the number of features also increases, which enable the model to capture more features or information for higher accuracy at the cost of efficiency. Besides, although the AUC on "TLSAN (ES = 48)" is higher than "TLSAN (ES = 32)", the improvement (0.108%) is not significant compared with the improvement (1.106%) from "ES = 16" to "TLSAN (ES = 32)". Simply adding features not only consumes computing power (slow down the speed), but also may force a certain feature to be split into sub-features, which may influence the performance. According to the results of Recall@20 and Precision@20 in <ref type="figure">Figure 6</ref> (a), as the embedding size increases, the ability of time-sensitive next-item recommendation also reach the summit. To achieve a balance between efficieny and accuracy, ES is set to 32.</p><p>(2) Heads (H): When multi-heads method is not adopted (that is, heads = 1), TLSAN's performance is not bad. As the number of heads increases, the  AUC reaches the optima when the heads = 8 as shown in <ref type="table" target="#tab_7">Table 5</ref> and <ref type="figure">Figure  6</ref> (b). On the one hand, the multi-heads can increase parallelism and capture the interconnections between various heads. On the other hand, because the embedding size is limited to 32, the number of features of items is inversely proportional to the number of heads. Only with the proper number of heads can the two factors reach equilibrium. H = 8 achieves the best for TLSAN on the Amazon Electronics dataset.</p><p>(3) Long-term sequence length (L s ): The behaviors that happened too long ago may hardly contribute to current prediction task, and considering all the behaviors will not only weaken users' current interests but also influence efficiency. As shown in <ref type="table" target="#tab_8">Table 6</ref>, AUC keeps increasing when L s grows. The reason is that TLSAN can capture users' long-term preferences better with larger L s , while the ability of capturing user short-term interests is already optimal in <ref type="figure">Figure 6</ref> (c). Besides, TLSAN performs best on Recall@20 and Precision@20 when L s is 10, which means time-sensitive prediction ability has reached the summit. In the balance, we set L s to 10. Because of GPU memory saving, the training speed is also increased.  <ref type="figure">Figure 7</ref>: Recall@20 and Precision@20 on TLSAN, NS (without short-term layer), NC (without categories), NU (without user categories) NP (without position embedding) and NG (without γ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">RQ3: Component Analysis</head><p>The purpose of this section is to explore how each main component in TLSAN contributes to the recommendation performance. Specifically, the main components of TLSAN are short-term feature-wise attention layer, dynamic user category extraction and personalized time position embedding module. Therefore, we will remove the corresponding components to obtain the following variants and compare them with TLSAN: NS (without shortterm layer), NC (without user and item categories), NU (without dynamic user categories), NP (without personalized time position embedding) and NG (without γ). The results on the Amazon Electronics dataset are shown in <ref type="table" target="#tab_9">Table 7</ref> and <ref type="figure">Figure 7</ref>.</p><p>As can be seen from <ref type="table" target="#tab_9">Table 7</ref> and <ref type="figure">Figure 7</ref>, the performance of those variants (incomplete models) is not as good as TLSAN, although some of them achieve better performance than baselines. Specifically, we will explain in the following four aspects:</p><p>(1) Many existing models consider long-term preferences of users, but they do not distinguish long-term preferences and short-term interests explicitly, so we remove short-term layer here to study its validity. As shown in <ref type="table" target="#tab_9">Table  7</ref> and <ref type="figure">Figure 7</ref>, we find that the NS model with short-term layer removed is not as good as other variants. For we set the long-term sequence length limit to L s , the model needs short-term sequence to provide enough information to learn both long-term preferences and short-term interests. Besides, we find that, the AUC of NS is better than some baselines such as PACA, but its performance on Recall@20 and Precision@20 is almost worse than all the baselines, which means short-term layer not only helps capture users' longterm preferences but also plays an important role in time-sensitive next-item recommendation.</p><p>(2) The comparisons between NC and NU show the effects of category attributes. As shown in in <ref type="table" target="#tab_9">Table 7</ref> and <ref type="figure">Figure 7</ref>, we can observe that categories are important for improving the performance of recommendation. Due to the effectiveness of other components, the performance of NC is still better than most of the baselines and almost the same with ATRank which utilizes categories. The improvement of introducing dynamic user category is not obvious on AUC, but obvious on Recall@20 and Precision@20. Especially, the improvement on capturing users' current preferences is greater than capturing users' long-term preferences when we introduce dynamic user category.</p><p>(3) TLSAN has improvement on Recall@20 and Precision@20, but almost no improvement on AUC compared to NP. The reason is that, in the personalized time position embedding module, the original time-aggregation embeddings can weaken the effect of long-term behaviors and emphasize short-term interests. Owing to personalized time position embedding, the time-sensitive next-item recommendation capability of our model TLSAN is improved further.</p><p>(4) NG considers personalized time position embedding, so it can capture users' short-term interests well. However, the AUC of NG is worse than NP, which means that personalized time position embedding without γ has difference in the order of magnitude with historical embedding. We utilize a global trainable parameter γ to automatically adjust the order of magnitude between them.</p><p>Considering the evaluation results by AUC, Recall@20 and Precision@20, we come to the following four conclusions: 1) both long and short-term preferences are important in performing accurate recommendation, and especially the latter plays a more significant role in strong time-sensitive next-item recommendations. 2) Dynamically classifying user categories according to their favorite item categories helps greatly in both capturing users' preferences and improving the recommendation performance. 3) The "personalized time-aggregation" effect is a phenomenon that exists in our daily life, which inspires us how to fully exploit users' behavior patterns. 4) Automatically tuning the order of magnitude between personalized time position embedding and historical embedding is helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In summary, we propose a new model: Time-aware Long-and Short-term Attention Network (TLSAN), which recommends the next most suitable item for the target users based on their historical behavior records in the following four steps: (1) To reinforce the category-aware correlations of user-user and user-item, we consider user dynamic favorite item category as dynamic user category. (2) Then we propose a new personalized time position embedding method to describe the "personalized time-aggregation" phenomenon.</p><p>(3) Long-and short-term feature-wise attention can generate long-term preferences and strengthened short-term interests of users to get their current preferences. (4)Finally, TLSAN can perform personalized sequential recommendation to target users based on their preferences. Extensive experiments are performed on Amazon datasets of different size and from different fields, and the results show that the proposed approach TLSAN outperforms stateof-the-art baselines. We further conduct parameter experiments to explore the process of parameter settings on TLSAN, and also show the effectiveness of each component through ablation experiments.</p><p>For future work, we plan to further exploit the phenomenon of "personalized time-aggregation" by non-linear operations for better recommendation. In addition, we will also try to incorporate more auxilary/side information such as images, audio, and comments, to further improve TLSAN's recommendation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Original time-aggregation embedding can be obtained from the dataset, we use reciprocal value to describe time decay. The darker the color, the smaller the value. White color means no time decay. Then we multiply (mask) the personalized time position embedding to it and tile the combination to 2d f dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Learning Algorithm of TLSAN Input: L: long-term item behaviors, S: short-term item behaviors, α: learning rate, d f : number of features, λ: l2-loss weight Output: optimal model parameters Θ = {U, I, W * , b * } 1: repeat 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>BPR-MF CNN CSAN-LSPM PACA Bi-0.7639 0.7620 0.7748 0.9016 0.8231 0.8953 0.9368 Home-Kitchen 0.7039 0.6352 0.7075 0.6820 0.6672 0.8165 0.7373 0.8230 0.8950 Video-Games 0.8809 0.6609 0.8598 0.8033 0.8449 0.8763 0.8598 0.9216 0.9459 Toys-Games 0.8139 0.6294 0.7788 0.7157 0.7708 0.8495 0.8012 0.8797 0.9309 same, which ignores user's personalized preferences on position. The performance of Bi-LSTM is quite good and the AUC value on the Clothing-Shoes datasets exceeds ATRank for its excellent ability of capturing time series information. However, both CNN and Bi-LSTM have slow training speeds as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Recall@K and Precision@K on Amazon Electronics dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>AUC progress on Amazon Electronics dataset. We only show the curve within 5 hours (18000 seconds) to show the different training speeds. TLSAN has already converged with the excellent performance, but some models (CNN, CSAN−, PACA, Bi-LSTM) have not converged within 5 hours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 Figure 6 :</head><label>86</label><figDesc>Recall@20 and Precision@20 on embedding size (ES), heads (H) and long-term sequence length (L s ) parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Key Notations and Their Descriptions</figDesc><table><row><cell cols="2">Notations Explanations</cell></row><row><cell>U, I, C</cell><cell>User, item and category ID sets</cell></row><row><cell>U, I, C</cell><cell>User, item and category embedding sets</cell></row><row><cell>L u t−1 , S u t , L u t</cell><cell>Long-term, short-term and all sequential behaviors at time t</cell></row><row><cell>L u t−1 , S u t</cell><cell>Long-term and short-term sequential behavior embeddings at time t</cell></row><row><cell>H u t−1</cell><cell>Time-aware history representation</cell></row><row><cell>P u</cell><cell>Personalized time position embedding for user u</cell></row><row><cell>W  *  , b  *</cell><cell>Trainable weight matrix and bias vector</cell></row><row><cell>L s</cell><cell>Long-term sequence length</cell></row><row><cell>d f</cell><cell>Embedding size</cell></row><row><cell>γ</cell><cell>Trainable parameter for adjusting the order of magnitude</cell></row><row><cell>c u t</cell><cell>Dynamic user category ID at time t</cell></row><row><cell>u e,t</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Amazon Datasets Statistics (After preprocessing)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>avg.</cell><cell>avg.</cell><cell>avg.</cell></row><row><cell>Datasets</cell><cell cols="4">#users #items #categories #samples</cell><cell>items</cell><cell>behaviors</cell><cell>behaviors</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>/category</cell><cell>/item</cell><cell>/user</cell></row><row><cell>Electronics</cell><cell cols="2">39991 22048</cell><cell>673</cell><cell>561100</cell><cell>32.8</cell><cell>25.4</cell><cell>14.0</cell></row><row><cell>CDs-Vinyl</cell><cell cols="2">24179 27602</cell><cell>310</cell><cell>470087</cell><cell>89.0</cell><cell>17.0</cell><cell>19.4</cell></row><row><cell cols="3">Clothing-Shoes 2010 1723</cell><cell>226</cell><cell>13157</cell><cell>7.6</cell><cell>7.6</cell><cell>6.5</cell></row><row><cell>Digital-Music</cell><cell cols="2">1659 1583</cell><cell>53</cell><cell>28852</cell><cell>29.9</cell><cell>18.2</cell><cell>17.4</cell></row><row><cell cols="2">Office-Products 1720</cell><cell>901</cell><cell>170</cell><cell>29387</cell><cell>5.3</cell><cell>32.6</cell><cell>17.0</cell></row><row><cell>Movies-TV</cell><cell cols="2">35896 28589</cell><cell>15</cell><cell>752676</cell><cell>1905.9</cell><cell>20.9</cell><cell>26.3</cell></row><row><cell>Beauty</cell><cell cols="2">3783 2658</cell><cell>179</cell><cell>54225</cell><cell>14.8</cell><cell>20.4</cell><cell>14.3</cell></row><row><cell cols="3">Home-Kitchen 11567 7722</cell><cell>683</cell><cell>143088</cell><cell>11.3</cell><cell>12.3</cell><cell>18.5</cell></row><row><cell>Video-Games</cell><cell cols="2">5436 4295</cell><cell>58</cell><cell>83748</cell><cell>74.1</cell><cell>19.5</cell><cell>15.4</cell></row><row><cell>Toys-Games</cell><cell cols="2">2677 2474</cell><cell>221</cell><cell>37515</cell><cell>11.2</cell><cell>15.2</cell><cell>14.0</cell></row></table><note>the training set. Furthermore, we consider the item immediately after the newest session as the test item to generate the test set.4.1.2. Baselines BPR-MF (traditional): Bayesian personalized ranking</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>AUC on embedding size (ES).</figDesc><table><row><cell cols="2">Parameters ES=16 TLSAN ES=48</cell></row><row><cell>AUC</cell><cell>0.9129 0.9230 0.9240</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>AUC on heads (H). As the embedding size increases by 16 every time, the AUC rises and plateaus when ES is 32 according toTable 4 and</figDesc><table><row><cell cols="2">Parameters H=1</cell><cell>H=2</cell><cell>H=4 TLSAN H=16</cell></row><row><cell>AUC</cell><cell cols="3">0.9199 0.9220 0.9201 0.9230 0.9186</cell></row><row><cell cols="2">(1) Embedding Size (ES):</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>AUC on long-term sequence length (L s ).</figDesc><table><row><cell cols="2">Parameters L s =5 TLSAN L s =15</cell></row><row><cell>AUC</cell><cell>0.9157 0.9230 0.9243</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>AUC on TLSAN, NS (without short-term layer), NC (without categories), NU (without user categories), NP (without position embedding) and NG (without γ).</figDesc><table><row><cell cols="2">components TLSAN</cell><cell>NS</cell><cell>NC</cell><cell>NU</cell><cell>NP</cell><cell>NG</cell></row><row><cell>AUC</cell><cell cols="6">0.9230 0.8395 0.8643 0.9225 0.9229 0.9017</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/TsingZ0/TLSAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>Bpr: Bayesian personalized ranking from implicit feedback</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent recommender networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM international conference on web search and data mining</title>
		<meeting>the tenth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep attention userbased collaborative filtering for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page" from="57" to="68" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequential recommendation with dual side neighbor-based collaborative relation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="465" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusing similarity models with markov chains for sparse sequential recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching non-negative matrix factorization with contextual embeddings for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Iltaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page" from="246" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic tensor factorization for recommendation and rating aggregation with multicriteria evaluation data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kurihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VBPR: visual bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personalized learning full-path recommendation model based on LSTM neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">444</biblScope>
			<biblScope unit="page" from="135" to="152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Position-aware context attention for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neural attention model for urban air quality inference: Learning the weights of monitoring stations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2151" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">User based collaborative filtering using fuzzy cmeans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="134" to="139" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using graph partitioning techniques for neighbour selection in user-based collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM conference on Recommender systems</title>
		<meeting>the sixth ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Collaborative topic regression with social matrix factorization for recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4684</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Personalized recommendation combining user interest and social circle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1763" to="1777" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Qualitative analysis of user-based and item-based prediction algorithms for recommendation agents, Engineering Applications of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papagelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Plexousakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="781" to="789" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Circle-based recommendation in online social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1267" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Atrank: An attention-based user behavior modeling framework for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4564" to="4571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Csan: Contextual selfattention network for user sequential recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="447" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Sequential recommender system based on hierarchical attention network</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3926" to="3932" />
		</imprint>
	</monogr>
	<note>IJCAI International Joint Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A formal statistical approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>Neural collaborative filtering</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time event embedding for poi recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A cross-domain hierarchical recurrent model for personalized session-based recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive attention-aware gated recurrent unit for sequential recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="317" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ask the gru: Multi-task learning for deep text recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5446" to="5455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pinidiyaarachchi, Target-specific siamese attention network for real-time object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thanikasalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1276" to="1289" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multimodal feature fusion by relational reasoning and attention for visual question answering, Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="116" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpretable predictions of clinical outcomes with an attention-based recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM International Conference on Bioinformatics</title>
		<meeting>the 8th ACM International Conference on Bioinformatics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling Heart Rate and Activity Data for Personalized Fitness Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Muhlstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1343" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross aggregation of multi-head attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="380" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention-Aware Multi-Task Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1867" to="1878" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluations of pleasurable experiences: The peak-end rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Rupert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="98" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint deep modeling of users and items using reviews for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Online personalized next-item recommendation via long short term preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<editor>X. Geng, B.-H</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
		<title level="m">PRICAI 2018: Trends in Artificial Intelligence</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="915" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Partial auc estimation and regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="623" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
