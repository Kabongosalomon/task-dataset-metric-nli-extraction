<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Personalizing Dialogue Agents: I have a dog, do you have pets too?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-25">25 Sep 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
							<email>saizheng.zhang@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">MILA ‡ Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
							<email>edinan@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">MILA ‡ Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">MILA ‡ Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
							<email>aszlam@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">MILA ‡ Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<email>dkiela@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">MILA ‡ Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">MILA ‡ Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Personalizing Dialogue Agents: I have a dog, do you have pets too?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-25">25 Sep 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown, our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy. It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting. Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses <ref type="bibr" target="#b23">Vinyals and Le, 2015)</ref>.</p><p>Common issues with chit-chat models include: (i) the lack of a consistent personality <ref type="bibr" target="#b7">(Li et al., 2016a)</ref> as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history <ref type="bibr" target="#b23">(Vinyals and Le, 2015)</ref>; 1 Work done while at Facebook AI Research. and (iii) a tendency to produce non-specific answers like "I don't know" <ref type="bibr" target="#b6">(Li et al., 2015)</ref>. Those three problems combine to produce an unsatisfying overall experience for a human to engage with. We believe some of those problems are due to there being no good publicly available dataset for general chit-chat.</p><p>Because of the low quality of current conversational models, and because of the difficulty in evaluating these models, chit-chat is often ignored as an end-application. Instead, the research community has focused on taskoriented communication, such as airline or restaurant booking <ref type="bibr" target="#b1">(Bordes et al., 2016)</ref>, or else singleturn information seeking, i.e. question answering <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref>. Despite the success of the latter, simpler, domain, it is well-known that a large quantity of human dialogue centers on socialization, personal interests and chit-chat <ref type="bibr" target="#b4">(Dunbar et al., 1997)</ref>. For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called "Meformers" <ref type="bibr" target="#b13">(Naaman et al., 2010)</ref>.</p><p>In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a configurable, but persistent persona, encoded by multiple sentences of textual description, termed a profile. This profile can be stored in a memory-augmented neural network and then used to produce more personal, specific, consistent and engaging responses than a persona-free model, thus alleviating some of the common issues in chit-chat models. Using the same mechanism, any existing information about the persona of the dialogue partner can also be used in the same way. Our models are thus trained to both ask and answer questions about personal topics, and the resulting dialogue can be used to build a model of the persona of the speaking partner.</p><p>To support the training of such models, we present the PERSONA-CHAT dataset, a new dialogue dataset consisting of 162,064 utterances between crowdworkers who were randomly paired and each asked to act the part of a given provided persona (randomly assigned, and created by another set of crowdworkers). The paired workers were asked to chat naturally and to get to know each other during the conversation. This produces interesting and engaging conversations that our agents can try to learn to mimic.</p><p>Studying the next utterance prediction task during dialogue, we compare a range of models: both generative and ranking models, including Seq2Seq models and Memory Networks <ref type="bibr" target="#b21">(Sukhbaatar et al., 2015)</ref> as well as other standard retrieval baselines. We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the next dialogue utterance. The PERSONA-CHAT dataset is designed to facilitate research into alleviating some of the issues that traditional chitchat models face, and with the aim of making such models more consistent and engaging, by endowing them with a persona. By comparing against chit-chat models built using the OpenSubtitles and Twitter datasets, human evaluations show that our dataset provides more engaging models, that are simultaneously capable of being fluent and consistent via conditioning on a persistent, recognizable profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional dialogue systems consist of building blocks, such as dialogue state tracking components and response generators, and have typically been applied to tasks with labeled internal dialogue state and precisely defined user intent (i.e., goal-oriented dialogue), see e.g. <ref type="bibr" target="#b28">(Young, 2000)</ref>. The most successful goal-oriented dialogue systems model conversation as partially observable Markov decision processes (POMDPs) <ref type="bibr" target="#b27">(Young et al., 2013)</ref>. All those methods typically do not consider the chit-chat setting and are more concerned with achieving functional goals (e.g. booking an airline flight) than displaying a personality. In particular, many of the tasks and datasets available are constrained to narrow domains .</p><p>Non-goal driven dialogue systems go back to Weizenbaum's famous program ELIZA <ref type="bibr" target="#b25">(Weizenbaum, 1966)</ref>, and hand-coded systems have continued to be used in applications to this day. For example, modern solutions that build an open-ended dialogue system to the Alexa challenge combine hand-coded and machine-learned elements <ref type="bibr">(Serban et al., 2017a)</ref>. Amongst the simplest of statistical systems that can be used in this domain, that are based on data rather than hand-coding, are information retrieval models <ref type="bibr" target="#b20">(Sordoni et al., 2015)</ref>, which retrieve and rank responses based on their matching score with the recent dialogue history. We use IR systems as a baseline in this work.</p><p>End-to-end neural approaches are a class of models which have seen growing recent interest. A popular class of methods are generative recurrent systems like seq2seq applied to dialogue <ref type="bibr" target="#b22">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b23">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b20">Sordoni et al., 2015;</ref><ref type="bibr" target="#b8">Li et al., 2016b;</ref><ref type="bibr" target="#b19">Serban et al., 2017b)</ref>. Rooted in language modeling, they are able to produce syntactically coherent novel responses, but their memory-free approach means they lack long-term coherence and a persistent personality, as discussed before. A promising direction, that is still in its infancy, to fix this issue is to use a memory-augmented network instead <ref type="bibr" target="#b21">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b3">Dodge et al., 2015)</ref> by providing or learning appropriate memories.  list available corpora for training dialogue systems. Perhaps the most relevant to learning chit-chat models are ones based on movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches <ref type="bibr" target="#b23">(Vinyals and Le, 2015;</ref><ref type="bibr" target="#b3">Dodge et al., 2015;</ref><ref type="bibr" target="#b8">Li et al., 2016b;</ref><ref type="bibr" target="#b19">Serban et al., 2017b)</ref>. Naively training on these datasets leads to models with the lack of a consistent personality as they will learn a model averaged over many different speakers. Moreover, the data does little to encourage the model to engage in understanding and maintaining knowledge of the dialogue partner's personality and topic interests.</p><p>According to 's survey, personalization of dialogue systems is "an important task, which so far has not received much attention". In the case of goal-oriented dialogue some work has focused on the agent being aware of the human's profile and adjusting the dialogue accordingly, but without a personality to the agent it-self <ref type="bibr" target="#b11">(Lucas et al., 2009;</ref><ref type="bibr" target="#b5">Joshi et al., 2017)</ref>. For the chit-chat setting, the most relevant work is <ref type="bibr" target="#b7">(Li et al., 2016a)</ref>. For each user in the Twitter corpus, personas were captured via distributed embeddings (one per speaker) to encapsulate individual characteristics such as background information and speaking style, and they then showed using those vectors improved the output of their seq2seq model for the same speaker. Their work does not focus on attempting to engage the other speaker by getting to know them, as we do here. For that reason, our focus is on explicit profile information, not hard-to-interpret latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PERSONA-CHAT Dataset</head><p>The aim of this work is to facilitate more engaging and more personal chit-chat dialogue. The PERSONA-CHAT dataset is a crowd-sourced dataset, collected via Amazon Mechanical Turk, where each of the pair of speakers condition their dialogue on a given profile, which is provided.</p><p>The data collection consists of three stages: (i) Personas: we crowdsource a set of 1155 possible personas, each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation, and 100 for test.</p><p>(ii) Revised personas: to avoid modeling that takes advantage of trivial word overlap, we crowdsource additional rewritten sets of the same 1155 personas, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging.</p><p>(iii) Persona chat: we pair two Turkers and assign them each a random (original) persona from the pool, and ask them to chat. This resulted in a dataset of 162,064 utterances over 10,907 dialogs, 15,602 utterances (1000 dialogs) of which are set aside for validation, and 15,024 utterances (968 dialogs) for test.</p><p>The final dataset and its corresponding data collection source code, as well as models trained on the data, are all available open source in ParlAI 2 .</p><p>In the following, we describe each data collection stage and the resulting tasks in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Personas</head><p>We asked the crowdsourced workers to create a character (persona) description using 5 sentences, providing them only a single example: 2 http://parl.ai "I am a vegetarian. I like swimming. My father used to work for Ford. My favorite band is Ma-roon5. I got a new job last month, which is about advertising design."</p><p>Our aim was to create profiles that are natural and descriptive, and contain typical topics of human interest that the speaker can bring up in conversation. Because the personas are not the real profiles of the Turkers, the dataset does not contain personal information (and they are told specifically not to use any). We asked the workers to make each sentence short, with a maximum of 15 words per sentence. This is advantageous both for humans and machines: if they are too long, crowdsourced workers are likely to lose interest, and for machines the task could become more difficult.</p><p>Some examples of the personas collected are given in <ref type="table" target="#tab_1">Table 1</ref> (left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Revised Personas</head><p>A difficulty when constructing dialogue datasets, or text datasets in general, is that in order to encourage research progress, the task must be carefully constructed so that is neither too easy nor too difficult for the current technology <ref type="bibr" target="#b24">(Voorhees et al., 1999)</ref>. One issue with conditioning on textual personas is that there is a danger that humans will, even if asked not to, unwittingly repeat profile information either verbatim or with significant word overlap. This may make any subsequent machine learning tasks less challenging, and the solutions will not generalize to more difficult tasks. This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent <ref type="bibr" target="#b2">(Chen et al., 2017)</ref>.</p><p>To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about "a related characteristic that the same person may have", hence the revisions could be rephrases, generalizations or specializations. For example "I like basketball" can be revised as "I am a big fan of Michael Jordan" not because they mean the same thing but because the same persona could contain both.</p><p>In the revision task, workers are instructed not to trivially rephrase the sentence by copying the original words. However, during the entry stage if a non-stop word is copied we issue a warning, I also went to school to work with technology. My mother is a medical doctor</p><p>The woman who gave birth to me is a physician. I am very shy.</p><p>I am not a social person. I like to build model spaceships.</p><p>I enjoy working with my hands.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Persona Chat</head><p>After collecting personas, we then collected the dialogues themselves, conditioned on the personas. For each dialogue, we paired two random crowdworkers, and gave them the instruction that they will chit-chat with another worker, while playing the part of a given character. We then provide them with a randomly chosen persona from our pool, different to their partners. The instructions are on purpose quite terse and simply ask them to "chat with the other person naturally and try to get to know each other". In an early study we noticed the crowdworkers tending to talk about themselves (their own persona) too much, so we also added the instructions "both ask questions and answer questions of your chat partner" which seemed to help. We also gave a bonus for high quality dialogs. The dialog is turn-based, with a maximum of 15 words per message. We again gave instructions to not trivially copy the character descriptions into the messages, but also wrote explicit code sending them an error if they tried to do so, using simple string matching. We define a minimum dialogue length which is randomly between 6 and 8 turns each for each dialogue. An example dialogue from the dataset is given in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation</head><p>We focus on the standard dialogue task of predicting the next utterance given the dialogue history, but consider this task both with and without the profile information being given to the learning agent. Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user.</p><p>We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both. These scenarios can be tried using either the original personas, or the revised ones. We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following . The latter consists of choosing N random distractor responses from other dialogues (in our setting, N =19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We consider two classes of model for next utterance prediction: ranking models and generative models. Ranking models produce a next utterance by considering any utterance in the training set as a possible candidate reply. Generative models generate novel sentences by conditioning on the dialogue history (and possibly, the persona), and then generating the response word-by-word. Note one can still evaluate the latter as ranking models by computing the probability of generating a given candidate, and ranking candidates by those scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline ranking models</head><p>We first consider two baseline models, an IR baseline <ref type="bibr" target="#b20">(Sordoni et al., 2015)</ref> and a supervised embedding model, Starspace <ref type="bibr" target="#b26">(Wu et al., 2017)</ref> 3 . While there are many IR variants, we adopt the simplest one: find the most similar message in the (training) dataset and output the response from that exchange. Similarity is measured by the tf-idf weighted cosine similarity between the bags of words. Starspace is a recent model that also performs information retrieval but by learning the similarity between the dialog and the next utterance by optimizing the embeddings directly for that task using the margin ranking loss and k-negative sampling. The similarity function sim(q, c ′ ) is the cosine similarity of the sum of word embeddings of the query q and candidate c ′ . Denoting the dictionary of D word embeddings as W which is a D × d matrix, where W i indexes the i th word (row), yielding its d-dimensional embedding, it embeds the sequences q and c ′ .</p><p>In both methods, IR and StarSpace, to incorporate the profile we simply concatenate it to the query vector bag of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ranking Profile Memory Network</head><p>Both the previous models use the profile information by combining it with the dialogue history, which means those models cannot differentiate between the two when deciding on the next utterance. In this model we instead use a memory network with the dialogue history as input, which then performs attention over the profile to find relevant lines from the profile to combine with the input, and then finally predicts the next utterance. We use the same representation and loss as in the Starspace model, so without the profile, the two models are identical. When the profile is available attention is performed by computing the similarity of the input q with the profile sentences p i , computing the softmax, and taking the weighted sum:</p><formula xml:id="formula_0">q + = q + s i p i , s i = Softmax(sim(q, p i ))</formula><p>where Softmax(z i ) = e z i / j e z j . One can then rank the candidates c ′ using sim(q + , c ′ ). One can also perform multiple "hops" of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Key-Value Profile Memory Network</head><p>The key-value (KV) memory network <ref type="bibr">(Miller et al., 2016)</ref> was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs. Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner. This allows the model to have a memory of past dialogues that it can directly use to help influence its prediction for the current conversation. The model we choose is identical to the profile memory network just described in the first hop over profiles, while in the second hop, q + is used to attend over the keys and output a weighted sum of values as before, producing q ++ . This is then used to rank the candidates c ′ using sim(q ++ , c ′ ) as before. As the set of (key-value) pairs is large this would make training very slow.</p><p>In our experiments we simply trained the profile memory network and used the same weights from that model and applied this architecture at test time instead. Training the model directly would presumably give better results, however this heuristic already proved beneficial compared to the original network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Seq2Seq</head><p>The input sequence x is encoded by applying h e t = LST M enc (x t | h e t−1 ). We use GloVe <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref> for our word embeddings. The final hidden state, h e t , is fed into the decoder LST M dec as the initial state h d 0 . For each time step t, the decoder then produces the probability of a word j occurring in that place via the softmax, i.e., p(y t,j = 1 | y t−1 , . . . , y 1 ) = exp(w j h d t )</p><formula xml:id="formula_1">K j ′ =1 exp(w j ′ h d t )</formula><p>.</p><p>The model is trained via negative log likelihood. The basic model can be extended to include persona information, in which case we simply prepend it to the input sequence x, i.e., x = ∀p ∈ P || x, where || denotes concatenation. For the OpenSubtitles and Twitter datasets trained in Section 5.2 we found training a language model (LM), essentially just the decoder part of this model, worked better and we report that instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Generative Profile Memory Network</head><p>Finally, we introduce a generative model that encodes each of the profile entries as individual memory representations in a memory network. As before, the dialogue history is encoded via LST M enc , the final state of which is used as the initial hidden state of the decoder. Each entry p i = p i,1 , . . . , p i,n ∈ P is then encoded via f (p i ) = |p i | j α i p i,j . That is, we weight words by their inverse term frequency: α i = 1/(1 + log(1 + tf)) where tf is computed from the GloVe index via Zipf's law 4 . Let F be the set of encoded memories. The decoder now attends over the encoded profile entries, i.e., we compute the mask a t , context c t and next inputx t as:</p><formula xml:id="formula_2">a t = sof tmax(F W a h d t ), c t = a ⊺ t F ;x t = tanh(W c [c t−1 , x t ]).</formula><p>If the model has no profile information, and hence no memory, it becomes equivalent to the Seq2Seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first report results using automated evaluation metrics, and subsequently perform an extrinsic evaluation where crowdsourced workers perform a human evaluation of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automated metrics</head><p>The main results are reported in <ref type="table" target="#tab_4">Table 3</ref>. Overall, the results show the following key points: Persona Conditioning Most models improve significantly when conditioning prediction on their own persona at least for the original (non-revised) versions, which is an easier task than the revised ones which have no word overlap. For example, the Profile Memory generation model has improved perplexity and hits@1 compared to Seq2Seq, and all the ranking algorithms (IR baseline, Starspace and Profile Memory Networks) obtain improved hits@1.</p><p>Ranking vs. Generative. Ranking models are far better than generative models at ranking. This is perhaps obvious as that is the metric they are optimizing, but still the performance difference is quite stark. It may be that the word-based probability which generative models use works well, but is not calibrated well enough to give a sentencebased probability which ranking requires. Human evaluation is also used to compare these methods, which we perform in Sec. 5.2.</p><p>Ranking Models. For the ranking models, the IR baseline is outperformed by Starspace due to its learnt similarity metric, which in turn is outperformed by Profile Memory networks due to the attention mechanism over the profiles (as all other parts of the models are the same   to consider neighboring dialogue history and next utterance pairs in the training set that are similar to the current dialogue, however when using persona information the performance is similar. Revised Personas. Revised personas are much harder to use. We do however still see some gain for the Profile Memory networks compared to none (0.354 vs. 0.318 hits@1). We also tried two variants of training: with the original personas in the training set or the revised ones, a comparison of which is shown in <ref type="table">Table 6</ref> of the Appendix. Training on revised personas helps, both for test examples that are in original form or revised form, likely due to the model be forced to learn more than simple word overlap, forcing the model to generalize more (i.e., learn semantic similarity of differing phrases).</p><p>Their Persona. We can also condition a model on the other speaker's persona, or both personas at once, the results of which are in Tables 5 and 6 in the Appendix. Using "Their persona" has less impact on this dataset. We believe this is because most speakers tend to focus on themselves when it comes to their interests. It would be interesting how often this is the case in other datasets. Certainly this is skewed by the particular instructions one could give to the crowdworkers. For example if we gave the instructions "try not to talk about yourself, but about the other's interests' likely these metrics would change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluation</head><p>As automated metrics are notoriously poor for evaluating dialogue <ref type="bibr" target="#b9">(Liu et al., 2016)</ref> we also perform human evaluation using crowdsourced workers. The procedure is as follows. We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3. In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat. Here, from the Turker's point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this). In this setting, for both the Turker and the model, the personas come from the test set pool.</p><p>After the dialogue, we then ask the Turker some additional questions in order to evaluate the quality of the model. We ask them to evaluate fluency, engagingness and consistency (scored between 1-5). Finally, we measure the ability to detect the other speaker's profile by displaying two possible profiles, and ask which is more likely to be the profile of the person the Turker just spoke to. More details of these measures are given in the Appendix.</p><p>The results are reported in <ref type="table" target="#tab_5">Table 4</ref> for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each. We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker). This effectively gives us upper bound scores which we can aim for with our models. Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following <ref type="bibr" target="#b23">Vinyals and Le (2015)</ref>. Example chats from a few of the models are shown in the Appendix in <ref type="table">Tables 7, 8</ref> <ref type="bibr">, 9, 10, 11 and 12.</ref> Firstly, we see a difference in fluency, engagingness and consistency between all PERSONA-CHAT models and the models trained on OpenSubtitles and Twitter. PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources. We also see suggestions of more subtle differences between the models, although these differences are obscured by the high variance of the human raters' evaluations. For example, in both the generative and ranking model cases, models endowed with a persona can be detected by the human conversation partner, as evidenced by the persona detection accuracies, whilst maintaining fluency and consistency compared to their nonpersona driven counterparts.</p><p>Finding the balance between fluency, engagement, consistency, and a persistent persona remains a strong challenge for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Profile Prediction</head><p>Two tasks could naturally be considered using PERSONACHAT: (1) next utterance prediction during dialogue, and (2) profile prediction given dialogue history. The main study of this work has been Task 1, where we have shown the use of profile information. Task 2, however, can be used to extract such information. While a full study is beyond the scope of this paper, we conducted some preliminary experiments, the details of which are in Appendix D. They show (i) human speaker's profiles can be predicted from their dialogue with high accuracy (94.3%, similar to human performance in <ref type="table" target="#tab_5">Table 4</ref>) or even from the model's dialogue (23% with KV Profile Memory) showing the model is paying attention to the human's interests. Further, the accuracies clearly improve with further dialogue, as shown in <ref type="table" target="#tab_1">Table 14</ref>. Combining Task 1 and Task 2 into a full system is an exciting area of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Discussion</head><p>In this work we have introduced the PERSONA-CHAT dataset, which consists of crowd-sourced dialogues where each participant plays the part of an assigned persona; and each (crowd-sourced) persona has a word-distinct paraphrase. We test various baseline models on this dataset, and show that models that have access to their own personas in addition to the state of the dialogue are scored as more consistent by annotators, although not more engaging. On the other hand, we show that models trained on PERSONA-CHAT (with or without personas) are more engaging than models trained on dialogue from other resources (movies, Twitter).</p><p>We believe PERSONA-CHAT will be a useful resource for training components of future dialogue systems. Because we have paired human generated profiles and conversations, the data aids the construction of agents that have consistent per-sonalities and viewpoints. Furthermore, predicting the profiles from a conversation moves chitchat tasks in the direction of goal-directed dialogue, which has metrics for success. Because we collect paraphrases of the profiles, they cannot be trivially matched; indeed, we believe the original and rephrased profiles are interesting as a semantic similarity dataset in their own right. We hope that the data will aid training agents that can ask questions about users' profiles, remember the answers, and use them naturally in conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Next Utterance Prediction Additional Evaluation Metrics</head><p>In <ref type="table" target="#tab_7">Table 5</ref> and <ref type="table">Table 6</ref> we show additional results on next utterance prediction. In particular we give results for conditioning on the other person's persona ("Their") instead of one's own ("Self") or the combination ("Both"). In particular, we see that, knowing the other's persona does not help precision on this data with these models. Finally, we show in <ref type="table">Table 6</ref> for the ranking models the performance difference from training on original personas versus revised ones. Revised personas give improved results, perhaps because the models are forced to learn more than trivial word overlap (i.e. semantic similarity of differing phrases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Example Dialogs between Humans and Models</head><p>In <ref type="table">Tables 7, 8</ref> <ref type="bibr">, 9, 11, 12</ref> we show example dialogues between Turkers and various models that were collected as part of the human evaluation of Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Human Evaluation Measures</head><p>After dialogues between humans and a model, we then ask the Turker some additional questions in order to evaluate the quality of the model. They are, in order:</p><p>• Fluency: We ask them to judge the fluency of the other speaker as a score from 1 to 5, where 1 is "not fluent at all", 5 is "extremely fluent", and 3 is "OK".</p><p>• Engagingness: We ask them to judge the engagingness of the other speaker disregarding fluency from 1-5, where 1 is "not engaging at all", 5 is "extremely engaging", and 3 is "OK".</p><p>• Consistency: We ask them to judge the consistency of the persona of the other speaker, where we give the example that "I have a dog" followed by "I have no pets" is not consistent. The score is again from 1-5.</p><p>• Profile Detection: Finally, we display two possible profiles, and ask which is more likely to be the profile of the person the Turker just spoke to. One profile is chosen at random, and the other is the true persona given to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Profile Prediction</head><p>While the main study of this work is the ability to improve next utterance classification by conditioning on a persona, one could naturally consider two tasks: (1) next utterance prediction during dialogue, and (2) profile prediction given dialogue history. In the main paper we show that Task 1 can be improved by using profile information. Task 2, however, can be used to extract such information.</p><p>In this section we conduct a preliminary study of the ability to predict the persona of a speaker given a set of dialogue utterances. We consider the dialogues between humans (PERSON 0) and our best performing model, the retrieval-based Key-Value Profile Memory Network (PERSON 1) from Section 5.2. We tested the ability to predict the profile information of the two speakers from the dialogue utterances of each speaker, considering all four combinations. We employ the same IR baseline model used in the main paper to predict profiles: it ranks profile candidates, either at the entire profile level (considering all the sentences that make up the profile as a bag) or at the sentence level (each sentence individually). We consider 100 negative profile candidates for each positive profile, and compute the error rate of predicting the true profile averaged over all dialogues and candidates. The results are given in <ref type="table" target="#tab_1">Table 13</ref>, both for the model conditioned on profile information, and the same KV Memory model that is not. The results indicate the following:</p><p>• It is possible to predict the humans profile from their dialogue utterances (PERSON 0, Profile 0) with high accuracy at both the profile and sentence level, independent of the model they speaking to.</p><p>• Similarly the model's profile can be predicted with high accuracy from its utterances (PER-SON 1, Profile 1) when it is conditioned on the profile, otherwise this is chance level (w/o Profile).</p><p>• It is possible to predict the model's profile from the human's dialogue, but with a lower accuracy (PERSON 0, Profile 1) as long as the model is conditioned on its own profile. This indicates the human responds to the model's utterances and pays attention to the model's interests.   <ref type="table">Table 6</ref>: Evaluation of dialog utterance prediction with ranking models using hits@1 in four settings: conditioned on the speakers persona ("self persona"), the dialogue partner's persona ("their persona"), both or none. The personas are either the original source given to Turkers to condition the dialogue, or the rewritten personas that do not have word overlap, explaining the poor performance of IR in that case.</p><p>• Similarly, the human's profile can be predicted from the model's dialogue, but with lower accuracy. Interestingly, the model without profile conditioning is better at this, perhaps because it does not concentrate on talking about itself, and pays more attention to responding to the human's interests. There appears to be a tradeoff that needs to be explored and understood here.</p><p>We also study the performance of profile prediction as the dialogue progresses, by computing error rates for dialogue lengths 1 to 8 (the longest length we consider in this work). The results, given in <ref type="table" target="#tab_1">Table 14</ref>, show the error rate of predicting the persona decreases in all cases as dialogue length increases.</p><p>Overall, the results in this section show that it is plausible to predict profiles given dialogue utterances, which is an important extraction task. Note that better results could likely be achieved with more sophisticated models.  <ref type="table" target="#tab_1">Table 13</ref>: Profile Prediction. Error rates are given for predicting either the persona of speaker 0 (Profile 0) or of speaker 1 (Profile 1) given the dialogue utterances of speaker 0 (PERSON 0) or speaker 1 (PERSON 1). This is shown for dialogues between humans (PERSON 0) and either the KV Profile Memory model ("KV Profile") which conditions on its own profile, or the KV Memory model ("KV w/o Profile") which does not.  <ref type="table" target="#tab_1">Table 14</ref>: Profile Prediction By Dialog Length. Error rates are given for predicting either the persona of speaker 0 (Profile 0) or of speaker 1 (Profile 1) given the dialogue utterances of speaker 0 (PERSON 0) or speaker 1 (PERSON 1). This is shown for dialogues between humans (PERSON 0) and the KV Profile Memory model averaged over the first N dialogue utterances from 100 conversations (where N is the "Dialogue Length"). The results show the accuracy of predicting the persona improves in all cases as dialogue length increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>I love the beach.To me, there is nothing like a day at the seashore.My dad has a car dealership My father sales vehicles for a living. I just got my nails done I love to pamper myself on a regular basis. I am on a diet now I need to lose weight. Horses are my favorite animal. I am into equestrian sports. I play a lot of fantasy videogames. RPGs are my favorite genre.</figDesc><table><row><cell>Original Persona</cell><cell>Revised Persona</cell></row></table><note>I have a computer science degree.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Example Personas (left) and their revised versions (right) from the PERSONA-CHAT dataset. The revised versions are designed to be characteristics that the same persona might have, which could be rephrases, generalizations or specializations.</figDesc><table><row><cell>Persona 1</cell><cell>Persona 2</cell></row><row><cell>I like to ski</cell><cell>I am an artist</cell></row><row><cell>My wife does not like me anymore</cell><cell>I have four children</cell></row><row><cell cols="2">I have went to Mexico 4 times this year I recently got a cat</cell></row><row><cell>I hate Mexican food</cell><cell>I enjoy walking for exercise</cell></row><row><cell>I like to eat cheetos</cell><cell>I love watching Game of Thrones</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Example dialog from the PERSONA-CHAT dataset. Person 1 is given their own persona (top left) at the beginning of the chat, but does not know the persona of Person 2, and vice-versa. They have to get to know each other during the conversation. and ask them to rephrase, guaranteeing that the instructions are followed. For example, "My father worked for Ford." can be revised to "My dad worked in the car industry", but not "My dad was employed by Ford." due to word overlap.Some examples of the revised personas collected are given inTable 1 (right).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). Finally KV Profile Memory networks outperform Profile Memory Networks in the no persona case due to the ability</figDesc><table><row><cell>Method</cell><cell cols="2">No Persona ppl hits@1</cell><cell cols="4">Original Persona Revised Persona ppl hits@1 ppl hits@1</cell></row><row><cell>Generative Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq2Seq</cell><cell cols="3">38.08 0.092 40.53</cell><cell>0.084</cell><cell>40.65</cell><cell>0.082</cell></row><row><cell>Profile Memory</cell><cell cols="3">38.08 0.092 34.54</cell><cell>0.125</cell><cell>38.21</cell><cell>0.108</cell></row><row><cell>Ranking Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IR baseline</cell><cell>-</cell><cell>0.214</cell><cell>-</cell><cell>0.410</cell><cell>-</cell><cell>0.207</cell></row><row><cell>Starspace</cell><cell>-</cell><cell>0.318</cell><cell>-</cell><cell>0.491</cell><cell>-</cell><cell>0.322</cell></row><row><cell>Profile Memory</cell><cell>-</cell><cell>0.318</cell><cell>-</cell><cell>0.509</cell><cell>-</cell><cell>0.354</cell></row><row><cell>KV Profile Memory</cell><cell>-</cell><cell>0.349</cell><cell>-</cell><cell>0.511</cell><cell>-</cell><cell>0.351</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of dialog utterance prediction with various models in three settings: without conditioning on a persona, conditioned on the speakers given persona ("Original Persona"), or a revised persona that does not have word overlap.</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Human Evaluation of various PERSONA-CHAT models, along with a comparison to human performance, and Twitter and OpenSubtitles based models (last 4 rows), standard deviation in parenthesis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of dialog utterance prediction with generative models in four settings: conditioned on the speakers persona ("self persona"), the dialogue partner's persona ("their persona"), both or none. The personas are either the original source given to Turkers to condition the dialogue, or the revised personas that do not have word overlap. In the "no persona" setting, the models are equivalent, so we only report once.</figDesc><table><row><cell></cell><cell cols="2">No Persona</cell><cell cols="2">Self Persona</cell><cell cols="2">Their Persona</cell><cell cols="2">Both Personas</cell></row><row><cell>Method</cell><cell cols="8">Orig Rewrite Orig Rewrite Orig Rewrite Orig Rewrite</cell></row><row><cell>IR baseline</cell><cell>0.214</cell><cell>0.214</cell><cell>0.410</cell><cell>0.207</cell><cell>0.181</cell><cell>0.181</cell><cell>0.382</cell><cell>0.188</cell></row><row><cell cols="2">Training on original personas</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Starspace</cell><cell>0.318</cell><cell>0.318</cell><cell>0.481</cell><cell>0.295</cell><cell>0.245</cell><cell>0.235</cell><cell>0.429</cell><cell>0.258</cell></row><row><cell>Profile Memory</cell><cell>0.318</cell><cell>0.318</cell><cell>0.473</cell><cell>0.302</cell><cell>0.283</cell><cell>0.267</cell><cell>0.438</cell><cell>0.266</cell></row><row><cell cols="2">Training on revised personas</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Starspace</cell><cell>0.318</cell><cell>0.318</cell><cell>0.491</cell><cell>0.322</cell><cell>0.271</cell><cell>0.261</cell><cell>0.432</cell><cell>0.288</cell></row><row><cell>Profile Memory</cell><cell>0.318</cell><cell>0.318</cell><cell>0.509</cell><cell>0.354</cell><cell>0.299</cell><cell>0.294</cell><cell>0.467</cell><cell>0.331</cell></row><row><cell cols="2">KV Profile Memory 0.349</cell><cell>0.349</cell><cell>0.511</cell><cell>0.351</cell><cell>0.291</cell><cell>0.289</cell><cell>0.467</cell><cell>0.330</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 :</head><label>12</label><figDesc>Example dialog between a human (Person 1) and the Language Model trained on the Twitter dataset (does not use persona).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Profile Level</cell><cell>Sentence Level</cell></row><row><cell>Speaker</cell><cell>Profile</cell><cell cols="3">KV Profile KV w/o Profile KV Profile KV w/o Profile</cell></row><row><cell cols="3">PERSON 0 Profile 0 0.057</cell><cell>0.017</cell><cell>0.173</cell><cell>0.141</cell></row><row><cell cols="3">PERSON 0 Profile 1 0.234</cell><cell>0.491</cell><cell>0.431</cell><cell>0.518</cell></row><row><cell cols="3">PERSON 1 Profile 0 0.254</cell><cell>0.112</cell><cell>0.431</cell><cell>0.349</cell></row><row><cell cols="3">PERSON 1 Profile 1 0.011</cell><cell>0.512</cell><cell>0.246</cell><cell>0.530</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">github.com/facebookresearch/StarSpace</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">tf = 1e6 * 1/(idx 1.07 )</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">] Nice ! How old are your children? [PERSON 2:] I have four that range in age from 10 to 21</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>] No, I do not have much time for TV. PERSON 2:] I usually spend my time painting: but, I love the show</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Evaluating prerequisite qualities for learning end-to-end dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06931</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil Dc</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human conversational behavior. Human nature</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="246" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boi</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faltings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07503</idno>
		<title level="m">Personalization in goal-oriented dialog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A diversity-promoting objective function for neural conversation models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<title level="m">A persona-based neural conversation model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01541</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Managing speaker identity and user profiles in a spoken dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R San</forename><surname>Ferreiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segundo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procesamiento del Lenguaje Natural</title>
		<meeting>esamiento del Lenguaje Natural</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Is it really about me?: message content in social awareness streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Boase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hui</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM conference on Computer supported cooperative work</title>
		<meeting>the 2010 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinnadhurai</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pieper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02349</idno>
		<title level="m">Sarath Chandar, Nan Rosemary Ke, et al. 2017a. A deep reinforcement learning chatbot</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A survey of available corpora for building data-driven dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05742</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06216</idno>
		<title level="m">Generative deep neural networks for dialogue: A short review</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06714</idno>
		<title level="m">A neural network approach to context-sensitive generation of conversational responses</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Elizaa computer program for the study of natural language communication between man and machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03856</idno>
		<title level="m">Starspace: Embed all the things! arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hi are you relaxing on this fine evening? [PERSON 2:] I am doing good and you? [PERSON 1:] I am well, just drinking some scotch and working on my writing [PERSON 2:] I love to cook and bake but I rarely make time for it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<date type="published" when="1769" />
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="1389" to="1402" />
		</imprint>
	</monogr>
	<note>] I understand. I am not allergic but I am terrified of dogs! [PERSON 2:] I have a cat. she has been with me through three wives, haha. PERSON 1:] Funny that you just told me you have no pets [PERSON 2:] I do not have any animals</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What is your novel about? [PERSON 2:] it is a love story. I really enjoy writing. do you write or read?</title>
	</analytic>
	<monogr>
		<title level="m">Example dialog between a human (Person 1) and the Seq2Seq model</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>] I do, but I prefer movies. PERSON 2:] I have not, I can not do that. I have to go to the east coast. PERSON 1:] Why can&apos;t you watch movies? [PERSON 2:] I have not. I have always had that done</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Example dialog between a human (Person 1) and the Generative Profile Memory Network with Self Persona</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>] the sea is shining. PERSON 1:] you at the beach, i love the beach [PERSON 2:] wait, wait, wait..</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Good afternoon! How are you? [PERSON 2:] i&apos;m not sure. I was just talking about the other day</title>
	</analytic>
	<monogr>
		<title level="m">Example dialog between a human (Person 1) and the Language Model trained on the Open-Subtitles 2018 dataset (does not use persona)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Do you have any children? I am a mother to 1 cat. PERSON 2:] this is the first time in history. PERSON 1:] Is it cold where you are? [PERSON 2:] I don&apos;t even know what I&apos;m talking about</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
