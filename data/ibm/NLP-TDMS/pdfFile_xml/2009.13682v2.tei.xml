<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is highly desirable yet challenging to generate image captions that can describe novel objects which are unseen in caption-labeled training data, a capability that is evaluated in the novel object captioning challenge (nocaps). In this challenge, no additional image-caption training data, other than COCO Captions, is allowed for model training. Thus, conventional Vision-Language Pre-training (VLP) methods cannot be applied. This paper presents VIsual VOcabulary pretraining (VIVO) that performs pre-training in the absence of caption annotations. By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of paired image-tag data to learn a visual vocabulary. This is done by pre-training a multi-layer Transformer model that learns to align image-level tags with their corresponding image region features. To address the unordered nature of image tags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct pre-training. We validate the effectiveness of VIVO by fine-tuning the pre-trained model for image captioning. In addition, we perform an analysis of the visual-text alignment inferred by our model. The results show that our model can not only generate fluent image captions that describe novel objects, but also identify the locations of these objects. Our single model has achieved new state-of-the-art results on nocaps and surpassed the human CIDEr score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Image captioning is a long-standing task in artificial intelligence <ref type="bibr" target="#b14">(Farhadi et al. 2010;</ref><ref type="bibr" target="#b22">Kulkarni et al. 2013;</ref><ref type="bibr" target="#b24">Kuznetsova et al. 2012;</ref><ref type="bibr" target="#b32">Mitchell et al. 2012;</ref><ref type="bibr" target="#b51">Yang et al. 2011;</ref><ref type="bibr" target="#b5">Fang et al. 2015)</ref>. The task is challenging in that it requires visual perception and recognition, and natural language generation grounded in perception and real-world knowledge <ref type="bibr" target="#b24">(Kuznetsova et al. 2012;</ref><ref type="bibr" target="#b51">Yang et al. 2011)</ref>. With recent progress in computer vision <ref type="bibr" target="#b17">(He et al. 2017;</ref><ref type="bibr" target="#b35">Ren et al. 2015)</ref>, natural language processing <ref type="bibr" target="#b9">(Devlin et al. 2018;</ref><ref type="bibr" target="#b34">Radford 2018;</ref><ref type="bibr" target="#b45">Vaswani et al. 2017)</ref>, and vision-language understanding <ref type="bibr" target="#b38">Sharma et al. 2018;</ref><ref type="bibr" target="#b55">Zhou et al. 2020a)</ref>, the performance on image captioning has been substantially improved on public benchmarks like COCO <ref type="bibr" target="#b5">(Chen et al. 2015)</ref> and Flickr30k <ref type="bibr" target="#b54">(Young et al. 2014</ref> A person holding a black umbrella and an accordion. <ref type="figure">Figure 1</ref>: VIVO pre-training uses paired image-tag data to learn a rich visual vocabulary where image region features and tags of the semantically similar objects are mapped into vectors that are close to each other. Fine-tuning is conducted on paired image-caption data that only cover a limited numbers of objects (in blue). During inference, our model can generalize to describe novel objects (in yellow) that are learnt during VIVO pre-training.</p><p>trained on such datasets with limited visual concepts generalize poorly to in-the-wild images <ref type="bibr" target="#b44">(Tran et al. 2016)</ref>.</p><p>To improve image captioning in the wild, the nocaps benchmark <ref type="bibr" target="#b0">(Agrawal et al. 2019)</ref> is developed to evaluate Novel Object Captioning (NOC) 1 at scale. The training data for nocaps is the COCO dataset consisting of image-caption pairs and the Open Images dataset <ref type="bibr" target="#b23">(Kuznetsova et al. 2020)</ref> containing bounding boxes and image-level tags. The test data consists of images selected from Open Images, containing nearly 400 objects that are not or rarely seen in the COCO dataset. This raises the challenge of how to generate captions that describe novel objects unseen in the paired image-caption training data. A common strategy is to resort to alternative data sources without caption supervision. Prior works on NOC <ref type="bibr" target="#b29">(Lu et al. 2018;</ref><ref type="bibr" target="#b49">Wu et al. 2018)</ref> propose to generate template sentences that can be filled in with detected visual concepts for NOC. However, the relationship between image and text is not fully explored in their frameworks. We will show that the performance of NOC can be significantly improved by pursuing image-text aligned representation learning.</p><p>In this paper, we present VIsual VOcabulary (VIVO) pretraining that leverages large amounts of vision data without caption annotations to learn a rich visual vocabulary for NOC. As shown in <ref type="figure">Figure 1</ref>, we define visual vocabulary as a joint embedding space where image region features and tags of semantically similar objects are mapped into vectors that are close to each other, e.g., "person" and "man", "accordion" and "instrument". Once the visual vocabulary is pre-trained, we can fine-tune the model using image-caption pairs for caption generation. Note that the dataset used for fine-tuning only covers a small subset of the most commonly occurred objects in the learnt visual vocabulary. Nevertheless, our model can generalize to any images that contain similar scenes (e.g., people sitting in couch in <ref type="figure">Figure 1</ref>) with novel objects unseen in the fine-tuning dataset, like "accordion", thanks to the pre-trained visual vocabulary.</p><p>The VIVO pre-training method is motivated to learn the cross-modality semantic alignment, similarly as in conventional Vision-Language Pre-training (VLP) methods. However, unlike existing VLP models which are pre-trained using image-caption pairs, VIVO is pre-trained on image-tag pairs. To the best of our knowledge, VIVO is the first VLP method that does not rely on caption annotations. Thus, it opens the possibility of leveraging, for VLP, many existing vision datasets originally developed for image tagging or object detection tasks like ImageNet <ref type="bibr" target="#b8">(Deng et al. 2009</ref>), Open Images <ref type="bibr" target="#b23">(Kuznetsova et al. 2020)</ref>, Objects365 , etc. Moreover, we can also leverage large amounts of images, paired with machine-generated tags as weak supervision signals, for VLP.</p><p>VIVO pre-training aims to learn a joint representation of visual and text input. We feed to a multi-layer Transformer model an input consisting of image region features and a paired image-tag set. We then randomly mask one or more tags, and ask the model to predict these masked tags conditioned on the image region features and the other tags. Given that tags are not ordered, we employ the Hungarian matching loss <ref type="bibr" target="#b41">(Stewart, Andriluka, and Ng 2016;</ref><ref type="bibr" target="#b3">Carion et al. 2020)</ref> for tag prediction optimization. Extensive experiments show that VIVO pre-training significantly improves the captioning performance on NOC. In addition, our model can precisely align the object mentions in a generated caption with the regions in the corresponding image.</p><p>In summary, we make the following contributions. • We propose a new VIVO pre-training method that leverages large amounts of vision data without caption annotations for vision-language representation learning. • We develop a Hungarian matching loss with masked tag prediction to conduct pre-training with image-tag pairs. • With a single model, our method achieves the new stateof-the-art result on the nocaps benchmark and surpasses the human CIDEr score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Work</head><p>Image Captioning Prior works on image captioning have focused on exploring different model structures and learn-ing methods for different applications. For example, <ref type="bibr" target="#b40">Song et al. (2019)</ref>; <ref type="bibr" target="#b47">Wang, Chen, and Hu (2019)</ref>; <ref type="bibr" target="#b15">Gao et al. (2019)</ref>; <ref type="bibr" target="#b19">Huang et al. (2019)</ref>; <ref type="bibr" target="#b33">Pan et al. (2020)</ref>; <ref type="bibr" target="#b16">Guo et al. (2020)</ref>; <ref type="bibr" target="#b7">Cornia et al. (2020)</ref> explore different attention mechanisms in captioning modeling. Other works improve the performance with reinforcement learning <ref type="bibr" target="#b36">(Rennie et al. 2017;</ref><ref type="bibr" target="#b25">Li, Chen, and Liu 2019;</ref><ref type="bibr" target="#b50">Yang et al. 2020)</ref> or adversarial learning <ref type="bibr">Dognin et al. 2019)</ref>. Different applications such as dense captioning <ref type="bibr" target="#b20">(Johnson, Karpathy, and Fei-Fei 2016;</ref><ref type="bibr" target="#b53">Yin et al. 2019;</ref><ref type="bibr" target="#b26">Li, Jiang, and Han 2019)</ref>, grounded captioning <ref type="bibr" target="#b30">(Ma et al. 2020;</ref><ref type="bibr" target="#b56">Zhou et al. 2020b</ref>), image captioning with reading comprehension <ref type="bibr" target="#b39">(Sidorov et al. 2020</ref>) have been studied. However, all these methods assume that most of the visual objects in test data are seen in training data. Thus, they do not work well for NOC, where the objects presented in test images are often unseen in the captionannotated training data.</p><p>Novel Object Captioning (NOC) NOC requires a model to generate image captions that describe novel objects that are unseen in the paired image-caption training data. Since the task setting resembles that in real-world applications, it draws growing interest in the research community. The early works, such as Deep Compositional Captioner <ref type="bibr" target="#b18">(Hendricks et al. 2016)</ref> and Novel Object Captioner <ref type="bibr" target="#b46">(Venugopalan et al. 2017)</ref>, propose to use unpaired image and sentence data to transfer knowledge among semantically similar visual concepts. Empirical evaluation on the COCO dataset by holding out 8 novel object categories suggests that these methods might be applicable to NOC. Recent studies propose to explicitly leverage the object detection results for NOC. <ref type="bibr" target="#b52">Yao et al. (2017)</ref> use LSTM-C with a copying mechanism to assemble the detected novel objects for caption generation. Neural Baby Talk <ref type="bibr" target="#b29">(Lu et al. 2018)</ref> and Decoupled Novel Object Captioner <ref type="bibr" target="#b49">(Wu et al. 2018)</ref> generate template sentences that are later filled in with visual concepts recognized by object detectors. Similarly, Constrained Beam Search <ref type="bibr" target="#b1">(Anderson et al. 2017</ref>) is exploited to generate captions that contain detected novel objects <ref type="bibr" target="#b0">(Agrawal et al. 2019)</ref>.</p><p>None of the aforementioned methods for NOC fully exploits the relationship between image and text, which we argue is crucial to the quality of generated captions. In this study, we pre-train a Transformer model to learn a visual vocabulary where object tags are aligned with their corresponding image feature representations in a semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision and Language Pre-training</head><p>Motivated by BERT <ref type="bibr" target="#b9">(Devlin et al. 2018</ref>), many VLP methods have been proposed to learn vision-language representations by pretraining large-scale Transformer models <ref type="bibr" target="#b43">Tan and Bansal 2019;</ref><ref type="bibr" target="#b42">Su et al. 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2020;</ref><ref type="bibr" target="#b55">Zhou et al. 2020a;</ref><ref type="bibr" target="#b27">Li et al. 2020)</ref>. Most existing VLP methods are developed for understanding tasks such as image-text retrieval and visual question answering. Only a few of them <ref type="bibr" target="#b55">(Zhou et al. 2020a;</ref><ref type="bibr" target="#b27">Li et al. 2020)</ref> can be applied to image captioning. But these methods use paired image-caption data for pre-training, and are not applicable to NOC. In this study, we break the dependency on image-caption pairs in VLP for the first time. The proposed VIVO pre-training learns vision-  language alignment on image-tag pairs, improving the image captioning results on both NOC and the general image captioning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>Recent image captioning models have achieved impressive results on the tasks where large amounts of paired imagecaption training data is available. But they generalize poorly to images in the wild, where there are a wide variety of visual objects that are unseen in the caption corpora for training. For example, the models trained on COCO Captions can faithfully describe images containing objects such as "people", "dogs", or "a couch", but fail to generate a reasonable caption for any image containing "an accordion" since the object is unseen in COCO Captions.</p><p>To address this problem, we propose a weakly supervised learning approach to pre-training image captioning models on image-tag pairs that, compared to image-caption pairs, are of larger amounts and contain many more diverse visual objects. Our approach uses a two-stage training scheme that consists of VIVO pre-training and fine-tuning. Figure 2 illustrates our approach using an example. First, in the pre-training stage <ref type="figure" target="#fig_0">(Figure 2(a)</ref>), an image captioning model learns to label image regions using tags (e.g., "person", "accordion") using image-tag pairs as training data, where the object "accordion" is included. Then in fine-tuning (Fig-ure 2(b)), given image-caption pairs and their corresponding object tags detected (e.g., "person" and "dog"), the model learns to map an image to a sentence conditioned on the detected objects, e.g., "[A] holding [B] ...", where [A] and [B] could attend to object tags. While the sentences are learned from image-caption pairs, the object tags may refer to novel visual objects that are unseen in image-caption pairs (but seen in image-tag data in this example). Thus, our model achieves the compositionality generalization, allowing for zero-shot generalization to novel objects for image captioning. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c), at inference time the model is able to recognize objects (e.g., "person", "accordion") and compose familiar constituents in a novel way to form a caption "a person holding an accordion".</p><p>The model architecture is shown in <ref type="figure">Figure 3</ref>. It consists of multiple Transformer layers to encode the input into a feature vector and a linear layer with softmax to generate the text description of the visual objects in the image. In what follows, we describe in detail the way the model is pretrained and fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIVO Pre-training</head><p>We pre-train the Transformer model on a large-scale dataset with abundant tags, e.g., the Open Images training set with 6.4K classes of image-level tags. Unlike many existing VLP methods that rely on image-caption pairs, VIVO pre-training is conducted solely on image-tag pairs, which are much eas-ier to collect by either human labeling or auto tagging. The training objective is to predict the missing (masked) tags given a bag of image-level tags and image regions. We denote the training set as</p><formula xml:id="formula_0">D = {I i , G i } N i=1</formula><p>with N images and their corresponding tags, where G i = {g ij } Li j=1 is a set of L i image-level tags that are associated with the image I i . These tags are textual labels of the visual objects presented in the image, e.g., "person", "cat", "dinning table", etc. In the rest of the paper, we omit the subscript i for simplicity.</p><p>We use a multi-layer Transformer model to learn a joint representation for both vision and language domains. The input to the Transformer model consists of image region features V and tag tokens T, where V = {v k } K k=1 are extracted from image I using a detector trained on Visual Genome dataset <ref type="bibr" target="#b2">(Anderson et al. 2018)</ref>, and T = {t j } T j=1 are tokenized tags in G. During training, some tokens are randomly masked out for the model to predict.</p><p>The main difference between a caption and a set of tags is that words in the caption are ordered while tags are not ordered. This unordered nature may result in ambiguity in tag prediction when two tags are masked out simultaneously. For example, if the masked tokens are "dog" and "cat", we can predict each token in either position without restricting to the original position or order in the input. To resolve this issue, we propose to use the Hungarian matching loss (Stewart, Andriluka, and Ng 2016; <ref type="bibr" target="#b3">Carion et al. 2020)</ref> to formulate the tag prediction as a set-matching problem.</p><p>We denote the set of M masked tokens asT</p><formula xml:id="formula_1">= {t m } M m=1</formula><p>where t m is the token id in the vocabulary, and the prediction probabilities of the corresponding representations in the final layer of Transformer as P</p><formula xml:id="formula_2">= {p i } M i=1</formula><p>where p i is the classification probabilities for the i-th masked position. Since the target tokens inT are unordered, we need an one-toone mapping fromT to P such that the prediction for each masked position is assigned one of the target tokens. Once such an assignment α is known, the loss is defined as:</p><formula xml:id="formula_3">L(T, P, α) = M i=1 (− log(p i (t α(i) )))<label>(1)</label></formula><p>where α is a permutation of the M indices, i.e., α(i) is the index of the target token assigned to the i-th prediction.</p><p>Since the assignment is unknown, we want α to be the best possible mapping betweenT and P. Formally, we define such best possible α to be the one that minimizes the following total cost among all the valid 2 permutations:</p><formula xml:id="formula_4">α = arg min α M i=1 C(p i , t α(i) ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">C(p i , t m ) = 1 − p i (t m )</formula><p>is the cost function of assigning the target t m to the i-th prediction. The reason why we use C(p i , t m ) instead of − log(p i (t α(i) )) as in <ref type="formula" target="#formula_3">(1)</ref> is that it is bounded. Now we can compute the final loss as L(T, P,α), where L is defined in (1) andα is defined in (2).</p><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref> (a), we use bi-directional attention mask in VIVO pre-training. In order to predict a missing tag, the model will have to resort to image region features and the other tags. So it learns a joint representation containing information from both image regions and textual tags. This facilitates the cross-modality alignment between representations of image regions and tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning and Inference</head><p>After pre-training, the Transformer model is fine-tuned on a dataset where both captions and tags are available, e.g., the COCO set annotated with tags from 80 object classes and captions. The tags can also be automatically generated using a pre-trained tagging or detection model. Given image regions and tags, the model learns to predict the conditional caption sentence where some positions are randomly masked out. More specifically, the input to the model during fine-tuning is a triplet of image region features V, a set of tags T and a caption C, where V and T are constructed in the same way as described in pre-training, and C is a sequence of tokens. During fine-tuning, we randomly mask out some of the tokens in a caption sentence for prediction, and optimize the model parameters using the cross-entropy loss. To make the model generate captions from left to right at inference time, during fine-tuning we apply the uni-directional attention mask on a caption sequence to prevent the positions from attending to subsequent positions.</p><p>During inference, we first extract image region features and detect tags from a given image. Then the model is applied to generate a sequence, one token at a time, until it outputs the end of sentence token or reaches the maximum length. At each step the model is auto-regressive, consuming the previously generated tokens as additional input when generating the next.</p><p>In the next section, we present extensive experimental results, showing that our model can generate captions to describe novel objects and that the alignment between image regions and tags, learned from VIVO pre-training, is crucial to the model's superior performance on NOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Settings</head><p>Datasets We use the Open Images V5 challenge training set, which has 1.7M images, for VIVO pre-training. We select 500 classes 3 from bounding box annotations and 6.4K classes from human verified image-level labels. The joint image-tag pairs, containing 6.4K unique classes in total, are used in VIVO pre-training. In the fine-tuning stage, the training data is the COCO training set of 118K images, each with 5 captions. We evaluate our model on the validation and test sets of nocaps, which consist of 4.5K and 10.6K images from the Open Images validation and test sets, respectively. Implementation Details We use the object detector from UpDown <ref type="bibr" target="#b2">(Anderson et al. 2018)</ref> to extract image region fea-tures, which are concatenated with scaled bounding boxes to form a 2054-dimension vector (2048D for the visual features and 6D for the bounding box encoding including topleft and bottom-up corners as well as the box's width and height). We use an object detector trained on the Open Images dataset to detect object tags for all datasets. For pretraining and fine-tuning, we also add the ground-truth tags from the training sets. No ground-truth tags are used on the nocaps validation and test sets. The Transformer model is initialized using BERT-base <ref type="bibr" target="#b9">(Devlin et al. 2018)</ref> where we add a linear layer to transform the image region features to the vectors with same size as the word embeddings.</p><p>In VIVO pre-training, we use a maximum of 50 image regions and 15 tag tokens per image. The model is trained for 160K iterations (about 100 epochs) with a batch size of 1024 and a learning rate of 5 × 10 −5 . In fine-tuning, we set the maximum caption length to 40 and the maximum tag length to 30. The model is trained for 30 epochs with a batch size of 256 and a learning rate of 5 × 10 −5 , optimized using the cross-entropy loss. To further boost the performance, we perform the SCST optimization <ref type="bibr" target="#b36">(Rennie et al. 2017</ref>) with a learning rate of 2 × 10 −6 for 5 epochs. During inference, we use greedy decoding to generate image captions with a maximum length of 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel Object Captioning</head><p>We compare our method with UpDown <ref type="bibr" target="#b2">(Anderson et al. 2018;</ref><ref type="bibr" target="#b0">Agrawal et al. 2019)</ref> and OSCAR 4 , which holds the state-of-the-art result on the nocaps benchmark. The training data for the baselines is the COCO dataset. Following prior settings, we also report the results after our model is optimized using SCST <ref type="bibr" target="#b36">(Rennie et al. 2017</ref>) and generates captions using Constrained Beam Search (CBS) <ref type="bibr" target="#b1">(Anderson et al. 2017</ref>).</p><p>The evaluation results on nocaps validation and test sets are shown in <ref type="table" target="#tab_4">Table 1</ref>. By leveraging VIVO pre-training on the Open Images dataset, our method has achieved significant improvement compared to all prior works. Our plain version (VIVO) already outperforms UpDown+ELMo+CBS and OSCAR by a large margin. It is worth noting that CBS brings absolute gains of 17.8% and 15.5% for UpDown and OSCAR, respectively, but it only improves VIVO by 3.8%. This suggests that our model is more capable of generating captions with novel objects without explicitly adding any constrains. Our best results are new state-of-the-art and surpasses the human CIDEr score on the overall dataset.</p><p>To quantitatively evaluate how well the model can describe novel objects, we also calculate the F1-score following <ref type="bibr" target="#b18">Hendricks et al. (2016)</ref>, where all the objects mentioned in the generated caption sentences are compared against the ground-truth object tags. <ref type="table" target="#tab_5">Table 2</ref> shows the comparison with OSCAR on the nocaps validation set. We see that VIVO improves OSCAR in F1-scores substantially especially for out-of-domain objects. This again verifies the effectiveness of VIVO pre-training in learning to recognize novel objects <ref type="bibr">4</ref> We compare with OSCAR base whose model size is the same as ours. In fact, our model with 12 layers and hidden size of 768 even outperforms the OSCAR large model.  <ref type="figure">Figure 3</ref>: Overview of our VIVO pre-trained Transformer model. Our model consists of multiple Transformer encoder layers followed by a linear layer and a softmax layer. We use masked tag prediction to conduct pre-training. To analyze the visual-text alignment, we use the outputs of the last layer of the encoder layers to estimate the cosine similarity between the image region and tag.</p><p>for NOC.</p><p>Although object tags are used in both VIVO pre-training and fine-tuning stages, we show that the model's capability of generating captions that precisely describe novel objects at inference time attributes largely to pre-training. We compare the distribution of object tags on COCO and nocaps, which are generated by the object detector trained on the Open Images dataset and used for fine-tuning and inference, respectively. As shown in <ref type="table" target="#tab_6">Table 3</ref>, COCO has a longtail distribution where 415 out of 568 categories amounts only to 2.43% of all the tags. The under-representation of novel objects makes the trained model statistically unlikely to generate plausible captions that describe these novel objects. Therefore, our VIVO pre-training, which mitigates the data imbalance issue by leveraging diverse tags in image-tag pairs, is crucial to improving model's generalization property, as empirically demonstrated on NOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-Text Alignment</head><p>To further understand the effects of VIVO pre-training in learning visual vocabulary, which aligns image regions with object tags, we show how the novel object tags can be grounded in image regions in <ref type="figure">Figure 4</ref>. Given the images from the Open Images validation set, we extract image region features using the same object detector from UpDown and generate captions from the captioning model with VIVO pre-training. After identifying the novel objects in the generated captions, as shown in <ref type="figure">Figure 3</ref>, we feed the novel object tags, together with the extracted image region features, to the VIVO pre-trained Transformer model. The output of the last encoder layer is used as the contextualized representation of the corresponding input. We then calculate the cosine similarity between representations of each pair of image region and object tag. We highlight the pairs with high scores in <ref type="figure">Figure 4</ref>. The result shows that our model can precisely     <ref type="bibr" target="#b27">Li et al. (2020)</ref>. The first OSCAR model is trained solely on Conceptual Captions (CC) <ref type="bibr" target="#b38">(Sharma et al. 2018)</ref>, as described in <ref type="bibr" target="#b27">Li et al. (2020)</ref>. The second OSCAR model is pre-trained using VIVO on Open Images (OI), and then fine-tuned on CC. As shown in   <ref type="bibr" target="#b21">(Karpathy and Fei-Fei 2015)</ref>. All results are based on single model with cross-entropy optimization.</p><p>improves the model performance across all metrics evaluated on the COCO test set, especially in CIDEr score. We do observe, however, that the gain on the COCO benchmark is not as substantial as that on the nocaps benchmark. We conjecture that this is due to the COCO dataset containing only a small number of visual concepts and thus diminishing the benefit of learning a large visual vocabulary. It is also worth noting that using machine-generated image tags rather than human-written captions makes it possible to utilize potentially unlimited amounts of images, which we will pursue in our future work.     <ref type="figure">Figure 4</ref>: Image captioning results on nocaps. B: our baseline without adding VIVO pre-training. V: our approach with VIVO pre-training. Red text represents novel objects. For each image, we show the similarity scores of each image region to the novel objects appear in the captions. The bounding box color is brighter when the similarity is higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We select a subset of 10% images from the Open Images training set to conduct an ablation study. We fine-tune with cross-entropy loss on the COCO dataset and report the performance on the nocaps validation set. Using a Larger Set of Tags We investigate whether using a larger set of tags in pre-training improves performance of the downstream image captioning task. We select 500 classes of objects, which are used to train the object detector, from the overall 6.4K classes of tags to conduct VIVO pre-training. As shown in <ref type="table" target="#tab_10">Table 5</ref>, VIVO pre-training with 500 classes significantly improves the performance on nocaps by 6.9% compared to no pre-training. Expanding the labels to 6.4K classes can further improve the performance, although the gain is limited due to the increased diversity of objects presented in test images. Using Hungarian Matching Loss We evaluate the effectiveness of the proposed Hungarian matching in VIVO pretraining to predict a set of tags. Training without Hungarian matching reduces the tag prediction to the standard masked language modeling task, which predicts the masked tokens in the same order as that in the input sequence. In addition, we also perform VIVO pre-training by masking only one token in input, which makes word order information not useful. The evaluation results on the nocaps validation set are in <ref type="table" target="#tab_11">Table 6</ref>. We can see that masking only one token is not effective, and using Hungarian matching leads to the best model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We have presented a weakly supervised learning approach to training image captioning models in two steps. First, a Transformer-based model is pre-trained on large amounts of image-tag pairs to learn a visual vocabulary without the need of using image-caption pairs which are harder to obtain. Then, the model is fine-tuned on image-caption pairs to learn to incorporate information from the pre-trained visual vocabulary and compose image captions that can describe novel visual objects unseen in the training data of imagecaption pairs.</p><p>Our experiments on the nocaps benchmark dataset demonstrate that our model achieves compositional generalization, allowing for zero-shot generalization to novel objects for image captioning. As a result, our best single model creates new state-of-the-art that surpasses the human CIDEr score on nocaps. A detailed analysis reveals that the generalization is attributed to a large degree to the visual vocabulary learned in model pre-training, which maps visual objects or regions with similar semantic meanings to feature vectors that are close to each other in a discrete semantic space.</p><p>Since our pre-training does not need paired image-caption data, one of our future works is to leverage large amounts of vision data, beyond image-tag pairs used in this paper, to significantly improve the quality of the visual vocabulary. In each figure, we use an image patch to represent its image feature, and use a marker "×" with the same color to indicate the same object tag. As shown in (a) and (d), the baseline with random initialization does not work well for visual-text alignment. In (b), the baseline with BERT initialization does not align the two modalities at first, but the alignment is improved after fine-tuning, as shown in (e). In contrast, our approach improves the visual-text alignment in both pre-training and fine-tuning, as shown in (c) and (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Vocabulary Visualization</head><p>To further understand the effects of VIVO, we conduct a qualitative comparison between the feature spaces learnt from the baselines and VIVO using t-SNE <ref type="bibr" target="#b31">(Maaten and Hinton 2008)</ref>. We randomly sample 30 object categories from the nocaps validation set, and visualize the representations of the image regions and object tags. <ref type="figure" target="#fig_2">Figure 5</ref> shows the comparison of two baselines and VIVO. The results show that VIVO compares favorably with the baselines in visual-text alignment.</p><p>We enlarge the t-SNE visualization results of <ref type="figure" target="#fig_2">Figure 5</ref>  <ref type="figure" target="#fig_2">Figure 5(e)</ref>, the alignment of the baseline is better for that objects that frequently occur in the caption corpora, e.g., motorcycle, pizza, but worse for novel objects, e.g., violin, drum, grape. (iv) VIVO improves the alignment overall, especially for novel objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Our transformer-based image captioning model consists of 12 transformer layers for encoding and a linear layer for prediction. Note that the model does not have any decoder layer. We use WordPiece embedding <ref type="bibr" target="#b48">(Wu et al. 2016</ref>) with a 30, 000 token vocabulary to represent input words, including both object tags and captions. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. In addition, we use the Faster R-CNN model from UpDown <ref type="bibr" target="#b2">(Anderson et al. 2018</ref>) to extract image region features and a tagging model trained on the Open Images dataset to predict tags. The transformer model is first pre-trained then fine-tuned, and applied iteratively at inference time to generate the output. Pre-training As described in the main text of the paper, our model consumes a set of tags as textual inputs during pre-training. In addition to the ground truth labels from the Open Images training set, we also use the predictions from our tagging model to enhance the label quality to mitigate that the labels in the Open Images dataset are not complete. We tokenize the tags, and concatenate the tokens into a sequence. We also add the special token [SEP] at the end of the sequence. Following masked language modeling of BERT, we randomly choose 15% of tokens for prediction, i.e., replacing the chosen token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged token 10% of the time. We concatenate the textual feature sequence and the visual feature sequence to form the input to the model. Fine-tuning The textual input encompasses a caption sentence (i.e., the ground truth of COCO Captions), and a set of tags (i.e., the prediction of the tagging model). The sequence for the caption always starts with the [CLS] token and ends with the [SEP] token. The sequence for tags is constructed in the same way as described in pre-training. To differentiate the caption from tags, we add a learned segment embedding to every token indicating whether it belongs to the caption or the tag sequence. In fine-tuning, we only mask tokens from the caption for prediction. The caption feature sequence, tag feature sequence and visual feature sequence are concatenated and fed into the model. Inference At inference time, the model's input contains three parts: a previous prediction for caption, a set of predicted tags, and image region features. At the beginning, the caption part is a [CLS] token followed by a [MASK] token. We feed the input made up of three parts to the model and get the prediction at the position of the [MASK] token. In the next step, we replace the previous [MASK] token with the prediction, and insert another [MASK] token at the end of the caption sequence. This step iterates until the prediction of the end of sentence token, i.e., the [SEP] token, or reaching the maximum length. In this way, the model generates a caption sentence from left to right.  <ref type="figure" target="#fig_2">Figure 5</ref>(e). The marker "×" with the same color indicates the same object class. We observe that the alignment is better for the objects commonly presenting in the caption corpora, e.g., pizza, motorcycle, but worse for novel objects, e.g., grape, violin, drum, strawberry.  <ref type="figure" target="#fig_2">Figure 5</ref>(f). The marker "×" with the same color indicates the same object class. Our model improves the visual-text alignment overall, especially for novel objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed two-stage training scheme. (a) In VIVO pre-training, we train a Transformer-based model on image-tag pairs for tag prediction, where it learns cross-modal representations for rich visual concepts. (b) In fine-tuning, we train the same model on limited image-caption pairs to learn how to generate captions conditional on the image and tags. (c) During inference, given the image and detected tags, our model is applied iteratively to generate a sequence of words describing novel objects in an auto-regressive manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>B</head><label></label><figDesc>: a large piece of art is displayed on the beach V: a turtle that is laying down on the beach B: a close up of a fruit with leaves V: a close up of a peach on a tree branch B: a group of four colored light up in the night sky V: a bunch of red lantern lights on a street B: a street light with a yellow light in the background V: a lamp that is on top of a pole B: a small orange vase with a handle on a table V: a cello is on display in a glass case B: a spider sitting on top of a plate on a dirt ground V: a spider sitting on the ground next to a coin B: a hamburger and fries on a plate V: a hamburger with lettuce and tomato on a plate with french fries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Feature space visualization results of the baselines and VIVO using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(e), Figure 5(c), and Figure 5(f) in Figure 6, Figure 7, and Figure 8, respectively. The results reveal some interesting findings: (i) We observe that VIVO pre-training is helpful in learning a better cross-modality alignment compared to the baselines. (ii) Fine-tuning with paired image-caption training data can further improve the alignment between two modalities. (iii) In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>t-SNE visualization of the baseline with BERT initialization and fine-tuning, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>t-SNE visualization of the VIVO pre-trained model, as shown inFigure 5(c). The marker "×" with the same color indicates the same object class. With the help of VIVO pre-training, we see that the image region features and object tags are better aligned compared to the baselines. t-SNE visualization of VIVO fine-tuned model, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). However, models Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</figDesc><table><row><cell>VIVO Pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>o</cell><cell>o</cell><cell>+</cell><cell></cell><cell>o</cell><cell>+</cell><cell>couch</cell></row><row><cell></cell><cell></cell><cell cols="3">+ o +</cell><cell>+</cell><cell>o +</cell><cell>o</cell></row><row><cell></cell><cell>accordion,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(image, tags)</cell><cell>instrument</cell><cell cols="5">visual vocabulary</cell><cell>man, person</cell></row><row><cell>Fine-tuning</cell><cell></cell><cell cols="3">Inference</cell><cell></cell></row><row><cell cols="2">A person holding a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dog sitting on a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>couch.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(image, sentence, tags)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Pre-training: learn visual vocabulary (b) Fine-tuning: learn sentence description (c) Inference: novel object captioning Multi-layer Transformer</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>accordion</cell></row><row><cell>Open Images</cell><cell>Multi-layer Transformer</cell></row><row><cell>6.4K tags</cell><cell></cell></row><row><cell>w/o caption</cell><cell></cell></row><row><cell></cell><cell>animal, person, [MASK], hat</cell></row><row><cell cols="2">COCO 80 objects w/ caption "A person holding a dog sitting on a couch." (a) [CLS] a person holding a Multi-layer Transformer [CLS] a [MASK] holding a [MASK] sitting on a couch. [SEP] person, dog, couch person dog black umbrella and [MASK] person, umbrella, accordion</cell><cell>A person holding a black umbrella and accordion.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on nocaps validation and test sets.</figDesc><table><row><cell>model</cell><cell cols="3">in-domain out-of-domain entire</cell></row><row><cell>OSCAR (Li et al. 2020)</cell><cell>39.5</cell><cell>15.7</cell><cell>20.7</cell></row><row><cell>VIVO</cell><cell>46.3</cell><cell>30.6</cell><cell>33.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison of F1-scores (in %) on object classes of Open Images, evaluated on the nocaps validation set. There are 504 classes in total. 105 of them are in-domain, which are 80 common classes from COCO and 25 objects frequently appearing in COCO Captions. The remaining 399 classes are the out-of-domain objects.</figDesc><table><row><cell>#occur in COCO (&lt;=)</cell><cell>0</cell><cell>10 100 1K 10K</cell></row><row><cell>#categories</cell><cell cols="2">194 274 415 522 563</cell></row><row><cell>percentage in COCO</cell><cell cols="2">0.0 0.14 2.43 15.62 64.01</cell></row><row><cell>percentage in nocaps</cell><cell cols="2">0.24 5.05 15.98 35.71 69.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Distribution of 568 object categories on COCO training images and nocaps validation images. Each column is a subset of object categories whose number of occurrences are below the threshold. The percentage is calculated by dividing the counts of those objects by the total counts of all objects in the dataset.align the mentions of these novel objects in captions with the corresponding image regions.</figDesc><table><row><cell>General Image Captioning</cell></row><row><cell>VIVO pre-training does not require the paired image-caption</cell></row><row><cell>data for model training as in conventional VLP methods. It</cell></row><row><cell>opens up an opportunity to leverage additional data sources</cell></row><row><cell>to improve image captioning models. To demonstrate the ef-</cell></row><row><cell>fectiveness of VIVO pre-training on general image caption-</cell></row><row><cell>ing tasks, we trained two versions of OSCAR, following the</cell></row><row><cell>setting in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>, VIVO pre-training</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Evaluation on COCO test set of Karpathy split</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Adding VIVO pre-training makes substantial improvement on NOC. Using more labels in pre-training also gives better results. All the models are fine-tuned on COCO and evaluated on the validation set of nocaps.</figDesc><table><row><cell>Loss</cell><cell cols="3">BLEU4 Meteor CIDEr SPICE</cell></row><row><cell>Mask only one token</cell><cell>20.6</cell><cell>25.2</cell><cell>74.9 11.8</cell></row><row><cell>w/o Hungarian matching</cell><cell>21.0</cell><cell>25.4</cell><cell>75.8 11.8</cell></row><row><cell>w/ Hungarian matching</cell><cell>21.2</cell><cell cols="2">25.4 77.8 12.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Ablation study of the proposed Hungarian match- ing loss. Results are evaluated on the entire validation set of nocaps.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use "NOC" to represent the task of novel object captioning and "nocaps" to refer to the nocaps benchmark.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For a tag tokenized into multiple tokens, the order of tokens within the tag cannot be changed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Only 500 out of 600 objects are used in the challenge set, as we further refine the labels by removing classes that are "parts" (e.g., human eyes).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Jianfeng Wang, Ehsan Azarnasab, Lin Liang, Pengchuan Zhang, Xiujun Li, Chunyuan Li, Jianwei Yang, Yu Wang, Houdong Hu, Furu Wei, Dong Li for valuable discussions and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<title level="m">nocaps: novel object captioning at scale. In ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving image captioning with conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNITER: Learning universal image-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meshed-Memory Transformer for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial semantic alignment for improved image captions</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deliberate attention networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Normalized and Geometry-Aware Self-Attention Network for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN. In ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale. IJCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Object Context for Dense Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to Generate Grounded Visual Captions without Localization Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">X-Linear Attention Networks for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">TextCaps: a Dataset for Image Captioning with Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Connecting Language to Images: A Progressive Attention-Guided Network for Simultaneous Image Captioning and Language Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carapcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sienkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Captioning images with diverse objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical attention network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decoupled novel object captioner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aloimonos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in image captioning for learning novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Context and attribute grounded dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">More Grounded Image Captioning by Distilling Image-Text Matching Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
