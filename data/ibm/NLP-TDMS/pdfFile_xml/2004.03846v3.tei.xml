<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Level Knowledge Distillation For Multilingual Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<email>yongjiang.jy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
							<email>nguyen.bach@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Level Knowledge Distillation For Multilingual Sequence Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student). We propose two novel KD methods based on structure-level information:</p><p>(1) approximately minimizes the distance between the student's and the teachers' structurelevel probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions. Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence labeling is an important task in natural language processing. Many tasks such as named entity recognition (NER) and part-of-speech (POS) tagging can be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing <ref type="bibr" target="#b15">(Jurafsky and Martin, 2009</ref>). Most of the previ-ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages <ref type="bibr" target="#b10">Huang et al., 2019a;</ref><ref type="bibr" target="#b31">Rahimi et al., 2019;</ref><ref type="bibr" target="#b12">Huang et al., 2019b;</ref><ref type="bibr" target="#b16">Keung et al., 2019)</ref>, but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual BERT (M-BERT) <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> is surprisingly good at zero-shot cross-lingual model transfer on tasks such as NER and POS tagging <ref type="bibr" target="#b28">(Pires et al., 2019)</ref>. M-BERT bridges multiple languages and makes training a multilingual sequence labeling model with high performance possible <ref type="bibr" target="#b44">(Wu and Dredze, 2019)</ref>. However, accuracy of the multilingual model is still inferior to monolingual models that utilize different kinds of strong pretrained word representations such as contextual string embeddings (Flair) proposed by <ref type="bibr" target="#b1">Akbik et al. (2018)</ref>.</p><p>To diminish the performance gap between monolingual and multilingual models, we propose to utilize knowledge distillation to transfer the knowledge from several monolingual models with strong word representations into a single multilingual model. Knowledge distillation <ref type="bibr" target="#b4">(Buciluǎ et al., 2006;</ref><ref type="bibr" target="#b9">Hinton et al., 2015)</ref> is a technique that first trains a strong teacher model and then trains a weak student model through mimicking the output probabilities <ref type="bibr" target="#b9">(Hinton et al., 2015;</ref><ref type="bibr" target="#b20">Lan et al., 2018;</ref><ref type="bibr" target="#b23">Mirzadeh et al., 2019)</ref> or hidden states <ref type="bibr" target="#b33">(Romero et al., 2014;</ref><ref type="bibr">Seunghyun Lee, 2019</ref>) of the teacher model. The student model can achieve an accuracy comparable to that of the teacher model and usually has a smaller model size through KD. Inspired by KD applied in neural machine translation (NMT) <ref type="bibr" target="#b17">(Kim and Rush, 2016)</ref> and multilingual NMT <ref type="bibr" target="#b40">(Tan et al., 2019)</ref>, our approach contains a set of monolingual teacher models, one for each language, and a single multilingual student model. Both groups of models are based on BiLSTM-CRF <ref type="bibr" target="#b19">(Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016)</ref>, one of the state-of-the-art models in sequence labeling. In BiLSTM-CRF, the CRF layer models the relation between neighbouring labels which leads to better results than simply predicting each label separately based on the BiLSTM outputs. However, the CRF structure models the label sequence globally with the correlations between neighboring labels, which increases the difficulty in distilling the knowledge from the teacher models. In this paper, we propose two novel KD approaches that take structure-level knowledge into consideration for multilingual sequence labeling. To share the structure-level knowledge, we either minimize the difference between the student's and the teachers' distribution of global sequence structure directly through an approximation approach or aggregate the global sequence structure into local posterior distributions and minimize the difference of aggregated local knowledge. Experimental results show that our proposed approach boosts the performance of the multilingual model in 4 tasks with 25 datasets. Furthermore, our approach has better performance in zero-shot transfer compared with the baseline multilingual model and several monolingual teacher models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence Labeling</head><p>BiLSTM-CRF <ref type="bibr" target="#b19">(Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016)</ref> is one of the most popular approaches to sequence labeling. Given a sequence of n word tokens x = {x 1 , · · · , x n } and the corresponding sequence of gold labels y * = {y * 1 , · · · , y * n }, we first feed the token representations of x into a BiL-STM to get the contextual token representations r = {r 1 , · · · , r n }. The conditional probability p(y|x) is defined by:</p><formula xml:id="formula_0">ψ(y , y, r i ) = exp(W T y r i + b y ,y )<label>(1)</label></formula><formula xml:id="formula_1">p(y|x) = n i=1 ψ(y i−1 , y i , r i ) y ∈Y(x) n i=1 ψ(y i−1 , y i , r i ) (2)</formula><p>where Y(x) denotes the set of all possible label sequences for x, ψ is the potential function, W y and b y ,y are parameters and y 0 is defined to be a special start symbol. W T y r i and b y ,y are usually called emission and transition scores respectively. During training, the negative log-likelihood loss for an input sequence is defined by:</p><formula xml:id="formula_2">L NLL = − log p(y * |x)</formula><p>BiLSTM-Softmax approach to sequence labeling reduces the task to a set of label classification problem by disregarding label transitions and simply feeding the emission scores W T r i into a softmax layer to get the probability distribution of each variable y i .</p><formula xml:id="formula_3">p(y i |x) = softmax(W T r i )<label>(3)</label></formula><p>The loss function then becomes:</p><formula xml:id="formula_4">L NLL = − n i=1 log p(y * i |x)</formula><p>In spite of its simplicity, this approach ignores correlations between neighboring labels and hence does not adequately model the sequence structure. Consequently, it empirically underperforms the first approach in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>A typical approach to KD is training a student network by imitating a teacher's predictions <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref>. The simplest approach to KD on BiLSTM-Softmax sequence labeling follows Eq. 3 and performs token-level distillation through minimizing the cross-entropy loss between the individual label distributions predicted by the teacher model and the student model:</p><formula xml:id="formula_5">L Token = − n i=1 |V| j=1 p t (y i = j|x) log p s (y i = j|x) (4)</formula><p>where p t (y i = j|x) and p s (y i = j|x) are the label distributions predicted by the teacher model and the student model respectively and |V| is the number of possible labels. The final loss of the student model combines the KD loss and the negative loglikelihood loss:</p><formula xml:id="formula_6">L = λL Token + (1 − λ)L NLL</formula><p>where λ is a hyperparameter. As pointed out in Section 2.1, however, sequence labeling based on Eq. 3 has the problem of ignoring structure-level knowledge. In the BiLSTM-CRF approach, we can also apply an Emission distillation through feeding emission scores in Eq. 3 and get emission probabilitiesp(y i |x), then the loss function becomes:</p><formula xml:id="formula_7">L Emission = − n i=1 |V| j=1p t (y i = j|x) logp s (y i = j|x) (5)</formula><p>3 Approach</p><p>In this section, we propose two approaches to learning a single multilingual sequence labeling model (student) by distilling structure-level knowledge from multiple mono-lingual models. The first approach approximately minimizes the difference between structure-level probability distributions predicted by the student and teachers. The second aggregates structure-level knowledge into local posterior distributions and then minimizes the difference between local distributions produced by the student and teachers. Our approaches are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Both the student and the teachers are BiLSTM-CRF models <ref type="bibr" target="#b19">(Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016)</ref>, one of the state-of-the-art models in sequence labeling. A BiLSTM-CRF predicts the distribution of the whole label sequence structure, so token-level distillation is no longer possible and structure-level distillation is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Top-K Distillation</head><p>Inspired by <ref type="bibr" target="#b17">Kim and Rush (2016)</ref>, we propose to encourage the student to mimic the teachers' global structural probability distribution over all possible label sequences:</p><formula xml:id="formula_8">L Str = − y∈Y(x) p t (y|x) log p s (y|x)<label>(6)</label></formula><p>However, |Y(x)| is exponentially large as it represents all possible label sequences. We propose two methods to alleviates this issue through efficient approximations of p t (y|x) using the k-best label sequences.</p><p>Top-K Eq. 6 can be seen as computing the expected student log probability with respect to the teacher's structural distribution:</p><formula xml:id="formula_9">L Str = −E pt(y|x) [log p s (y|x)]<label>(7)</label></formula><p>The expectation can be approximated by sampling from the teacher's distribution p t (y|x). However, unbiased sampling from the distribution is difficult. We instead apply a biased approach that regards the k-best label sequences predicted by the <ref type="table">Table 1</ref>: Example of computing the structural knowledge for a sequence of 3 tokens with a label set of {T, F }. ψ(y k−1 , y k , r k ) represents the potential formulated in Eq. 1. Each Label Seq. Probs. is defined in Eq. 2 for the corresponding label sequence. Top-2 represents the two label sequences with the highest scores and Weights are their corresponding weights for KD <ref type="bibr">(Eq. 8,</ref><ref type="bibr">9)</ref>. α(y k ), β(y k ) and the posterior distribution q(y k |x) are computed based on Eq. 11, 12 and 10 respectively. We assume that ψ(y 0 , y 1 , r 1 ) = 1 regardless of whether y 1 is T or F .</p><formula xml:id="formula_10">ψ(y k−1 , y k , r k ) LABEL SEQ. PROBS. STRUCTURAL KNOWLEDGE y1 y2 y3 Prob. y1 y2 y3 Weights k = 2 F F F 0.035 Top-2 T T F 0.57 y k−1 \ y k y2 = F y2 = T F F T 0.316 F F T 0.43 y1 = F 2 1/2 F T F 0.105 α(y k = F ) 1.00 2.50 10.83 y1 = T 1/2 2 F T T 0.007 α(y k = T ) 1.00 2.50 8.13 k = 3 T F F 0.009 β(y k = F ) 8.79 3.33 1.00 y k−1 \ y k y3 = F y3 = T T F T 0.079 β(y k = T ) 10.17 4.25 1.00 y2 = F 1/3 3 T T F 0.422 q(y k = F |x) 0.46 0.44 0.57 y2 = T 4 1/4 T T T 0.026 q(y k = T |x) 0.54 0.56 0.43</formula><p>teacher model as our samples. We use a modified Viterbi algorithm to predict the k-best label sequences T = {ŷ 1 , . . . ,ŷ k }. Eq. 7 is then approximated as:</p><formula xml:id="formula_11">L Top-K = − 1 k ŷ∈T log p s (ŷ|x)<label>(8)</label></formula><p>This can also be seen as data augmentation through generating k pseudo target label sequences for each input sentence by the teacher.</p><p>Weighted Top-K The Top-K method is highly biased in that the approximation becomes worse with a larger k . A better method is to associate weights to the k samples to better approximate p t (y|x).</p><formula xml:id="formula_12">p t (y|x) =    pt(y|x) ŷ∈T pt(ŷ|x) y ∈ T 0 y / ∈ T</formula><p>Eq. 7 is then approximated as:</p><formula xml:id="formula_13">L Top-WK = − y∈T p t (y|x) log p s (y|x) (9)</formula><p>This can be seen as the student learning weighted pseudo target label sequences produced by the teacher for each input sentence. The Top-K approach is related to the previous work on model compression in neural machine translation <ref type="bibr" target="#b17">(Kim and Rush, 2016)</ref> and multilingual neural machine translation <ref type="bibr" target="#b40">(Tan et al., 2019)</ref>. In neural machine translation, producing k-best label sequences is intractable in general and in practice, beam search decoding has been used to approximate the k-best label sequences. However, for linear-chain CRF model, k-best label sequences can be produced exactly with the modified Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Posterior Distillation</head><p>The Top-K is approximate with respect to the teacher's structural distribution and still is slow on large k. Our second approach tries to distill structure-level knowledge based on tractable local (token-wise) distributions q(y k |x), which can be exactly computed.</p><formula xml:id="formula_14">q(y k |x) = {y 1 ,...,yn}\y k p(y 1 , . . . , y n |x) = {y 1 ,...,yn}\y k n i=1 ψ(y i−1 , y i , r i ) Z (10) ∝ α(y k ) × β(y k ) α(y k ) = {y 0 ,...,y k−1 } k i=1 ψ(y i−1 , y i , r i ) (11) β(y k ) = {y k+1 ,...,yn} n i=k+1 ψ(y i−1 , y i , r i ) (12)</formula><p>where Z is the denominator of Eq. 2 that is usually called the partition function and α(y k ) and β(y k ) are calculated in forward and backward pass utilizing the forward-backward algorithm. We assume that β(y n ) = 1. Given the local probability distribution for each token, we define the KD loss function in a similar manner with the token-level distillation in Eq. 5.</p><formula xml:id="formula_15">L Pos. = − n i=1 |V| j=1 q t (y i = j|x) log q s (y i = j|x)<label>(13)</label></formula><p>The difference between token-level distillation and posterior distillation is that posterior distillation is based on BiLSTM-CRF and conveys global </p><formula xml:id="formula_16">D i ∈ D do 5: for (x i j , y i j ) ∈ D i do 6:</formula><p>Teacher model Ti reads the input x i j and predicts probability distributionsp i j required for KD. 7:</p><p>Append (x i j , y i j ,p i j ) into the new training dataset D. 8: end for 9: end for 10: 11: while S &lt; S do 12: S = S + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>for mini-batch (x, y,p) sampled fromD do 14:</p><p>Compute the KD loss LKD(x,p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Compute the golden target loss LNLL(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Compute the final loss L = λLKD + (1 − λ)LNLL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>Update</p><formula xml:id="formula_17">θ: θ = θ -η * ∂L/∂θ . 18: if λ − τ &gt; 0 do 19:</formula><p>Update interpolation factor λ: λ = λ − τ 20: else 21:</p><p>Update interpolation factor λ: λ = 0 22: end if 23: end while structural knowledge in the local probability distribution.</p><p>Posterior distillation has not been used in the related research of knowledge distillation in neural machine translation because of intractable computation of local distributions. In sequence labeling, however, local distributions in a BiLSTM-CRF can be computed exactly using the forward-backward algorithm.</p><p>An example of computing the structural knowledge discussed in this and last subsections is shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Knowledge Distillation</head><p>Let D = {D 1 , . . . , D l } denotes a set of training data with l languages. D i denotes the corpus of the i-th language that contains multiple sentence and label sequence pairs</p><formula xml:id="formula_18">D i = {(x i j , y i j )} m i j=1 .</formula><p>To train a single multilingual student model from multiple monolingual pretrained teachers, for each input sentence, we first use the teacher model of the corresponding language to predict the pseudo targets (k-best label sequences or posterior distribution for posterior distillation). Then the student jointly learns from the gold targets and pseudo targets in training by optimizing the following loss function:</p><formula xml:id="formula_19">L ALL = λL KD + (1 − λ)L NLL</formula><p>where λ decreases from 1 to 0 throughout training following <ref type="bibr" target="#b5">Clark et al. (2019)</ref>, L KD is one of the <ref type="bibr">Eq. 5,</ref><ref type="bibr">8,</ref><ref type="bibr">9,</ref><ref type="bibr">13</ref> or an averaging of Eq. 9, 13. The overall distillation process is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Dataset We use datasets from 4 sequence labeling tasks in our experiment.</p><p>• CoNLL NER: We collect the corpora of 4 languages from the CoNLL 2002 and 2003 shared task <ref type="bibr" target="#b41">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b42">Tjong Kim Sang and De Meulder, 2003)</ref> • WikiAnn NER <ref type="bibr" target="#b25">(Pan et al., 2017)</ref>: The dataset contains silver standard NER tags that are annotated automatically on 282 languages that exist in Wikipedia. We select the data of 8 languages from different language families or from different language subgroups of Indo-European languages. We randomly choose 5000 sentences from the dataset for each language except English, and choose 10000 sentences for English to reflect the abundance of English corpora in practice. We split the dataset by 8:1:1 for training/development/test.</p><p>• Universal Dependencies (UD) <ref type="bibr" target="#b24">(Nivre et al., 2016)</ref>: We use universal POS tagging annotations in the UD datasets. We choose 8 languages from different language families or language subgroups and one dataset for each language.</p><p>• Aspect Extraction: The dataset is from an aspect-based sentiment analysis task in SemEval-2016 Task 5 <ref type="bibr" target="#b30">(Pontiki et al., 2016)</ref>. We choose subtask 1 of the restaurants domain which has the most languages in all domains 1 , and split 10% of the training data as the development data.    Model Configurations In our experiment, all the word embeddings are fixed and M-BERT token embeddings are obtained by average pooling. We feed the token embeddings into the BiLSTM-CRF for decoding. The hidden size of the BiLSTM layer is 256 for the monolingual teacher models and 600 or 800 for the multilingual student model depending on the dataset as larger hidden size for the multilingual model results in better performance in our experiment. The settings of teacher and student models are as follows:</p><p>• Monolingual Teachers: Each teacher is trained with a dataset of a specific language. We use M-BERT concatenated with languagespecific Flair (Akbik et al., 2018) embeddings and fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref> word embeddings as token embeddings 2 for all the monolingual teacher models.</p><p>• Multilingual Student: The student model is trained with the datasets of all the languages combined. We only use M-BERT as token embeddings for the multilingual student model.</p><p>Training For model training, the mini-batch size is set to 2000 tokens. We train all models with SGD optimizer with a learning rate of 0.1 and anneal the learning rate by 0.5 if there is no improvements on the development set for 10 epochs. For all models, we use a single NVIDIA Tesla V100 GPU for training including the student model. We tune the loss interpolation anneal rate in {0.5, 1.0} and the k value of Top-K ranging from [1, 10].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We report results of the following approaches.  • Baseline represents training the multilingual model with the datasets of all the languages combined and without knowledge distillation.</p><p>• Emission is the KD method based on Eq. 5.</p><p>• Top-K, Top-WK and Posterior are our KD methods formulated by Eq. 8, Eq. 9 and Eq. 13 resprectively.</p><p>• Pos.+Top-WK is a mixture of posterior and weighted Top-K distillation.</p><p>We also report the results of monolingual models as Teachers and multilingual BiLSTM-Softmax model with token-level KD based on Eq. 4 as Softmax and Token for reference. <ref type="table" target="#tab_2">Table 2</ref>, 3, and 4 show the effectiveness of our approach on 4 tasks over 25 datasets. In all the tables, we report scores averaged over 5 runs. Observation #0. BiLSTM-Softmax models perform inferior to BiLSTM-CRF models in most cases in the multilingual setting: The results show that the BiLSTM-CRF approach is stronger than the BiLSTM-Softmax approach on three of the four tasks, which are consistent with previous work on sequence labeling <ref type="bibr" target="#b22">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b32">Reimers and Gurevych, 2017;</ref><ref type="bibr" target="#b46">Yang et al., 2018)</ref>. The token-level KD approach performs almost the same as the BiLSTM-Softmax baseline in most of the tasks except the Aspect Extraction task. Observation #1. Monolingual teacher models outperform multilingual student models: This is probably because the monolingual teacher models are based on both multilingual embeddings M-BERT and strong monolingual embeddings (Flair/fastText). The monolingual embedding may provide additional information that is not available to the multilingual student models. Furthermore, note that the learning problem faced by a multilingual student model is much more difficult than that of a teacher model because a student model has to handle all the languages using roughly the same model size as a teacher model. Observation #2. Emission fails to transfer knowledge: Emission outperforms the baseline  <ref type="table">Table 6</ref>: Averaged results of zero-shot transfer on another 28 languages of the NER task and 24 languages of the POS tagging task.</p><p>only on 12 out of 25 datasets. This shows that simply following the standard approach of knowledge distillation from emission scores is not sufficient for the BiLSTM-CRF models. Observation #3. Top-K and Top-WK outperform the baseline: Top-K outperforms the baseline on 15 datasets. It outperforms Emission on average on Wikiann NER and Aspect Extraction and is competitive with Emission in the other two tasks. Top-WK outperforms the baseline on 18 datasets and it outperforms Top-K in all the tasks. Observation #4. Posterior achieves the best performance on most of the tasks: The Posterior approach outperforms the baseline on 21 datasets and only underperforms the baseline by 0.12 on 2 languages in WikiAnn and by 0.01 on one language in UD POS tagging. It outperforms the other methods on average in all the tasks except that is slightly underperforms Pos.+Top-WK in the CoNLL NER task. Observation #5. Top-WK+Posterior stays in between: Pos.+Top-WK outperforms both Top-WK and Posterior only in the CoNLL NER task. In the other three tasks, its performance is above that of Top-WK but below that of Posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-shot Transfer</head><p>We use the monolingual teacher models, multilingual baseline models and our Posterior and Pos.+Top-WK models trained on the CoNLL NER datasets to predict NER tags on the test sets of 7 languages in WikiAnn that used in Section 4.2. Table 5 shows the results. For the teacher models, we report the maximum score over all the teachers for  We also conduct experiments on zero-shot transferring over other 28 languages on WikiAnn NER datasets and 24 languages on UD POS tagging datasets. The averaged results are shown in <ref type="table">Table  6</ref>. The NER experiment shows that our approaches outperforms Baseline on 24 out of 28 languages and the Posterior is stronger than Pos.+Top-WK by 0.29 F1 score on average. The POS tagging experiment shows that our approach outperforms Baseline on 20 out of 24 languages. For more details, please refer to the Appendices A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">KD with Weaker Teachers</head><p>To show the effectiveness of our approach, we train weaker monolingual teachers using only M-BERT embeddings on four datasets of the CoNLL NER task. We run Posterior distillation and keep the setting of the student model unchanged. In this setting, Posterior not only outperforms the baseline, but also outperforms the teacher model on average. This shows that our approaches still work when the teachers have the same token embeddings as the student. By comparing <ref type="table" target="#tab_9">Table 7</ref> and 2, we can also see that stronger teachers lead to better students.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">k Value in Top-K</head><p>To show how the k value affects the performance of Top-K and Top-WK distillation methods, we compare the models with two distillation methods and different k values on the CoNLL NER task. <ref type="figure" target="#fig_1">Figure  2</ref> shows that Top-K drops dramatically when k gets larger while Top-WK performs stably. Therefore   Top-WK is less sensitive to the hyper-parameter k and might be practical in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Training Time and Memory Consumption</head><p>We compare the training time of different approaches on the CoNLL NER task and report the results in <ref type="table" target="#tab_11">Table 8</ref>. Our Top-WK and Posterior approaches take 1.45 and 1.63 times the training time of the Baseline approach. For the memory consumption in training, the GPU memory cost does not vary significantly for all the approaches, while the CPU memory cost for all the KD approaches is about 2 times that of the baseline model, because training models with KD requires storing predictions of the teachers in the CPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Multilingual Sequence Labeling Many important tasks such as NER and POS tagging can be reduced to a sequence labeling problem. Most of the recent work on multilingual NER <ref type="bibr" target="#b39">(Täckström, 2012;</ref><ref type="bibr" target="#b8">Fang et al., 2017;</ref><ref type="bibr" target="#b7">Enghoff et al., 2018;</ref><ref type="bibr" target="#b31">Rahimi et al., 2019;</ref> and POS tagging <ref type="bibr" target="#b36">(Snyder et al., 2009;</ref><ref type="bibr" target="#b29">Plank and Agić, 2018)</ref> focuses on transferring the knowledge of a specific language to another (low-resource) language.</p><p>For example,  proposed crosslingual transfer learning for NER focusing on bootstrapping Japanese from English, which has a different character set than Japanese.</p><p>Pretrained Word Representations Recent progress on pretrained word representations such as ELMo <ref type="bibr" target="#b26">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b47">(Yang et al., 2019)</ref> significantly improve the performance of multiple NLP tasks. Multilingual BERT is a pretrained BERT model incorporating 104 languages into a single multilingual model. <ref type="bibr" target="#b28">Pires et al. (2019)</ref> showed its ability of generalization and zero-shot transfer learning on NER and POS tagging and <ref type="bibr" target="#b16">Keung et al. (2019)</ref> used adversarial learning with M-BERT and significantly improved zero-resource cross-lingual NER. On the tasks of NER and POS tagging, Flair embeddings <ref type="bibr" target="#b1">(Akbik et al., 2018</ref><ref type="bibr" target="#b0">(Akbik et al., , 2019</ref> is a state-of-the-art method based on character-level language models. <ref type="bibr" target="#b37">Straka et al. (2019)</ref> found that concatenating Flair embeddings with BERT embeddings outperforms other mixtures of ELMo, BERT and Flair embeddings in most of the subtasks on the CoNLL 2018 Shared Task <ref type="bibr" target="#b48">(Zeman and Hajič, 2018</ref>) datasets on 54 languages, which inspired us to use M-BERT + Flair embeddings as the word representation of teachers.</p><p>Knowledge Distillation Knowledge distillation has been used to improve the performance of small models with the guidance of big models, with applications in natural language processing <ref type="bibr" target="#b17">(Kim and Rush, 2016;</ref><ref type="bibr" target="#b18">Kuncoro et al., 2016;</ref><ref type="bibr" target="#b40">Tan et al., 2019;</ref><ref type="bibr" target="#b5">Clark et al., 2019;</ref><ref type="bibr" target="#b38">Sun et al., 2019)</ref>, computer vision <ref type="bibr" target="#b2">(Ba and Caruana, 2014)</ref> and speech recognition <ref type="bibr" target="#b11">(Huang et al., 2018)</ref>. For simple classification problems, there is a variety of work on tasks such as sentiment analysis <ref type="bibr" target="#b5">(Clark et al., 2019)</ref>, image recognition <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref> and cross-lingual text classification <ref type="bibr" target="#b45">(Xu and Yang, 2017)</ref>. For structured prediction problems, there are lines of work on neural machine translation <ref type="bibr" target="#b17">(Kim and Rush, 2016;</ref><ref type="bibr" target="#b40">Tan et al., 2019)</ref>, connectionist temporal classification in the field of speech recognition <ref type="bibr" target="#b11">(Huang et al., 2018)</ref> and dependency parsing <ref type="bibr" target="#b18">(Kuncoro et al., 2016;</ref><ref type="bibr" target="#b21">Liu et al., 2018)</ref>. Many recent researches on BERT with knowledge distillation are focused on distilling a large BERT model into a smaller one. <ref type="bibr" target="#b43">(Tsai et al., 2019)</ref> distilled a large M-BERT model into a three layer M-BERT model for sequence labeling and achieved a competitively high accuracy with significant speed improvements. <ref type="bibr" target="#b13">(Jiao et al., 2019)</ref> proposed TinyBERT for natural language understanding. <ref type="bibr" target="#b34">(Sanh et al., 2019)</ref> proposed a distilled version of the BERT model which achieves a 60% faster speed and maintains 97% performance of the larger BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion on Flair/M-BERT Fine-tuning</head><p>Previous work has discussed and empirically investigated two ways of adapting monolingual pretrained embedding models to monolingual downstream tasks <ref type="bibr" target="#b27">(Peters et al., 2019)</ref>: either fixing the models and using them for feature extraction, or fine-tuning them in downstream tasks. They found that both settings have comparable performance in most cases. <ref type="bibr" target="#b44">Wu and Dredze (2019)</ref> found that fine-tuning M-BERT with the bottom layers fixed provides further performance gains in multilingual setting. In this paper, we mainly focus on the first approach and utilize the pretrained embedding as fixed feature extractor because Flair/M-BERT finetuning is too slow for our large-scale experimental design of multilingual KD. Designing a cheap and fast fine-tuning approach for pretrained embedding models might be an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper our major contributions are the two structure-level methods to distill the knowledge of monolingual models to a single multilingual model in sequence labeling: Top-K knowledge distillation and posterior distillation. The experimental results show that our approach improves the performance of multilingual models over 4 tasks on 25 datasets. The analysis also shows that our model has stronger zero-shot transfer ability on unseen languages on the NER and POS tagging task. Our code is publicly available at https://github.   <ref type="table">Table 10</ref>: F1 scores of zero-shot transfer on the UD POS tagging datasets. ∆ represents the difference of F1 score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Structure-level knowledge distillation approaches. Mono/Multi represents Monolingual and Multilingual, respectively. Pos. represents the posterior distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Averaged F1 scores on the CoNLL NER task versus the k values of Top-K distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 KD for Multilingual Sequence Labeling 1: Input: Training corpora D = {D 1 , . . . , D l } with l languages, monolingual models T = {T 1 , . . . , T l } pretrained on the corresponding training corpus, learning rate η, multilingual student model M with parameters θ, total training epochs S, loss interpolation coefficient λ, interpolation annealing rate τ . 2: Initialize: Randomly initialize multilingual model parameters θ. Set the current training epoch S = 0, current loss interpolation λ = 1. Create an new empty training dataset</figDesc><table><row><cell>D.</cell></row><row><cell>3:</cell></row><row><cell>4: for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Spanish German Avg. Turkish Spanish Dutch English Russian Avg. REF TEACHERS 92.43 91.90 89.19 84.00 89.38 59.29 74.29 72.85 72.80 71.77 70.20 SOFTMAX 90.08 88.99 87.72 81.40 87.05 52.39 71.54 68.86 65.87 66.85 65.10 TOKEN 90.02 88.87 88.24 81.30 87.11 52.56 72.12 69.33 66.81 67.20 65.61 BASE BASELINE 90.13 89.11 88.06 82.16 87.36 55.79 72.02 69.35 67.54 68.02 66.54 EMISSION 90.28 89.31 88.65 81.96 87.55 51.52 72.60 69.10 67.21 68.52 65.79 OURS TOP-K 90.57 89.33 88.61 81.99 87.62 55.74 73.13 69.81 67.99 69.21 67.18</figDesc><table><row><cell>Task</cell><cell cols="2">CoNLL NER</cell><cell cols="2">SemEval 2016 Aspect Extraction</cell></row><row><cell cols="2">Approach English Dutch TOP-WK 90.52 89.24 88.64</cell><cell>82.15 87.64 56.40</cell><cell>72.81 69.33 68.16</cell><cell>69.42 67.22</cell></row><row><cell>POSTERIOR</cell><cell>90.68 89.41 88.57</cell><cell>82.22 87.72 56.69</cell><cell>73.47 69.98 68.11</cell><cell>69.22 67.49</cell></row><row><cell cols="2">POS.+TOP-WK 90.53 89.58 88.66</cell><cell>82.31 87.77 55.00</cell><cell>73.97 70.15 67.83</cell><cell>69.76 67.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results in F1 score of CoNLL 2002/2003 NER task and Aspect Extraction of SemEval 2016 Task 5.</figDesc><table><row><cell></cell><cell>Approach</cell><cell cols="6">English Tamil Basque Hebrew Indonesian Persian Slovenian French Avg.</cell></row><row><cell></cell><cell>TEACHERS</cell><cell>83.80 86.72 94.68</cell><cell>83.72</cell><cell>90.48</cell><cell>90.37</cell><cell>91.66</cell><cell>90.29 88.97</cell></row><row><cell>REF</cell><cell>SOFTMAX</cell><cell>81.86 80.72 93.72</cell><cell>77.11</cell><cell>90.64</cell><cell>90.03</cell><cell>91.05</cell><cell>88.18 86.66</cell></row><row><cell></cell><cell>TOKEN</cell><cell>81.33 80.88 93.56</cell><cell>77.47</cell><cell>90.50</cell><cell>89.83</cell><cell>91.08</cell><cell>87.93 86.57</cell></row><row><cell>BASE</cell><cell>BASELINE EMISSION</cell><cell>82.56 82.39 94.13 82.54 82.23 94.37</cell><cell>78.89 78.45</cell><cell>91.11 90.92</cell><cell>90.23 89.92</cell><cell>91.62 91.56</cell><cell>88.92 87.48 89.47 87.43</cell></row><row><cell></cell><cell>TOP-K</cell><cell>82.39 82.94 94.13</cell><cell>78.93</cell><cell>90.93</cell><cell>90.12</cell><cell>91.56</cell><cell>89.25 87.53</cell></row><row><cell>OURS</cell><cell>TOP-WK POSTERIOR</cell><cell>82.55 82.71 94.44 83.03 83.02 94.35</cell><cell>78.79 78.77</cell><cell>91.18 91.75</cell><cell>90.22 90.11</cell><cell>91.37 91.95</cell><cell>89.32 87.57 89.65 87.83</cell></row><row><cell></cell><cell cols="2">Pos.+Top-WK 82.77 82.81 94.47</cell><cell>78.87</cell><cell>91.18</cell><cell>90.31</cell><cell>91.84</cell><cell>89.42 87.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>F1 scores in the WikiAnn NER task.</figDesc><table><row><cell></cell><cell>TEACHERS</cell><cell>96.94</cell><cell>97.54</cell><cell>96.81</cell><cell>95.01</cell><cell>99.10</cell><cell>94.02</cell><cell>98.07 93.01 96.31</cell></row><row><cell>REF</cell><cell>SOFTMAX</cell><cell>95.61</cell><cell>96.25</cell><cell>96.59</cell><cell>90.66</cell><cell>97.94</cell><cell>92.56</cell><cell>96.62 86.58 94.10</cell></row><row><cell></cell><cell>TOKEN</cell><cell>95.66</cell><cell>96.28</cell><cell>96.47</cell><cell>90.82</cell><cell>97.95</cell><cell>92.70</cell><cell>96.58 86.41 94.11</cell></row><row><cell>BASE</cell><cell>BASELINE EMISSION</cell><cell>95.71 95.63</cell><cell>96.18 96.21</cell><cell>96.60 96.52</cell><cell>90.64 90.76</cell><cell>97.89 97.98</cell><cell>92.62 92.64</cell><cell>96.63 86.19 94.06 96.61 86.66 94.13</cell></row><row><cell></cell><cell>TOP-K</cell><cell>95.74</cell><cell>96.27</cell><cell>96.56</cell><cell>90.66</cell><cell>97.96</cell><cell>92.58</cell><cell>96.64 86.57 94.12</cell></row><row><cell>OURS</cell><cell>TOP-WK POSTERIOR</cell><cell>95.68 95.71</cell><cell>96.23 96.34</cell><cell>96.58 96.59</cell><cell>90.73 90.91</cell><cell>97.89 97.99</cell><cell>92.62 92.72</cell><cell>96.62 86.74 94.14 96.69 87.36 94.29</cell></row><row><cell></cell><cell cols="2">POS.+TOP-WK 95.74</cell><cell>96.27</cell><cell>96.47</cell><cell>90.84</cell><cell>98.02</cell><cell>92.58</cell><cell>96.73 86.97 94.20</cell></row></table><note>Approach English Hebrew Japanese Slovenian French Indonesian Persian Tamil Avg.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Accuracies in UD POS tagging.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of zero-shot transfer in the NER task (CoNLL ⇒ WikiAnn).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>English Dutch Spanish German Avg. TEACHERS 90.63 89.65 88.05 81.81 87.54 BASELINE 90.13 89.11 88.06 82.16 87.36 POSTERIOR 90.57 89.17 88.61 82.16 87.63</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Posterior distillation with weaker teachers.</figDesc><table><row><cell>each language. The results show that multilingual</cell></row><row><cell>models significantly outperform the teacher models.</cell></row><row><cell>For languages such as Tamil and Hebrew, which</cell></row><row><cell>are very different from the languages in the CoNLL</cell></row><row><cell>datasets, the performance of the teacher models</cell></row><row><cell>drops dramatically compared with the multilingual</cell></row><row><cell>models. It shows that the language specific features</cell></row><row><cell>in teacher models limits their generalizability on</cell></row><row><cell>new languages. Our multilingual models, Poste-</cell></row><row><cell>rior and Pos.+Top-WK outperform the baseline</cell></row><row><cell>on all the languages. Emission slightly underper-</cell></row><row><cell>forms Baseline, once again showing its ineffective-</cell></row><row><cell>ness in knowledge distillation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Training time of the Baseline and KD ap-</cell></row><row><cell>proaches on CoNLL NER datasets. The training time</cell></row><row><cell>of KD approaches includes teachers predicting and stu-</cell></row><row><cell>dent training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>F1 scores of zero-shot transfer on the WikiAnn NER datasets. ∆ represents the difference of F1 score.</figDesc><table><row><cell></cell><cell>ar</cell><cell>bg</cell><cell>ca</cell><cell>cs</cell><cell>da</cell><cell>de</cell><cell>es</cell><cell>eu</cell><cell>fi</cell></row><row><cell>(1): TEACHER</cell><cell>47.85</cell><cell>48.24</cell><cell>80.04</cell><cell>51.62</cell><cell>53.79</cell><cell>44.35</cell><cell>81.03</cell><cell>44.29</cell><cell>51.50</cell></row><row><cell>(2): BASELINE</cell><cell>80.82</cell><cell>88.59</cell><cell>89.95</cell><cell>87.55</cell><cell>88.35</cell><cell>87.70</cell><cell>91.32</cell><cell>69.62</cell><cell>80.06</cell></row><row><cell>(3): EMISSION</cell><cell>80.85</cell><cell>88.62</cell><cell>90.00</cell><cell>87.56</cell><cell>88.47</cell><cell>87.89</cell><cell>91.27</cell><cell>69.68</cell><cell>80.10</cell></row><row><cell>(4): POSTERIOR</cell><cell>80.95</cell><cell>88.26</cell><cell>89.77</cell><cell>87.50</cell><cell>88.68</cell><cell>87.79</cell><cell>91.48</cell><cell>70.03</cell><cell>80.52</cell></row><row><cell>(5): POSTERIOR+TOP-K</cell><cell>80.77</cell><cell>88.30</cell><cell>89.77</cell><cell>87.46</cell><cell>88.58</cell><cell>87.84</cell><cell>91.29</cell><cell>70.17</cell><cell>80.38</cell></row><row><cell>∆: (4)-(1)</cell><cell>33.10</cell><cell>40.02</cell><cell>9.73</cell><cell>35.88</cell><cell>34.89</cell><cell>43.44</cell><cell>10.45</cell><cell>25.74</cell><cell>29.02</cell></row><row><cell>∆: (5)-(1)</cell><cell>32.92</cell><cell>40.06</cell><cell>9.73</cell><cell>35.84</cell><cell>34.79</cell><cell>43.49</cell><cell>10.26</cell><cell>25.88</cell><cell>28.88</cell></row><row><cell>∆: (4)-(2)</cell><cell>0.12</cell><cell>-0.33</cell><cell>-0.18</cell><cell>-0.05</cell><cell>0.33</cell><cell>0.09</cell><cell>0.15</cell><cell>0.41</cell><cell>0.47</cell></row><row><cell>∆: (5)-(2)</cell><cell>-0.05</cell><cell>-0.30</cell><cell>-0.18</cell><cell>-0.09</cell><cell>0.23</cell><cell>0.14</cell><cell>-0.03</cell><cell>0.55</cell><cell>0.32</cell></row><row><cell>∆: (4)-(3)</cell><cell>0.09</cell><cell>-0.36</cell><cell>-0.24</cell><cell>-0.06</cell><cell>0.21</cell><cell>-0.10</cell><cell>0.20</cell><cell>0.34</cell><cell>0.42</cell></row><row><cell>∆: (5)-(3)</cell><cell>-0.08</cell><cell>-0.33</cell><cell>-0.23</cell><cell>-0.10</cell><cell>0.11</cell><cell>-0.05</cell><cell>0.02</cell><cell>0.49</cell><cell>0.28</cell></row><row><cell></cell><cell>hi</cell><cell>hr</cell><cell>it</cell><cell>ko</cell><cell>nl</cell><cell>no</cell><cell>pl</cell><cell>pt</cell><cell>ro</cell></row><row><cell>(1): TEACHER</cell><cell>33.09</cell><cell>69.40</cell><cell>79.33</cell><cell>37.90</cell><cell>40.02</cell><cell>50.86</cell><cell>48.68</cell><cell>77.66</cell><cell>70.45</cell></row><row><cell>(2): BASELINE</cell><cell>76.41</cell><cell>88.28</cell><cell>93.66</cell><cell>58.47</cell><cell>87.30</cell><cell>88.84</cell><cell>85.26</cell><cell>93.38</cell><cell>86.20</cell></row><row><cell>(3): EMISSION</cell><cell>76.15</cell><cell>88.17</cell><cell>93.74</cell><cell>58.65</cell><cell cols="2">87.32 88.94</cell><cell>85.27</cell><cell>93.49</cell><cell>86.15</cell></row><row><cell>(4): POSTERIOR</cell><cell cols="2">76.64 88.46</cell><cell>93.70</cell><cell>59.09</cell><cell>87.19</cell><cell>88.91</cell><cell>85.31</cell><cell>93.42</cell><cell>86.33</cell></row><row><cell>(5): POSTERIOR+TOP-K</cell><cell>76.44</cell><cell>88.34</cell><cell>93.83</cell><cell>58.85</cell><cell>87.20</cell><cell>88.83</cell><cell>85.60</cell><cell>93.15</cell><cell>86.57</cell></row><row><cell>∆: (4)-(1)</cell><cell>43.55</cell><cell>19.06</cell><cell>14.37</cell><cell>21.19</cell><cell>47.17</cell><cell>38.05</cell><cell>36.63</cell><cell>15.76</cell><cell>15.88</cell></row><row><cell>∆: (5)-(1)</cell><cell>43.35</cell><cell>18.94</cell><cell>14.50</cell><cell>20.95</cell><cell>47.18</cell><cell>37.97</cell><cell>36.92</cell><cell>15.49</cell><cell>16.12</cell></row><row><cell>∆: (4)-(2)</cell><cell>0.23</cell><cell>0.18</cell><cell>0.03</cell><cell>0.62</cell><cell>-0.11</cell><cell>0.07</cell><cell>0.05</cell><cell>0.03</cell><cell>0.13</cell></row><row><cell>∆: (5)-(2)</cell><cell>0.03</cell><cell>0.06</cell><cell>0.17</cell><cell>0.38</cell><cell>-0.10</cell><cell>0.00</cell><cell>0.34</cell><cell>-0.23</cell><cell>0.36</cell></row><row><cell>∆: (4)-(3)</cell><cell>0.50</cell><cell>0.29</cell><cell>-0.05</cell><cell>0.45</cell><cell>-0.13</cell><cell>-0.03</cell><cell>0.04</cell><cell>-0.07</cell><cell>0.18</cell></row><row><cell>∆: (5)-(3)</cell><cell>0.30</cell><cell>0.18</cell><cell>0.09</cell><cell>0.21</cell><cell>-0.11</cell><cell>-0.10</cell><cell>0.33</cell><cell>-0.34</cell><cell>0.41</cell></row><row><cell></cell><cell>ru</cell><cell>sk</cell><cell>sr</cell><cell>sv</cell><cell>tr</cell><cell>zh</cell><cell>Avg.</cell><cell></cell><cell></cell></row><row><cell>(1): TEACHER</cell><cell>50.81</cell><cell>56.09</cell><cell>70.04</cell><cell>50.63</cell><cell>54.93</cell><cell>51.55</cell><cell>56.01</cell><cell></cell><cell></cell></row><row><cell>(2): BASELINE</cell><cell>88.15</cell><cell>87.67</cell><cell>89.70</cell><cell>89.73</cell><cell>71.49</cell><cell>70.24</cell><cell>84.11</cell><cell></cell><cell></cell></row><row><cell>(3): EMISSION</cell><cell>88.10</cell><cell>87.73</cell><cell>89.60</cell><cell>89.91</cell><cell>71.68</cell><cell>70.72</cell><cell>84.17</cell><cell></cell><cell></cell></row><row><cell>(4): POSTERIOR</cell><cell>88.22</cell><cell>87.83</cell><cell cols="2">89.95 89.96</cell><cell>71.93</cell><cell cols="2">70.93 84.28</cell><cell></cell><cell></cell></row><row><cell>(5): POSTERIOR+TOP-K</cell><cell>88.10</cell><cell>87.84</cell><cell>89.92</cell><cell>89.69</cell><cell>71.99</cell><cell>70.74</cell><cell>84.24</cell><cell></cell><cell></cell></row><row><cell>∆: (4)-(1)</cell><cell>37.41</cell><cell>31.74</cell><cell>19.91</cell><cell>39.33</cell><cell>17.00</cell><cell>19.38</cell><cell>28.28</cell><cell></cell><cell></cell></row><row><cell>∆: (5)-(1)</cell><cell>37.29</cell><cell>31.75</cell><cell>19.88</cell><cell>39.06</cell><cell>17.06</cell><cell>19.19</cell><cell>28.23</cell><cell></cell><cell></cell></row><row><cell>∆: (4)-(2)</cell><cell>0.07</cell><cell>0.16</cell><cell>0.25</cell><cell>0.23</cell><cell>0.44</cell><cell>0.69</cell><cell>0.17</cell><cell></cell><cell></cell></row><row><cell>∆: (5)-(2)</cell><cell>-0.05</cell><cell>0.18</cell><cell>0.22</cell><cell>-0.04</cell><cell>0.50</cell><cell>0.50</cell><cell>0.12</cell><cell></cell><cell></cell></row><row><cell>∆: (4)-(3)</cell><cell>0.12</cell><cell>0.10</cell><cell>0.35</cell><cell>0.05</cell><cell>0.24</cell><cell>0.21</cell><cell>0.12</cell><cell></cell><cell></cell></row><row><cell>∆: (5)-(3)</cell><cell>0.00</cell><cell>0.11</cell><cell>0.32</cell><cell>-0.21</cell><cell>0.31</cell><cell>0.02</cell><cell>0.07</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Subtask 1 of the restaurants domain contains 6 languages but we failed to get the French dataset as the dataset is not accessible from the provided crawling toolkit.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use fastText + M-BERT instead if the Flair embedding is not available for a certain language.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://en.wikipedia.org/wiki/List_ of_ISO_639-1_codes</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the National Natural Science Foundation of China (61976139).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>In this appendices, we use ISO 639-1 codes 3 to represent each language for simplification. <ref type="table">Table 9</ref>, 10 shows performance of zero-shot transfer on the NER and POS tagging datasets. Our Posterior approach outperforms Baseline in 24 out of 28 languages on NER and 20 out of 24 languages on POS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Zero-shot Transfer</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="724" to="728" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BAM! born-again multi-task networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1595</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5931" to="5937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-resource named entity recognition via multi-source projection: Not quite there yet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Vium Enghoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</title>
		<meeting>the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning how to active learn: A deep reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crosslingual multi-level adversarial transfer to enhance low-resource name tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1383</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-05" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3823" to="3833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge distillation for sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhehuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1589</idno>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3703" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What matters for neural cross-lingual named entity recognition: An empirical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1672</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6396" to="6402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">Tinybert: Distilling bert for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for Japanese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penny</forename><surname>Karanasou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Gaspers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-2023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis -Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
	<note>Industry Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and Language Processing</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>2Nd Edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Bhardwaj</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1355" to="1360" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distilling an ensemble of greedy dependency parsers into one MST parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1180</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7517" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distilling knowledge for searchbased structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1393" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed-Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
		<idno>abs/1902.03393</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1659" to="1666" />
		</imprint>
	</monogr>
	<note>Portorož. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</title>
		<meeting>the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distant supervision from disparate sources for low-resource partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="620" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orphée</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotelnikov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>Nuria Bel, Salud María Jiménez-Zafra, and Gülşen Eryigit; San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Massively multilingual transfer for NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="151" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal hyperparameters for deep lstm-networks for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.6550</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing -NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphbased knowledge distillation by multi-head attention network</title>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<editor>Byung Cheol Song Seunghyun Lee</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adding more languages improves unsupervised multilingual part-of-speech tagging: a Bayesian non-parametric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07448</idno>
		<title level="m">Evaluating contextualized embeddings on 54 languages in pos tagging, lemmatization and dependency parsing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4314" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nudging the envelope of direct transfer methods for multilingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>the NAACL-HLT Workshop on the Induction of Linguistic Structure<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Small and practical BERT models for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amelia</forename><surname>Archer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1374</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3632" to="3636" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cross-lingual distillation for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1130</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3879" to="3889" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Association for Computational Linguistics</publisher>
			<pubPlace>Brussels, Belgium</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
