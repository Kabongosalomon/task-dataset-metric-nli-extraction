<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building Computationally Efficient and Well-Generalizing Person Re-Identification Models with Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Sovrasov</surname></persName>
							<email>sovrasov.vladislav@itmm.unn.ru</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Sidnev</surname></persName>
							<email>dmitry.sidnev@intel.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Lobachevsky State University of Nizhni Novgorod</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IOTG Computer Vision (ICV)</orgName>
								<orgName type="institution" key="instit2">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">IOTG Computer Vision (ICV)</orgName>
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Building Computationally Efficient and Well-Generalizing Person Re-Identification Models with Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work considers the problem of domain shift in person re-identification. Being trained on one dataset, a reidentification model usually performs much worse on unseen data. Partially this gap is caused by the relatively small scale of person re-identification datasets (compared to face recognition ones, for instance), but it is also related to training objectives. We propose to use the metric learning objective, namely AM-Softmax loss, and some additional training practices to build wellgeneralizing, yet, computationally efficient models. We use recently proposed Omni-Scale Network (OSNet) architecture combined with several training tricks and architecture adjustments to obtain state-of-the art results in cross-domain generalization problem on a large-scale MSMT17 dataset in three setups: MSMT17-all→DukeMTMC, MSMT17-train→Market1501 and MSMT17-all→Market1501. Training code and the models are available online in the GitHub repository 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, deep learning approaches have taken leading positions in many computer vision tasks such as image classification, object detection, semantic segmentation, face recognition, person re-identification <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref> and many others. Modern ways to solve these tasks are based on convolutional neural networks (CNN). Common weak point of CNNs is domain shift <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The impact of this problem depends on the nature of the computer vision task and diversity of available training data. The person re-identification task is to build a discriminative identity-preserving person descriptor for performing largescale person retrieval from diverse video streams coming from different cameras under varying lighting and background conditions. Latest advances in face recognition <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> can make one think that domain shift is not a significant issue for person re-identification as well, since both tasks are solved using similar approaches of building identity-preserving descriptors. Unfortunately, performance of person re-identification CNNs drastically degrades on unseen data captured under different (compared to the training data) conditions. Many works are devoted to the cross-domain adaptation problem <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> because this question is critical for practical use of reidentification models.  IDs Images Cameras VIPeR <ref type="bibr" target="#b19">[20]</ref> 632 1264 2 GRID <ref type="bibr" target="#b20">[21]</ref> 251 1275 6 CUHK01 <ref type="bibr" target="#b21">[22]</ref> 971 3882 2 CUHK03 <ref type="bibr" target="#b4">[5]</ref> 1467 28192 2 Market1501 <ref type="bibr" target="#b22">[23]</ref> 1501 32668 6 DukeMTMC-ReID <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> 1812 36411 8 MSMT17 <ref type="bibr" target="#b11">[12]</ref> 4101 <ref type="bibr">126411 15</ref> We believe that huge domain shift problem experienced by the current state-of-the-art person re-identification models is due to the following reasons:</p><p>• Relatively small size of the available datasets (see statistics in <ref type="table" target="#tab_0">Table I</ref>). Currently the largest and the most challenging public person re-identification dataset is MSMT17 <ref type="bibr" target="#b11">[12]</ref>. It consists of 126142 images and contains 4101 identities. At the same time, the most popular face recognition dataset MS-Celeb-1M <ref type="bibr" target="#b12">[13]</ref> has about 10M images of 1M identities. Such a huge scale allows models trained on the MS-Celeb-1M to be robust in benchmarks containing unseen data <ref type="bibr" target="#b7">[8]</ref> from other domains. • The nature of the person re-identification problem itself: appearance of a person seems to be not as discriminative as appearance of their face. For example, distinguishing two people in similar dark clothes from the back side can be challenging even for a human. • A common way of building re-identification models is to use ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as a backbone with the crossentropy loss function <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Given the small amount of the training data it tends to overfit and provides poor discriminative qualities. To some extent it is alleviated by regularization techniques and/or various loss functions <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>, but there are more effective ways to do that.</p><p>In this work we use AM-Softmax <ref type="bibr" target="#b8">[9]</ref> loss within OSNet <ref type="bibr" target="#b25">[26]</ref> architecture to get discriminative features that allow us to obtain fast cross-domain networks that generalize well to unseen data.</p><p>In brief, the key contributions of this paper can be summa-arXiv:2003.07618v2 [cs.CV] 7 Jul 2020 rized as follows:</p><p>• We apply the metric-learning approach (AM-Softmax <ref type="bibr" target="#b8">[9]</ref> loss) to further improve generalization of re-identification models in the cross-domain setup. • We explore some additional training tricks and modifications of the OSNet to make lightweight models that are strong enough in the cross-domain setup and require significantly less computations than the top-performing ones. • Our combination of adjustments to OSNet obtains SOTA results in three cross domain setups MSMT17-all→DukeMTMC, MSMT17-train→Market1501, MSMT17-all→Market1501 among approaches that do not use data from the target domains in training. • In order to see which models are suitable for realtime applications, we evaluate the performance of the developed models on a desktop CPU.</p><p>II. RELATED WORK a) Efficient CNN architectures built for person reidentification: There is a wide variety of approaches to person re-identification, but to our best knowledge, only a few of them aim to develop a CNN architecture specifically designed for the re-id task from scratch <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. RMNet proposed in <ref type="bibr" target="#b26">[27]</ref> is a lightweight and computationally efficient architecture aimed to work on low-power devices. It is based on the ResNet <ref type="bibr" target="#b13">[14]</ref> paradigm mixed with other techniques for building efficient CNNs <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. This network trained by a carefully designed procedure reaches similar results to the relatively heavy ResNet-50-based alternatives like MGN <ref type="bibr" target="#b15">[16]</ref> and HPM <ref type="bibr" target="#b14">[15]</ref>.</p><p>The next generation of efficient architectures for person reidentification is based on Res2Net <ref type="bibr" target="#b29">[30]</ref> multi-scale paradigm and presented in <ref type="bibr" target="#b25">[26]</ref>. It also employs insights from the lightweight architectures <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref> and SE blocks <ref type="bibr" target="#b31">[32]</ref> to build efficient multi-scale residual blocks.</p><p>Later, OSNet has been adapted for the cross-domain scenario <ref type="bibr" target="#b32">[33]</ref> by incorporating Instance normalization <ref type="bibr" target="#b33">[34]</ref> (IN) layers. The optimal placement of INs is found by the NAS <ref type="bibr" target="#b34">[35]</ref> technique. The resulting OSNet-AIN shows great crossdomain generalization outperforming some of the recently proposed label-free methods that use target data <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. b) Loss functions for person re-identification: The person re-identification task aims to find a mapping function that translates the image domain into the high-dimensional vector space f : I → R N such that images of the same person captured under different conditions become close in the vector space. At the same time, vectors corresponding to different persons should be far from each other. The most popular way to address this problem is to treat it as multi-class classification and solve using the cross-entropy (CE) loss <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Later, it has been explored that CE favors separability of features rather than discriminative properties <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. To overcome this drawback, several margin-based variations of the CE loss were introduced <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Despite advantages of the marginbased softmax losses, they are not widespread yet in the person re-identification field and only several works use them for single domain training <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p><p>Alternative approach to learn a discriminative mapping is to use the triplet loss <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b41">[42]</ref>. This approach requires carefully designed hard mining strategies due to a vast amount of possible triplets. Often the triplet loss is added as a complementary one to cross entropy <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref>. c) Cross domain re-identification: After emergence of several large-scale person re-identification benchmarks <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref> the problem of domain shift between them has become clearly visible. Domain adaptation, unsupervised and cross-domain methods have been designed to handle this problem.</p><p>Domain adaptation approach assumes availability of labels on source domain and unlabeled data from the target one. The training is performed on a mix of labeled and unlabeled data. And the evaluation is performed on the target domain (if test data from the target domain exists). Former domain adaptation methods utilized GANs to generate new data with the distribution similar to the target domain in offline or online mode <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Latter approaches try to adapt to source domain via solving auxiliary tasks on the target data <ref type="bibr" target="#b45">[46]</ref> or to use unsupervised methods <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref> that don't require labels at all.</p><p>Although fully unsupervised methods are attractive, there is a performance gap between them and supervised approaches <ref type="bibr" target="#b49">[50]</ref>. So, from the practical perspective, only supervised crossdomain methods are suitable for real life applications at this moment. Besides, once trained, well-generalizing model can be deployed without any retraining.</p><p>Cross-domain generalization is addressed by adjusting the architecture of CNNs <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b50">[51]</ref>, designing task-specific loss functions <ref type="bibr" target="#b9">[10]</ref> or adversarial training <ref type="bibr" target="#b10">[11]</ref>. Improvements in each of this fields can be transferred to unsupervised or semi-supervised methods since many of them use parts of supervised training procedures under the hood. Our work pays attention to the loss function, model architecture and details of the training process (augmentation, schedules, data sampling) to build better cross domain models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD A. Loss function for deep metric learning</head><p>To learn the mentioned identity-preserving mapping function f : I → R N there are currently two main approaches: to use an identity classification loss <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> acting as a global rule or use local rules such as triplet loss <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Both approaches are sub-optimal and currently the best results are obtained by its combination <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref>. The purpose of the triplet loss in the mentioned papers is to fix a poor ability of the CE to learn a well-generalizing mapping. CE loss can perform well in the single-domain setup, because training and testing subsets of public datasets <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref> are very similar (they contain randomly sampled identities from the same scenes). But in cross domain setup we should have a stronger and more structured supervision to control the properties of f . The family of angular margin-based losses  <ref type="bibr" target="#b51">[52]</ref>, AM-Softmax <ref type="bibr" target="#b8">[9]</ref>, ArcFace <ref type="bibr" target="#b7">[8]</ref>) allows us to achieve that. We use AM-Softmax since it provides the required properties and is easier to optimize than SphereFace or ArcFace even on noisy data. AM-Softmax is defined by the following formula:</p><formula xml:id="formula_0">L ASM = − i log p i ,<label>(1)</label></formula><formula xml:id="formula_1">p i = e s(W T y i fi−m) e s(W T y i fi−m) + j,j =yi e sW T y j fj ,<label>(2)</label></formula><p>where f i ∈ R N are l 2 -normalized outputs of the mapping f , W ∈ R N ×M are l 2 -normalized weights of the linear layer transforming f i to the space of logits, m and s are a margin between classes and a scale of features. Non-zero margin forces the loss not only to make vectors f i closer to their prototypes W T yi in terms of the cosine distance, but also to create margins between different classes. That process makes the mapping f discriminative. The scale parameter s controls the degree of similarity between f i and W T yi required to generate sharp distribution p i . High value of s corresponds to the case, when the similarity between f i and W T yi should be just slightly greater, than between f i and W T j , j = y i to get close to one-hot distribution p i .</p><p>Since AM-Softmax loss defines a strong global placement rule for vectors f on a high-dimensional hypersphere, it needs to be slightly relaxed to prevent overfitting. Authors of <ref type="bibr" target="#b52">[53]</ref> evaluate several approaches to decrease the sharpness of the distribution p i (2) on easy samples and simple subtraction of entropy from the softmax loss gives them best results. Taking that into account, we will define the AM-Softmaxbased identity loss as:</p><formula xml:id="formula_2">L id = [L ASM + α i p i log p i ] + ,<label>(3)</label></formula><p>where p i are from (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model architecture</head><p>As stated earlier, we use OSNet as a baseline CNN architecture for the person re-identification task. Like Res2Net, it provides a multi-scale residual block, but at the same time, OSBlock is lightweight. Compared to the standard residual block, OSBlock has larger theoretical receptive field. Hence, it can provide aggregation and processing of global context starting from shallow layers. It seems to be the major contributing factor that allows OSNet to outperform many ResNet-50 based approaches <ref type="bibr" target="#b25">[26]</ref> without any sophisticated training tricks, requiring about 2.0 billions of the floating point operations while ResNet-50 requires 5.3 billions for the same input resolution 256 × 128.</p><p>OSNet already has a well-balanced architecture, so there are only minor adjustments from our side:</p><p>• By default, OSNet uses a global average pooling operation to aggregate spatial features into a vector. We replace it with the global depthwise convolution <ref type="bibr" target="#b53">[54]</ref>. It allows us to make aggregation of the final feature map more flexible since each channel and each spatial position has a learnable weight instead of uniform weights in case of the average pooling. Global depthwise convolution also slightly increases the capacity of the lightweight models without introducing noticeable overhead. • It has been explored that InstanceNorms can boost the performance in cross-domain re-identification <ref type="bibr" target="#b50">[51]</ref>. Following this practice, we insert InstanceNorms before the first convolution and after it (instead of BatchNorms) at least to decrease the color distribution shift. AM-Softmax loss treats the normalized representation vectors f i as points on the hypersphere ||f || l2 = 1. Such points can have both positive and negative-signed coordinates. To let the model produce the output vector with negative components too, we add PReLU activation layer instead of ReLU in the original OSNet. Also we use 256-dimensional output layer, while the original OSNet comes with a 512-dimensional one.  <ref type="figure" target="#fig_1">Figure 1</ref>), that gives a lot of false matches containing the orange car. AM-Softmax-based model gives near-perfect Top-10 in the first two cases, but fails in the third one. All the images are taken from DukeMTMC-ReID, the models are trained on MSMS17-all AM-Softmax generates more structured representation than Softmax, so we can learn a compact embedding space. As a bonus, low-dimensional embeddings lead to faster distance computation, averaging and any other operations performed on the extracted embeddings.</p><p>We refer to the modified OSNet architecture as OSNet-IAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>a) Datasets and cross-domain training issues: For training and evaluation we use 3 largest publicly available datasets: MSMT17 <ref type="bibr" target="#b11">[12]</ref>, Market1501 <ref type="bibr" target="#b22">[23]</ref> and DukeMTMC-ReID <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Their statistics are shown in the <ref type="table" target="#tab_0">Table I</ref>. Among them, MSMT17 has drastically more identities and images. We are focused on large scale data, thus we use MSMT17-train as a core data subset for all of our experiments and add MSMT17test, Market1501 or DukeMTMC-ReID depending on setup.</p><p>In single domain setup, better performance on training data, generally, means better performance on the test (given that we avoid overfitting). For the cross-domain scenario that's not always true. For instance, if the majority of identities presented in the training split wear shorts, we shouldn't expect great  performance on a test split where most people wear trousers, since we have a drastic appearance bias in these two datasets.</p><p>In case of large-scale data this problem is not so severe, but the model can still capture less obvious dataset-specific details   <ref type="bibr">48.66</ref> or noise. In the cross-domain setup we have to make models slightly underfitted to provide higher performance on unseen data. b) Training strategy: To avoid overfitting we use intensive data augmentations with a long training schedule. We train all models with AMSGrad optimizer <ref type="bibr" target="#b55">[56]</ref> for 65 epochs. The initial learning rate is 0.0015. It's dropped by a factor of 10 twice: at the epochs 40 and 50. Values of the scale s = 0.3 and margin m = 0.35 are taken from the original paper <ref type="bibr" target="#b8">[9]</ref> without changes. Regularization parameter α from <ref type="formula" target="#formula_2">(3)</ref> is set to 0.3, although, the model seems to be insensitive to this parameter in the range α ∈ [0.1, 0.5]. Batch size is 64. Each batch is sampled from 16 randomly chosen identities, totally we have 4 images per identity in a batch. If one or several of the sampled identities have less than 4 images, additional identities are randomly picked to complete the batch. We found this sampling strategy to be effective in combination with the AM-Softmax loss. It allows us to partially alleviate imbalance in the amount of images per identity (see <ref type="figure" target="#fig_4">Figure 3</ref>). Initial weights are taken from the original OSNet pre-trained on the ImageNet <ref type="bibr" target="#b56">[57]</ref>. Also we use a kind of warm-up proposed in <ref type="bibr" target="#b25">[26]</ref>: for the first 5 epochs the base network pre-trained on ImageNet is frozen and only the randomly initialized classifier and depthwise global convolution layers are trained. Data augmentation includes random color transformations (jittering in the HSV space, conversion to grayscale), spatial deformations (random rotation, horizontal flip, padding) and image distortion (random erase <ref type="bibr" target="#b57">[58]</ref>, drawing of random figures and grids). In case of training on MSMT17-train set we observed overfitting. To handle that we add Gaussian Continuous Dropout layers <ref type="bibr" target="#b58">[59]</ref> with µ = 0.1, σ = 0.03 after the convolutional branch of the each OSBlock. We don't use this regularization when training on larger sets.</p><p>Our training code is based on the Torchreid library <ref type="bibr" target="#b59">[60]</ref> and available as a part of the OpenVINO R Training Extensions toolkit 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results in cross-domain person re-id</head><p>We evaluate the scalability of OSNet-IAP in data and model size dimensions (see <ref type="table" target="#tab_0">Table II</ref>). Following the original OSNet, we use multiplier β ∈ {0.25, 0.5, 0.75, 1.0} to control the amount of channels. OSNet-IAP reacts smoothly to varying β. The most dramatic performance drop occurs between β = 0.5 and β = 0.25. Evaluation subset of MSMT17 is 3x larger than the training one. Combining train and test subsets of MSMT17 gives OSNet-IAP additional 6% to rank-1 on Market1501 and almost 11% on DukeMTMC-ReID. OSNet-IAP 0.25x performs at the same level as OSNet-IAP 1.0x trained with 4x fewer amount of the data. Combining MSMT17-all with DukeMTMC-ReID brings a significant improvement on Mar-ket1501 as well as combining MSMT17-all and Market1501 gives a clear boost on DukeMTMC-ReID. We've also collected a private person re-identification dataset containing 216284 images of 7187 identities. The dataset is built from single-camera scenes and thus it is not so as representative as MSMT17. But, still, when we combine this data with MSMT17 we observe a clear boost on both DukeMTMC-ReID and Market1501 domains. This indicates that OSNet-IAP can further benefit from a more diverse dataset and we believe that increasing the amount of public carefully designed large-scale datasets will push the limits of cross-domain generalization in person re-identification task to a new level. a) Comparison with the state-of-the-art: Recently, crossdomain methods demonstrated an impressive progress in zeroshot transfer. ADFL <ref type="bibr" target="#b54">[55]</ref> incorporates attention mechanism into ResNet-50 and shows great results on Market1501 and DukeMTMC-ReID using only MSMT17-train as the source data. Even still, OSNet-IAP 1.0x slightly outperforms it in MSMT17-train→Market1501 setup (see <ref type="table" target="#tab_0">Table III</ref>). SOTAlevel single domain model ABD-Net <ref type="bibr" target="#b16">[17]</ref> demonstrates worse performance than any of the considered cross-domain methods, although using the same training data.</p><p>With larger training set, OSNet-IAP 1.0x significantly improves performance in both MSMT17-all→Market1501, MSMT17-all→DukeMTMC-ReID setups outperforming the original cross-domain variations of OSNet <ref type="figure">(IBN and AIN)</ref>. At the same time, OSNet-IAP surpasses unsupervised domain adaptation approaches that use unlabeled target data and MSMT17 with labels as an auxiliary dataset (for pre-training or assigning soft labels, etc.). Cross-domain model should perform equally well in all domains including the target one. SOTA-level cross-domain models demonstrate good performance on the target data, although their results are significantly lower than the highest score in the same domain setup (see <ref type="table" target="#tab_0">Table IV</ref>). OSNet-IAP 1.0x shows better performance on target (MSMT17) domain than on unseen ones (Market1501 and DukeMTMC-ReID, see <ref type="table" target="#tab_0">Table III</ref>), yielding the first place to ADFL. Thus, OSNet-IAP outperforms ADFL in MSMT17-train→Market1501, but yields in MSMT17-train→DukeMTMC-ReID and MSMT17-train→MSMT17-test setups. Considering all of the above, OSNet-IAP tends to be underfitted on MSMT17-train (this leads to worse score on DukeMTMC-ReID, since it's somewhat similar to MSMT17) and more biased towards Mar-ket1501 than ADFL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation study</head><p>In this section we'll evaluate the contribution of each of the training tricks and architecture adjustments to the final cross-domain score of OSNet-IAP. We take OSNet 1x with Softmax without augmentations as a baseline, MSMT17-all as a source domain, Market1501 and DukeMTMC-ReID as target domains. Results are shown in <ref type="table" target="#tab_3">Table V</ref>. Each component slightly increases the overall metric, but the most noticeable improvement is due to the AM-Softmax loss, basic augmentations (flip, color jitter) and sampling strategy. Color augmentation is a crucial step in the cross-domain training pipeline, since it allows us to mitigate the gap between color distributions of different domains. Instance Normalization of input data at shallow layers also helps in that. AM-Softmaxbased model noticeably benefits from the sampling of uniform number of images per identity in each batch. This technique allows to handle differences in the number of samples per identity in the training dataset: all identities have equal probability to appear in a batch instead of having a correlation with the number of samples representing an identity.</p><p>To prove the actual discriminative ability of AM-Softmaxguided features, we randomly picked sets of 200 wellrepresented identities that have 20 or more images from several datasets. Then we extracted normalized embeddings using OSNet-IAP 1.0x trained on MSMT17-train with and without AM-Softmax, computed centroids for each identity and estimated average pairwise cosine distance between the obtained centroids. Results of this experiment are presented in the <ref type="table" target="#tab_0">Table VI</ref>. AM-Softmax generates wide distance margins between centroids, corresponding to different identities, even on unseen domains (Market1501 and DukeMTMC-ReID). On the contrary, softmax-based model creates narrow distance margins between centroids, even on the data from the source domain (MSMT17-test). For all the evaluated models the average inter-centroid distance on the source domain is greater than on unseen ones. Thus, the average pairwise distance between centroids could be considered as a similarity measure between domains. Model fitted to the MSMT17 distribution splits different identities from DukeMTMC-ReID better than ones from Market1501. This fact proves that MSMT17 is more similar with DukeMTMC-ReID than with Market1501. Training with AM-Sofrmax also leads to better discrimination between background and foreground without introducing any explicit attention mechanism (see quantitative analysis on the <ref type="figure" target="#fig_1">Figure 1 and Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance evaluation on CPU</head><p>Performance is a crucial point for practical re-identification models. Modern multi-camera multi-target person tracking (MCMT) approaches use tracking-by-detection and hierarchical feature clustering paradigms that compute re-identification features for each detected person in multiple video streams <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Fast and accurate re-id model in combination with real-time detector would allow to achieve close to real-time performance in MCMT. In this work we provide a number of models demonstrating a trade-off between cross-domain re-identification accuracy and performance on CPU. <ref type="table" target="#tab_0">Table  VII</ref> shows the performance numbers. Any approach based on ResNet-50 is slower than OSNet-IAP 1.0x by a factor of two. Our top-performing model OSNet-IAP 1.0x is slightly slower than the original OSNet 1.0x because of the adaptive pooling and Instance Normalization. The inference of our lightest model OSNet-IAP 0.25x takes only 1ms. That allows processing about 30 persons in real time while outperforming much heavier approaches OSNetx1.0-IBN <ref type="bibr" target="#b25">[26]</ref> and MAR <ref type="bibr" target="#b35">[36]</ref> on Market1501 (see <ref type="table" target="#tab_0">Table III</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work we showcased the effectiveness of the metriclearning approach in the cross domain person re-identification task. We proposed to use the margin-based loss to extract discriminative features and showed that this approach substantially improves cross-domain generalization. Also, we utilized a number of training techniques and architecture adjustments to further boost our results to the state-of-the-art level while maintaining compact size of our model. Extensive experiments validated the effectiveness of the proposed method as well as each training component individually. We proved the realtime performance of our approach and compared it against widespread ResNet-50-based solution by benchmarking on CPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 https://github.com/opencv/openvino training extensions/tree/develop/ pytorch toolkit/object reidentification/person reidentification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Activation maps of OSNet-IAP 1.0x trained with the Softmax loss (b) Activation maps of OSNet-IAP 1.0x trained with the AM-Softmax loss The difference between activations at the last feature map of the models trained with Softmax and AM-Softmax. The model with Softmax pays attention to the car in all the presented cases while AM-Softmax-based model fails to discriminate car and person only in one case (on the third map from the left). All the images are taken from DukeMTMC-ReID, models are trained on MSMS17-all (SphereFace</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Top10 images retrieved by OSNet-IAP 1.0x trained with the Softmax loss (b) Top10 images retrieved by OSNet-IAP 1.0x trained with the AM-Softmax loss Retrieving results from the models trained with Softmax and AM-Softmax. Query images are on the left side. Incorrect matches are enclosed by red rectangles. The model trained with Softmax generates corrupted activations on all three query images (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Distribution of the amount of images per identity in MSMT17 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF LARGE-SCALE PERSON RE-IDENTIFICATION DATASETS Dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="7">CROSS-DOMAIN RESULTS OBTAINED BY OSNET-IAP IN DIFFERENT SETUPS</cell></row><row><cell>Model, train data</cell><cell></cell><cell cols="2">MSMT17-test</cell><cell cols="2">Market1501</cell><cell cols="2">DukeMTMC-ReID</cell></row><row><cell></cell><cell></cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>OSNet-IAP 1.0x, MSMT17-train</cell><cell></cell><cell>77.97</cell><cell>48.66</cell><cell>69.27</cell><cell>40.27</cell><cell>64.00</cell><cell>42.16</cell></row><row><cell>OSNet-IAP 0.25x, MSMT17-all</cell><cell></cell><cell>-</cell><cell>-</cell><cell>72.09</cell><cell>42.14</cell><cell>62.97</cell><cell>41.86</cell></row><row><cell>OSNet-IAP 0.5x, MSMT17-all</cell><cell></cell><cell>-</cell><cell>-</cell><cell>77.49</cell><cell>48.98</cell><cell>70.92</cell><cell>50.87</cell></row><row><cell>OSNet-IAP 0.75x, MSMT17-all</cell><cell></cell><cell>-</cell><cell>-</cell><cell>79.78</cell><cell>52.49</cell><cell>73.83</cell><cell>53.85</cell></row><row><cell>OSNet-IAP 1.0x, MSMT17-all</cell><cell></cell><cell>-</cell><cell>-</cell><cell>82.66</cell><cell>55.70</cell><cell>74.91</cell><cell>56.6</cell></row><row><cell cols="2">OSNet-IAP 1.0x, MSMT17-all + DukeMTMC-ReID-all</cell><cell>-</cell><cell>-</cell><cell>83.52</cell><cell>58.02</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">OSNet-IAP 1.0x, MSMT17-all + Market1501-all</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.66</cell><cell>58.93</cell></row><row><cell cols="2">OSNet-IAP 1.0x, MSMT17-all + Private data</cell><cell>-</cell><cell>-</cell><cell>85.87</cell><cell>58.58</cell><cell>77.06</cell><cell>58.59</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMPARISON WITH THE CURRENT STATE-OF-THE-ART METHODS IN THE CROSS-DOMAIN RE-IDENTIFICATION</cell></row><row><cell>Method</cell><cell>Train data</cell><cell></cell><cell></cell><cell cols="2">Market1501</cell><cell cols="2">DukeMTMC-ReID</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>ABD-Net [17] (single domain) a</cell><cell>MSMT17-train</cell><cell></cell><cell></cell><cell>50.15</cell><cell>25.95</cell><cell>48.7</cell><cell>29.9</cell></row><row><cell>CE-FAT [10]</cell><cell>MSMT17-train</cell><cell></cell><cell></cell><cell>52.80</cell><cell>25.40</cell><cell>50.90</cell><cell>31.30</cell></row><row><cell>ADIN [11]</cell><cell>MSMT17-train</cell><cell></cell><cell></cell><cell>59.10</cell><cell>30.30</cell><cell>60.70</cell><cell>39.10</cell></row><row><cell>CDB [6]</cell><cell>MSMT17-train</cell><cell></cell><cell></cell><cell>64.80</cell><cell>36.60</cell><cell>64.50</cell><cell>43.3</cell></row><row><cell>ADFL [55]</cell><cell>MSMT17-train</cell><cell></cell><cell></cell><cell>68.00</cell><cell>37.70</cell><cell>66.30</cell><cell>46.20</cell></row><row><cell>OSNet-IAP 1.0x (Ours)</cell><cell>MSMT17-train</cell><cell></cell><cell></cell><cell>69.27</cell><cell>40.27</cell><cell>64.00</cell><cell>42.16</cell></row><row><cell>OSNetx1.0-IBN [26]</cell><cell>MSMT17-all</cell><cell></cell><cell></cell><cell>66.50</cell><cell>37.20</cell><cell>67.40</cell><cell>45.60</cell></row><row><cell>MAR [36]</cell><cell cols="3">MSMT17-all+Market(U)/Duke(U)</cell><cell>67.70</cell><cell>40.00</cell><cell>67.10</cell><cell>48.00</cell></row><row><cell>OSNetx1.0-AIN [33]</cell><cell>MSMT17-all</cell><cell></cell><cell></cell><cell>71.10</cell><cell>52.70</cell><cell>70.10</cell><cell>43.30</cell></row><row><cell>PAUL [37]</cell><cell cols="3">MSMT17-all+Market(U)/Duke(U)</cell><cell>68.50</cell><cell>40.10</cell><cell>72.00</cell><cell>53.20</cell></row><row><cell>OSNet-IAP 0.75x (Ours)</cell><cell>MSMT17-all</cell><cell></cell><cell></cell><cell>79.78</cell><cell>52.49</cell><cell>73.83</cell><cell>53.85</cell></row><row><cell>OSNet-IAP 1.0x (Ours)</cell><cell>MSMT17-all</cell><cell></cell><cell></cell><cell>82.66</cell><cell>55.70</cell><cell>74.91</cell><cell>56.60</cell></row></table><note>a Measured by us using the official model and evaluation script from https://github.com/TAMU-VITA/ABD-Net</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="3">SAME DOMAIN PERFORMANCE OF CROSS-DOMAIN METHODS ON</cell></row><row><cell>MSMT17</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>ABD-Net [17] (single domain)</cell><cell>82.30</cell><cell>60.80</cell></row><row><cell>CE-FAT [10]</cell><cell>69.40</cell><cell>39.20</cell></row><row><cell>ADFL [55]</cell><cell>78.20</cell><cell>48.80</cell></row><row><cell>OSNet-IAP 1.0x (Ours)</cell><cell>77.97</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V ABLATION</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">STUDY ON MSMT17-ALL DATASET</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell cols="2">Market1501</cell><cell>DukeMTMC-ReID</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell></cell><cell cols="2">OSNet 1x with Softmax</cell><cell>67.04</cell><cell>38.65</cell><cell>65.53</cell><cell>46.78</cell></row><row><cell></cell><cell cols="2">+ Random flip &amp; Color jitter</cell><cell>71.38</cell><cell>37.07</cell><cell>67.10</cell><cell>46.88</cell></row><row><cell></cell><cell cols="2">+ AM-Softmax</cell><cell>74.70</cell><cell>46.47</cell><cell>69.52</cell><cell>49.60</cell></row><row><cell></cell><cell cols="2">+ Depthwise pooling</cell><cell>75.18</cell><cell>46.55</cell><cell>70.38</cell><cell>50.16</cell></row><row><cell></cell><cell cols="2">+ Instance Norms</cell><cell>75.53</cell><cell>46.96</cell><cell>70.92</cell><cell>50.91</cell></row><row><cell></cell><cell cols="2">+ Uniform identity sampling</cell><cell>81.44</cell><cell>53.93</cell><cell>73.38</cell><cell>55.06</cell></row><row><cell></cell><cell cols="2">+ Advanced augmentations</cell><cell>82.66</cell><cell>55.70</cell><cell>74.91</cell><cell>56.60</cell></row><row><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">AVERAGE PAIRWISE COSINE DISTANCE BETWEEN CENTROIDS</cell><cell></cell><cell></cell></row><row><cell cols="3">CORRESPONDING TO 200 RANDOMLY PICKED IDENTITIES FROM</cell><cell></cell><cell></cell></row><row><cell cols="3">DIFFERENT DATASETS. MODELS ARE TRAINED ON MSMT17-TRAIN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>OSNet-IAP 1.0x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>w/o AM-Softmax</cell><cell>OSNet-IAP 1.0x</cell><cell></cell><cell></cell></row><row><cell></cell><cell>+ Softmax</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Market-1501</cell><cell>0.35</cell><cell>0.71</cell><cell></cell><cell></cell></row><row><cell>DukeMTMC-ReID</cell><cell>0.36</cell><cell>0.81</cell><cell></cell><cell></cell></row><row><cell>MSMT17-test</cell><cell>0.41</cell><cell>0.91</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>ON THE INTEL R CORE TM I7-6700K 4.00GHZ CPU IN OPENVINO TM R3 2019 TOOLKIT. BATCH SIZE IS SET TO 1, INPUT RESOLUTION IS 256 × 128, INFERENCE PRECISION IS FP32</figDesc><table><row><cell>Model</cell><cell>GFLOPs</cell><cell>Parameters, M</cell><cell>FPS</cell><cell>Latency, ms</cell></row><row><cell>ResNet-50</cell><cell>5.30</cell><cell>23.50</cell><cell>74.51</cell><cell>13.42</cell></row><row><cell>OSNet 1.0x</cell><cell>1.99</cell><cell>2.05</cell><cell>162.90</cell><cell>6.13</cell></row><row><cell>OSNet-IAP 1.0x</cell><cell>1.99</cell><cell>2.12</cell><cell>157.64</cell><cell>6.34</cell></row><row><cell>OSNet-IAP 0.75x</cell><cell>1.17</cell><cell>1.24</cell><cell>250.65</cell><cell>3.99</cell></row><row><cell>OSNet-IAP 0.5x</cell><cell>0.56</cell><cell>0.60</cell><cell>441.64</cell><cell>1.26</cell></row><row><cell>OSNet-IAP 0.25x</cell><cell>0.17</cell><cell>0.18</cell><cell>911.93</cell><cell>1.09</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/opencv/openvino training extensions/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fairest of them all: Establishing a strong baseline for cross-domain person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marchwica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="2502" to="2511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In defense of the triplet loss again: Learning robust person re-identification with fast approximated triplet loss and label distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Calibrated domain-invariant learning for highly generalizable large scale re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3240508.3240552</idno>
		<ptr target="https://doi.org/10.1145/3240508.3240552" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
	<note>ser. ICML 16. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="499" to="515" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-camera activity correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>Computer Vision -ACCV 2012, K. M. Lee, Y. Matsushita, J. M. Rehg, and Z. Hu</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="31" to="44" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3774" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast and accurate person re-identification with rmnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izutov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and textless1mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning generalisable omni-scale representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06827</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2143" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3628" to="3637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Person reidentification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spherereid: Deep hypersphere manifold embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fei</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1047320319300100" />
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spectral feature transformation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1811.11405</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Eanet: Enhancing alignment for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno>abs/1812.11369</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Selfsimilarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1811.10144</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Frustratingly easy person re-identification: Generalizing person re-id in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno>abs/1905.03422</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking person re-identification with confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adaimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CCBR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention: A big surprise for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno>abs/1904.09237</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Continuous dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3926" to="3937" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Torchreid: A library for deep learning person re-identification in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10093</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multi-target, multi-camera tracking by hierarchical clustering: Recent progress on dukemtmc project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">State-aware reidentification feature for multi-target multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
