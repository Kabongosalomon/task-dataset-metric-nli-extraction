<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Knowledge Propagation for Image-to-Video Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
							<email>xinqian.gu@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
							<email>bpma@ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Knowledge Propagation for Image-to-Video Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many scenarios of Person Re-identification (Re-ID), the gallery set consists of lots of surveillance videos and the query is just an image, thus Re-ID has to be conducted between image and videos. Compared with videos, still person images lack temporal information. Besides, the information asymmetry between image and video features increases the difficulty in matching images and videos. To solve this problem, we propose a novel Temporal Knowledge Propagation (TKP) method which propagates the temporal knowledge learned by the video representation network to the image representation network. Specifically, given the input videos, we enforce the image representation network to fit the outputs of video representation network in a shared feature space. With back propagation, temporal knowledge can be transferred to enhance the image features and the information asymmetry problem can be alleviated. With additional classification and integrated triplet losses, our model can learn expressive and discriminative image and video features for image-to-video re-identification. Extensive experiments demonstrate the effectiveness of our method and the overall results on two widely used datasets surpass the state-of-the-art methods by a large margin. Code is available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-identificaiton (Re-ID) aims to find the sample among the gallery set which has the same identity with a given query. In general, Re-ID problems fall into two categories: image-based Re-ID <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref> and video-based Re-ID <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>. The main difference is that the query and gallery in image-based Re-ID are both images, while the query and gallery in video-based Re-ID are both videos.</p><p>However, in many real-world scenarios, the gallery set is usually constituted by lots of surveillance videos, while the query only consists of one image, thus Re-ID has to be conducted between image and videos. One instance is rapidly locating and tracking a criminal suspect among a mass of surveillance videos according to one photo of the suspect (e.g., Boston marathon bombings event). Due to its crucial role in video surveillance system, image-to-video (I2V) Re-ID <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> has attracted increasing attention in recent years.</p><p>In I2V Re-ID, the query is a still image, while the gallery of videos contains additional temporal information. Some research <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44]</ref> indicates that modeling temporal relations between video frames makes the gallery video features robust to disturbing conditions. However, these methods ignore that the query of I2V Re-ID only consists of one still image and lacks temporal information. As a result, on one hand, the image feature cannot benefit from the advantages of modeling temporal relations (see <ref type="figure" target="#fig_0">Figure 1</ref>). On the other hand, the information asymmetry between image and video features increases the difficulty in measuring the image-tovideo similarity. Hence, it is essential and desirable to de-velop a method to supplement temporal information to image feature representation.</p><p>In this paper, we propose a novel Temporal Knowledge Propagation (TKP) method to address the problems of ignored image temporal representation and information asymmetry in I2V Re-ID. This is inspired by knowledge distillation <ref type="bibr" target="#b10">[11]</ref>, which transfers dark knowledge from a large and powerful teacher network to a smaller and faster student network. In our TKP method, the temporal knowledge learned by video representation network is propagated to image representation network. During training, given the same input videos, we enforce the frame features extracted by the image representation network to match the outputs of video representation network in a shared feature space. After training with back propagation, the temporal knowledge can be naturally transferred from video representation network to image representation network. In the test stage, we use the trained image representation network to extract the query image features. Thanks to the transferred temporal knowledge, the extracted image features manifest robustness to disturbing conditions just like the video frame features (see <ref type="figure" target="#fig_0">Figure 1</ref>). Meanwhile, the information asymmetry problem between image and video features is addressed, thus it is much easier to measure the similarity between images and videos.</p><p>Extensive experiments validate the effectiveness of the proposed method. For instance, on MARS dataset, our method improves the performance from 67.1% to 75.6% (+8.5%) w.r.t. top-1 accuracy, surpassing the state-of-theart methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>I2V Re-ID. Recently, several relevant methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> are proposed for I2V Re-ID task. Among them, Zhu et al. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> firstly investigate this problem and propose a heterogeneous dictionary pair learning framework to map image features and video features to a shared feature space. Wang et al. <ref type="bibr" target="#b30">[31]</ref> try to use deep learning based methods to solve this problem and Zhang et al. <ref type="bibr" target="#b36">[37]</ref> use LSTM to model the temporal information of gallery videos to enhance the robustness of the video features. However, these methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b36">37]</ref> neglect that there is no temporal information in query image features. In contrast, our proposed TKP method transfers temporal knowledge learned by video representation network to image representation network, which can effectively reduce the information asymmetry between image and video features.</p><p>Modeling Temporal Relations. Dealing with temporal relations between video frames is of central importance in video feature extraction. A natural solution is to apply Recurrent Neural Networks (RNN) to model sequences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. There are also some methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref> that use 3D convolution (C3D) to process temporal neighborhoods. However, both RNN and C3D only process one local neighborhood at a time. Recently, Wang et al. <ref type="bibr" target="#b32">[33]</ref> propose to use non-local operation to capture long-range dependencies, which achieves higher results on video classification task. In this paper, we also attempt to utilize non-local operation to model temporal relations in person videos.</p><p>Knowledge Distillation. Knowledge distillation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref> is a widely used technique to transfer knowledge from a teacher network to a student network. Generally, it is used to transfer from a powerful and large network to a faster and small network. In contrast, our TKP method is the first time to transfer temporal knowledge from video representation network to image representation network. Besides, instead of using a well-trained teacher, our image-to-video representation learning and temporal knowledge transferring are trained simultaneously.</p><p>As for the distillation forms, Hinton et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref> minimize the Kullback-Leibler divergence of the final classification probability of teacher network and student network to transfer knowledge. In contrast, Bengio et al. <ref type="bibr" target="#b24">[25]</ref> directly minimize the Mean Square Error (MSE) of the middle outputs of these two networks. For deep metric learning tasks, Chen et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref> transfer knowledge via cross sample similarities. In this paper, we transfer temporal knowledge by minimizing the MSE of the image features and the corresponding video frame features in a shared feature space, which is similar to <ref type="bibr" target="#b24">[25]</ref> in loss design but different in models. Besides, we also formulate the TKP loss based on cross sample distances in the shared feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>In this section, we first introduce the overall network architecture for the proposed TKP method. Secondly, the details of image representation network and video representation network are illustrated. Then, our TKP method is presented, followed by the final objective function and sampling strategy. Finally, these two learned networks are used to perform I2V Re-ID testing.</p><p>The framework for our proposed TKP method in I2V Re-ID training is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. Given input video clips, the image representation network extracts visual information of single-frame images, while the video representation network extracts visual information and deals with temporal relations between video frames simultaneously. Temporal knowledge propagation from the video representation network to image representation network is formulated by constructing the TKP loss. By minimizing the TKP loss, together with the classification and triplet losses, the image features and video features are mapped to a shared feature space. Details of the proposed method are given as follows.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Representation Network</head><p>We use ResNet-50 <ref type="bibr" target="#b8">[9]</ref> without final fully-connected layer as image representation network for visual feature learning. To enrich the granularity of image features, we remove the last down-sampling operation of ResNet-50 following <ref type="bibr" target="#b29">[30]</ref>.</p><p>Given N person video clips V = {V n } N n=1 , each V n contains T frames V n = {F n1 , F n2 , ..., F nT } (Unless specified, we set T = 4). If we discard the temporal relations between video frames, these video clips V can be considered as a set of individual images {F nt } N,T n=1,t=1 . Hence, we can use the image representation network F img (·) to extract features of these images, for all n, t,</p><formula xml:id="formula_0">i nt = F img (F nt ),<label>(1)</label></formula><p>where i nt ∈ R D is the corresponding image feature of video frame F nt . As for ResNet-50, D is 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Representation Network</head><p>To model the visual and temporal information of video sequences simultaneously, we combine CNN with nonlocal neural network <ref type="bibr" target="#b32">[33]</ref> as video representation network. The non-local block computes the response at a position as a weighted sum of the features at all positions in the input feature map. It can naturally handle the temporal relations between video frames. <ref type="table" target="#tab_1">Table 1</ref> shows the model structure of our video representation network on the backbone of ResNet-50. Specifically, we add two non-local blocks to res 3 and three non-local blocks to res 4 and remove the last down-sampling operation in res 5 to enrich granularity. Given an input video clip</p><formula xml:id="formula_1">V n = {F n1 , F n2 , ..., F nT } with T frames, the video repre- sentation network F vid (·) is defined as: {f n1 , f n2 , ..., f nT } = F vid (F n1 , F n2 , ...F nT ),<label>(2)</label></formula><p>where f nt ∈ R D , t = 1, ..., T is the video frame feature of F nt . With temporal average pooling, multiple video frame features of a video clip can be integrated to a video feature v n ∈ R D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Knowledge Propagation</head><p>In general, the performance of Re-ID highly depends on the robustness of feature representation. It has been proved that modeling temporal relations between video frames makes the person appearance representations robust to large variations <ref type="bibr" target="#b34">[35]</ref>. However, image representation network takes in still images and cannot process temporal relations, thus the output image features cannot benefit from temporal knowledge. To solve this problem, we propose TKP method which enforces the outputs of image representation network to fit the robust outputs of video representation network in the shared feature space. By back propagation algorithm, the image representation network can learn temporal knowledge from the video frame features. Consequently, the features extracted by image representation network are assigned, though not directly, some video temporal information.</p><p>Specifically, given the input video clips V, we can use Eq. (1) and Eq. (2) to extract image features i nt and video frame features f nt for all n = 1, ..., N, t = 1, ..., T . Since F vid (·) extracts visual information and deals with temporal relations between video frames simultaneously, f nt not only contains the visual information of video frame F nt , but also involves temporal relations with other frames. In order to use video frame feature f nt to propagate the temporal knowledge to image representation network, we formulate the TKP method as an optimization problem from the following two ways.</p><p>Propagation via Features. The first way is enforcing image representation network to fit the robust video frame features in a shared feature space. In this case, TKP method can be formulated to minimize the MSE between the image features and the corresponding video frame features:</p><formula xml:id="formula_2">L F T KP = 1 N T N n=1 T t=1 i nt − f nt 2 2 ,<label>(3)</label></formula><p>where · 2 denotes l 2 distance. Eq. (3) can be considered as simplified moving least squares <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>, which is capable of reconstructing continuous functions from a set of labeled samples. Here, our target is to reconstruct the image representation function F img (·) from the video frame representations</p><formula xml:id="formula_3">(F nt , f nt ) N,T n=1,t=1</formula><p>. This formulation is similar to Fit-Nets <ref type="bibr" target="#b24">[25]</ref>, except that the outputs of teacher network and student network in <ref type="bibr" target="#b24">[25]</ref> are mapped to the same dimension via an extra convolutional regressor. In contrast, the outputs of the image and video representation networks in our framework have the same dimension and the network construction is similar, thus we do not need additional convolutional regressor.</p><p>Propagation via Cross Sample Distances. Another way to propagate temporal knowledge from video representations to image representations may resort to neural network embedding. The structure of the target embedding space is characterized by cross sample distances. For all video frame features {f nt } N,T n=1,t=1 , we compute the cross sample Euclidean distances matrix as D vid ∈ R N T ×N T . To estimate the image representation in the embedding space, we constrain that the cross image distances D img ∈ R N T ×N T are consistent with the cross video frame distances D vid . In this way, the temporal information, as well as the sample distribution, are propagated to the image representation network. The TKP loss is formulated as:</p><formula xml:id="formula_4">L D T KP = 1 N T D img − D vid 2 F ,<label>(4)</label></formula><p>where · F denotes Frobenius norm. This formulation is similar to multidimensional scaling <ref type="bibr" target="#b16">[17]</ref>, except that we use a deep network to model the embedding function F img (·) instead of directly computing the embedded features via eigen-decomposition. Eq. (3) and Eq. (4) transfer knowledge from different levels and are complementary to each other. The empirical comparison of the two ways is provided in Section 4.3.</p><p>Note that both image and video networks use ResNet-50 as backbone. The only difference is that video network add extra non-local blocks to model temporal information. Given the same inputs, TKP loss enforces these two networks to output similar features. Obviously, the weights of additional non-local blocks being 0 is the optimal solution of minimizing TKP loss. In that case, the non-local blocks can not capture any temporal information. So updating video network by TKP deteriorates modeling temporal knowledge. Unless specified, in our emperiments, L F T KP and L F T KP are not back-propagated through the video representation network during model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>Besides the TKP loss, additional identification losses are also needed to learn discriminative features for image-tovideo re-identification. In this paper, we utilize the widely used classification loss and integrated triplet loss. In fact, other identification losses are also applicable. Classification Loss. Considering person identities as category-level annotations, we build two shared weights classifiers to map the image features and video features to a shared identity space. The classifiers are implemented as a linear layer following by a softmax operation and the output channel is the number of identities of training set. The classification loss L C can be formulated as the cross entropy error between the predicted identities and the correct labels. Integrated Triplet Loss. We also use triplet loss with hard sample mining <ref type="bibr" target="#b9">[10]</ref> to constrain the relative sample distances in the shared feature space. Specifically, we integrate four kinds of triplet losses, image-to-video (I2V), video-to-image (V2I), image-to-image (I2I) and video-tovideo (V2V) triplet losses. The final triplet loss L T is defined as:</p><formula xml:id="formula_5">L T = L I2V + L V 2I + L I2I + L V 2V ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">L I2V = m + max vp∈S + a d(i a , v p ) − min vn∈S − a d(i a , v n ) + ,<label>(6)</label></formula><formula xml:id="formula_7">L V 2I = m + max ip∈S + a d(v a , i p ) − min in∈S − a d(v a , i n ) + ,<label>(7)</label></formula><formula xml:id="formula_8">L I2I = m + max ip∈S + a d(i a , i p ) − min in∈S − a d(i a , i n ) + ,<label>(8)</label></formula><formula xml:id="formula_9">L V 2V = m + max vp∈S + a d(v a , v p ) − min vn∈S − a d(v a , v n ) + .<label>(9)</label></formula><p>Here m is a pre-defined margin, d(·, ·) denotes the Euclidean distance, and [·] + = max(0, ·). S + a and S − a are the positive and negative sample sets of the anchor sample (i a or v a ) respectively.</p><p>Among the four losses, Eq. (6) and Eq. <ref type="formula" target="#formula_7">(7)</ref> constrain the distance between image feature and video feature, which can improve the between-modality feature discriminativity. In contrast, Eq. (8) and Eq. (9) constrain within-modality relative distances, which makes our model distinguish the fine-grained differences between different identities within the same modality. The between-modality and withinmodality losses are complementary and their integration can improve image-to-video representation learning.</p><p>Objective Function. The image-to-video representation learning and temporal knowledge transferring are trained simultaneously. The final objective function is formulated as the combination of classification loss, integrated triplet loss and the proposed TKP loss:</p><formula xml:id="formula_10">L = L C + L T + L F T KP + L D T KP .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Sampling Strategy</head><p>To better train the model with multiple losses, we design a particular sampling strategy. For each batch, we randomly select P persons. For each person, we randomly select K video clips, each with T frames. All the P × K = N video clips are fed into video representation network. Meanwhile, all N × T frames form an image batch and are fed into image representation network. In this way, all the samples in a mini batch can be reused to compute these three losses in Eq. (10), which can reduce the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Image-to-video Re-ID Testing</head><p>In the test stage, each query is a still image and the gallery set consists of masses of person videos. The process of image-to-video Re-ID testing is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Specifically, we use the learned image representation network after TKP to extract image feature of the query and the gallery video features are extracted by video representation network. After feature extraction, we compute the distances between the query feature and each gallery video feature and then conduct image-to-video retrieval according to the distances.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocol</head><p>Datasets. We evaluate our method on MARS <ref type="bibr" target="#b40">[41]</ref>, DukeMTMC-VideoReID (Duke) <ref type="bibr" target="#b33">[34]</ref> and iLIDS-VID <ref type="bibr" target="#b31">[32]</ref> datasets. Among them, MARS and Duke are multi-camera datasets, while iLIDS-VID is captured by only two cameras. The amounts of person videos on MARS, Duke and iLIDS-VID are 20478, 5534 and 600 respectively, and the average lengths of person videos on these three datasets are 58, 168 and 71 respectively.</p><p>Evaluation Protocol. All the three datasets above are video Re-ID datasets. For multi-camera datasets (MARS and Duke), we just use the first frame of every query video as query image to perform I2V Re-ID testing following <ref type="bibr" target="#b30">[31]</ref>. For iLIDS-VID, we use the first frames of all person videos captured by the first camera for both training and testing in order to be consistent with <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. We use the Cumulative Matching Characteristics (CMC) to evaluate the performance of each approach. For iLIDS-VID, the experiment is repeated 10 times and the average result is presented. For multi-camera datasets, we also report the mean Average Precision (mAP) <ref type="bibr" target="#b41">[42]</ref> as a complement to CMC.</p><p>The comparison experiments are mainly conducted on MARS and Duke, since these two datasets have fixed training/testing splits, which is convenient for extensive evaluation. We also present the final results on iLIDS-VID as well as MARS to compare with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We pre-train ResNet-50 on ImageNet <ref type="bibr" target="#b25">[26]</ref> and adopt the method in <ref type="bibr" target="#b32">[33]</ref> to initialize the non-local blocks. During training, we randomly sample 4 frames with a stride of 8 frames from the original full-length video to form an input video clip. For the original video less than 32 frames, we duplicate it to meet the length. The parameters P and K in Section 3.5 are both set to 4. The input video frames are resized to 256 × 128 pixels. Only horizontal flip is used for data augmentation. We adopt Adaptive Moment Estimation (Adam) <ref type="bibr" target="#b15">[16]</ref> with weight decay 0.0005 to optimize the parameters. The model is trained for 150 epochs in total. The learning rate is initialized to 0.0003 and divided by 10 after every 60 epochs. For iLIDS-VID, we first pre-train the model on large-scale dataset and then fine-tune it on iLIDS-VID following <ref type="bibr" target="#b30">[31]</ref>.</p><p>In the test phase, the query image features are extracted by image representation model. For each gallery video, we first split it into several 32-frame clips. For each clip, we utilize video representation model to extract video representation. The final video feature is the averaged representation of all clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on I2V Re-ID</head><p>To validate the effectiveness of the proposed TKP method for I2V Re-ID, we implement and test a baseline and several variants of our model. The configurations of these approaches are presented in <ref type="table" target="#tab_3">Table 2</ref>. Among them, baseline only adopts classification loss and triplet loss for image-to-video representation learning. TKP-F and TKP-D extra use Eq. (3) and Eq. (4) to transfer temporal knowledge respectively. TKP combines these two transfer ways during training. The results of I2V Re-ID on the MARS and Duke dataset can be seen in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Compared with baseline, TKP-F and TKP-D consistently improve the performance by a large margin. Specifically, TKP-F increases the mAP by 8.7% and 8.6% on MARS and Duke respectively. For TKP-D, the improvement of mAP is 7.6% on MARS, and 9.3% on Duke. This comparison shows that temporal knowledge transfer is essential for image-to-video representation learning. We also compare the combination method TKP with TKP-F and TKP-D. It can be seen that a farther improvement is achieved. This result demonstrates that these two transfer ways transfer temporal knowledge from different perspectives and are complementary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">How does TKP Work?</head><p>To investigate how TKP works, we extra use the image representation networks trained in Section 4.3 to conduct image-to-image (I2I) Re-ID tests, where only the first frames of the original query and gallery videos are utilized.  Moreover, we also use the trained video representation networks to perform video-to-video (V2V) Re-ID experiments, where the original full-length query and gallery videos are used. The experiment results are also presented in <ref type="table" target="#tab_3">Table 2</ref>. Compared with baseline, different transfer methods consistently improve the I2I Re-ID performance on both MARS and Duke dataset. Especially, TKP increases the mAP from 49.2% to 55.0% (+5.8%) on the MARS dataset. Moreover, the V2V Re-ID performance of different transfer methods is close to baseline. The comparisons demonstrate that the proposed TKP method can improve the robustness of learned image features, meanwhile, does not reduce the discriminativity of video features. In addition, with the transferred temporal knowledge, the information asymmetry between image and video features can also be alleviated, thus the I2V Re-ID performance gains more improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison among I2I, I2V and V2V Re-ID</head><p>I2I (image-based) Re-ID is a task where the query and each gallery are both images, while the query and each gallery in V2V (video-based) Re-ID are both videos. In I2V setting, the query is an image while each gallery is a video. We compare the three different tasks under the same configurations and the comparisons on the MARS and Duke dataset are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Due to the lack of additional visual and temporal information, the performance of I2V Re-ID is lower than that of V2V Re-ID, and I2I Re-ID is   lower than I2V Re-ID. Especially, the performance gap on Duke is much larger. The reason can be attributed that the average length of the videos on Duke is longer than that on MARS. When we only use one frame of the original video to conduct I2I and I2V tests, the information loss is more serious. But our proposed TKP method can transfer temporal knowledge to image features, thus the performance gap can be greatly reduced on these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison with State-of-the-art Methods</head><p>We compare the proposed approach with state-of-the-art I2V Re-ID methods on the iLIDS-VID and MARS datasets. The results are presented in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_5">Table 4</ref> respectively. Among them, PSDML <ref type="bibr" target="#b42">[43]</ref>, LERM <ref type="bibr" target="#b13">[14]</ref>, XQDA <ref type="bibr" target="#b20">[21]</ref>, KISSME <ref type="bibr" target="#b17">[18]</ref> and PHDL <ref type="bibr" target="#b43">[44]</ref> are handcrafted feature based methods, while ResNet-50 <ref type="bibr" target="#b8">[9]</ref>+XQDA <ref type="bibr" target="#b20">[21]</ref>, TMSL <ref type="bibr" target="#b36">[37]</ref> and P2SNet <ref type="bibr" target="#b30">[31]</ref> are deep learning based methods. It can be seen that deep learning based methods significantly outperform the traditional methods with handcrafed features, while our method further surpasses the existing deep learning based methods by a large margin. Since Duke is a newly released dataset, existing methods have not conducted I2V Re-ID experiments on it. Therefore, we do not compare with stateof-the-art methods on this dataset. Anyway, the results of our method can be seen in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Note that the V2V Re-ID performance decides the upper bound of the I2V Re-ID performance. We also compare the proposed approach with state-of-the-art V2V Re-ID methods on the MARS dataset. To compare fairly, for multisnippets <ref type="bibr" target="#b2">[3]</ref>, we use the results without optical flow. As shown in <ref type="table" target="#tab_6">Table 5</ref>, our TKP consistently outperforms these methods. As for the iLIDS-VID dataset, since we do not use all of the training set (only use the first frames of all videos captured by the first camera), we do not compare V2V Re-ID results with these methods on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visualization</head><p>The visualization of feature maps. We visualise the feature maps of the image features before/after TKP in <ref type="figure">Figure 5</ref>. It can be seen that the original image features only pay attention to some local discriminative regions. After TKP transferring temporal knowledge, the learned image representations can focus on more foreground and manifest rubustness to occlusion and blur just like video features in <ref type="figure" target="#fig_0">Figure 1</ref>, which benefits I2V matching. So the final I2V performance can be improved significantly.</p><p>The visualization of feature distribution. We also visualize the distribution of the learned features before/after TKP using t-SNE <ref type="bibr" target="#b21">[22]</ref> as shown in <ref type="figure">Figure 6</ref>. Before temporal knowledge transferring, the image features and video features with the same identity are incompact. After TKP    transferring temporal knowledge, the feature distributions of these two modalities become more consistent. Therefore, it is easier to measure the similarity between image and video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Ablation Study</head><p>Whether the non-local blocks are required? In our framework, we use non-local blocks to model temporal relations between video frames. To verify whether the nonlocal blocks are required or not, we remove the non-local blocks from the methods baseline and TKP-F. And the video frame feature f nt in Eq. <ref type="formula" target="#formula_2">(3)</ref> is replaced by the video feature v n after temporal average pooling. As shown in <ref type="table" target="#tab_7">Table 6</ref>, when the non-local blocks are removed, TKP-F still supasses baseline by a reasonable margin. But the performance and improvement without non-local blocks are both lower than those with non-local blocks. We argue that, compared with simple temporal average pooling, non-local blocks can model temporal information much better, which makes temporal knowledge propagation more effective as well.</p><p>Whether TKP loss should propagate gradient to video representation network? As discussed in Section 3.3, enforcing TKP loss to propagate gradient to video representaiton network will degenerate video representations w.r.t. temporal knowledge. To verify this, we add two additional experiments and the results are reported in <ref type="table" target="#tab_8">Table 7</ref>. It can be seen that, when the gradient of TKP loss is propagated to video network, TKP-F and TKP-D can still increase the performance of I2I and I2V Re-ID by a considerable margin. But these two methods consistently gain lower V2V performance. If the back propagation to video network is banned, all I2I, I2V and V2V results can be further improved.</p><p>Whether using a pre-trained video model is beneficial for the convergence of networks? Our method aims to solve I2V matching, but the pre-trained video model supervised by V2V loss function may not be optimal for I2V matching. To verify this, we add an experiment which uses a pre-trained video model to perform knowledge propagation. As shown in <ref type="table" target="#tab_9">Table 8</ref>, though the method pre-trained outperforms baseline, it is inferior to TKP which learns two networks simultaneously.</p><p>The influence of different triplet losses. To explore this, we perform the experiments with different kinds of triplet losses. As shown in <ref type="table" target="#tab_10">Table 9</ref>, the method Integrated tri. using integrated triplet loss surpasses I2V tri. which only uses I2V triplet loss. With additional classification loss, baseline outperforms these two methods.</p><p>The influence of the video clip size T . By varying T , we show the experimental results in <ref type="figure" target="#fig_6">Figure 7</ref>. It can be seen that the best top-1 and mAP are consistently achieved when T is 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel TKP method for I2V Re-ID. TKP can transfer temporal knowledge from video representation network to image representation network. With the transferred temporal knowledge, the robustness of the image features can be improved and the information asymmetry between image and video features can also be alleviated. Extensive experiments demonstrate the effectiveness of our method and the results on two widely used datasets significantly surpass start-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualisation of the feature maps. Due to lack of temporal information, compared with video features, the query image feature only pays attention to some local regions. With the proposed TKP method, temporal knowledge can be transferred to enhance the image feature and the learned image feature focuses on more foreground of person image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework of TKP method in I2V Re-ID training. SAP and TAP represent spatial average pooling and temporal average pooling respectively. The classification loss and triplet loss are used to guide image-to-video representation learning. The blue arrow represents the process of TKP via features, while the green arrow represents the process of TKP via cross sample distances. And the red arrow denotes the back propagation process of TKP loss. The representation learning and temporal knowledge transferring are trained simultaneously. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The pipeline of I2V Re-ID testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison among I2I, I2V and V2V Re-ID on the MARS and Duke datasets. With the proposed TKP method, the performance gap can be significantly reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The visualisation of feature maps before/after TKP on the (a) MARS and (b) Duke datasets. Best viewed in color.BeforeAfter The visualisation of feature distribution before/after TKP on the MARS dataset. Circle represents video feature, while cross represents image feature. Different colors denote different identities. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>The results with different T on the MARS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The architecture of video representation network. Each input video clip contains 4 frames, each frame with 256 × 128 pixels.</figDesc><table><row><cell>res5</cell><cell>residual block ×3</cell><cell>4 × 16 × 8 × 2048</cell></row><row><cell></cell><cell>spatial average pool</cell><cell>4 × 2048</cell></row><row><cell></cell><cell>temporal average pool</cell><cell>2048</cell></row></table><note>layer output size conv1 7 × 7, stride 2, 2 4 × 128 × 64 × 64 pool 3 × 3 max, stride 2, 2 4 × 64 × 32 × 64 res2 residual block ×3 4 × 64 × 32 × 256 res3 residual block × 2 non-local block × 1 × 2 4 × 32 × 16 × 512 res4 residual block × 2 non-local block × 1 × 3 4 × 16 × 8 × 1024</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The results of I2V, I2I and V2V Re-ID on the MARS and Duke datasets. In I2I setting, only the first frames of the query and gallery samples are used. In V2V setting, the full-length query videos and gallery videos are used. All the image features in I2I and I2V Re-ID are extracted by the learned image representation network. All the video features in I2V and V2V Re-ID are extracted by the learned video representation network.</figDesc><table><row><cell></cell><cell>Losses</cell><cell></cell><cell>MARS</cell><cell>Duke</cell></row><row><cell>Models</cell><cell>LC LT L F T KP</cell><cell>L D T KP</cell><cell cols="2">I2V Re-ID top-1 mAP top-1 mAP top-1 mAP top-1 mAP top-1 mAP top-1 mAP I2I Re-ID V2V Re-ID I2V Re-ID I2I Re-ID V2V Re-ID</cell></row><row><cell>baseline</cell><cell></cell><cell></cell><cell cols="2">67.1 55.5 65.9 49.2 83.4 72.6 67.5 65.6 60.4 52.8 93.2 91.3</cell></row><row><cell>TKP-F</cell><cell></cell><cell></cell><cell cols="2">75.0 64.2 71.0 54.7 83.2 72.6 76.8 74.2 63.0 54.5 93.6 91.5</cell></row><row><cell>TKP-D</cell><cell></cell><cell></cell><cell cols="2">75.0 63.1 70.3 55.0 84.1 72.9 76.5 74.9 62.0 53.5 93.3 91.4</cell></row><row><cell>TKP</cell><cell></cell><cell></cell><cell cols="2">75.6 65.1 71.0 55.0 84.0 73.3 77.9 75.9 63.4 54.8 94.0 91.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art I2V Re-ID methods on the iLIDS-VID dataset.</figDesc><table><row><cell>Models</cell><cell cols="3">top-1 top-5 top-10 top-20</cell></row><row><cell cols="2">PSDML [43] 13.5 33.8</cell><cell>45.6</cell><cell>56.3</cell></row><row><cell>LERM [14]</cell><cell>15.3 37.1</cell><cell>49.7</cell><cell>62.0</cell></row><row><cell>XQDA [21]</cell><cell>16.8 38.6</cell><cell>52.3</cell><cell>63.6</cell></row><row><cell cols="2">KISSME [18] 17.6 41.7</cell><cell>55.3</cell><cell>68.7</cell></row><row><cell>PHDL [44]</cell><cell>28.2 50.4</cell><cell>65.9</cell><cell>80.4</cell></row><row><cell>TMSL [37]</cell><cell>39.5 66.9</cell><cell>79.6</cell><cell>86.6</cell></row><row><cell>P2SNet [31]</cell><cell>40.0 68.5</cell><cell>78.1</cell><cell>90.0</cell></row><row><cell>TKP</cell><cell>54.6 79.4</cell><cell>86.9</cell><cell>93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: Comparison with state-of-the-art I2V Re-ID methods on</cell></row><row><cell>the MARS dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="3">top-1 top-5 top-10 mAP</cell></row><row><cell>P2SNet [31]</cell><cell>55.3 72.9</cell><cell>78.7</cell><cell>-</cell></row><row><cell cols="2">ResNet-50 [9]+XQDA [21] 67.2 81.9</cell><cell>86.1</cell><cell>54.9</cell></row><row><cell>TKP</cell><cell>75.6 87.6</cell><cell>90.9</cell><cell>65.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">: Comparison with state-of-the-art V2V Re-ID methods on</cell></row><row><cell>the MARS dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="4">top-1 top-5 top-10 mAP</cell></row><row><cell>SDM [38]</cell><cell cols="2">71.2 85.7</cell><cell>91.8</cell><cell>-</cell></row><row><cell>MGCAM [29]</cell><cell>77.2</cell><cell>-</cell><cell>-</cell><cell>71.2</cell></row><row><cell>DuATM [28]</cell><cell cols="2">78.7 90.9</cell><cell>-</cell><cell>62.3</cell></row><row><cell cols="3">multi-snippets [3] 81.2 92.1</cell><cell>-</cell><cell>69.4</cell></row><row><cell>DRSA [20]</cell><cell>82.3</cell><cell>-</cell><cell>-</cell><cell>65.8</cell></row><row><cell>TKP</cell><cell cols="2">84.0 93.7</cell><cell>95.7</cell><cell>73.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The I2V Re-ID results with/without non-local blocks on the MARS dataset. w/ NL? denotes whether the model contains non-local blocks or not. The performance improvement is provided in brackets.</figDesc><table><row><cell>Models</cell><cell>w/ NL?</cell><cell>top-1</cell><cell>mAP</cell></row><row><cell>baseline</cell><cell>-</cell><cell>66.1</cell><cell>51.8</cell></row><row><cell>TKP-F</cell><cell>-</cell><cell>68.9(+2.8)</cell><cell>57.8(+6.0)</cell></row><row><cell>baseline</cell><cell></cell><cell>67.1</cell><cell>55.5</cell></row><row><cell>TKP-F</cell><cell></cell><cell>75.0(+7.9)</cell><cell>64.2(+8.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The results with/without the TKP loss propagating gradient to video representation network on the MARS dataset. BP2v? denotes whether the gradient of TKP loss is propagated to video representation network or not.</figDesc><table><row><cell cols="2">Models BP2v?</cell><cell>I2I Re-ID top-1 mAP top-1 mAP top-1 mAP I2V Re-ID V2V Re-ID</cell></row><row><cell>baseline</cell><cell></cell><cell>65.9 49.2 67.1 55.5 83.4 72.6</cell></row><row><cell>TKP-F</cell><cell>-</cell><cell>66.6 51.0 72.7 60.3 78.5 66.6 71.0 54.7 75.0 64.2 83.2 72.6</cell></row><row><cell>TKP-D</cell><cell>-</cell><cell>66.3 50.3 74.2 61.7 79.3 66.1 70.3 55.0 75.0 63.1 84.1 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison with the method using pre-trained video model on the MARS dataset.</figDesc><table><row><cell cols="4">Models baseline pre-trained TKP</cell></row><row><cell>top-1</cell><cell>67.1</cell><cell>73.2</cell><cell>75.6</cell></row><row><cell>mAP</cell><cell>55.5</cell><cell>61.5</cell><cell>65.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparing the methods with different identification losses on the MARS dataset.</figDesc><table><row><cell cols="4">Models I2V tri. Integrated tri. baseline</cell></row><row><cell>top-1</cell><cell>54.4</cell><cell>59.1</cell><cell>67.1</cell></row><row><cell>mAP</cell><cell>42.6</cell><cell>47.3</cell><cell>55.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable person reidentification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Darkrank: Accelerating deep metric learning via cross sample similarities transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A two stream siamese convolutional neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahjung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep multi-metric learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>ArXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vrstc: Occlusion-free video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning euclidean-to-riemannian metric for pointto-set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The approximation power of moving leastsquares. Mathematics of Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niall</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yue Hei Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image deformation using moving least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Mcphail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">P2snet : Can an image match a video for person re-identification in an end-to-end way?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Toppush video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjie</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hard-aware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Image-to-video person re-identification with temporally memorized similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoquan</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-shot pedestrian re-identification via sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From point to set: Extend the learning of distance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning heterogeneous dictionary pair with feature projection matrix for pedestrian video retrieval via single query image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Image to video person re-identification by learning heterogeneous dictionary pair with feature projection matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TIFS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
