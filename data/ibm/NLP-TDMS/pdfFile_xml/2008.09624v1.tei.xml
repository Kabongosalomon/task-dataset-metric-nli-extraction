<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimization of Graph Neural Networks with Natural Gradient Descent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Rasool</forename><surname>Izadi</surname></persName>
							<email>mizadi@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering, † Applied and Computational Mathematics and Statistics</orgName>
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Fang</surname></persName>
							<email>yfang5@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering, † Applied and Computational Mathematics and Statistics</orgName>
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stevenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering, † Applied and Computational Mathematics and Statistics</orgName>
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Lin</surname></persName>
							<email>lizhen.lin@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering, † Applied and Computational Mathematics and Statistics</orgName>
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimization of Graph Neural Networks with Natural Gradient Descent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose to employ information-geometric tools to optimize a graph neural network architecture such as the graph convolutional networks. More specifically, we develop optimization algorithms for the graph-based semi-supervised learning by employing the natural gradient information in the optimization process. This allows us to efficiently exploit the geometry of the underlying statistical model or parameter space for optimization and inference. To the best of our knowledge, this is the first work that has utilized the natural gradient for the optimization of graph neural networks that can be extended to other semi-supervised problems. Efficient computations algorithms are developed and extensive numerical studies are conducted to demonstrate the superior performance of our algorithms over existing algorithms such as ADAM and SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms</head><p>Graph neural network, Fisher information, natural gradient descent, network data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>proposed to capture graph representations in neural networks <ref type="bibr" target="#b19">[20]</ref>, have been used for semisupervised learning in a variety of problems like node classification, link predictions, and so on.</p><p>The goal of each GNN layer is to transform features while considering the graph structure by aggregating information from connected or neighboring nodes. When there is only one graph, the goal of node classification becomes predicting node labels in a graph while only a portion of node labels are available (even though the model might have access to the features of all nodes). Inspired by the advance of convolutional neural networks <ref type="bibr" target="#b13">[14]</ref> in computer vision <ref type="bibr" target="#b11">[12]</ref>, Graph Convolutional Network (GCN) <ref type="bibr" target="#b9">[10]</ref> employs the spectra of graph Laplacian for filtering signals and the kernel can be approximated using Chebyshev polynomials or functions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>. GCN has become a standard and popular tool in the emerging field of geometric deep learning <ref type="bibr" target="#b1">[2]</ref>.</p><p>From the optimization perspective, Stochastic Gradient Descent (SGD)-based methods that use an estimation of gradients have been popular choices due to their simplicity and efficiency.</p><p>However, SGD-based algorithms may be slow in convergence and hard to tune on large datasets.</p><p>Adding extra information about gradients, may help with the convergence but are not always possible or easy to obtain. For example, using second-order gradients like the Hessian matrix, resulting in the Newton method, is among the best choices which, however, are not easy to calculate especially in NNs. When the dataset is large or samples are redundant, NNs are trained using methods built on top of SGD like AdaGrad <ref type="bibr" target="#b3">[4]</ref> or Adam <ref type="bibr" target="#b8">[9]</ref>. Such methods use the gradients information from previous iterations or simply add more parameters like momentum to the SGD. Natural Gradient Descent (NGD) <ref type="bibr" target="#b0">[1]</ref> provides an alternative based on the secondmoment of gradients. Using an estimation of the inverse of the Fisher information matrix (simply Fisher), NGD transforms gradients into so-called natural gradients that showed to be much faster compared to the SGD in many cases. The use of NGD allows efficient exploration of the geometry of the underlying parameter space in the optimization process. Also, Fisher information plays a pivotal role throughout statistical modeling <ref type="bibr" target="#b15">[16]</ref>. In frequentist statistics, Fisher information is used to construct hypothesis tests and confidence intervals by maximum likelihood estimators. In Bayesian statistics, it defines the Jeffreys's prior, a default prior commonly used for estimation problems and nuisance parameters in a Bayesian hypothesis test. In minimum description length,</p><p>Fisher information measures the model complexity and its role in model selection within the minimum description length framework like AIC and BIC. Under this interpretation, NGD is invariant to any smooth and invertible reparameterization of the model, while SGD-based methods highly depend on the parameterization. For models with a large number of parameters like DNN, Fisher is so huge that makes it almost impossible to evaluate natural gradients. Thus, for faster calculation it is preferred to use an approximation of Fisher like Kronecker-Factored Approximate Curvature (KFAC) <ref type="bibr" target="#b17">[18]</ref> that are easier to store and inverse.</p><p>Both GNN and training NNs with NGD have been active areas of research in recent years but, to the best of our knowledge, this is the first attempt on using NGD in the semi-supervised learning. In this work, a new framework for optimizing GNNs is proposed that takes into account the unlabeled samples in the approximation of Fisher. Section II provides an overview of related topics such as semi-supervised learning, GNN, and NGD. The proposed algorithm is described in section III and a series of experiments are performed in section IV to evaluate the method's efficiency and sensitivity to hyper-parameters. Finally, the work is concluded in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM AND BACKGROUND</head><p>In this section, first, the graph-based semi-supervised learning with a focus on least-squared regression and cross-entropy classification is defined. Required backgrounds on the optimization and neural networks are provided in the subsequent sections. A detailed description of the notation is summarized in the <ref type="table">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem</head><p>Consider an information source q(x) generating independent samples x i ∈ X, the target distribution q(y|x) associating y i ∈ Y to each x i , and the adjacency distribution q(a|x, x )</p><p>representing the link between two nodes given their covariates levels x and x . The problem of learning q(y|x) is to estimate some parameters θ that minimizes the cost function</p><formula xml:id="formula_0">r(θ) = E x,x ∼q(x),a∼q(a|x,x ),y∼q(y|x) [l(y, f (x, x , a; θ))]<label>(1)</label></formula><p>where the loss function l(y,ŷ) measures the prediction error between y andŷ. Also, x and a show sequences of x and a, respectively. As q(x), q(a|x, x ), and q(y|x) are usually unknown or unavailable, the cost r(θ) is estimated using samples from these distributions. Furthermore, it is often more expensive to sample from q(y|x) than q(x) and q(a|x, x ) resulting in the different number of samples from each distribution being available.</p><p>Let X = X 0 to be a d 0 × n matrix of n ≥ 1 i.i.d x i samples from q(x) (equivalent to X ∼ q(X)). It is assumed that n × n adjacency matrix A = [a ij ] is sampled from q(a|x i , x j ) for i, j = 1, . . . , n (equivalent to A ∼ q(A|X)). One can consider (X, A) to be a graph of n  denotes the diagonal degree matrix. Also, denote Y to be a d m ×n matrix ofn &lt; n samples y i from q(y|x i ) for i = 1, . . . ,n and z = [1(i ∈ {1, . . . ,n})] n i=1 to be the training mask vector.</p><p>Note that 1(condition) is 1 if the condition is true and 0 otherwise. Thus, an empirical cost can be estimated byr</p><formula xml:id="formula_1">(θ) = 1 nn i=1 l(y i , f (x i , X, A; θ)),<label>(2)</label></formula><p>where f (x i , X, A; θ) shows the processed x i when having access to n − 1 extra samples and links between them. Note that as X contains x i (the ith column), f (x i , X, A; θ) and f (X, A; θ) are used interchangeably.</p><p>Assuming p(y|f (X, A; θ)) = p θ (y|X, A) to be an exponential family with natural parameters in F, the loss function becomes l(y, f (X, A; θ)) = − log p(y|f (X, A; θ)).</p><p>In the least-squared regression,</p><formula xml:id="formula_3">p(y|f (X, A; θ)) = N (y|f (X, A; θ), σ 2 )<label>(4)</label></formula><p>for fixed σ 2 and F = Y = R. In the cross-entropy classification to c classes,</p><formula xml:id="formula_4">p(y = k|f (X, A; θ)) = exp(f k )/ c j=1 exp(f j )<label>(5)</label></formula><p>for F = R c and Y = {1, . . . , c}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter estimation</head><p>Having the first order approximation of r(θ) around a point θ 0 ,</p><formula xml:id="formula_5">r(θ) ≈ r(θ 0 ) + g T 0 (θ − θ 0 ),<label>(6)</label></formula><p>the gradient descent can be used to update parameter θ iteratively:</p><formula xml:id="formula_6">θ t+1 = θ t − ηBg 0<label>(7)</label></formula><p>where η &gt; 0 denotes the learning rate, g 0 = g(θ 0 ) is the gradient at θ 0 for g(θ) = ∂r(θ) ∂θ <ref type="bibr" target="#b7">(8)</ref> and B shows a symmetric positive definite matrix called preconditioner capturing the interplay between the elements of θ. In SGD, B = I and g 0 is approximated by:</p><formula xml:id="formula_7">g 0 = 1 nn i=1 ∂l(y i , f (X, A; θ)) ∂θ<label>(9)</label></formula><p>wheren ≥ 1 can be the mini-batch (a randomly drawn subset of the dataset) size.</p><p>To take into the account the relation between θ elements, one can use the second order approximation of r(θ):</p><formula xml:id="formula_8">r(θ) ≈ r(θ 0 ) + g T 0 (θ − θ 0 ) + 1 2 (θ − θ 0 ) T H 0 (θ − θ 0 ),<label>(10)</label></formula><p>where H 0 = H(θ 0 ) denotes the Hessian matrix at θ 0 for</p><formula xml:id="formula_9">H(θ) = ∂ 2 r(θ) ∂θ T θ .<label>(11)</label></formula><p>Thus, having the gradients of r(θ) around θ as:</p><formula xml:id="formula_10">g(θ) ≈ g 0 + H 0 (θ − θ 0 ),<label>(12)</label></formula><p>the parameters can be updated using:</p><formula xml:id="formula_11">θ t+1 = (I − ηBH 0 )θ t − ηB(g 0 − H 0 θ 0 ).<label>(13)</label></formula><p>The convergence of Eq. 13 heavily depends on the selection of η and the distribution of I −ηBH 0 eigenvalues. Note that update rules Eqs. 7 and 13 coincides at B = H −1 0 resulting the Newton's method. As it is not always possible or desirable to obtain Hessian, several preconditioners are suggested to adapt the information geometry of the parameter space.</p><p>In NGD, the preconditioner is defined to be the inverse of Fisher Information matrix:</p><formula xml:id="formula_12">F (θ) :=E x,y∼p(x,y;θ) [∇ θ ∇ T θ ] (14) =E x∼q(x),y∼p(y|x;θ) [∇ θ ∇ T θ ]<label>(15)</label></formula><p>where p(x, y; θ) := q(x)p(y|x; θ) and</p><formula xml:id="formula_13">∇ θ := −∇ θ log p(x, y; θ).<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neural Networks</head><p>A neural network is a mapping from the input space X to the output space F through a series of m layers. Layer k ∈ {1, . . . , m}, projects d k−1 -dimensional input x k−1 to d k -dimensional output x k and can be expressed as:</p><formula xml:id="formula_14">x k = φ k (W k x k−1 )<label>(17)</label></formula><p>where φ k is an element-wise non-linear function and W k is the</p><formula xml:id="formula_15">d k × d k−1 -dimensional weight matrix.</formula><p>The bias is not explicitly mentioned as it could be the last column of W k when x k has an extra unit element. Let the θ = [θ 1 , . . . , θ m ] to be the parameters of an m-layer neural network formed by stacking m vectors of dimension d k d k−1 for k = 1, . . . , m and dim(</p><formula xml:id="formula_16">x) = d 0 such that dim(θ) = m k=1 d k d k−1 . The parameters of the k'th layer, θ k = vec(W k ) for vec(W k ) = [w 1 , . . . , w d k ]</formula><p>, is also shaped by piling up rows of W k . Their gradients, ∇ θ k , could be written as:</p><formula xml:id="formula_17">∇ θ k = ∂l ∂θ k = ∂x k ∂θ k T ∂l ∂x k<label>(18)</label></formula><formula xml:id="formula_18">for d k × d k d k−1 -dimensional matrix ∂x k /∂θ k and d k -dimensional vector ∂l/∂x k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Graph Neural Networks</head><p>The Graph Neural Network (GNN) extends the NN mapping to the data represented in graph domains <ref type="bibr" target="#b19">[20]</ref>. The basic idea is to use related samples when the adjacency information is available. In other words, the input to the k'th layer, x k−1 is transformed intox k−1 that take into the account unlabeled samples using the adjacency such that p(x k−1 , A) = p(x k−1 ). Therefore, for each node i = 1, . . . , n, the Eq. 17 can be written by a local transition function (or a single message passing step) as:</p><formula xml:id="formula_19">x k,i = f k (x k−1,i , x k−1,i , x 0,i , x 0,i ; W k )<label>(19)</label></formula><p>where x k,i denotes all the information coming from nodes connected to the ith node at the kth layer. The subscripts here are used to indicate both the layer and the node, i.e. x k,i means the state embedding of node i in the layer k. Also, the local transition Eq. 19, parameterized by W k , is shared by all nodes that includes the information of the graph structure, and x 0,i = x i .</p><p>The Graph Convolutional Network (GCN) is a one of the GNN with the message passing operation as a linear approximation to spectral graph convolution, followed by a non-linear activation function as:</p><formula xml:id="formula_20">x k,i =f k (x k−1,i , x k−1,i ; W k )<label>(20)</label></formula><formula xml:id="formula_21">X k =φ k (W k X k−1Ã ) (21) =φ k (W kXk−1 )<label>(22)</label></formula><p>where φ k is a element-wise nonlinear activation function such as RELU(x) = max(x, 0), W k is a d k × d k−1 parameter matrix that needs to be estimated.Ã denotes the normalized adjacency matrix defined by:Ã</p><formula xml:id="formula_22">= (D + I) −1/2 (A + I)(D + I) −1/2<label>(23)</label></formula><p>to overcome the overfitting issue due to the small number of labeled samplesn. In fact, a GCN layer is basically a NN (Eq. 17) where the input x k−1 is initially updated intox k−1 using a so-called renormalization trick such thatx k−1,i = n j=1ã i,j x k−1,i whereÃ = [ã i,j ]. Comparing Eq. 20 with the more general Eq. 19, the local transition function f k is defined as a linear combination followed by a nonlinear activation function. For classifying x into c classes, having a c-dimensional x m as the output of the last layer with a Softmax activation function, the loss between the label y and the prediction x m becomes:</p><formula xml:id="formula_23">l(y, x m ) = − c j=1 1(x m,j = j) log x m,j .<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The basic idea of preconditioning is to capture the relation between the gradients of parameters ∇ θ . This relation can be as complete as a matrix B (for example, NGD) representing the pairwise relation between elements of ∇ θ or as simple as a weighting vector (for example, Adam) with the same size as ∇ θ resulting in a diagonal B. Considering the flow of gradients ∇ θ,t over the training time as input features, the goal of preconditioning is to extract useful features that help with the updating rule. One can consider the preconditioner to be the expected value of</p><formula xml:id="formula_24">B(x, y) = [b ij ] −1 for b ij = b i,j (x, y) = b(∇ θi ||∇ θj ) 1 .<label>(25)</label></formula><p>In methods with a diagonal preconditioner like Adam, B(x, y) = diag(∇ θ ∇ θ ), the pairwise relation between gradients is neglected. Preconditioners like Hessian inverse in Newton's method with the form of b ij = ∂∇ θi /∂θ j are based on the second derivative that encodes the cost curvature in the parameter space. In NGD and similar methods, this curvature is approximated using the second moment of gradient b ij = ∇ θi ∇ θj , as an approximation of Hessian, in some empirical cases (see <ref type="bibr" target="#b12">[13]</ref> for a detailed discussion).</p><p>In this section, a new preconditioning algorithm, motivated by natural gradient, is proposed for graph-based semi-supervised learning that improves the convergence of Adam and SGD with intuitive and insensitive hyper-parameters. The natural gradient is a concept from information geometry and stands for the steepest descent direction in the Riemannian manifold of probability distributions <ref type="bibr" target="#b0">[1]</ref>, where the distance in the distribution space is measured with a special Riemannian metric. This metric depends only on the properties of the distributions themselves and not their parameters, and in particular, it approximates the square root of the KL divergence within a small neighborhood <ref type="bibr" target="#b16">[17]</ref>. Instead of measuring the distance between the parameters θ and θ , the cost is measured by the KL divergence between their distributions p(θ) and p(θ ).</p><p>Consequently, the steepest descent direction in the statistical manifold is the negative gradient <ref type="bibr" target="#b0">1</ref> Note that the adjacency matrix provides the relation between x samples where the preconditioning matrix includes the relation between the elements of ∇ θ preconditioned with the Fisher information matrix F (θ). The validation cost on three different datasets is shown in <ref type="figure" target="#fig_1">Fig. 1</ref> where preconditioning is applied to both Adam and SGD. datasets. Also, <ref type="figure" target="#fig_1">Fig. 1d, 1e</ref>, and 1f (bottom row) reveal that the suggested SGD-KFAC methods (green and red curves) achieve a remarkably faster convergence than the vanilla SGD method (blue and orange curves) on all three datasets.</p><p>As the original NGD (Eq. 14) is defined based on a prediction function with access only to a single sample, p(y|f (x; θ)), Fisher information matrix with the presence of the adjacency distribution becomes:</p><formula xml:id="formula_25">F (θ) = E x,x ,a,y∼p(x,x ,a,y;θ) [∇ θ ∇ T θ ] (26) = E x,x ∼q(x),a∼q(a|x,x ),y∼p(y|x,x ,a;θ) [∇ θ ∇ T θ ].<label>(27)</label></formula><p>With n samples of q(x), i.e. X and n 2 samples of q(a|X), i.e. A, Fisher can be estimated as:</p><formula xml:id="formula_26">F (θ) = E y∼p(y|X,A;θ) [∇ θ ∇ T θ ],<label>(28)</label></formula><p>where ∇ θ = −∇ θ log p(X, A, y; θ).</p><p>In fact, to evaluate the expectation in Eq. 26, q(X) and q(A|X) are approximated withq(X) andq(A|X) using {x i } n i=1 and A, respectively. However, there are onlyn samples fromq(y|x j ) as an approximation of q(y|x j ) for the following replacement:</p><formula xml:id="formula_28">p(y|X, A; θ) ≈q(y|x i ).<label>(30)</label></formula><p>Therefore, an empirical Fisher can be obtained bŷ</p><formula xml:id="formula_29">F (θ) = 1 nn i=1 ∇ θ,i ∇ T θ,i =n i=1 B i (θ)<label>(31)</label></formula><p>for</p><formula xml:id="formula_30">∇ θ,i = −∇ θ log p(y i |X, A; θ) (32) B i (θ) = B(X, A, y i ; θ).<label>(33)</label></formula><p>From the computation perspective, the matrix B i (θ) can be very large, for example, in neural networks with multiple layers, the parameters could be huge, so it needs to be approximated too. In networks characterized with Eqs. 17 or 20, a simple solution would be ignoring the cross-layer terms so that B i (θ) −1 and consequently B i (θ) turns into a block-diagonal matrix:</p><formula xml:id="formula_31">B i (θ) = diag(B 1,i , . . . , B m,i )<label>(34)</label></formula><p>In KFAC, the diagonal block B k,i , corresponded to k'th layer with the dimension d k d k−1 ×d k d k−1 , is approximated with the Kronecker product of the inverse of two smaller matrices U k,i and V k,i as:</p><formula xml:id="formula_32">B k,i = (U k,i ⊗ V k,i ) −1 = U −1 k,i ⊗ V −1 k,i .<label>(35)</label></formula><p>For ∇ θ,i = [∇ T θ 1 ,i , . . . , ∇ T θm,i ] T , the preconditioned gradient B k,i ∇ θ k ,i can be computed using the identity</p><formula xml:id="formula_33">B k,i ∇ θ k ,i = U −1 k,i ⊗ V −1 k,i vec( ∂l ∂W k ) (36) = vec(U −1 k,i ∂l ∂W k V −1 k,i ).<label>(37)</label></formula><p>Noting that: For a graph with n nodes, adjacency matrix A, and the training set {(x i , y i )}n i=1 + {x i } n i=n+1 , U k and V k are estimated in two ways: (1) using onlyn labeled samples, and (2) including n −n unlabeled samples. In the first method, U k and V k are estimated by:</p><formula xml:id="formula_34">∂l ∂W k = ∂l ∂x k φ k (W kxk−1 ) x T k−1 (38) = u k,i v T k,i ,<label>(39)</label></formula><formula xml:id="formula_35">U k = 1 n ∂l ∂X k φ k (W kXk−1 ) ∂l ∂X k φ k (W kXk−1 ) T (40) V k = 1 nX k−1X T k−1 .<label>(41)</label></formula><p>Note that both ∂l/∂X k and φ k (W kXk−1 ) are d k × n matrices and the last n −n columns of ∂l/∂X k are zero. However, as unlabeled samples are not used in the first method, one needs to evaluate loss function for i =n + 1, . . . , n, which can be done by samplingŷ i from p(y|x; θ).</p><p>In the second method, these new samples are added to the empirical cost aŝ</p><formula xml:id="formula_36">r(θ) = 1 nn i=1 l(y i , f (X, A; θ)) + λ n −n n i=n+1 l(y i , f (X, A; θ)),<label>(42)</label></formula><p>where 0 ≤ λ ≤ 1 denotes the regularization hyper-parameter for controlling the cost of predicted labels and λ = 0 results the first method. As the prediction improves over the course of training, λ can be a function of iteration t, for example here, it is defined to be:</p><formula xml:id="formula_37">λ(t) := t max(t) γ ,<label>(43)</label></formula><p>where max(t) shows the maximum number of iterations and γ is the replaced regularization hyper-parameter. Algorithm 1 shows the preconditioning step for modifying gradients of each layer at any iteration such that gradients are first, transformed using two matrices of V −1 k and U −1 k , then sent to the optimization algorithm for updating parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relation between Fisher and Hessian</head><p>The Hessian of the cost function: for k = 1, . . . , m dõ</p><formula xml:id="formula_38">H θ r(θ) = E X,</formula><formula xml:id="formula_39">x k−1,i = n j=1ã i,j x k−1,j u k−1,i = ∂l/∂x k φ k (W kxk−1,i ) v k−1,i =x k−1,i U k = n i=1 (z i + (1 − z i )λ)u k−1,i u T k−1,i /(n + λn) V k = n i=1 (z i + (1 − z i )λ)v k,i v T k,i /(n + λn) U −1 k = INVERSE(U k ) V −1 k = INVERSE(V k ) output V −1 k ∇W k U −1 k function INVERSE(X) output (X + −1/2 I) −1</formula><p>can also be approximated usingq(X),q(A|X), andq(y|x i ) resulting the empirical Hessian to beĤ θ r(θ) :=</p><formula xml:id="formula_40">1 nn i=1 H θ l(y i , f (X, A; θ)),<label>(45)</label></formula><p>which is equivalent to the empirical Fisher Eq. 31 when p(X, A, y; θ) is estimated withq(X)q(A|X)q(y|x i ) for i = 1, . . . ,n (see Lemma 1 in the appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, the performance of the proposed algorithm is evaluated compared to Adam and SGD on several datasets for the task of node classification in single graphs. The task is assumed to be transductive when all the features are available for training but only a portion of labels are used in the training. First, a detailed description of datasets and the model architecture are provided. Then, the general optimization setup, commonly used for the node classification, is specified. The last part includes the sensitivity to hyper-parameter and training time analysis in addition to validation cost convergence and the test accuracy. All the experiments are conducted mainly using Pytorch <ref type="bibr" target="#b18">[19]</ref> and Pytorch Geometric <ref type="bibr" target="#b4">[5]</ref>, two open-source Python libraries for automating differentiation and working with graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Three citation datasets with the statistics shown in <ref type="table" target="#tab_1">Table II</ref> are used in the experiments <ref type="bibr" target="#b20">[21]</ref>.</p><p>Cora, CiteSeer, and PubMed are single graphs in which nodes and edges correspond to documents and citation links, respectively. A sparse feature vector (document keywords) and a class label are associated with each node. Several splits of these datasets are used in the node classification task. The first split, 20 instances are randomly selected for training, 500 for validation, and 1000</p><p>for the test; the rest of the labels are not used <ref type="bibr" target="#b22">[23]</ref>. In the second split, all nodes except 500+1000</p><p>validation and test nodes are used for the training <ref type="bibr" target="#b2">[3]</ref>. To evaluate the overfitting behavior, the third split exploits all labels for training excluding 500 + 500 nodes for the validation and test <ref type="bibr" target="#b14">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architectures</head><p>In the node classification using a NN followed by Softmax function (Eq. 5), the class with maximum probability is chosen to be the predicted node label. A 2-layer GCN with a 64dimensional hidden variable is used for comparing different optimization methods. In the first layer, the activation function ReLU is followed by a dropout function with a rate of 0.5. The loss function is evaluated as the negative log-likelihood of Softmax (Eq. 5) of the last layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization</head><p>The weights of parameters are initialized like the original GCN <ref type="bibr" target="#b9">[10]</ref> and input vectors are row-normalized accordingly <ref type="bibr" target="#b6">[7]</ref>. The model is trained for 200 epochs without any early stopping and the learning rate of 0.01. The Adam and SGD are used with the weight decay of 5 × 10 −4 and the momentum of 0.9, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>The optimization performance is measured by both the minimum validation cost and the test accuracy for the best validation cost. The validation cost of training a 2-layer GCN with a 64-dimensional hidden variable is used for comparing optimization methods (Adam and SGD)</p><p>with their preconditioned version (Adam-KFAC and SGD-KFAC). For each method, unlabeled samples are used in the training process with a ratio controlled by γ. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the validation cost of four methods based on Adam (upper row) and SGD (bottom row) for all three Citation datasets. The test accuracy of a 2-layer GCN trained using four different methods on three split are shown in Tab. III, IV, and V. Reported values of test accuracy in tables are averages and 95% confidence intervals over 10 runs for the best hyper-parameters tuned on the second split of the CiteSeer dataset. Note that the test accuracy may not always reflect the performance of the optimization method as the objective function (cross-entropy) is not the same as the prediction function (argmax). However, in most cases, the proposed method achieves better accuracy compared to Adam (the first row in all tables). As a fixed learning rate 0.01 is used in all methods, SGD has a very slow convergence and does not provide competitive results.</p><p>The importance of hyper-parameters , γ are shown in <ref type="figure" target="#fig_4">Fig. 2. Figures 2a and 2d</ref> depict the sensitivity of Adam and SGD to the parameter, respectively. As the inverse of directly affects the same factor as the learning rate η, the smaller the , the faster the convergence. However, choosing very small results in larger confidence intervals which are not desirable. The effect of γ on Adam and SGD are depicted in figures 2b and 2e, respectively. When using Adam, due to its faster convergence compared to SGD, smaller γ, i.e. using more predictions leads to much wider confidence intervals. In other words, the training process dominated by more labels results in a more stable convergence with a smaller variance. Thus, for a stable estimation, λ or γ must be tuned with respect to the optimization algorithm because of their sensitivity to  (e) SGD  <ref type="figure" target="#fig_4">Fig. 2a and 2d</ref> show that smaller results in a faster convergence with a probable cost of larger variance as it inversely scales the same factor as the learning rate. As depicted in <ref type="figure" target="#fig_4">Fig. 2b and 2e</ref>, the larger the γ, the more stable the convergence (the more confined confidence intervals). Finally, it can be seen in <ref type="figure" target="#fig_4">Fig. 2c and 2f</ref>  To examine the time complexity of the proposed method, the validation costs of Adam-KFAC and SGD-KFAC are compared with Adam and SGD when training on the second split of Citation datasets with respect to the training time for 200 epochs <ref type="figure" target="#fig_5">(Fig. 3)</ref>. The training on Cora and PubMed <ref type="figure" target="#fig_5">(Fig. 3b and 3c</ref>) takes a shorter time compared to the training on CitSeer <ref type="figure" target="#fig_5">(Fig. 3a)</ref> mainly because of the dimension of input features as it directly enlarges the size of the Fisher matrix. As shown in <ref type="figure" target="#fig_5">Fig. 3</ref>, the proposed SGD-KFAC method (red curve) converges much faster than the vanilla SGD as expected. Surprisingly, SGD-KFAC outperforms Adam and even Adam-KFAC methods in all datasets implying that the naive SGD with a natural gradient preconditioner can lead to a faster convergence than Adam-based methods. Another interesting observation is that Adam-based methods demonstrate similar performances in all experiments making them independent of the dataset while SGD-based methods show different overfitting behavior. shows the highest convergence rate among all other methods and it is slightly faster than Adam-KFAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we introduced a novel optimization framework for graph-based semi-supervised learning. After the distinct definition of semi-supervised problems with the adjacency distribution, we provided a comprehensive review of topics like semi-supervised learning, graph neural network, and preconditioning optimization (and NGD as its especial case). We adopted a commonly used probabilistic framework covering least-squared regression and cross-entropy classification. In the node classification task, our proposed method showed to improve Adam and SGD not only in the validation cost but also in the test accuracy of GCN on three splits of Citation datasets. Extensive experiments were provided on the sensitivity to hyper-parameters and the time complexity. As the first work, to the best of our knowledge, on the preconditioned optimization of graph neural networks, we not only achieved the best test accuracy but also empirically showed that it can be used with both Adam and SGD.</p><p>As the preconditioner may significantly affect Adam, illustrating the relation between NGD and Adam and effectively combining them can be a promising direction for future work. We also aim to deploy faster approximation methods than KFAC like <ref type="bibr" target="#b5">[6]</ref> and better sampling methods for exploiting unlabeled samples. Finally, since this work is mainly focused on single parameter layers, another possible research path would be adjusting KFAC to, for example, residual layers <ref type="bibr" target="#b7">[8]</ref>.</p><p>ACKNOWLEDGMENT YF and LL were partially supported by NSF grants DMS Career 1654579 and DMS 1854779.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F</head><label></label><figDesc>Fisher information matrix B Preconditioning matrix r(θ) The cost of parameters θ l(y,ŷ) The loss between y andŷ q(x) The source distribution q(y|x) The target distribution q(a|x, x ) The adjacency distribution p(y|f (X, A; θ)) The prediction distribution φ(·) An element-wise nonliear function ∇ θ f Gradient of scalar f wrt. θ J θ f Jacobian of vector f wrt. θ H θ f Hessian of scalar f wrt. θ Element-wise multiplication operation nodes in which the ith column of X shows the covariate at the node i and D = diag( j a ij )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The validation costs of four optimization methods on the second split of Citation datasets over 10 runs. A 2-layer GCN with a 64-dimensional hidden variable is used in all experiments. As shown inFig. 1a, 1b, and 1c (upper row), the proposed Adam-KDAC methods (green and red curves) outperform vanilla Adam methods (blue and orange curves) on all three</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>U 1 kand V − 1 k</head><label>11</label><figDesc>k and V k blocks are approximated with the expected values of u k,i u T k,i and v k,i v T k,i respectively where dim(u k ) = d k , dim(v k ) = d k−1 . Finally, U −are evaluated by taking inverses of U k + −1/2 and V k + −1/2 for being the regularization hyper-parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>A,y∼p(X,A,y;θ) [H θ l(y, f (X, A; θ))] (44) Semi-Supervised Preconditioning Require: ∇W k Gradient of parameters for k = 1, . . . , m Regularization hyper-parameters n = dim(z) n = (z) A = (D + I) −1/2 (A + I)(D + I) −1/2 = [ã ij ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>The sensitivity of , γ, and updating frequency on validation costs of Adam-KFAC (upper) and SGD-KFAC (below) when training on the second split of CiteSeer dataset over 10 runs. A 2-layer GCN with a 64-dimensional hidden variable is used in all experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>The validation costs of four optimization methods with respect to the training time on the second split of Citation datasets over 10 runs. A 2-layer GCN with a 64-dimensional hidden variable is used in all experiments. The proposed SGD-KFAC method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CITATION</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">NETWORK DATASETS STATISTICS</cell></row><row><cell>Dataset</cell><cell>Nodes</cell><cell cols="3">Edges Classes Features</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4732</cell><cell>6</cell><cell>3,703</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell></row><row><cell cols="3">Pubmed 19,717 44,338</cell><cell>3</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>TEST ACCURACY OF FOUR OPTIMIZATION METHODS ON THE FIRST SPLIT OF CITATION DATASETS OVER 10 RUNS. A 2-LAYER GCN WITH A 64-DIMENSIONAL HIDDEN VARIABLE IS USED IN ALL EXPERIMENTS.</figDesc><table><row><cell></cell><cell>CiteSeer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>Adam</cell><cell>71.66 ± 0.61</cell><cell>81.20 ± 0.25</cell><cell>79.72 ± 0.30</cell></row><row><cell>Adamγ</cell><cell>74.28 ± 0.67</cell><cell>82.42 ± 0.33</cell><cell>80.06 ± 0.34</cell></row><row><cell>Adam-KFAC</cell><cell>71.94 ± 0.53</cell><cell>81.68 ± 0.25</cell><cell>79.48 ± 0.28</cell></row><row><cell>Adam-KFACγ</cell><cell>70.24 ± 0.66</cell><cell>82.84 ± 0.87</cell><cell>76.94 ± 0.59</cell></row><row><cell>SGD</cell><cell>20.38 ± 8.92</cell><cell>23.14 ± 5.17</cell><cell>45.76 ± 3.04</cell></row><row><cell>SGDγ</cell><cell>17.64 ± 6.18</cell><cell>17.26 ± 8.41</cell><cell>46.20 ± 4.35</cell></row><row><cell>SGD-KFAC</cell><cell>71.82 ± 0.48</cell><cell>82.06 ± 0.34</cell><cell>77.20 ± 0.63</cell></row><row><cell>SGD-KFACγ</cell><cell>73.52 ± 0.22</cell><cell>81.70 ± 0.79</cell><cell>79.20 ± 0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV THE</head><label>IV</label><figDesc>TEST ACCURACY OF FOUR OPTIMIZATION METHODS ON THE SECOND SPLIT OF CITATION DATASETS OVER 10 RUNS. A 2-LAYER GCN WITH A 64-DIMENSIONAL HIDDEN VARIABLE IS USED IN ALL EXPERIMENTS. THE TEST ACCURACY OF FOUR OPTIMIZATION METHODS ON THE THIRD SPLIT OF CITATION DATASETS OVER 10 RUNS. A 2-LAYER GCN WITH A 64-DIMENSIONAL HIDDEN VARIABLE IS USED IN ALL EXPERIMENTS. the convergence rate. Since the Fisher matrix does not change considerably at each iteration, an experiment is performed to explore the sensitivity of validation loss to the frequency of updating Fisher. In Figures 2c and 2f, the validation cost over time is evaluated for updating Fisher every 4, 8, . . . , 128 iterations. When Fisher is updated more frequently, its computation takes more time hence the training process lasts longer (having other hyper-parameters fixed). Increasing the update frequency does not affect the performance to some extent, however, it largely reduces the training time. As updating Fisher every 50 or 100 iterations, does not affect the final validation cost to a great extent, to speed up the training process, Fisher is updated every 50 epochs in all of the experiments.</figDesc><table><row><cell>Validation Cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Validation Cost</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Validation Cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CiteSeer 12</cell><cell></cell><cell></cell><cell cols="2">Cora</cell><cell></cell><cell cols="3">Pubmed 150</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell cols="2">125</cell><cell>150</cell><cell>175</cell><cell>200</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell cols="2">Adam</cell><cell></cell><cell></cell><cell cols="4">78.68 ± 0.83</cell><cell cols="4">87.36 ± 0.47 Epoch</cell><cell cols="3">87.78 ± 0.14</cell><cell></cell><cell>Time (s)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Adam</cell><cell cols="3">Adamγ</cell><cell></cell><cell cols="8">77.98 ± 0.39 (b) Adam 87.28 ± 0.34</cell><cell cols="3">87.52 ± 0.30</cell><cell cols="2">(c) Adam</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Adam-KFAC</cell><cell cols="11">79.50 ± 0.15 87.60 ± 0.20 88.46 ± 0.24</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Adam-KFACγ</cell><cell cols="4">79.42 ± 0.32</cell><cell cols="4">86.60 ± 0.30</cell><cell cols="3">87.88 ± 0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Validation Cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SGD SGDγ SGD-KFAC</cell><cell cols="11">20.80 ± 2.12 20.96 ± 5.22 79.48 ± 0.40 87.54 ± 0.43 89.08 ± 0.18 31.90 ± 0.00 43.22 ± 1.42 31.90 ± 0.00 40.82 ± 0.33 Validation Cost 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8 1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SGD-KFACγ</cell><cell cols="4">77.32 ± 0.27 9 12</cell><cell cols="4">87.42 ± 0.24</cell><cell cols="3">88.18 ± 0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell></cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(d) SGD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CiteSeer</cell><cell></cell><cell></cell><cell cols="2">Cora</cell><cell></cell><cell cols="3">Pubmed</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Adam</cell><cell></cell><cell></cell><cell cols="4">79.80 ± 0.66</cell><cell cols="4">89.44 ± 0.41</cell><cell cols="3">87.16 ± 0.71</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Adamγ</cell><cell></cell><cell cols="4">79.64 ± 0.32</cell><cell cols="4">89.60 ± 0.91</cell><cell cols="3">87.44 ± 0.27</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Adam-KFAC</cell><cell cols="11">80.52 ± 0.14 90.16 ± 0.59 87.84 ± 0.21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Adam-KFACγ</cell><cell cols="4">80.52 ± 0.22</cell><cell cols="4">89.24 ± 0.64</cell><cell cols="3">87.36 ± 0.37</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SGD</cell><cell></cell><cell></cell><cell cols="4">15.04 ± 1.70</cell><cell cols="4">32.80 ± 0.00</cell><cell cols="3">41.96 ± 0.44</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SGDγ</cell><cell></cell><cell></cell><cell cols="4">16.12 ± 5.30</cell><cell cols="4">32.80 ± 0.00</cell><cell cols="3">41.20 ± 0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SGD-KFAC</cell><cell cols="11">79.76 ± 0.75 89.88 ± 0.14 89.36 ± 0.57</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SGD-KFACγ</cell><cell cols="4">78.52 ± 0.28</cell><cell cols="4">88.72 ± 0.38</cell><cell cols="3">87.88 ± 0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>that since performances are similar under different updating frequencies, selecting a relatively large frequency (50) can reduce the training time substantially.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>APPENDIX Lemma 1. The expected value of the Hessian of − log p(X, A, y; θ) is equal to Fisher informa-</p><p>Proof. The Hessian of f (θ) can be written as the Jacobian of ∇ θ f :</p><p>So for the Hessian of the negative log-likelihood becomes:</p><p>= − H θ p(X, A, y; θ).p(X, A, y; θ) p(X, A, y; θ).p(X, A, y; θ) (50) − ∇ θ p(X, A, y; θ)∇ θ p(X, A, y; θ) T p(X, A, y; θ).p(X, A, y; θ) (51)</p><p>Taking the expectation over p(X, A, y; θ), the first term turns into zero:</p><p>and Fisher is defined as the expected value of the second term.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast approximate natural gradient descent in a kronecker factored eigenbasis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9550" to="9560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Limitations of the empirical Fisher approximation for natural gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4156" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tutorial on Fisher information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="40" to="55" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">New insights and perspectives on the natural gradient method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1193</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI magazine</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
