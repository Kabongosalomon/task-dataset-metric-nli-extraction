<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dual-Source Approach for 3D Pose Estimation from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia, Simulation</orgName>
								<orgName type="laboratory">Virtual Reality Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Krüger</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Gokhale Method Institute</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia, Simulation</orgName>
								<orgName type="laboratory">Virtual Reality Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Dual-Source Approach for 3D Pose Estimation from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One major challenge for 3D pose estimation from a single RGB image is the acquisition of sufficient training data. In particular, collecting large amounts of training data that contain unconstrained images and are annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of images with annotated 2D poses and the second source consists of accurate 3D motion capture data. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient and robust 3D pose retrieval. In our experiments, we show that our approach achieves state-of-the-art results and is even competitive when the skeleton structure of the two sources differ substantially.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human 3D pose estimation from a single RGB image is a very challenging task. One approach to solve this task is to collect training data, where each image is annotated with the 3D pose. A regression model, for instance, can then be learned to predict the 3D pose from the image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. In contrast to 2D pose estimation, however, acquiring accurate 3D poses for an image is very elaborate. Popular datasets like HumanEva <ref type="bibr" target="#b22">[23]</ref> or Human3.6M <ref type="bibr" target="#b12">[13]</ref> synchronized cameras with a commercial marker-based system to obtain 3D poses for images. This requires a very expensive hardware setup and the requirements for marker-based system like studio environment and attached markers prevent the capturing of realistic images.</p><p>Instead of training a model on pairs consisting of an image and a 3D pose, we propose an approach that is able to incorporate 2D and 3D information from two different training sources. The first source consists of images with annotated 2D pose. Since 2D poses in images can be manually annotated, they do not impose any constraints regarding the environment from where the images are taken. Indeed any image from the Internet can be annotated and used. The * authors contributed equally second source is accurate 3D motion capture data captured in a lab, e.g., as in the CMU motion capture dataset <ref type="bibr" target="#b7">[8]</ref> or the Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref>. We consider both sources as independent, i.e., we do not know the 3D pose for any training image. To integrate both sources, we propose a dual-source approach as illustrated in <ref type="figure">Fig. 1</ref>. To this end, we first convert the motion capture data into a normalized 2D pose space, and separately learn a regressor for 2D pose estimation from the image data. During inference, we estimate the 2D pose and retrieve the nearest 3D poses using an approach that is robust to 2D pose estimation errors. We then jointly estimate a mapping from the 3D pose space to the image, identify wrongly estimated 2D joints, and estimate the 3D pose. During this process, the 2D pose can also be refined and the approach can be iterated to update the estimated 3D and 2D pose. We evaluate our approach on two popular datasets for 3D pose estimation. On both datasets, our approach achieves state-of-the-art results and we provide a thorough evaluation of the approach. In particular, we analyze the impact of differences of the skeleton structure between the two training sources, the impact of the accuracy of the used 2D pose estimator, and the impact of the similarity of the training and test poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A common approach for 3D human pose estimation is to utilize multiple images captured by synchronized cameras <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. The requirement of a multi-camera system in a controlled environment, however, limits the applicability of these methods. Since 3D human pose estimation from a single image is very difficult due to missing depth information, depth cameras have been utilized for human pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref>. However, current depth sensors are limited to indoor environments and cannot be used in unconstrained scenarios. Earlier approaches for monocular 3D human pose estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref> utilize discriminative methods to learn a mapping from local image features (e.g. HOG, SIFT, etc.) to 3D human pose or use a CNN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Since local features are sensitive to noise, these methods often assume that the location and scale of the human is given, e.g., in the form of an accurate bounding i </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Sources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Capture Dataset</head><p>Annotated 2D Images <ref type="figure">Figure 1</ref>: Overview. Our approach relies on two training sources. The first source is a motion capture database that contains only 3D poses. The second source is an image database with annotated 2D poses. The motion capture data is processed by pose normalization and projecting the poses to 2D using several virtual cameras. This gives many 3D-2D pairs where the 2D poses serve as features. The image data is used to learn a pictorial structure model (PSM) for 2D pose estimation where the unaries are learned by a random forest. Given a test image, the PSM predicts the 2D pose which is then used to retrieve the normalized nearest 3D poses. The final 3D pose is then estimated by minimizing the projection error under the constraint that the solution is close to the retrieved poses, which are weighted by the unaries of the PSM. The steps (red arrows) in the dashed box can be iterated by updating the binaries of the PSM using the retrieved poses and updating the 2D pose.</p><p>box or silhouette. While the approach <ref type="bibr" target="#b11">[12]</ref> still relies on the known silhouette of the human body, it partially overcomes the limitations of local image features by segmenting the body parts and using a second order hierarchical pooling process to obtain robust descriptors. Instead of predicting poses with a low 3D joint localization error, an approach for retrieving semantic meaningful poses is proposed in <ref type="bibr" target="#b18">[19]</ref>.</p><p>The 3D pictorial structure model (PSM) proposed in <ref type="bibr" target="#b13">[14]</ref> combines generative and discriminative methods. Regression forests are trained to estimate the probabilities of 3D joint locations and the final 3D pose is inferred by the PSM. Since inference is performed in 3D, the bounding volume of the 3D pose space needs to be known and the inference requires a few minutes per frame.</p><p>Besides of a-priori knowledge about bounding volumes, bounding boxes or silhouettes, these approaches require sufficient training images with annotated 3D poses. Since such training data is very difficult to acquire, we propose a dual-source approach that does not require training images with 3D annotations, but exploits existing motion capture datasets to estimate the 3D human pose.</p><p>Estimating 3D human pose from a given 2D pose by exploiting motion capture data has been addressed in a few works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. In <ref type="bibr" target="#b32">[33]</ref>, the 2D pose is manually annotated in the first frame and tracked in a video. A nearest neighbor search is then performed to retrieve the closest 3D poses. In <ref type="bibr" target="#b20">[21]</ref> a sparse representation of 3D human pose is constructed from a MoCap dataset and fitted to manually annotated 2D joint locations. The approach has been extended in <ref type="bibr" target="#b28">[29]</ref> to handle poses from an off-the-shelf 2D pose estimator <ref type="bibr" target="#b30">[31]</ref>. The same 2D pose estimator is also used in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> to constrain the search space of 3D poses. In <ref type="bibr" target="#b25">[26]</ref> an evolutionary algorithm is used to sample poses from the pose space that correspond to the estimated 2D joint positions. This set is then exhaustively evaluated according to some anthropometric constraints. The approach is extended in <ref type="bibr" target="#b24">[25]</ref> such that the 2D pose estimation and 3D pose estimation are iterated. In contrast to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref>, <ref type="bibr" target="#b24">[25]</ref> deals with 2D pose estimation errors. Our approach also estimates 2D and 3D pose but it is faster and more accurate than the sampling based approach <ref type="bibr" target="#b24">[25]</ref>.</p><p>Action specific priors learned from the MoCap data have also been proposed for 3D pose tracking <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref>. These approaches, however, are more constrained by assuming that the type of motion is known in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>In this work, we aim to predict the 3D pose from an RGB image. Since acquiring 3D pose data in natural environments is impractical and annotating 2D images with 3D pose data is infeasible, we do not assume that our training data consists of images annotated with 3D pose. Instead, we propose an approach that utilizes two independent sources of training data. The first source consists of motion capture data, which is publically available in large quantities and that can be recorded in controlled environments. The second source consists of images with annotated 2D poses, which is also available and can be easily provided by humans. Since we do not assume that we know any relations between the sources except that the motion capture data includes the poses we are interested in, we preprocess the sources first independently as illustrated in <ref type="figure">Fig. 1</ref>. From the image data, we learn a pictorial structure model (PSM) to predict 2D poses from images. This will be discussed in Section 4. The motion capture data is prepared to efficiently retrieve 3D poses that could correspond to a 2D pose. This part is described in Section 5.1. We will show that the retrieved poses are insufficient for estimating the 3D pose. Instead, we estimate the pose by minimizing the projection error under the constraint that the solution is close to the retrieved poses (Section 5.2). In addition, the retrieved poses can be used to update the PSM and the process can be iterated (Section 5.3). In our experiments, we show that we achieve very good results for 3D pose estimation with only one or two iterations.</p><p>The models for 2D pose estimation and the source code for 3D pose estimation are publicly available. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">2D Pose Estimation</head><p>In this work, we adopt a PSM that represents the 2D body pose x with a graph G = (J , L), where each vertex corresponds to 2D coordinates of a particular body joint i, and edges correspond to the kinematic constraints between two joints i and j. We assume that the graph is a tree structure which allows efficient inference. Given an image I, the 2D body pose is inferred by maximizing the following posterior distribution,</p><formula xml:id="formula_0">P (x|I) ∝ i∈J φ i (x i |I) (i,j)∈L φ i,j (x i , x j ),<label>(1)</label></formula><p>where the unary potentials φ i (x i |I) correspond to joint templates and define the probability of the i th joint at location x i . The binary potentials φ i,j (x i , x j ) define the deformation cost of joint i from its parent joint j. The unary potentials in (1) can be modeled by any discriminative model, e.g., SVM in <ref type="bibr" target="#b30">[31]</ref> or random forests in <ref type="bibr" target="#b8">[9]</ref>. In this work, we choose random forest based joint regressors. We train a separate joint regressor for each body joint. Following <ref type="bibr" target="#b8">[9]</ref>, we model binary potentials for each joint i as a Gaussian mixture model with respect to its parent j. We obtain the relative joint offsets between two adjacent joints in the tree structure and cluster them into c = 1, . . . , C clusters using k-means clustering. The offsets in each cluster are then modeled with a weighted Gaussian distribution as,</p><formula xml:id="formula_1">γ c ij exp − 1 2 d ij − µ c ij T Σ c ij −1 d ij − µ c ij<label>(2)</label></formula><p>with mean µ c ij , covariance Σ c ij and d ij = (x i −x j ). The weights γ c ij are set according to the cluster frequency p(c|i, j) α with a normalization constant α = 0.1 <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">3D Pose Estimation</head><p>While the PSM for 2D pose estimation is trained on the images with 2D pose annotations as shown in <ref type="figure">Fig. 1</ref>, we now describe an approach that makes use of a second dataset with 3D poses in order to predict the 3D pose from an image. Since the two sources are independent, we first have to establish relations between 2D poses and 3D poses. This is achieved by using an estimated 2D pose as query for 3D pose retrieval (Section 5.1). The retrieved poses, however, contain many wrong poses due to errors in 2D pose estimation, 2D-3D ambiguities and differences of the skeletons in the two training sources. It is therefore necessary to fit the 3D poses to the 2D observations. This will be described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">3D Pose Retrieval</head><p>In order to efficiently retrieve 3D poses for a 2D pose query, we preprocess the motion capture data. We first normalize the poses by discarding orientation and translation information from the poses in our motion capture database. We denote a 3D normalized pose with X and the 3D normalized pose space with Ψ. As in <ref type="bibr" target="#b32">[33]</ref>, we project the normalized poses X ∈ Ψ to 2D using orthographic projection. We use 144 virtual camera views with azimuth angles spanning 360 degrees and elevation angles in the range of 0 and 90 degree. Both angles are uniformly sampled with step size of 15 degree. We further normalize the projected 2D poses by scaling them such that the y-coordinates of the joints are within the range of [−1, 1]. The normalized 2D pose space is denoted by ψ and does not depend on a specific camera model or coordinate system. This step is illustrated in <ref type="figure">Fig. 1</ref>. After a 2D pose is estimated by the approach described in Section 4, we first normalize it according to ψ, i.e., we translate and scale the pose such that the y-coordinates of the joints are within the range of [−1, 1], then use it to retrieve 3D poses. The distance between two normalized 2D poses is given by the average Euclidean distance of the joint positions. The K-nearest neighbours in ψ are efficiently retrieved by a kd-tree <ref type="bibr" target="#b14">[15]</ref>. The retrieved normalized 3D poses are the corresponding poses in Ψ. An incorrect 2D pose estimation or even an imprecise estima-tion of a single joint position, however, can effect the accuracy of the 3D pose retrieval and consequently the 3D pose estimation. We therefore propose to use several 2D joint sets for pose retrieval where each joint set contains a different subset of all joints. The joint sets are shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. While J all contains all joints, the other sets J up , J lw , J lt and J rt contain only the joints of the upper body, lower body, left hand side and right hand side of the body, respectively. In this way we are able to compensate for 2D pose estimation errors, if at least one of our joint sets does not depend on the wrongly estimated 2D joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3D Pose Estimation</head><p>In order to obtain the 3D pose X, we have to estimate the unknown projection M from the normalized pose space Ψ to the image and infer which joint set J s explains the image data best. To this end, we minimize the energy</p><formula xml:id="formula_2">E(X, M, s) = ω p E p (X, M, s)+ω r E r (X, s)+ω a E a (X, s) (3) consisting of the three weighted terms E p , E r and E a .</formula><p>The first term E p (X, M, s) measures the projection error of the 3D pose X and the projection M:</p><formula xml:id="formula_3">E p (X, M, s) = i∈Js M (X i ) − x i 2 1 4 ,<label>(4)</label></formula><p>where x i is the joint position of the predicted 2D pose and X i is the 3D joint position of the unknown 3D pose. The parameter s defines the set of valid 2D joint estimates and the error is only computed for the joints of the corresponding joint set J s .</p><p>The second term enforces that the pose X is close to the retrieved 3D poses X k s for a joint set J s :</p><formula xml:id="formula_4">E r (X, s) = k w k,s i∈J all X k s,i − X i 2 1 4 .<label>(5)</label></formula><p>In contrast to (4), the error is computed over all joints but the set of nearest neighbors depends on s. In our experiments, we will show that an additional weighting of the nearest neighbors by w k,s improves the 3D pose estimation accuracy.</p><p>Although the term E r (X, s) penalizes already deviations from the retrieved poses and therefore enforces implicitly anthropometric constraints, we found it useful to add an additional term that enforces anthropometric constraints on the limbs:</p><formula xml:id="formula_5">E a (X, s) = k w k,s   (i,j)∈L L k s,i,j − L i,j 2   1 4 ,<label>(6)</label></formula><p>where L i,j denotes the limb length between two joints.</p><p>Minimizing the energy E(X, M, s) (3) over the discrete variable s and the continuous parameters X and M would be expensive. We therefore propose to obtain an approximate solution where we estimate the projection M first. For the projection, we assume that the intrinsic parameters are given and only estimate the global orientation and translation. The projectionM s is estimated for each joint set J s with s ∈ {up, lw, lt, rt, all} by minimizinĝ</p><formula xml:id="formula_6">M s = arg min M K k=1 E p (X k s , M, s)<label>(7)</label></formula><p>using non-linear gradient optimization. Given the estimated projectionsM s for each joint set, we then optimize over the discrete variable s:</p><formula xml:id="formula_7">s = arg min s∈{up,lw,lt,rt,all} K k=1 E(X k s ,M s , s) .<label>(8)</label></formula><p>As a result, we obtainŝ andM =Mŝ and finally minimizê</p><formula xml:id="formula_8">X = arg min X E(X,M,ŝ)<label>(9)</label></formula><p>to obtain the 3D pose.</p><p>Implementation details Instead of obtainingŝ by minimizing <ref type="bibr" target="#b7">(8)</ref>,ŝ can also be estimated by maximizing the posterior distribution for the 2D pose <ref type="bibr" target="#b0">(1)</ref>. To this end, we project all retrieved 3D poses to the image by</p><formula xml:id="formula_9">x k s,i =M s X k s,i .<label>(10)</label></formula><p>The binary potentials φ i,j (x i , x j |X s ), which are mixture of Gaussians, are then computed from the projected full body poses for each set andŝ is inferred by the maximum posterior probability:</p><formula xml:id="formula_10">(x,ŝ) = arg max x,s    i∈J φ i (x i |I) i,j∈L φ i,j (x i , x j |X s )    .<label>(11)</label></formula><p>Finally, the refined 2D posex is used to compute the projection error E p (X,M,ŝ) in <ref type="bibr" target="#b8">(9)</ref>.</p><p>In addition, we weight the nearest neighbors by</p><formula xml:id="formula_11">w k,s = i∈J φ i (x k s,i |I),<label>(12)</label></formula><p>to keep only the K w poses with the highest weights, and normalize them by</p><formula xml:id="formula_12">w k,s = w k,s − min k (w k ,s ) max k (w k ,s ) − min k (w k ,s ) .<label>(13)</label></formula><p>The dimensionality of X can be reduced by applying PCA to the weighted poses. We thoroughly evaluate the impact of the implementation details in Section 6.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Iterative Approach</head><p>The approach can be iterated by using the refined 2D posex <ref type="bibr" target="#b10">(11)</ref> as query for 3D pose retrieval (Section 5.1) as illustrated in <ref type="figure">Fig. 1</ref>. Having more than one iteration is not very expensive since many terms like the unaries need to be computed only once and the optimization of (7) can be initialized by the results of the previous iteration. The final pose estimation (9) also needs to be computed only once after the last iteration. In our experiments, we show that two iterations are sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate the proposed approach on two publicly available datasets, namely HumanEva-I <ref type="bibr" target="#b22">[23]</ref> and Hu-man3.6M <ref type="bibr" target="#b12">[13]</ref>. Both datasets provide accurate 3D poses for each image and camera parameters. For both datasets, we use a skeleton consisting of 14 joints, namely head, neck, ankles, knees, hips, wrists, elbows and shoulders. For evaluation, we use the 3D pose error as defined in <ref type="bibr" target="#b25">[26]</ref>. The error measures the accuracy of the relative pose up to a rigid transformation. To this end, the estimated skeleton is aligned to the ground-truth skeleton by a rigid transformation and the average 3D Euclidean joint error after alignment is measured. In addition, we use the CMU motion capture dataset <ref type="bibr" target="#b7">[8]</ref> as training source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation on HumanEva-I Dataset</head><p>We follow the same protocol as described in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref> and use the provided training data to train our approach while using the validation data as test set. As in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>, we report our results on every 5 th frame of the sequences walking (A1) and jogging (A2) for all three subjects (S1, S2, S3) and camera C1. For 2D pose estimation, we train regression forests and PSMs for each activity as described in <ref type="bibr" target="#b8">[9]</ref>. The regression forests for each joint consists of 8 trees, each trained on 700 randomly selected training images from a particular activity. While we use c = 15 mixtures per part (2) for the initial pose estimation, we found that 5 mixtures are enough for pose refinement (Section 5.2) since the retrieved 2D nearest neighbours strongly reduce the variation compared to the entire training data. In our experiments, we consider two sources for the motion capture data, namely HumanEva-I and the CMU motion capture dataset. We first evaluate the parameters of our approach using the entire 49K 3D poses of the HumanEva training set as motion capture data. Although the training data for 2D pose estimation and the 3D pose data are from the same dataset, the sources are separated and it is unknown which 3D pose corresponds to which image. S1(A1) S2(A1) S3(A1) S1(A2) S2(A2) S3(A2)  <ref type="figure">Figure 3</ref>: (a) Using only joint set J all . (b) Using all joint sets J s and estimatingŝ using <ref type="bibr" target="#b7">(8)</ref>. (c) All joint sets J s and estimatingŝ using <ref type="bibr" target="#b10">(11)</ref>. <ref type="bibr" target="#b15">16</ref> 32  <ref type="figure">Figure 4</ref>: Impact of the number of nearest neighbours K and weighting of nearest neighbours K w . The results are reported for subject S3 with walking action (A1, C1) using the CMU dataset (a-b) and HumanEva (c-d) for 3D pose retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Parameters</head><p>Joint Sets J . For 3D pose retrieval (Section 5.1), we use several joint sets J s with s ∈ {up, lw, lt, rt, all}. For the evaluation, we use only one iteration and K = 256 without weighting. The results in <ref type="figure">Fig. 3</ref> show the benefit of using several joint sets. Estimatingŝ using (11) instead of (8) also reduces the pose estimation error.</p><p>Nearest Neighbours. The impact of weighting the retrieved 3D poses and the number of nearest neighbours is evaluated in <ref type="figure">Fig. 4</ref>. The results show that the weighting reduces the pose estimation error independently of the used motion capture dataset. Without weighting more nearest neighbours are required. If not otherwise specified, we use K = 256 and K w = 64 for the rest of the paper. If the average of the retrieved K or K w poses is used instead of optimizing <ref type="bibr" target="#b8">(9)</ref>, the errors are 55.7mm and 48.9mm, respectively, as compared to 53.2mm and 47.5mm by optimizing <ref type="bibr" target="#b8">(9)</ref>. PCA can be used to reduce the dimension of X. <ref type="figure">Fig. 5(a)</ref> evaluates the impact of the number of principal components. Good results are achieved for 10-26 components, but the exact number is not critical. In our experiments, we use 18.</p><p>Energy Terms. The impact of the weights ω r , ω p and ω a in (3) is reported in <ref type="figure">Fig. 5(b-d)</ref>. Without the term E r , the  <ref type="figure">Figure 5</ref>: (a) Impact of the number of principal components. The error is reported for subject S3 with action jogging (A2, C1) using the CMU dataset for 3D pose retrieval. (b-d) Impact of the weights ω r , ω p and ω a in <ref type="formula">(3).</ref> error is very high. This is expected since the projection error E p is evaluated on the joint set Jŝ. If Jŝ does not contain all joints, the optimization is not sufficiently constrained without E r . Since E r is already weighted by the image consistency of the retrieved poses, E p does not result in a large drop of the error, but refines the 3D pose. The additional anthropometric constraints E a slightly reduce the error in addition. In our experiments, we use ω p = 0.55, ω r = 0.35, and ω a = 0.065.</p><p>Iterations. We finally evaluate the benefit of having more than one iteration (Section 5.3). <ref type="figure" target="#fig_4">Fig. 6</ref> compares the pose estimation error for one and two iterations. For completeness, the results for nearest neighbours without weighting are included. In both cases, a second iteration decreases the error on nearly all sequences. A third iteration does not reduce the error further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Comparison with State-of-the-art</head><p>In our experiments, we consider two sources for the motion capture data, namely HumanEva-I and the CMU motion capture dataset.</p><p>HumanEva-I Dataset. We first use the entire 49K 3D poses of the training data as motion capture data and compare our approach with the state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. Although the training data for 2D pose estimation and 3D pose data are from the same dataset, our approach considers them as two different sources and does not know the 3D pose for a training image. We report the 3D pose error for each sequence and the average error in <ref type="table">Table 1</ref>. While there is no method that performs best for all sequences, our approach outperforms all other methods in terms of average 3D pose error. The approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6]</ref> achieve a similar error, but they rely on stronger assumptions. In <ref type="bibr" target="#b13">[14]</ref> the ground-truth information S1(A1) S2(A1) S3(A1) S1(A2) S2(A2) S3(A2) 0  is used to compute a 3D bounding volume and the inference requires around three minutes per image since the approach uses a 3D PSM. The first iteration of our approach takes only 19 seconds per image 2 and additional 8 seconds for a second iteration.</p><p>In <ref type="bibr" target="#b5">[6]</ref> background subtraction is performed to obtain the human silhouette, which is used to obtain a tight bounding box. The approach also uses 20 joints instead of 14, which therefore results in a different 3D pose error. We therefore use the publicly available source code <ref type="bibr" target="#b5">[6]</ref> and evaluate the method for 14 joints and provide the human bounding box either from ground-truth data (GT-BB) or from our 2D pose estimation (Est-BB). The results in <ref type="table">Table 1</ref> show that the error significantly increases for <ref type="bibr" target="#b5">[6]</ref> when the same skeleton is used and the bounding box is not given but estimated.</p><p>CMU Motion Capture Dataset. In contrast to the other methods, we do not assume that the images are annotated by 3D poses but use motion capture data as a second training source. We therefore evaluate our approach using the CMU motion capture dataset <ref type="bibr" target="#b7">[8]</ref> for our 3D pose retrieval. We use one third of the CMU dataset and downsample the CMU dataset from 120Hz to 30Hz, resulting in 360K 3D poses. Since the CMU skeleton differs from the HumanEva skeleton, the skeletons are mapped to the HumanEva dataset by linear regression. The results are shown in <ref type="table">Table 1</ref>(b). As expected the error is higher due to the differences of the datasets, but the error is still low in comparison to the other methods.</p><p>To analyze the impact of the motion capture data more in detail, we have evaluated the pose error for various modifications of the data in <ref type="table" target="#tab_3">Table 2</ref>. We first remove the walking sequences from the motion capture data. The error increases for the walking sequences since the dataset does not contain poses related to walking sequences any more, but the error is still comparable with the other state-of-the-art methods ( <ref type="table">Table 1</ref>). The error for the jogging sequences actually decreases since the dataset contains less poses that are not related to jogging. In order to analyze how much of Methods Walking (A1, C1) Jogging (A2, C1) Average S1 S2 S3 S1 S2 S3 Kostrikov et al. <ref type="bibr" target="#b13">[14]</ref> 44.0 ± 15.9 <ref type="bibr" target="#b29">30</ref>  the difference between the HumanEva and the CMU motion capture data can be attributed to the skeleton, we mapped the HumanEva poses to the CMU skeletons. As shown in <ref type="table" target="#tab_3">Table 2</ref>(c), the error increases significantly. Indeed, over 60% of the error increase can be attributed to the difference of skeletons. In <ref type="table" target="#tab_5">Table 3</ref> we also compare the error of our refined 2D poses with other approaches. We report the 2D pose error for <ref type="bibr" target="#b8">[9]</ref>, which corresponds to our initial 2D pose estimation as described in Section 4. In addition, we also compare our method with <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref> using publicly available source codes. The 2D error is reduced by pose refinement using either of the two motion capture datasets and is lower than for the other methods. In addition, the error is further decreased by a second iteration. Some qualitative results are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation on Human3.6M Dataset</head><p>The protocol originally proposed for the Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref> uses the annotated bounding boxes and the training data only from the action class of the test data. Since this protocol simplifies the task due to the small pose</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Walking (A1, C1) Jogging (A2, C1) Avg. S1 S2 S3 S1 S2 S3 <ref type="bibr" target="#b8">[9]</ref> 9    variations for a single action class and the known scale, a more realistic protocol has been proposed in <ref type="bibr" target="#b13">[14]</ref> where the scale is unknown and the training data comprises all action classes. We follow the protocol <ref type="bibr" target="#b13">[14]</ref> and use every 64 th frame of the subject S11 for testing. Since the Human3.6M dataset comprises a very large number of training samples, we increased the number of regression trees for 2D pose estimation to 30 and the number of mixtures of parts to c = 40, where each tree is trained on 10K randomly selected training images. We use the same 3D pose error for evaluation and perform the experiments with 3D pose data from Human3.6M and the CMU motion capture dataset. In the first case, we use six subjects (S1, S5, S6, S7, S8 and S9) from Human3.6M and eliminate very similar 3D poses. We   <ref type="table">Table 5</ref>: The average 3D pose error (mm) on the Human3.6M dataset for all actions of subject S11.  consider two poses as similar when the average Euclidean distance of the joints is less than 1.5mm. This resulted in 380K 3D poses. In the second case, we use the CMU pose data as described in Section 6.1.2. The results are reported in <ref type="table" target="#tab_6">Tables 4 and 5</ref>. <ref type="table" target="#tab_6">Table 4</ref> shows that our approach outperforms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6]</ref>. On this datasets, a second iteration reduces the pose error by less than 1mm. <ref type="figure" target="#fig_8">Fig. 8</ref> provides a more detailed analysis and shows that more joints are estimated with an error below 100mm in comparison to the other methods. When using CMU motion capture dataset, the error is again higher due to differences of the datasets but still competitive.</p><p>We also investigated the impact of the accuracy of the initially estimated 2D poses. If we initialize the approach with the 2D ground-truth poses, the 3D pose error is drastically reduced as shown in <ref type="table" target="#tab_6">Table 4</ref>(b) and <ref type="figure" target="#fig_8">Fig. 8</ref>. This indicates that the 3D pose error can be further reduced by improving the used 2D pose estimation method. In <ref type="table" target="#tab_6">Table 4</ref>(c), we also report the error when the 3D poses of the test sequences are added to the motion capture dataset. While the error is reduced, the impact is lower compared to accurate 2D poses or differences of the skeletons (CMU). The error for each action class is given in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have presented a novel dual-source approach for 3D pose estimation from a single RGB image. One source is a MoCap dataset with 3D poses and the other source are images with annotated 2D poses. In our experiments, we demonstrate that our approach achieves stateof-the-art results when the training data are from the same dataset, although our approach makes less assumptions on training and test data. Our dual-source approach also allows to use two independent sources. This makes the approach very practical since annotating images with accurate 3D poses is often infeasible while 2D pose annotations of images and motion capture data can be collected separately without much effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different joint sets. J up is based on upper body joints, J lw lower body joints, J lt left body joints, J rt right body joints and J all is composed of all body joints. The selected joints are indicated by the large green circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Joint set: Jall (b) Joint set: Js with Eq. (8) (c) Joint set: Js with Eq. (11)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Impact of the number of iterations and weighting of nearest neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) 2D pose estimated as in Section 4 (b) 2D pose from groundtruth. (c) MoCap dataset includes 3D pose of subject S11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Four examples from HumanEva-I. From left to right: estimated 2D pose x (Section 4); retrieved 3D poses from all joint sets (Section 5.1); retrieved 3D poses from inferred joint set Jŝ (Section 5.2); retrieved 3D poses weighted by w k,ŝ (13); refined 2D posex<ref type="bibr" target="#b10">(11)</ref>; estimated 3D poseX (9) shown from two different views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>. Our method Iter−I (CMU). Our method Iter−I (H3.6M). Our method Iter−I (H3.6M) with 2D gt. Our method Iter−I (H3.6M) with 3D gt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Comparison on the Human3.6M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.9 ± 12.0 41.7 ± 14.9 57.2 ± 18.5 35.0 ± 9.9 33.3 ± 13.0 40.3 ± 14.0 Wang et al. [29] 71.9 ± 19.0 75.7 ± 15.9 85.3 ± 10.3 62.6 ± 10.2 77.7 ± 12.1 54.4 ± 9.0 71.3 ± 12.7 Radwan et al. [20] 75.1 ± 35.6 99.8 ± 32.6 93.8 ± 19.3 79.2 ± 26.4 89.8 ± 34.2 99.4 ± 35.1 89.5 ± 30.5 Simo-Serra et al. [25] 65.1 ± 17.4 48.6 ± 29.0 73.5 ± 21.4 74.2 ± 22.3 46.6 ± 24.7 32.2 ± 17.5 56.7 ± 22.0 Simo-Serra et al. [26] 99.6 ± 42.6 108.3 ± 42.3 127.4 ± 24.0 109.2 ± 41.5 93.1 ± 41.1 115.8 ± 40.6 108.9 ± 38.7 Results of the proposed approach with one or two iterations and motion capture data from the HumanEva-I dataset. (b) Results with motion capture data from the CMU dataset.</figDesc><table><row><cell cols="2">Bo et al. [6] (GT-BB)</cell><cell cols="2">46.4 ± 20.3 30.3 ± 10.5</cell><cell>64.9 ± 35.8</cell><cell>64.5 ± 27.5 48.0 ± 17.0 38.2 ± 17.7</cell><cell>48.7 ± 21.5</cell></row><row><cell cols="2">Bo et al. [6] (Est-BB)</cell><cell cols="2">54.8 ± 40.7 36.7 ± 20.5</cell><cell>71.3 ± 39.8</cell><cell>74.2 ± 47.1 51.3 ± 18.1 48.9 ± 34.2</cell><cell>56.2 ± 33.4</cell></row><row><cell>Bo et al. [6]*</cell><cell></cell><cell cols="2">38.2 ± 21.4 32.8 ± 23.1</cell><cell>40.2 ± 23.2</cell><cell>42.0 ± 12.9 34.7 ± 16.6 46.4 ± 28.9</cell><cell>39.1 ± 21.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Our Approach (MoCap from HumanEva dataset)</cell></row><row><cell>Iteration-I</cell><cell></cell><cell cols="2">40.1 ± 34.5 33.1 ± 27.7</cell><cell>47.5 ± 35.2</cell><cell>48.6 ± 33.3 43.6 ± 31.5 40.0 ± 27.9</cell><cell>42.1 ± 31.6</cell></row><row><cell>Iteration-II</cell><cell></cell><cell cols="2">35.8 ± 34.0 32.4 ± 26.9</cell><cell>41.6 ± 35.4</cell><cell>46.6 ± 30.4 41.4 ± 31.4 35.4 ± 25.2</cell><cell>38.9 ± 30.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Our Approach (MoCap from CMU dataset)</cell></row><row><cell>Iteration-I</cell><cell></cell><cell cols="2">54.5 ± 23.7 54.2 ± 21.4</cell><cell>64.2 ± 26.7</cell><cell>76.2 ± 23.8 74.5 ± 19.6 58.3 ± 23.7</cell><cell>63.6 ± 23.1</cell></row><row><cell>Iteration-II</cell><cell></cell><cell cols="2">52.2 ± 20.5 51.0 ± 15.1</cell><cell>62.8 ± 27.4</cell><cell>74.5 ± 23.2 72.4 ± 20.6 56.8 ± 21.4</cell><cell>61.6 ± 21.4</cell></row><row><cell cols="6">Table 1: Comparison with other state-of-the-art approaches on the HumanEva-I dataset. The average 3D pose error (mm) and</cell></row><row><cell cols="6">standard deviation are reported for all three subjects (S1, S2, S3) and camera C1. * denotes a different evaluation protocol.</cell></row><row><cell>(a) MoCap data</cell><cell cols="2">Walking (A1, C1) S1 S2 S3</cell><cell>Jogging (A2, C1) S1 S2 S3</cell><cell>Avg.</cell></row><row><cell>(a) HuEva</cell><cell cols="3">40.1 33.1 47.5 48.6 43.6 40.0</cell><cell>42.1</cell></row><row><cell cols="4">(b) HuEva\Walking 70.5 60.4 86.9 46.5 40.4 38.8</cell><cell>57.3</cell></row><row><cell>(c) HuEva-Retarget</cell><cell cols="3">59.5 43.9 63.4 61.0 51.2 55.7</cell><cell>55.8</cell></row><row><cell>(d) CMU</cell><cell cols="3">54.5 54.2 64.2 76.2 74.5 58.3</cell><cell>63.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Impact of the MoCap data.</figDesc><table><row><cell>(a) MoCap from Hu-</cell></row><row><cell>manEva dataset. (b) MoCap from HumanEva dataset with-</cell></row><row><cell>out walking sequences. (c) MoCap from HumanEva dataset</cell></row><row><cell>but skeleton is retargeted to CMU skeleton. (d) MoCap</cell></row><row><cell>from CMU dataset. The average 3D pose error (mm) is</cell></row><row><cell>reported for the HumanEva-I dataset with one iteration.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>2D pose estimation error (pixels) after refinement.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Our Approach</cell></row><row><cell>Methods</cell><cell>[14]</cell><cell>[6]</cell><cell cols="2">Human3.6M (Iter-I) CMU (Iter-I) (a) (b) (c)</cell></row><row><cell cols="4">3D Pose Error 115.7 117.9 108.3 70.5 95.2</cell><cell>124.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison on the Human3.6M dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://pages.iai.uni-bonn.de/iqbal_umar/ ds3dpose/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">2D pose estimation with a pyramid of 6 scales and scale factor 0.85 (10 sec.); 3D pose retrieval (0.12 sec.); estimating projection and 2D pose refinement (7.7 sec.); 3D pose estimation (0.15 sec.); image size 640×480 pixels; measured on a 12-core 3.2GHz Intel processor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Hashim Yasin gratefully acknowledges the Higher Education Commission of Pakistan for providing the financial support. The authors would also like to acknowledge the financial support from the DFG Emmy Noether program (GA 1927/1-1) and DFG research grant (KR 4309/2-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d human pose from silhouettes by relevance vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recovering 3d human pose from monocular images. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A data-driven approach for real-time full body pose reconstruction from a depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Twin gaussian processes for structured prediction. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast algorithms for large scale conditional 3d prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Carnegie mellon university graphics lab: Motion capture database</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note>mocap.cs.cmu.edu. 1, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonlinear body pose estimation from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterated Second-Order Label Sensitive Pooling for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast local and global similarity searches in large motion capture databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tautges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recovering 3d human body configurations using shape contexts. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Looselimbed people: Estimating 3d human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single Image 3D Human Pose Estimation from Noisy Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative density propagation for 3d human motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coupled action recognition and pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Model based full body human motion reconstruction from video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIRAGE</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
