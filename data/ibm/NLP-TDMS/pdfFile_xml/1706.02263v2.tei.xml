<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Matrix Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den Berg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam University of Amsterdam University of Amsterdam</orgName>
								<address>
									<settlement>CIFAR 1</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam University of Amsterdam University of Amsterdam</orgName>
								<address>
									<settlement>CIFAR 1</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam University of Amsterdam University of Amsterdam</orgName>
								<address>
									<settlement>CIFAR 1</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolutional Matrix Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the explosive growth of e-commerce and social media platforms, recommendation algorithms have become indispensable tools for many businesses. Two main branches of recommender algorithms are often distinguished: content-based recommender systems <ref type="bibr" target="#b23">[24]</ref> and collaborative filtering models <ref type="bibr" target="#b8">[9]</ref>. Content-based recommender systems use content information of users and items, such as their respective occupation and genre, to predict the next purchase of a user or rating of an item. Collaborative filtering models solve the matrix completion task by taking into account the collective interaction data to predict future ratings or purchases.</p><p>In this work, we view matrix completion as a link prediction problem on graphs: the interaction data in collaborative filtering can be represented by a bipartite graph between user and item nodes, with observed ratings/purchases represented by links. Content information can naturally be included in this framework <ref type="bibr" target="#b0">1</ref> Canadian Institute for Advanced Research in the form of node features. Predicting ratings then reduces to predicting labeled links in the bipartite useritem graph.</p><p>We propose graph convolutional matrix completion (GC-MC): a graph-based auto-encoder framework for matrix completion, which builds on recent progress in deep learning on graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>. The auto-encoder produces latent features of user and item nodes through a form of message passing on the bipartite interaction graph. These latent user and item representations are used to reconstruct the rating links through a bilinear decoder.</p><p>The benefit of formulating matrix completion as a link prediction task on a bipartite graph becomes especially apparent when recommender graphs are accompanied with structured external information such as social networks. Combining such external information with interaction data can alleviate performance bottlenecks related to the cold start problem. We demonstrate that our graph auto-encoder model efficiently combines interaction data with side information, without resorting to recurrent frameworks as in <ref type="bibr" target="#b21">[22]</ref>. The paper is structured as follows: in Section 2 we introduce our graph auto-encoder model for matrix completion. Section 3 discusses related work. Experimental results are shown in Section 4, and conclusion and future research directions are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Matrix completion as link prediction in bipartite graphs</head><p>Consider a rating matrix M of shape N u × N v , where N u is the number of users and N v is the number of items. Entries M ij in this matrix encode either an observed rating (user i rated item j) from a set of discrete possible rating values, or the fact that the rating is unobserved (encoded by the value 0). See <ref type="figure">Figure 1</ref> for an illustration. The task of matrix completion or recommendation can be seen as predicting the value of unobserved entries in M .  <ref type="figure">Figure 1</ref>: Left: Rating matrix M with entries that correspond to user-item interactions (ratings between <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> or missing observations (0). Right: User-item interaction graph with bipartite structure. Edges correspond to interaction events, numbers on edges denote the rating a user has given to a particular item. The matrix completion task (i.e. predictions for unobserved interactions) can be cast as a link prediction problem and modeled using an end-to-end trainable graph auto-encoder.</p><p>In an equivalent picture, matrix completion or recommendation can be cast as a link prediction problem on a bipartite user-item interaction graph. More precisely, the interaction data can be represented by an undirected graph G = (W, E, R) with entities consisting of a collection of user nodes</p><formula xml:id="formula_0">u i ∈ U with i ∈ {1, ..., N u }, and item nodes v j ∈ V with j ∈ {1, ..., N v }, such that U ∪ V = W.</formula><p>The edges (u i , r, v j ) ∈ E carry labels that represent ordinal rating levels, such as r ∈ {1, ..., R} = R. This connection was previously explored in <ref type="bibr" target="#b17">[18]</ref> and led to the development of graphbased methods for recommendation.</p><p>Previous graph-based approaches for recommender systems (see <ref type="bibr" target="#b17">[18]</ref> for an overview) typically employ a multistage pipeline, consisting of a graph feature extraction model and a link prediction model, all of which are trained separately. Recent results, however, have shown that results can often be significantly improved by modeling graph-structured data with end-to-end learning techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> and specifically with graph auto-encoders <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref> for unsupervised learning and link prediction. In what follows, we introduce a specific variant of graph auto-encoders for the task of recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph auto-encoders</head><p>We revisit graph auto-encoders which were originally introduced in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref> as an end-to-end model for unsupervised learning <ref type="bibr" target="#b29">[30]</ref> and link prediction <ref type="bibr" target="#b13">[14]</ref> on undirected graphs. We specifically consider the setup introduced in <ref type="bibr" target="#b13">[14]</ref>, as it makes efficient use of (convolutional) weight sharing and allows for inclusion of side information in the form of node features. Graph autoencoders are comprised of 1) a graph encoder model Z = f (X, A), which take as input an N ×D feature matrix X and a graph adjacency matrix A, and produce an N × E node embedding matrix Z = [z T 1 , . . . , z T N ] T , and 2) a pairwise decoder modelǍ = g(Z), which takes pairs of node embeddings (z i , z j ) and predicts respective entriesǍ ij in the adjacency matrix. Note that N denotes the number of nodes, D the number of input features, and E the embedding size.</p><p>For bipartite recommender graphs G = (W, E, R), we can reformulate the encoder as</p><formula xml:id="formula_1">[U, V ] = f (X, M 1 , . . . , M R ), where M r ∈ {0, 1}</formula><p>Nu×Nv is the adjacency matrix associated with rating type r ∈ R, such that M r contains 1's for those elements for which the original rating matrix M contains observed ratings with value r. U and V are now matrices of user and item embeddings with shape N u × E and N v × E, respectively. A single user (item) embedding takes the form of a real-valued vector U i,: (V j,: ) for user i (item j).</p><p>Analogously, we can reformulate the decoder asM = g(U, V ), i.e. as a function acting on the user and item embeddings and returning a (reconstructed) rating matrixM of shape N u × N v . We can train this graph auto-encoder by minimizing the reconstruction error between the predicted ratings inM and the observed ground-truth ratings in M . Examples of metrics for the reconstruction error are the root mean square error, or the cross entropy when treating the rating levels as different classes.</p><p>We shall note at this point that several recent stateof-the-art models for matrix completion <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> can be cast into this framework and understood as a special case of our model. An overview of these models is provided in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph convolutional encoder</head><p>In what follows, we propose a particular choice of encoder model that makes efficient use of weight sharing across locations in the graph and that assigns separate processing channels for each edge type (or rating type) r ∈ R. The form of weight sharing is inspired by a recent class of convolutional neural networks that operate directly on graph-structured data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>, in the sense that the graph convolutional layer performs local operations that only take the first-order neighborhood of a node into account, whereby the same transformation is applied across all locations in the graph.</p><p>This type of local graph convolution can be seen as a form of message passing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, where vector-valued messages are being passed and transformed across edges of the graph. In our case, we can assign a specific transformation for each rating level, resulting in edgetype specific messages µ j→i,r from items j to users i of the following form:</p><formula xml:id="formula_2">µ j→i,r = 1 c ij W r x j .<label>(1)</label></formula><p>Here, c ij is a normalization constant, which we choose to either be |N i | (left normalization) or |N i ||N j | (symmetric normalization) with N i denoting the set of neighbors of node i. W r is an edge-type specific parameter matrix and x j is the (initial) feature vector of node j. Messages µ i→j,r from users to items are processed in an analogous way. After the message passing step, we accumulate incoming messages at every node by summing over all neighbors N i,r under a specific edge-type r, and by subsequently accumulating them into a single vector representation:</p><formula xml:id="formula_3">h i = σ accum j∈Ni,1 µ j→i,1 , . . . , j∈N i,R µ j→i,R ,<label>(2)</label></formula><p>where accum(·) denotes an accumulation operation, such as stack(·), i.e. a concatenation of vectors (or matrices along their first dimension), or sum(·), i.e. summation of all messages. σ(·) denotes an element-wise activation function such as the ReLU(·) = max(0, ·).</p><p>To arrive at the final embedding of user node i, we transform the intermediate output h i as follows:</p><formula xml:id="formula_4">u i = σ(W h i ) .<label>(3)</label></formula><p>The item embedding v i is calculated analogously with the same parameter matrix W . In the presence of user-and item-specific side information we use separate parameter matrices for user and item embeddings. We will refer to (2) as a graph convolution layer and to (3) as a dense layer. Note that deeper models can be built by stacking several layers (in arbitrary combinations) with appropriate activation functions. In initial experiments, we found that stacking multiple convolutional layers did not improve performance and a simple combination of a convolutional layer followed by a dense layer worked best.</p><p>It is worth mentioning that the model demonstrated here is only one particular possible, yet relatively simple choice of an encoder, and other variations are potentially worth exploring. Instead of a simple linear message transformation, one could explore variations where µ j→i,r = nn(x i , x j , r) is a neural network in itself. Instead of choosing a specific normalization constant for individual messages, such as done here, one could employ some form of attention mechanism, where the individual contribution of each message is learned and determined by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bilinear decoder</head><p>For reconstructing links in the bipartite interaction graph we consider a bilinear decoder, and treat each rating level as a separate class. Indicating the reconstructed rating between user i and item j withM ij , the decoder produces a probability distribution over possible rating levels through a bilinear operation followed by the application of a softmax function:</p><formula xml:id="formula_5">p(M ij = r) = e u T i Qrvj s∈R e u T i Qsvj ,<label>(4)</label></formula><p>with Q r a trainable parameter matrix of shape E × E, and E the dimensionality of hidden user (item) repre-sentations u i (v j ). The predicted rating is computed asM</p><formula xml:id="formula_6">ij = g(u i , v j ) = E p(Mij =r) [r] = r∈R r p(M ij = r) .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model training</head><p>Loss function During model training, we minimize the following negative log likelihood of the predicted ratingsM ij :</p><formula xml:id="formula_7">L = − i,j;Ωij =1 R r=1 I[r = M ij ] log p(M ij = r) ,<label>(6)</label></formula><p>with I[k = l] = 1 when k = l and zero otherwise. The matrix Ω ∈ {0, 1} Nu×Ni serves as a mask for unobserved ratings, such that ones occur for elements corresponding to observed ratings in M , and zeros for unobserved ratings. Hence, we only optimize over observed ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node dropout</head><p>In order for the model to generalize well to unobserved ratings, it is trained in a denoising setup by randomly dropping out all outgoing messages of a particular node, with a probability p dropout , which we will refer to as node dropout. Messages are rescaled after dropout as in <ref type="bibr" target="#b27">[28]</ref>. In initial experiments we found that node dropout was more efficient in regularizing than message dropout. In the latter case individual outgoing messages are dropped out independently, making embeddings more robust against the presence or absence of single edges. In contrast, node dropout also causes embeddings to be more independent of particular user or item influences. We furthermore also apply regular dropout <ref type="bibr" target="#b27">[28]</ref> to the hidden layer units <ref type="formula" target="#formula_4">(3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-batching</head><p>We introduce mini-batching by sampling contributions to the loss function in Eq. (6) from different observed ratings. That is, we sample only a fixed number of contributions from the sum over user and item pairs. By only considering a fixed number of contributions to the loss function, we can remove respective rows of users and items in M 1 , ..., M R in Eq. (7) that do not appear in the current batch. This serves both as an effective means of regularization, and reduces the memory requirement to train the model, which is necessary to fit the full model for MovieLens-10M into GPU memory. We experimentally verified that training with mini-batches and full batches leads to comparable results for the MovieLens-1M dataset while adjusting for regularization parameters. For all datasets except for the MovieLens-10M, we opt for fullbatch training since it leads to faster convergence than training with mini-batches in this particular setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Vectorized implementation</head><p>In practice, we can use efficient sparse matrix multiplications, with complexity linear in the number of edges, i.e. O(|E|), to implement the graph auto-encoder model. The graph convolutional encoder (Eq. 3), for example in the case of left normalization, can be vectorized as follows:</p><formula xml:id="formula_8">U V = f (X, M 1 , . . . , M R ) = σ H u H v W T ,<label>(7)</label></formula><p>with</p><formula xml:id="formula_9">H u H v = σ R r=1 D −1 M r XW T r ,<label>(8)</label></formula><p>and</p><formula xml:id="formula_10">M r = 0 M r M T r 0</formula><p>.</p><p>The summation in <ref type="formula" target="#formula_9">(8)</ref> can be replaced with concatenation, similar to <ref type="bibr" target="#b1">(2)</ref>. In this case D denotes the diagonal node degree matrix with nonzero elements D ii = |N i |.</p><p>Vectorization for an encoder with a symmetric normalization, as well as vectorization of the bilinear decoder, follows in an analogous manner. Note that it is only necessary to evaluate observed elements inM , given by the mask Ω in Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Input feature representation and side information</head><p>Features containing information for each node, such as content information, can in principle be injected into the graph encoder directly at the input-level (i.e. in the form of an input feature matrix X). However, when the content information does not contain enough information to distinguish different users (or items) and their interests, feeding the content information directly into the graph convolution layer leads to a severe bottleneck of information flow. In such cases, one can include side information in the form of user and item feature vectors x f i (for node i) via a separate processing channel directly into the the dense hidden layer:</p><formula xml:id="formula_12">u i = σ(W h i + W f 2 f i ) with f i = σ(W f 1 x f i + b) ,<label>(10)</label></formula><p>where W f 1 and W f 2 are trainable weight matrices, and b is a bias. The weight matrices and bias vector are different for users and items. The input feature matrix X = [x T 1 , . . . , x T N ] T containing the node features for the graph convolution layer is then chosen as an identity matrix, with a unique one-hot vector for every node in the graph. For the datasets considered in this paper, the user (item) content information is of limited size, and we thus opt to include this as side information while using Eq. (10).</p><p>In <ref type="bibr" target="#b28">[29]</ref>, Strub et al. propose to include content information along similar lines, although in their case the proposed model is strictly user-or item-based, and thus only supports side information for either users or items.</p><p>Note that side information does not necessarily need to come in the form of per-node feature vectors, but can also be provided in the form of, e.g., graph-structured, natural language, or image data. In this case, the dense layer in <ref type="formula" target="#formula_2">(10)</ref> is replaced by an appropriate differentiable module, such as a recurrent neural network, a convolutional neural network, or another graph convolutional network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Weight sharing</head><p>In the collaborative filtering setting with one-hot vectors as input, the columns of the weight matrices W r play the role of latent factors for each separate node for one specific rating value r. These latent factors are passed onto connected user or item nodes through message passing. However, not all users and items necessarily have an equal number of ratings for each rating level. This results in certain columns of W r to be optimized significantly less frequently than others. Therefore, some form of weight sharing between the matrices W r for different r is desirable to alleviate this optimization problem. Following <ref type="bibr" target="#b31">[32]</ref>, we therefore implement the following weight sharing setup:</p><formula xml:id="formula_13">W r = r s=1 T s .<label>(11)</label></formula><p>We will refer to this type of weight sharing as ordinal weight sharing due to the increasing number of weight matrices included for higher rating levels.</p><p>As an effective means of regularization of the pairwise bilinear decoder, we resort to weight sharing in the form of a linear combination of a set of basis weight matrices P s :</p><formula xml:id="formula_14">Q r = n b s=1 a rs P s ,<label>(12)</label></formula><p>with s ∈ (1, ..., n b ) and n b being the number of basis weight matrices. Here, a rs are the learnable coefficients that determine the linear combination for each decoder weight matrix Q r . Note that in order to avoid overfitting and to reduce the number of parameters, the number of basis weight matrices n b should naturally be lower than the number of rating levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Auto-encoders User-or item-based auto-encoders <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref> are a recent class of state-of-the-art collabo-rative filtering models that can be seen as a special case of our graph auto-encoder model, where only either user or item embeddings are considered in the encoder. AutoRec by Sedhain et al. <ref type="bibr" target="#b26">[27]</ref> is the first such model, where the user's (or item's) partially observed rating vector is projected onto a latent space through an encoder layer, and reconstructed using a decoder layer with mean squared reconstruction error loss.</p><p>The CF-NADE algorithm by Zheng et al. <ref type="bibr" target="#b31">[32]</ref> can be considered as a special case of the above auto-encoder architecture. In the user-based setting, messages are only passed from items to users, and in the item-based case the reverse holds. Note that in contrast to our model, unrated items are assigned a default rating of 3 in the encoder, thereby creating a fully-connected interaction graph. CF-NADE imposes a random ordering on nodes, and splits incoming messages into two sets via a random cut, only one of which is kept. This model can therefore be seen as a denoising auto-encoder, where part of the input space is dropped out at random in every iteration.</p><p>Factorization models Many of the most popular collaborative filtering algorithms fall into the class of matrix factorization (MF) models. Methods of this sort assume the rating matrix to be well approximated by a low rank matrix: M ≈ U V T , with U ∈ R Nu×k and V ∈ R Ni×k , with k N u , N i . The rows of U and V can be seen as latent feature representations of users and items, representing an encoding for their interests through their rating pattern. Probabilistic matrix factorization (PMF) by Salakhutdinov et al. <ref type="bibr" target="#b19">[20]</ref> assumes that the ratings contained in M are independent stochastic variables with Gaussian noise. Optimization of the maximum likelihood then leads one to minimize the mean squared error between the observed entries in M and the reconstructed ratings in U V T . BiasedMF by Koren et al. <ref type="bibr" target="#b15">[16]</ref> improves upon PMF by incorporating a user and item specific bias, as well as a global bias. Neural network matrix factorization (NNMF) <ref type="bibr" target="#b6">[7]</ref> extends the MF approach by passing the latent user and item features through a feed forward neural network. Local low rank matrix approximation by Lee et al. <ref type="bibr" target="#b16">[17]</ref>, introduces the idea of reconstructing rating matrix entries using different (entry dependent) combinations of low rank approximations.</p><p>Matrix completion with side information In matrix completion (MC) <ref type="bibr" target="#b2">[3]</ref>, the objective is to approximate the rating matrix with a low-rank rating matrix. Rank minimization, however, is an intractable problem, and Candes &amp; Recht <ref type="bibr" target="#b2">[3]</ref> replaced the rank minimization with a minimization of the nuclear norm (the sum of the singular values of a matrix), turning the objective function into a tractable convex one. Inductive matrix completion (IMC) by Jain &amp; Dhillon, 2013 and Xu et al., 2013 incorporates content information of users and items in feature vectors and approximates the observed elements of the rating matrix as M ij = x T i U V T y j , with x i and y j representing the feature vector of user i and item j respectively.</p><p>The geometric matrix completion (GMC) model proposed by <ref type="bibr">Kalofolias et al. in 2014 [12]</ref> introduces a regularization of the MC model by adding side information in the form of user and item graphs. In <ref type="bibr" target="#b24">[25]</ref>, a more efficient alternating least squares optimization optimization method (GRALS) is introduced to the graph-regularized matrix completion problem. Most recently, Monti et al. <ref type="bibr" target="#b21">[22]</ref> suggested to incorporate graph-based side information in matrix completion via the use of convolutional neural networks on graphs, combined with a recurrent neural network to model the dynamic rating generation process. Their work is different from ours, in that we model the rating graph directly using a graph convolutional encoder/decoder approach that predicts unseen ratings in a single, noniterative step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on a number of common collaborative filtering benchmark datasets: MovieLens 2 (100K, 1M, and 10M), Flixster, Douban, and YahooMusic. The datasets consist of user ratings for items (such as movies) and optionally incorporate additional user/item information in the form of features. For Flixster, Douban, and YahooMusic we use preprocessed subsets of these datasets provided by <ref type="bibr" target="#b21">[22]</ref> 3 . These datasets contain sub-graphs of 3000 users and 3000 items and their respective user-user and item-item interaction graphs (if available). Dataset statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For all experiments, we choose from the following settings based on validation performance: accumulation function (stack vs. sum), whether to use ordinal weight sharing in the encoder, left vs. symmetric normalization, and dropout rate p dropout ∈ {0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. Unless otherwise noted, we use a Adam <ref type="bibr" target="#b12">[13]</ref> with a learning rate of 0.01, weight sharing in the decoder with 2 basis weight matrices, and layer sizes of 500 and 75 for the graph convolution (with ReLU) and dense layer (no activation function), respectively. We evaluate our model on the held out test sets using an exponential moving average of the learned model parameters with a decay factor set to 0.995. MovieLens 100K For this task, we compare against matrix completion baselines that make use of side information in the form of user/item features. We report performance on the canonical u1.base/u1.test train/test split. Hyperparameters are optimized on a 80/20 train/validation split of the original training set. Side information is present both for users (e.g. age, gender, and occupation) and movies (genres). Following Rao et al. <ref type="bibr" target="#b24">[25]</ref>, we map the additional information onto feature vectors for users and movies, and compare the performance of our model with (GC-MC+Feat) and without the inclusion of these features (GC-MC) . Note that GMC <ref type="bibr" target="#b11">[12]</ref>, GRALS <ref type="bibr" target="#b24">[25]</ref> and sRGCNN <ref type="bibr" target="#b21">[22]</ref> represent user/item features via a k-nearest-neighbor graph. We use stacking as an accumulation function in the graph convolution layer in Eq. (2), set dropout equal to 0.7, and use left normalization. GC-MC+Feat uses 10 hidden units for the dense side information layer (with ReLU activation) as described in Eq. 10. We train both models for 1,000 full-batch epochs. We report RMSE scores averaged over 5 runs with random initializations <ref type="bibr" target="#b3">4</ref> . Results are summarized in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MovieLens 1M and 10M</head><p>We compare against current state-of-the-art collaborative filtering algorithms, such as AutoRec <ref type="bibr" target="#b26">[27]</ref>, LLorma <ref type="bibr" target="#b16">[17]</ref>, and CF-NADE <ref type="bibr" target="#b31">[32]</ref>. Results are reported as averages over the same five 90/10 training/test set splits as in <ref type="bibr" target="#b31">[32]</ref> and summarized in <ref type="table" target="#tab_2">Table 4</ref>. Model choices are validated on an internal 95/5 split of the training set. For ML-1M we use accumulate messages through summation in Eq. (2), use a dropout rate of 0.7, and symmetric normalization. As ML-10M has twice the number of rating classes, we use twice the number of basis function matrices in the decoder. Furthermore, we use stacking accumulation, a dropout of 0.3 and symmetric normalization. We train for 3,500 full-batch epochs, and 18,000 mini-batch iterations (20 epochs with batch size 10,000) on the ML-1M and ML-10M dataset, respectively.</p><p>Flixster, Douban and YahooMusic These datasets contain user and item side information in the form of graphs. We integrate this graph-based side information into our framework by using the adjacency vector (normalized by degree) as a feature vector for the respective user/item. For a single dense feature embedding layer, this is equivalent to performing a graph convolution akin to <ref type="bibr" target="#b14">[15]</ref> on the user-user or itemitem graph. <ref type="bibr" target="#b4">5</ref> We use a dropout rate of 0.7, and 64 hidden units for the dense side information layer (with ReLU activation) as described in Eq. 10. We use a left normalization, and messages in the graph convolution layer are accumulated by concatenation (as opposed to summation). All models are trained for 200 epochs. For hyperparameter selection, we set aside a separate 80/20 train/validation split from the original training set in <ref type="bibr" target="#b21">[22]</ref>. For final model evaluation, we train on the full original training set from <ref type="bibr" target="#b21">[22]</ref> and report test set performance. Results are summarized in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-100K + Feat</head><p>MC <ref type="bibr" target="#b2">[3]</ref> 0.973 IMC <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref> 1.653 GMC <ref type="bibr" target="#b11">[12]</ref> 0.996 GRALS <ref type="bibr" target="#b24">[25]</ref> 0.945 sRGCNN <ref type="bibr" target="#b21">[22]</ref> 0.929 GC-MC (Ours) 0.910 GC-MC+Feat 0.905 <ref type="table">Table 2</ref>: RMSE scores 6 for the MovieLens 100K task with side information on a canonical 80/20 training/test set split. Side information is either presented as a nearest-neighbor graph in user/item feature space or as raw feature vectors. Baseline numbers are taken from <ref type="bibr" target="#b21">[22]</ref>.</p><p>Cold-start analysis To gain insight into how the GC-MC model makes use of side information, we study the performance of our model in the presence of users with only very few ratings (cold-start users). We adapt the ML-100K benchmark dataset, so that for a fixed number of cold-start users N c all ratings except for a minimum number N r are removed from the training set (chosen at random with a fixed seed across experiments). Note that ML-100K in its original form contains only users with at least 20 ratings.</p><p>We analyze model performance for N r ∈ {1, 5, 10} and 6 Results for our model slightly differ from the previous version of this paper, as we chose a different method for weight sharing in the encoder for consistency across experiments. See Section 2.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Flixster Douban YahooMusic GRALS  <ref type="table">Table 3</ref>: Average RMSE test set scores for 5 runs on Flixster, Douban, and YahooMusic, all of which include side information in the form of user and/or item graphs. We replicate the benchmark setting as in <ref type="bibr" target="#b21">[22]</ref>. For Flixster, we show results for both user/item graphs (right number) and user graph only (left number). Baseline numbers are taken from <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-1M ML-10M</head><p>PMF <ref type="bibr" target="#b19">[20]</ref> 0.883 -I-RBM <ref type="bibr" target="#b25">[26]</ref> 0.854 0.825 BiasMF <ref type="bibr" target="#b15">[16]</ref> 0.845 0.803 NNMF <ref type="bibr" target="#b6">[7]</ref> 0.843 -LLORMA-Local <ref type="bibr" target="#b16">[17]</ref> 0.833 0.782 I-AUTOREC <ref type="bibr" target="#b26">[27]</ref> 0.831 0.782 CF-NADE <ref type="bibr" target="#b31">[32]</ref> 0.829 0.771 GC-MC (Ours) 0.832 0.777  <ref type="bibr" target="#b31">[32]</ref>) without the use of side information. Baseline scores are taken from <ref type="bibr" target="#b31">[32]</ref>. For CF-NADE, we report the bestperforming model variant.  N c ∈ {0, 50, 100, 150}, both with and without using user/item features as side information (see <ref type="figure" target="#fig_3">Figure 3</ref>). Hyperparameters and test set are chosen as before, i.e. we report RMSE on the complete canonical test set split. The benefit of incorporating side information, such as user and item features, is especially pronounced in the presence of many users with only a single rating.</p><p>Discussion On the ML-100K task with side information, our model outperforms related methods by a significant margin. Remarkably, this is even the case without the use of side information. Most related to our method is sRGCNN by Monti et al. <ref type="bibr" target="#b21">[22]</ref> that uses graph convolutions on the nearest-neighbor graphs of users and items, and learns representations in an iterative manner using recurrent neural networks. Our results demonstrate that a direct estimation of the rating matrix from learned user/item representations using a simple decoder model can be more effective, while being computationally more efficient.</p><p>Our results on ML-1M and ML-10M demonstrate that it is possible to scale our method to larger datasets, putting it into the vicinity of recent state-of-the-art collaborative filtering user-or item-based methods in terms of predictive performance. At this point, it is important to note that several techniques introduced in CF-NADE <ref type="bibr" target="#b31">[32]</ref>, such as layer-specific learning rates, a special ordinal loss function, and the auto-regressive modeling of ratings, can be seen as orthogonal to our approach and can be used in conjunction with our framework.</p><p>For the Flixster, Douban, and YahooMusic datasets our model achieves state-of-the-art results, while using a single hyperparameter setting across all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we have introduced graph convolutional matrix completion (GC-MC): a graph auto-encoder framework for the matrix completion task in recommender systems. The encoder contains a graph convolution layer that constructs user and item embeddings through message passing on the bipartite user-item interaction graph. Combined with a bilinear decoder, new ratings are predicted in the form of labeled edges.</p><p>The graph auto-encoder framework naturally generalizes to include side information for both users and items. In this setting, our proposed model outperforms recent related methods by a large margin, as demonstrated on a number of benchmark datasets with feature-and graph-based side information. We further show that our model can be trained on larger scale datasets through stochastic mini-batching. In this setting, our model achieves results that are competitive with recent state-of-the-art collaborative filtering.</p><p>In future work, we wish to extend this model to largescale multi-modal data (comprised of text, images, and other graph-based information), such as present in many realisitic recommendation platforms. In such a setting, the GC-MC model can be combined with recurrent (for text) or convolutional neural networks (for images). To address scalability, it is necessary to develop efficient approximate schemes, such as subsampling local neighborhoods <ref type="bibr" target="#b9">[10]</ref>. Finally, attention mechanisms <ref type="bibr" target="#b0">[1]</ref> provide a promising future avenue for extending this class of models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of a forward-pass through the GC-MC model, which is comprised of a graph convolutional encoder [U, V ] = f (X, M 1 , . . . , M R ) that passes and transforms messages from user to item nodes, and vice versa, followed by a bilinear decoder model that predicts entries of the (reconstructed) rating matrixM = g(U, V ), based on pairs of user and item embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Cold-start analysis for ML-100K. Test set RMSE (average over 5 runs with random initialization) for various settings, where only a small number of ratings N r is kept for a certain number of cold-start users N c during training. Standard error is below 0.001 and therefore not shown. Dashed and solid lines denote experiments without and with side information, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of users, items and ratings for each of the MovieLens datasets used in our experiments. We further indicate rating density and rating levels.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Users Items</cell><cell>Features</cell><cell cols="3">Ratings Density Rating levels</cell></row><row><cell>Flixster</cell><cell>3,000</cell><cell cols="2">3,000 Users/Items</cell><cell>26,173</cell><cell>0.0029</cell><cell>0.5, 1, . . . , 5</cell></row><row><cell>Douban</cell><cell>3,000</cell><cell>3,000</cell><cell>Users</cell><cell>136,891</cell><cell>0.0152</cell><cell>1, 2, . . . , 5</cell></row><row><cell>YahooMusic</cell><cell>3,000</cell><cell>3,000</cell><cell>Items</cell><cell>5,335</cell><cell>0.0006</cell><cell>1, 2, . . . , 100</cell></row><row><cell>MovieLens 100K (ML-100K)</cell><cell>943</cell><cell cols="2">1,682 Users/Items</cell><cell>100,000</cell><cell>0.0630</cell><cell>1, 2, . . . , 5</cell></row><row><cell>MovieLens 1M (ML-1M)</cell><cell>6,040</cell><cell>3,706</cell><cell>-</cell><cell>1,000,209</cell><cell>0.0447</cell><cell>1, 2, . . . , 5</cell></row><row><cell>MovieLens 10M (ML-10M)</cell><cell cols="2">69,878 10,677</cell><cell cols="2">-10,000,054</cell><cell>0.0134</cell><cell>0.5, 1, . . . , 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Comparison of average test RMSE scores on five 90/10 training/test set splits (as in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://grouplens.org/datasets/movielens/ 3 https://github.com/fmonti/mgcnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Standard error less than 0.001.<ref type="bibr" target="#b4">5</ref> With a row-normalized adjacency matrix instead of the symmetric normalization from<ref type="bibr" target="#b14">[15]</ref>. Both perform similarly in practice.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Jakub Tomczak, Christos Louizos, Karen Ullrich and Peter Bloem for helpful discussions and comments. This project is supported by the SAP Innovation Center Network.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06443</idno>
		<title level="m">Neural network matrix factorization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using collaborative filtering to weave an information tapestry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Oki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<title level="m">ductive representation learning on large graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0626</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Provable inductive matrix completion. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.1717</idno>
		<title level="m">Matrix completion on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Local low-rank matrix approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<editor>Sanjoy Dasgupta and David McAllester</editor>
		<meeting>the 30th International Conference on Machine Learning (ICML)<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="17" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recommendation as link prediction in bipartite graphs: A graph kernel-based machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="880" to="890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model cnns. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Content-based recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Billsus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The adaptive web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hybrid recommender system based on autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Gaudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
	<note>DLRS 2016</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Speedup matrix completion with side information: Application to multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2301" to="2309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
