<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Multi-Person Pose Estimation and Semantic Part Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangting</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<email>alan.yuille@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>90095, 21218</postCode>
									<settlement>Los Angeles, Baltimore</settlement>
									<region>CA, MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Multi-Person Pose Estimation and Semantic Part Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation and semantic part segmentation are two complementary tasks in computer vision. In this paper, we propose to solve the two tasks jointly for natural multi-person images, in which the estimated pose provides object-level shape prior to regularize part segments while the part-level segments constrain the variation of pose locations. Specifically, we first train two fully convolutional neural networks (FCNs), namely Pose FCN and Part FCN, to provide initial estimation of pose joint potential and semantic part potential. Then, to refine pose joint location, the two types of potentials are fused with a fully-connected conditional random field (FCRF), where a novel segment-joint smoothness term is used to encourage semantic and spatial consistency between parts and joints. To refine part segments, the refined pose and the original part potential are integrated through a Part FCN, where the skeleton feature from pose serves as additional regularization cues for part segments. Finally, to reduce the complexity of the FCRF, we induce human detection boxes and infer the graph inside each box, making the inference forty times faster.</p><p>Since there's no dataset that contains both part segments and pose labels, we extend the PASCAL VOC part dataset [6] with human pose joints 1 and perform extensive experiments to compare our method against several most recent strategies. We show that our algorithm surpasses competing methods by 10.6% in pose estimation with much faster speed and by 1.5% in semantic part segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation (i.e. predicting the position of joints for each human instance) and semantic part segmentation (i.e. decomposing humans into semantic part regions) are two crucial and correlated tasks in analysing humans from images. They provide richer representations for many 1 https://sukixia.github.io/paper.html dependent tasks, e.g. fine-grained recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17]</ref>, action recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref>, image/video retrieval <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16]</ref>, person-identification <ref type="bibr" target="#b23">[24]</ref> and video surveillance <ref type="bibr" target="#b22">[23]</ref>.</p><p>Recently, dramatic progress has been made on pose estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref> and human part segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20]</ref> with the advent of powerful convolutional neural networks (CNN) <ref type="bibr" target="#b18">[19]</ref> and the availability of pose/segment annotations on large-scale datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>. However, the two tasks are mostly solved independently without considering their correlations. As shown in the middle column in <ref type="figure" target="#fig_0">Fig. 1</ref>, for pose estimation, by designing loss w.r.t. the joints solely, it may omit the knowledge of dense pixel-wise part appearance coherence, yielding joints located outside of human instance or misleading joints when two people are close to each other. On the other hand, for part segmentation, through training that only respects pixel-wise part labels, it lacks proper overall human shape regularization, yielding missing/errorneous predictions when appearance cues are weak or missing.</p><p>In fact, the two tasks are complementary, and solving them jointly can reduce the learning difficulty in addressing each of them individually. As shown in the right column <ref type="figure" target="#fig_0">Fig. 1</ref>, by handling the two tasks jointly, the ambiguity in pose estimation (e.g. out of instance region) can be corrected by considering semantic part segments, while the estimated pose skeleton provides object-level context and regularity to help part segments align with human instances, e.g. over the details of arms and legs where appearance cues are missing.</p><p>Specifically, we illustrate our framework in <ref type="figure" target="#fig_1">Fig. 2</ref>. Firstly, given an image that contains multiple people, we train two FCNs: Pose FCN and Part FCN. Similar to <ref type="bibr" target="#b14">[15]</ref>, the Pose FCN outputs the pixel-wise joint score map, i.e. the potential of joints at each pixel (how likely a type of joint is located at certain pixel), and also outputs the joint neighbour score map, i.e. the potential of the location likelihood of neighboring joints for each joint type. The Part FCN produces the part score map for each semantic part type. Secondly, the three types of information are fused through a FCRF to refine the human joint locations, where a novel smoothness term on both part segments and joint proposals (generated from the initially estimated pixel-wise joint score map) are applied to encourage the consistency between segments and joints. Thirdly, the refined pose joints are re-organized into pose features that encode overall shape information, and are fed into a second-stage Part FCN as an additional input besides the initial part score map, yielding better segmentation results. To reduce the complexity of the FCRF, rather than infer over the full image as <ref type="bibr" target="#b14">[15]</ref>, we adopt a human detector <ref type="bibr" target="#b25">[26]</ref> to first get the bounding box for each human instance and resize each instance region in a similar way to <ref type="bibr" target="#b32">[33]</ref>. Our whole inference procedure is then performed within each resized region.</p><p>Last but not the least, in order to train and evaluate our method, we augment the challenging PASCAL-Person-Part dataset <ref type="bibr" target="#b5">[6]</ref> with 14 human pose joint locations through manual labeling and make the annotations public. This dataset includes 3533 images that contain large variation of human poses, scales and occlusion. We evaluate our method over this dataset, and show that our approach outperforms the most recent competing methods for both tasks. In particu-lar, our method is more effective and much faster (8 seconds versus 4 minutes) than DeeperCut <ref type="bibr" target="#b14">[15]</ref> which is arguably the most effective algorithm for multi-person pose estimation.</p><p>In summary, the contributions of this paper lay in three folds: (1) to our best knowledge, we are the first to explore and demonstrate the complementary property of multiperson pose estimation and part segmentation with deep learned potentials; (2) by combining detection boxes in the pipeline, we reduce the complexity of FCRF inference over the full image, yielding better efficiency; (3) we extend the well labelled PASCAL-Person-Part dataset with human joints and demonstrate the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Pose estimation. Traditional approaches use graphical models to combine spatial constraints with local observations of joints, based on low-level features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>. With the growing popularity of deep learning, recent methods rely on strong joint detectors trained by DCNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>, and often use a simple graphical model (e.g. tree model, And-Or graph) to select and assemble joints into a valid pose configuration. These recent methods perform much better than traditional ones, but the localization of joints is still inaccurate (e.g. sometimes outside the human body) and they still struggle when there are multiple people overlapping each other. Other approaches discard graphical models by modeling the spatial dependencies of joints within DC-NNs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>. These approaches perform well on relatively simple datasets, but their ability to handle large pose variations in natural multi-person datasets is limited. A very recent work, Deeper-Cut <ref type="bibr" target="#b14">[15]</ref>, addresses the multi-person issue explicitly, using integer linear programming to cluster joint candidates into multiple human instances and assign joint types to each joint candidate. Deeper-Cut handles multi-person overlapping well, but is very time-consuming (4 minutes per image) and its performance on datasets with large scale variation is not fully satisfactory. Our method improves in these aspects by introducing a segment-joint consistency term that yields better localization of flexible joints such as wrists and ankles, and an effective scale handling strategy (using detected boxes and smart box rescaling) that can deal with humans of different sizes.</p><p>Semantic part segmentation. Previous approaches either use graphical models to select and assemble region proposals <ref type="bibr" target="#b33">[34]</ref>, or use fully convolutional neural networks (FCNs) <ref type="bibr" target="#b21">[22]</ref> to directly produce pixel-wise part labels. Traditional graphical models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b10">11]</ref> find it difficult to handle the large variability of pose and occlusion in natural images. FCN-type approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, though simple and fast, give coarse part details due to FCN's inherent invariance property, and can have local confusion errors (e.g. la-  <ref type="bibr" target="#b4">[5]</ref> to combine the part segmentation results of three fixed scales. Xia et al. build an hierarchical model that adapts to object scales and part scales using "auto-zoom" <ref type="bibr" target="#b32">[33]</ref>. We treat these two methods as our baselines, and demonstrate the advantages of our part segmentation approach. Most recently, researchers design and adopt more powerful network architectures such as Graph Long Short-Term Memory (LSTM) <ref type="bibr" target="#b19">[20]</ref> and DeepLab with Deep Residual Net <ref type="bibr" target="#b3">[4]</ref>, greatly improving the performance. We prove that our method is complementary and can be added to these networks to further improve the performance.</p><p>Joint pose estimation and part segmentation. Yamaguchi et al. perform pose estimation and semantic part segmentation sequentially for clothes parsing, using a CRF with low-level features <ref type="bibr" target="#b34">[35]</ref>. Ladicky et al. combine the two tasks in one principled formulation, using also lowlevel features <ref type="bibr" target="#b17">[18]</ref>. Dong et al. combine the two tasks with a manually designed And-Or graph <ref type="bibr" target="#b9">[10]</ref>. These methods demonstrate the complementary properties of the two tasks on relatively simple datasets, but they cannot deal with images with large pose variations or multi-person overlapping, mainly due to the less powerful features they use or the poor quality of their part region proposals. In contrast, our model combines FCNs with graphical models, greatly boosting the representation power of models to handle large pose variation. We also introduce novel part segment consistency terms for pose estimation and novel pose consistency terms for part segmentation, further improving the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Given an image I with size h×w, our task is to output a pixel-wise part segmentation map L s , and a list of scored pose configurations C p = {(c i , s i )|i = 1, 2, . . . , k i }, where c i is the location of all 14 pose joint types for the person and s i is the score of this pose configuration.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, for each human detection box, we first use Pose FCN and Part FCN to give initial estimation of pose location and part segmentation. Then a FCRF is used to refine pose estimation and a second-stage Part FCN is adopted for part refinement. Specifically, we first extract human bounding boxes with Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>, and resize the image region within each detection box following <ref type="bibr" target="#b32">[33]</ref> so that small people are enlarged and extra large people are shrunk to a fixed size. The resized box regions serve as input to Pose FCN and Part FCN. Pose FCN adopts the network architecture of ResNet-101 proposed in <ref type="bibr" target="#b13">[14]</ref>, while for Part FCN we use DeepLab-LargeFOV <ref type="bibr" target="#b2">[3]</ref>.</p><p>Pose FCN outputs two feature maps: (1) the pixel-wise joint score map P j , which is a matrix with shape h×w× 14 representing the probability of each joint type locating at each pixel. (2) the pixel-wise joint neighbor score map P n , which is a h×w×364 matrix representing the probability of expected neighbor location for each joint. Here, the dimension of 364 is obtained by 14×13×2, which means for each joint the we estimate the other 13 joint locations using the offset (δx, δy). Following the definition of parts in <ref type="bibr" target="#b2">[3]</ref>, Part FCN outputs a part score map P s including 7 classes: 6 part labels and 1 background label.</p><p>Given the three score maps, we design a novel segmentjoint smoothness term for our FCRF to obtain refined pose estimation results (detailed in Sec. 3.1). To obtain better part segmentation results, we further design a second-stage Part FCN, which takes joint input of first-stage part scores and derived feature maps from refined poses (detailed in Sec. 3.2). Finally, the estimated poses from each bounding box are merged through a Non-Maximum Suppression (NMS) strategy detailed in Sec. 4.1. For part segmentation, we merge the segment score map from different boxes using score averaging similar to <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Human Pose Estimation</head><p>In this section, we explain how we unify the three score maps (i.e. P j , P n and P s ) to estimate poses in each human detection box.</p><p>Following DeeperCut <ref type="bibr" target="#b14">[15]</ref>, we adopt a FCRF to obtain robust context for assembling the proposed joints into human instances. To reduce the complexity of the FCRF, rather than consider all the pixels, we generate 6 candidate locations for each joint from the joint score map P j by non-maximum suppression (NMS). Formally, the FCRF for the graph is formulated as G = {V, E}, where the node set V = {c 1 , c 2 , . . . , c n } represents all the candidate locations of joints and the edge set E = {(c i , c j )|i = 1, 2, . . . , n, j = 1, 2, . . . , n, i &lt; j} is the edges connecting all the locations. The label to predict for each node is its joint type l ci ∈ {0, · · · , K}, where K = 14 is the number of joint types and type 0 represents that the node belongs to background and is not selected. Besides, we also predict whether two nodes belong to the same person, i.e. l ci,cj ∈ {0, 1}, where 1 indicates the two nodes are for the same person. Let L = {l ci |c i ∈ V} ∪ {l ci,cj |(c i , c j ) ∈ E}. The target we want to optimize is:</p><formula xml:id="formula_0">min L ci∈V ψ i (l ci ) + (ci,cj )∈E ψ i,j (l ci , l cj , l ci,cj ) (1)</formula><p>where the unary term is defined as ψ i = log</p><formula xml:id="formula_1">1−Pj (lc i )</formula><p>Pj (lc i ) , which is a log-likelihood at location c i based on the Pose-CNN output, the joint score map P j .</p><p>In contrast, the pairwise term is determined by both the joint neighbor score map P n and the segmentation score map P s . Formally,</p><formula xml:id="formula_2">ψ i,j = l ci,cj log 1 − P i,j (l ci , l cj |P n , P s ) P i,j (l ci , l cj |P n , P s )<label>(2)</label></formula><p>where P i,j (l ci , l cj ) = 1 1+exp(−ω·f (ci,cj ,lc i ,lc j )) , obtained from logistic regression results w.r.t. a combined feature vector f from f (P n ) and f (P s ), in which we omit c i , c j , l ci , l cj for simplicity.</p><p>The feature vector f (P n ) encodes information to help decide whether the two proposals belong to the same person. We borrow the idea proposed in <ref type="bibr" target="#b14">[15]</ref>, and here we explain how the feature is extracted for paper completeness. Given the location of two joint proposals c i , c j and their corresponding label l ci , l cj , we first derive a direct vector from c i to c j , denoted as v i,j . In addition, given c i , l ci , l cj , based on the joint neighbor offset score map P n , we may find an estimated location of l cj respecting c i though computing c j = c i + (δx, δy) i,j . We denote the direct vector from c i to the estimated location as v i,j . Similar vectors v j,i , v j,i can be extracted in the same way.</p><formula xml:id="formula_3">Feature f (P n ) = [ |v j,i − v j,i |, |v i,j − v i,j |, &lt; v j,i , v j,i &gt;, &lt; v i,j , v i,j &gt; ]</formula><p>, in which |. − .| is the euclidean distance between two vectors and &lt; . , . &gt; is the angle between two vectors.</p><p>The feature vector f (P s ) considers the correlation between joints and segments. Intuitively, joints are the connection points of parts. If two joints are neighboring joints, using forehead and neck as an example, the head joint should be located inside the head segment region and near the head segment boundary while the neck joint should be located in either head or body region and near the common boundary of body and head. Moreover, the connected line between forehead joint and neck joint should fall inside the head region. These segment-based heuristic cues provide strong constrains for the location of joints. We design f (P s ) w.r.t. this idea. Formally, each joint type is associated with one or two semantic parts and each neighbouring joint type pair is associated with one semantic part type.</p><p>Based on the part segmentation label map L s inferred from P s , here we introduce the feature f (P s ) using the example of forehead and neck. For details, please see the supplementary material. Suppose l ci = forehead and l cj = neck, then our feature from segment includes 4 components: (1) a 2-d binary feature, with the first dimension indicating whether c i is inside the head region, and the second dimension indicating whether it is around the boundary of the head region; (2) a 4-d binary feature, with the first 2-d feature indicating c j w.r.t. the head region same as (1), and the rest 2-d feature indicating c j w.r.t. the torso region respectively; (3) a 1-d feature indicating the proportion of pixels on the line segment between c i and c j that fall inside the head region; (4) a 1-d feature indicating the intersect-overunion (IOU) between an oriented rectangle computed from c i and c j (with aspect ratio = 2.5:1) and the head region. We only extract the full feature for neighboring joints. For the joints locating far away like head and feet, we drop the third and the fourth components of the feature and set them to be 0. We validate the parameters for aspect ratio through a mean human shape following <ref type="bibr" target="#b26">[27]</ref>.</p><p>Based on the unary and pairwise terms described above, the FCRF infers the best labels L for the generated joint proposals c 1 , c 2 , . . . , c n , selecting and assembling them into a list of pose configurations. We adapt the inference algorithm introduced in <ref type="bibr" target="#b14">[15]</ref>, transforming the FCRF into an integer linear programming (ILP) problem with additional constraints from L. For each detection box, the inference algorithm gives the labels L for joint proposals within 1 sec. and we can acquire a list of pose configurations based on L, with pose score equal to the sum of unary scores for all visible joints. For each detection box, we choose only one pose configuration whose center is closest to the detection box center, and add that pose configuration to our final pose estimation result. We also experiment with the strategy of extracting multiple pose configurations from each detection box since there might be multiple people in the detection box, but find this strategy doesn't improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic Part Segmentation</head><p>We train a part segmentation model (the second-stage Part FCN) to segment an image into semantic parts with estimated high-quality pose configurations C p . We define two pose feature maps from C p : a joint label map and a skeleton label map, and use them as inputs to the second-stage Part FCN in addition to the original part score map. For the joint label map, we draw a circle with radius 3 at each joint location in C p . For the skeleton label map, we draw a stick with width 7 between neighbouring joints in C p . <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the two simple and intuitive feature maps.</p><p>The second-stage Part FCN is much lighter than the first-stage Part FCN since we already have the part score map P s predicted. We concatenate the 2 dimension feature map from estimated poses with the original part score map, yielding a 7 + 2 dimension inputs, and stacked 3 additional convolutional layers with kernel size as 7, kernel dimension as 128 and Relu as activation function. Our final part segmentation is then derived using the argmax value from the output part score map.</p><p>To learn all the parameters, we adopt a stage-wise strategy, i.e. first learn Pose FCN and the first-stage Part FCN, then the FCRF, and finally the second-stage Part FCN, which roughly take 3 days to train. For inference, our framework takes roughly 6s per-image. It is possible for us to do learning and inference iteratively. However, we found it's practically inefficient and the performance improvement is marginal. Thus, we only do the refinement once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Data. We perform extensive experiments on our manually labeled dataset, PASCAL-Person-Part <ref type="bibr" target="#b5">[6]</ref>, which provides joint and part segment annotations for PASCAL person images with large variation in pose and scale. There are 14 annotated joint types (i.e. forehead, neck, left/right shoulder, l/r elbow, l/r wrist, l/r waist, l/r knee and l/r ankle) and we combine the part labels into 6 semantic part types (i.e. head, torso, upper arm, lower arm, upper leg and lower leg). We only use those images containing humans for training (1716 images) and validation (1817 images). We only experiment on this dataset because other datasets do not have both pose and part segment annotations.</p><p>Generation of joint proposals. We apply the Faster R-CNN detector to produce human detection boxes, and perform a NMS procedure with detection score threshold = 0.6 and box IOU overlap threshold = 0.6. For each human detection box, we generate 6 joint proposals per joint type from the joint score map outputted by Pose FCN, using a NMS procedure with joint score threshold = 0.2 and proposal distance threshold = 16.</p><p>Generation of final pose configurations. For each detection box, the FCRF selects and assembles joint proposals into a series of pose configurations, with pose score defined as the sum of all unary joint scores (in logarithm form). For each missing joint, we regard its unary score as 0.2. To combine pose configurations from all the detection boxes, we design a NMS prodedure which considers the overlap of head bounding box, upper-body bounding box, lowerbody bounding box and whole-body bounding box inferred from the pose configurations. For two pose configurations, the one with a lower pose score will be filtered if their IOU overlap exceeds 0.65 for head boxes, or 0.5 for upperbody/lower-body boxes, or 0.4 for whole-body boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human Pose Estimation</head><p>Previous evaluation metrics (e.g. PCK and PCP) do not penalize false positives that are not part of the groundtruth. So following <ref type="bibr" target="#b14">[15]</ref>, we compare our model with other state-of-the-arts by Mean Average Precision (mAP). Briefly speaking, pose configurations in C pose I are first matched to groundtruth pose configurations according to the pose box overlap, and then the AP for each joint type is computed and  reported. Each groundtruth can only be matched to one estimated pose configuration. Unassigned pose configurations in C pose I are all treated as false positives. We compare our method with two other state-of-theart approaches: (1) Chen &amp; Yuille <ref type="bibr" target="#b6">[7]</ref>, a tree-structured model designed specifically for single-person estimation in presence of occlusion, using unary scores and imagedependent pairwise terms based on DCNN features; (2) Deeper-Cut <ref type="bibr" target="#b14">[15]</ref>, an integer linear programming model that jointly performs multi-person detection and multi-person pose estimation. These two methods both use strong graphical assembling models. We also build two other baselines, which use simple And-Or graphs for assembling instead of the FCRF in our model. One is "AOG-Simple", which only uses geometric connectivity between neighbouring joints. The other one is "AOG-Seg", which adds part segment consistency features to "AOG-Simple". The part segment consistency features are the same as the segment-joint smoothness feature we use in the FCRF. To test the effectiveness of our proposed part segment consistency, we also list the result of our model w/o the consistency features ("Our Model (w/o seg)"). The results are shown in Tab. 1. Our model outperforms all the other methods, and by comparing our model with "AOG-Simple" and "AOG-Seg", we can see that a good assembling model is really necessary for challenging multi-person images like those in PASCAL.</p><p>Our proposed part segment consistency features not only help the overall pose estimation results, but also improve the accuracy of the detailed joint localization. Previous evaluation metrics (e.g. PCP, PCK and mAP) treat any joint estimate within a certain distance of the groundtruth to be correct, and thus they do not encourage joint estimates to be as close as possible to the groundtruth. Therefore, we design a new evaluation metric called Average Distance of Keypoints (ADK). For each groundtruth pose configura-   tion, we compute its reference scale to be half of the distance between the forehead and neck, then find the only pose configuration estimate among the generated pose configuration proposals that has the highest overlap with the groundtruth configuration. For each joint that is visible in both the groundtruth configuration and the estimated configuration, the relative distance (w.r.t. the reference scale) between the estimated location and the groundtruth location is computed. Finally, we compute the average distance for each joint type across all the testing images. The result is shown in Tab. 2. It can be seen that our model reduces the average distance of keypoints significantly for wrists and lower-body joints by employing consistency with semantic part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human Semantic Part Segmentation</head><p>We evaluate the part segmentation results in terms of mean pixel IOU (mIOU) following previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>. In Tab. 3, we compare our model with two other state-ofthe-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> as well as one inferior baseline of our own model (i.e. the output part label map L s of the firststage part FCN, without the help of pose information).</p><p>We also list the numbers of our model using the more advanced network architecture ResNet-101 <ref type="bibr" target="#b3">[4]</ref> instead of VGG-16 <ref type="bibr" target="#b2">[3]</ref> for Part FCN. It can be seen that our model surpasses previous methods and the added pose information is effective for improving the segmentation results. When using ResNet-101, our model further boosts the performance to 64.39%.</p><p>Besides, we evaluate part segmentation w.r.t. different sizes of human instances in Tab. 4, following <ref type="bibr" target="#b32">[33]</ref>. Our model performs especially well for small-scale people, surpassing other state-of-the-arts by over 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Size   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Evaluation</head><p>Human pose estimation. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we visually demonstrate our pose estimation results on PASCAL-Person-Part, comparing them with the recent state-of-the-art Deeper-Cut <ref type="bibr" target="#b14">[15]</ref> and also a sub-model of ours ("Our Model (w/o seg)") which does not consider part segment consistency. This shows that our model gives more accurate prediction of heads, arms and legs, and is especially better at handling people of small scale (see the 6 th and 7 th row of <ref type="figure" target="#fig_2">Fig. 3</ref>) and extra large scale (see the first two rows of <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>Human semantic part segmentation. <ref type="figure">Fig. 4</ref> visually illustrates the advantages of our model over two other recent methods, Attention <ref type="bibr" target="#b4">[5]</ref> and HAZN <ref type="bibr" target="#b32">[33]</ref>, which adopt the same basic network structure as ours. Our model estimates the overall part configuration more accurately. For example, in the 2 rd row of <ref type="figure">Fig. 4</ref>, we correctly labels the right arm of the person while the other two baseline methods label it as upper-leg and lower-leg. Furthermore, our model gives clearer details of arms and legs (see the last three rows of <ref type="figure">Fig. 4)</ref>, especially for small-scale people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Attention HAZN Our Model Ground-truth <ref type="figure">Figure 4</ref>: Visual comparison of human semantic part segmentation on PASCAL-Person-Part <ref type="bibr" target="#b5">[6]</ref>. Our method is compared against two recent state-of-the-art methods: Attention <ref type="bibr" target="#b4">[5]</ref> and HAZN <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we demonstrate the complementary properties of human pose estimation and semantic part segmentation in complex multi-person images. We present an efficient framework that performs the two tasks iteratively and improves the results of each task. For human pose estimation, we adopt a fully-connected CRF that jointly performs human instance clustering and joint labeling, using deeplearned features and part segment based consistency features. This model gives better localization of joints, especially for arms and legs. For human semantic segmentation, we train a FCN that uses estimated pose configurations as shape and location priors, successfully correcting local con-fusions of people and giving clearer details of arms and legs.</p><p>We also adopt an effective "auto-zoom" strategy that deals with object scale variation for both tasks and helps reduces the inference time of the CRF by a factor of 40. We test our approach on the challenging PASCAL-Person-Part dataset and show that it outperforms state-of-the-art methods for both tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Joint human pose estimation and semantic part segmentation improve both tasks. (a) input image. (b) pose estimation and semantic part segmentation results before joint inference. (c) pose estimation and semantic part segmentation results after joint inference. Note that comparing (b1) and (c1), our result recovers the missing forehead joint and corrects the location error of right elbow and right wrist for the woman on the right. Comparing (b2) and (c2), our result gives more accurate details of lower arms and upper legs than (b2) for both people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework of our approach for joint pose estimation and part segmentation. Initial joint scores and part segment scores are fused to yield better pose estimation results, and then the estimated poses are used to refine part segmentation.beling arms as legs, labeling background regions as arms, etc.) if the person is in a non-typical pose, or when there are some other object/person nearby with similar appearance. Two recent works improve on FCN-type approaches by paying attention to the large scale variation in natural images. Chen et al. learn pixel-wise weights through an attention model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparison of human pose estimation on PASCAL-Person-Part [6]. Our full model is compared against Deeper-Cut [15] and a variant of our model ("Our Model (w/o seg)") that doesn't consider part segment consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean Average Precision (mAP) of Human Pose Estimation on PASCAL-Person-Part.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="5">Head Torso U-arms L-arms U-legs L-legs Background Ave.</cell></row><row><cell>Attention [5]</cell><cell>81.47 59.06 44.15</cell><cell>42.50</cell><cell>38.28 35.62</cell><cell>93.65</cell><cell>56.39</cell></row><row><cell>HAZN [33]</cell><cell>80.76 60.50 45.65</cell><cell>43.11</cell><cell>41.21 37.74</cell><cell>93.78</cell><cell>57.54</cell></row><row><cell>Our model (VGG-16, w/o pose)</cell><cell>79.83 59.72 43.84</cell><cell>40.84</cell><cell>40.49 37.23</cell><cell>93.55</cell><cell>56.50</cell></row><row><cell>Our model (VGG-16, final)</cell><cell>80.21 61.36 47.53</cell><cell>43.94</cell><cell>41.77 38.00</cell><cell>93.64</cell><cell>58.06</cell></row><row><cell cols="2">Our model (ResNet-101, w/o pose) 84.95 67.21 52.81</cell><cell>51.37</cell><cell>46.27 41.03</cell><cell>94.96</cell><cell>62.66</cell></row><row><cell>Our model (ResNet-101, final)</cell><cell>85.50 67.87 54.72</cell><cell>54.30</cell><cell>48.25 44.76</cell><cell>95.32</cell><cell>64.39</cell></row></table><note>Average Distance of Keypoints (ADK) (%) of Hu- man Pose Estimation on PASCAL-Person-Part.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mean Pixel IOU (mIOU) (%) of Human Semantic Part Segmentation on PASCAL-Person-Part.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Image</cell><cell>Deeper-Cut</cell><cell>Our Model (w/o seg)</cell><cell>Our Model (full)</cell></row></table><note>Mean Pixel IOU (mIOU) (%) of Human Seman- tic Part Segmentation w.r.t. Size of Human Instance on PASCAL-Person-Part.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We are deeply grateful for the support from ONR N00014-15-1-2356, NSF award CCF-1317376 and Army Research Office ARO 62250-CS, and also for the free GPUs provided by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<title level="m">Bird species categorization using pose normalized deep convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06550</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03339</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09065</idno>
		<title level="m">Structured feature learning for pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deformable mixture parsing model with parselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3408" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03170</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Content-based retrieval of human actions from realistic video databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06789</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3578" to="3585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCAS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07063</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Application of an incremental svm algorithm for on-line human recognition from video surveillance using texture and color features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boukharouba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boonaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lecoeuche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="140" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human identification using body prior and generalized emd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1441" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting 3d people from 2d pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="185" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint object and part segmentation using deep learned potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1573" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical part-based models for human parsing and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3075" to="3102" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pose-guided human parsing by an and/or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Retrieving similar styles to parse clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1028" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
