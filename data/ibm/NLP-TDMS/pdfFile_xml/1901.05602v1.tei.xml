<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
							<email>zhaojian90@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Xie</surname></persName>
							<email>mxie@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengsheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
							<email>jianshu@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face anti-spoofing (a.k.a presentation attack detection) has drawn growing attention due to the high security demand in face authentication systems. Existing CNN-based approaches usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of unseen scenes. In this paper, we try to boost the generalizability and applicability of these methods by designing a CNN model with two major novelties. First, we propose a simple yet effective Total Pairwise Confusion (TPC) loss for CNN training, which enhances the generalizability of the learned Presentation Attack (PA) representations. Secondly, we incorporate a Fast Domain Adaptation (FDA) component into the CNN model to alleviate negative effects brought by domain changes. Besides, our proposed model, which is named Generalizable Face Authentication CNN (GFA-CNN), works in a multi-task manner, performing face anti-spoofing and face recognition simultaneously. Experimental results show that GFA-CNN outperforms previous face anti-spoofing approaches and also well preserves the identity information of input face images. arXiv:1901.05602v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the recent noticeable advances, the security of face recognition systems is still vulnerable to Presentation Attacks (PA) with printed photos or replayed videos. To counteract PA, face anti-spoofing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref> is developed and serves as a pre-step prior to face recognition.</p><p>Earlier face anti-spoofing approaches mainly adopt handcrafted features, like LBP <ref type="bibr" target="#b7">[8]</ref>, HoG <ref type="bibr" target="#b15">[16]</ref> and SURF <ref type="bibr" target="#b4">[5]</ref>, to find the differences between live and spoofing faces. In <ref type="bibr" target="#b26">[27]</ref>, CNN was used for face anti-spoofing for the first time, with remarkable performance achieved in intradatabase tests. Following their work, a number of CNN-  <ref type="figure">Figure 1</ref>: Our CNN framework works in a multi-task manner, addressing face recognition and face anti-spoofing at one shot. It leverages total pairwise confusion (TPC) loss and fast domain adaption (FDA) to enhance the generalizability of the learned Presentation Attack (PA) feature and improve face anti-spoofing performance across different scenes.</p><p>based methods have been proposed, almost all treating face anti-spoofing as a binary (live vs. spoofing) classification problem. However, given the enormous solution space of CNN, these methods tend to suffer overfitting and poor generalizability to new PA patterns and environments. In this work, we attempt to enable an anti-spoofing system to be deployed in various environments, i.e. with good generalizability.</p><p>For CNN-based methods, an important clue to differentiate live vs. spoofing faces is the spoof pattern, including color distortion, moiré pattern, shape deformation, spoofing artifacts (e.g., reflection), etc. During CNN model training, strong patterns make more contributions, and the resultant model is more discriminative for them. However, if these patterns are absent in the testing data, the performance would severely drop. The CNN-based methods tend to overfit to some strong spoof patterns and thus suffer poor generalizability <ref type="bibr" target="#b18">[19]</ref>. Apart from overfitting, domain shift <ref type="bibr" target="#b17">[18]</ref> is also an important reason for the poor generalizability of face anti-spoofing methods. A domain here refers to a certain environment where an image is acquisited, consisting of various factors such as illumination, background, facial appearance, camera type, etc. Considering the huge diversity of real world environments, it is very common that different samples have different domains. For example, the domains of two paper attacks may be quite different even in case of the same face if reproduced with different pieces of paper (e.g. glossy vs. rough paper). Such domain variance may lead to distribution dissimilarity of different samples in the feature space and cause the models to fail on new domains. Based on the above observations, we propose a new Total Pairwise Confusion (TPC) loss to balance the contributions of all involved spoof patterns, and also employ a Fast Domain Adaptation (FDA) model <ref type="bibr" target="#b10">[11]</ref> to narrow the distribution discrepancy of samples from different domains in the feature space. We then obtain a Generalizable Face Authentication CNN model, shorted as GFA-CNN. Different from prior methods that take face anti-spoofing as a pre-step of face authentication, our GFA-CNN works in a multi-task manner, performing simultaneously face anti-spoofing and face recognition, as shown in <ref type="figure">Fig. 1</ref>. Since the CNN layers of the two tasks share the same parameters, our model works with high efficiency.</p><p>Extensive experiments on five popular benchmarks for face anti-spoofing demonstrate the superiority of our method over the state-of-the-arts. Our code and trained models will be available upon acceptance. Our contributions are summarized as follows:</p><p>• We propose a Total Pairwise Confusion (TPC) loss to effectively relieve the overfitting problems of CNNbased face anti-spoofing models to dataset-specific spoof patterns, which improves generalizability of face anti-spoofing methods. • We incorporate the Fast Domain Adaptation (FDA) model to learn more robust Presentation Attack (PA) representations, which reduces domain shift in the fea-ture space. • We develop a multi-task CNN model for face authentication. Our GFA-CNN performs jointly face antispoofing and face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most previous approaches for face anti-spoofing exploit texture differences between live and spoofing faces with pre-defined features such as LBP <ref type="bibr" target="#b7">[8]</ref>, HoG <ref type="bibr" target="#b15">[16]</ref>, and SURF <ref type="bibr" target="#b4">[5]</ref>, which are subsequently fed to a supervised classifier (e.g., SVM, LDA) for binary classification. However, such handcrafted features are very sensitive to different illumination conditions, camera devices, specific identities, etc. Though noticeable performance achieved under the intradataset protocol, the sample from a different environment may fail the model. In order to obtain features with better generalizability, some approaches leverage temporal information, e.g. making use of the spontaneous motions of the live faces, such as eye-blinking <ref type="bibr" target="#b19">[20]</ref> and lip motion <ref type="bibr" target="#b14">[15]</ref>. Though these methods are effective against photo attacks, they become vulnerable when attackers simulate these motions through a paper with eye/mouth positions cut.</p><p>Recently, deep learning based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> have been proposed to address face anti-spoofing. They use CNNs to learn highly discriminative representations by taking face anti-spoofing as a binary classification problem. However, most of them easily suffer overfitting. Current publicly available face anti-spoofing datasets are too limited to cover various potential spoofing types. A very recent work <ref type="bibr" target="#b18">[19]</ref> by Liu et al. leverages the depth map and rPPG signal as auxiliary supervision to train CNN instead of treating face anti-spoofing as a simple binary classification problem in order to avoid overfitting. Another critical issue for face anti-spoofing is domain shift. To bridge the gap between training and testing domains, <ref type="bibr" target="#b16">[17]</ref> generalizes CNN to unknown conditions by minimizing the feature distribution dissimilarity across domains, i.e. minimizing the Maximum Mean Discrepancy distance among representations.</p><p>To our best knowledge, almost all previous works take face anti-spoofing as a pre-step prior to face recognition and address it as a binary classification problem. Compared with previous literature, we solve face anti-spoofing and face recognition at one shot. A most related work to ours is <ref type="bibr" target="#b22">[23]</ref>, which proposed a two-tier framework to ensure the authenticity of the user to the recognition system, namely, monitoring whether the user has passed the biometric system as a live or spoofing one. It performs authentication based on fingerprint, palm vein print, face, etc., with two separated tiers: the anti-spoofing is powered by CNN learned representations while the recognition is based on pre-defined handcrafted features like ORB points.</p><p>Different with <ref type="bibr" target="#b22">[23]</ref>, we build our GFA-CNN in a multitask manner, our framework can recognize the identity of a given face, and meanwhile judge whether the face is a live or spoofing one. It is worth mentioning that for face recognition, our method achieves single-model accuracy up to 97.1% on the LFW database <ref type="bibr" target="#b11">[12]</ref>, which is even comparable to state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generalizable Face Authentication CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Task Network Architecture</head><p>The proposed Generalizable Face Authentication CNN (GFA-CNN) is able to jointly address face recognition and face anti-spoofing in a mutual boosting way. The network has two branches: the face anti-spoofing branch and the face recognition branch. Each branch consists of 5 blocks of CNN layers and 3 fully connected (FC) layers, and each block contains 3 CNN layers. The parameters are shared between these two branches. The face anti-spoofing branch is trained by minimizing TPC loss and face anti-spoofing loss (Anti-loss), while the face recognition branch is trained by optimizing face recognition loss (Recg-loss). The antispoofing branch takes as input raw face images with background, while the recognition branch takes cropped faces as input. Before fed to the face anti-spoofing branch, the training images are transferred to a target domain by a given target-domain image. In testing phase, each query image is transferred to the target domain and then propagated forward the network. The CNN blocks are structured the same with the convolution part of VGG16. Before training, the CNN blocks are first trained on the VGG-face dataset to obtain fundamental weights for face recognition. The FC layers of face antispoofing and face recognition branches have the same structure except for the output dimension of the last FC layer. The face anti-spoofing branch takes 2 dimensions for the last FC layer, while the dimensions of the last FC layer in the face recognition branch depend on the number of subjects involved in training. The overall objective function is</p><formula xml:id="formula_0">L = Lanti + λ1 * Lid + λ2 * Ltpc,<label>(1)</label></formula><p>where L anti and L recg are the cross entropy losses for face anti-spoofing and face recognition respectively, L tpc is the Total Pairwise Confusion (TPC) loss, and λ 1 and λ 2 are the weighting parameters among different losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Total Pairwise Confusion Loss</head><p>In order to learn Presentation Attack (PA) representations that are adaptable to varying environment conditions, we propose a novel Total Pairwise Confusion (TPC) loss. Our inspiration comes from the pairwise confusion (PC) loss <ref type="bibr" target="#b9">[10]</ref> that tackles the overfitting issue in fine-grained visual classification by intentionally introducing confusion in the feature activations. We modify their confusion implementation to make it applicable to the face anti-spoofing task. Our TPC loss is defined as</p><formula xml:id="formula_1">Ltpc(xi, xj) = M i =j ||ψ(xi) − ψ(xj)|| 2 2 ,<label>(2)</label></formula><p>where x i and x j are two randomly selected images (sample pair), M is the total number of sample pairs involved in training and ψ(x) denotes the representations of the second fully connected layer of the face anti-spoofing branch.</p><p>Our L tpc differs from the original PC loss in two-fold: 1) TPC loss minimizes the distribution distance of a random sample pair from the training set, rather than the sample pair from two different categories, to force CNN to learn slightly less discriminative features. 2) We minimize the Euclidean distance in the feature space while the original PC loss minimizes the distance in the probability space (output of softmax) to make samples in the same pair have a similar conditional probability distribution.</p><p>Our modifications are based on below considerations: 1) With face anti-spoofing taken as a binary classification issue, confusion across categories would not excessively affect the discriminability of the PA feature on differentiating live vs. spoofing samples. 2) Face samples related to the same subject would usually cluster in the feature space, and implementing confusion on all samples could compact and homogenize the whole feature distribution (see <ref type="figure" target="#fig_2">Fig. 3</ref>), thus benefiting generalization performance. 3) As a binary classification problem of simpler structure, regularizing the model within the feature space would be more useful than imposing regularization within the output probabilistic space.</p><p>Our L tpc can effectively improve the generalizability of PA representations. This can be understood as follows. Suppose there are K components in the PA representations, each corresponding to one spoof pattern, which is called a Spoof-pattern Specific Feature (SSF) in this work. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, different SSFs contribute differently to the final decision. If we define the feature for a live and a spoofing sample as F l = (f 1 l , f 2 l , ..., f K l ) and F s = (f 1 s , f 2 s , ..., f K s ), respectively, where f i l is the i th SSF of the live sample and f i s is the i th SSF of the spoofing sample. The SSFs are ranked based on their importance to the classification of live vs. spoofing. On one hand, L anti aims to enlarge the distance between F l and F s for better discrimination. On the other hand, L tpc attempts to narrow the difference between F l and F s . As f 1 l/s contributes the most to the differentiation of live and spoofing samples, it will be impaired the most by L tpc . However, the contributions of less important SSFs, such as f K−1 l/s and f K l/s , will be enhanced by L anti to offset the impaired discriminative ability. In this trade-off game, the contributions of all SSFs tend to be equalized, meaning more spoof patterns are involved in the decision rather than just a couple of strong spoof patterns specific to the training set. This could effectively alleviate overfitting risks. If some spoof patterns disappear in testing, a fair decision can still be achieved by other patterns, ensuring CNN would not overfit to some specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fast Domain Adaptation</head><p>Besides the proposed TPC loss that balances the contribution of each spoof pattern, we also apply FDA to reduce domain shift in the feature space to further improve the generalizability of our framework.</p><p>Generally, an image contains two components: content and appearance <ref type="bibr" target="#b20">[21]</ref>. The appearance information (e.g., colors, localised structures) makes up the style of images from a certain domain and is mostly represented by features in the bottom layers of CNN <ref type="bibr" target="#b12">[13]</ref>. For face anti-spoofing, the domain variance among face samples may introduce the distribution dissimilarity in the feature space and hurt antispoofing performance. Here, we employ the FDA to alleviate negative effects brought by domain changes. The FDA consists of an image transformation network f (·) that generates a synthetic image y from a given image x: y = f (x), and a loss network ϕ(·) that computes content reconstruction loss L content and domain reconstruction loss L domain .</p><p>Let ϕ j (·) be the j th layer of ϕ(·) with the shape of C j × H j × W j . The content reconstruction loss penalizes the output image y when it deviates in content from the input x. We thus minimize the Euclidean distance between the feature representations of y and x:</p><formula xml:id="formula_2">Lcontent = 1 CjHjWj ||ϕj(y) − ϕj(x)|| 2 2 .<label>(3)</label></formula><p>The domain reconstruction loss enables the output image y to have the same domain with the target-domain image y d . We then minimize the squared Frobenius norm of the difference between the Gram matrices of y and y d :</p><formula xml:id="formula_3">Ldomain = 1 CjHjWj ||Gj(y) − Gj(y d )|| 2 F .<label>(4)</label></formula><p>The Gram matrix is computed by reshaping ϕ j into a matrix κ, G j = κκ T /C j H j W j . Then the optimal imageŷ is generated by solving the following objective function: y = arg min P (λcLcontent(y, x) + λsLdomain(y, y d )),</p><p>where P is the optimal parameters of network f (·), x is the content image, y = f (x), y d is the target-domain image, and λ c and λ s are scalars. By solving Eqn. (5), x is transferred toŷ, preserving the content of x with the domain of y d . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We evaluate GFA-CNN on five face antispoofing benchmarks: CASIA-FASD <ref type="bibr" target="#b27">[28]</ref>, Replay-Attack  [8], MSU-MFSD <ref type="bibr" target="#b25">[26]</ref>, Oulu-NPU <ref type="bibr" target="#b6">[7]</ref> and SiW <ref type="bibr" target="#b18">[19]</ref>. CASIA-FASD and MSU-MFSD are small datasets, containing 50 and 35 subjects, respectively. Oulu-NPU and SiW are high-resolution databases published very recently. Oulu-NPU contains 4 testing protocols: Protocol 1 evaluates the environment condition variations; Protocol 2 examines the influences of different spoofing mediums; Protocol 3 estimates the effects of different input cameras; Protocol 4 considers all the challenges above. We conduct intradatabase tests on MSU-MFSD and Oulu-NPU, respectively. Cross-database tests are performed between CASIA-FASD vs. Replay-Attack and MSU-MFSD vs. Replay-Attack, respectively. The face recognition performance is evaluated on SiW, which contains 165 subjects with large variations in poses, illumination, expressions (PIE), and different distances from subject to camera. The LFW, the most widely used benchmark for face recognition, is also used to evaluate the face recognition performance. Implementation Details. The proposed GFA-CNN is implemented with TensorFlow <ref type="bibr" target="#b0">[1]</ref>. We use Adam optimizer with a learning rate beginning at 0.0003 and decaying half after every 2,000 steps. The batch size is set as 32. λ 1 and λ 2 in Eqn. (1) are set as 0.1 and 2.5e −5 , respectively. All experiments are performed according to the protocols provided in the datasets. The CNN layers are pre-trained on the VGG-face dataset <ref type="bibr" target="#b21">[22]</ref>. For data balance, we triple the live samples in the training set of CASIA-FASD, MSU-MFSD and Replay-Attack with horizontal and vertical flipping, while doubling the live samples in the training set of SiW by just flipping horizontally.</p><p>Evaluation Metrics. We have two evaluation protocols, intra-test and cross-test, which test samples from and not from the domain of the training set, respectively. We report our results with the following metrics. Intra-test evaluation:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We perform ablation analysis to reveal the role of TPC loss and FDA in our framework. We retrain the proposed network by adding/ablating TPC and FDA. As shown in Tab. 1, if TPC is removed, the HTER of intra-test on MFSD drops by 2.9% (w/ FDA) and 4.1% (w/o FDA), respectively. Since Replay-Attack is usually free of severe overfitting, it is reasonable to see the improved performance is not significant when using FDA, 0.3% (w/ FDA) and 0.6% (w/o FDA) on HTER.</p><p>For cross-test, if TPC is ablated, the HTER dramatically decreases by over 10% for MFSD → Replay 1 , and over 8% for Replay → MFSD, no matter FDA is used or not. The best cross-test result is achieved by using both TPC and FDA, indicating FDA can further improve the generalizability of the proposed method.</p><p>To evaluate the feature diversity between domains w/ and w/o FDA, we calculate the feature divergence via symmetric KL divergence. Similar to <ref type="bibr" target="#b20">[21]</ref>, we denote the mean value of a channel from the feature embedding of CNN as F . Given a Gaussian distribution of F , with mean µ and variance σ 2 , the symmetric KL divergence of this channel between domain A and B is</p><formula xml:id="formula_5">D(FA||FB) = KL(FA||FB) + KL(FB||FA).<label>(6)</label></formula><formula xml:id="formula_6">KL(FA||FB) = log σA σB + σ 2 A + (µA − µB) 2 2µ 2 B − 1 2 .<label>(7)</label></formula><p>Denote D(F iA ||F iB ) as the symmetric KL divergence of the i th channel. Then the average feature divergence of the layer is defined as</p><formula xml:id="formula_7">D(LA||LB) = 1 C C i=1 D(FiA||FiB),<label>(8)</label></formula><p>where C is the channel number of this layer. This metric measures the distance between the feature distributions of Methods EER(%) LBP + SVM baseline 14.7 DoG + LBP + SVM baseline 23.1 IDA + SVM <ref type="bibr" target="#b25">[26]</ref> 8.58 Color LBP <ref type="bibr" target="#b2">[3]</ref> 10.8 Color texture <ref type="bibr" target="#b3">[4]</ref> 4.9 Color SURF <ref type="bibr" target="#b4">[5]</ref> 2.2 GFA-CNN (ours) 7.5  domain A and B. We calculate the feature divergence of each layer in a CNN model for comparison. In particular, we randomly select 5,000 face samples from MSU-MFSD and Replay-Attack, respectively. Each dataset is considered as one domain. These samples are then fed to a pre-trained VGG16 <ref type="bibr" target="#b23">[24]</ref> model to calculate the KL divergence at each layer following Eqn. <ref type="bibr" target="#b7">(8)</ref>. The comparison results are shown in <ref type="figure">Fig. 6</ref>. As can be seen, with the FDA, the feature divergence between MSU-MFSD and Replay-Attack is significantly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face Anti-spoofing Evaluation</head><p>Intra-Test. We perform intra-test on MSU-MFSD and Oulu-NPU. Tab. 2 shows the comparisons of our method with other state-of-the-art methods on MSU-MFSD. For Oulu-NPU, we refer to the face anti-spoofing competition results in <ref type="bibr" target="#b1">[2]</ref> and use the best two for each protocol for comparison. All results are reported in Tab. 3. As shown in Tab. 2, GFA-CNN achieves the EER of 7.5%, ranking the 3 rd among all the compared methods. This result is satisfactory considering GFA-CNN is not designed blindly to pursue high performance in the intratest setting. In our experiments, we find the proposed TPC loss may slightly decrease the intra-test performance, mainly because TPC loss impairs the contributions of sev- eral strongest SSFs w.r.t the training datasets. The weakening of these dataset-specific features may in turn affect the intra-test performance (however, they may improve the performance in cross-test). According to Tab. 3, our method achieves the lowest ACER in 3 out of 4 protocols. For the most challenging protocol 4, we achieve the ACER of 8.9%, which is 1.1% lower than the best performer.</p><p>Cross-Test. To demonstrate the strong generalizability of GFA-CNN, we perform cross-test on CASIA-FASD, Replay-Attack, and MSU-MFSD by comparing with other state-of-the-arts. We adopt the most widely used crosstest settings: CASIA-FASD vs. Replay-Attack and MSU-MFSD vs. Replay-Attack, and report comparison results in Tab. 4. As can be seen, GFA-CNN achieves the lowest HTERs in cross-test: CASIA → Replay, MFSD → Replay and Replay → MFSD. Especially for Replay → MFSD, GFA-CNN reduces the cross-testing HTER by 8.3% compared with the best state-of-the-art. However, we also observe GFA-CNN has a relatively worse HTER compared with the best method on Replay Attack → CASIA-FASD. This is probably due to the "quality degradation" by FDA when the resolution of a sourcedomain image to be transferred is much higher than that of the target-domain image. During the cross-testing on Replay-Attack → CASIA-FASD, the target-domain image is selected from Replay-Attack with a low-resolution of 320 × 240. However, CASIA-FASD contains quite a num-   ber of images with high-resolution of 720 × 1280. Such a "resolution gap" leads to a "quality degradation" of FDA, as shown in the rightmost image in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Face Recognition Evaluation</head><p>We further evaluate the face recognition performance of our GFA-CNN on SiW and LFW. Since our method is not targeted specifically at face recognition, we only adopt VGG-16 as the baseline. On LFW, we follow the provided protocol to perform testing. On SiW we use 90 subjects for training and the other 75 subjects for testing, which is its default data splitting. This dataset also provides a frontal legacy face image corresponding to each subject. At the testing phase, we select the legacy image w.r.t each subject of the testing set as the gallery faces, and use all images in the testing set (including both live and spoofing) as the probe faces.</p><p>The ROC curves of face verification are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. As can be observed, GFA-CNN achieves competitive results to VGG16 on LFW, 97.1% and 97.6%, respectively. However, when testing on SiW, the declined accuracy of GFA-CNN is much lower than that of VGG16: the accuracy of GFA-CNN reduces by 4.5%, while VGG16 drops by 14%. The degraded performance is mainly due to face reproduction by spoofing mediums, in which some of the finer facial details might be lost. However, GFA-CNN still achieves satisfactory performance compared with VGG16. This is mainly because the face anti-spoofing and face recognition tasks mutually enhance each other, making the representations learned for face recognition less sensitive to spoof patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussions on Multi-task Setting</head><p>In this subsection, we investigate how the multi-task learning affects model performance for face anti-spoofing. We retrain our model without the face recognition branch, keep hyper-parameters unchanged and evaluate with the same protocol as the GFA-CNN. From the experiments, we observe the multi-task training slightly decreases the intratest performance of face anti-spoofing (dropping 2.5% and 0.3% on MSU-MFSD and Replay-Attack, respectively). This is reasonable, since the single model learns to perform two different tasks. However, two advantages are achieved compared with the single task training. Firstly, the training process becomes more stable with the Anti-loss decreasing gradually, rather than dropping sharply after some steps by single task training, suggesting multi-task setting can help overcome overfitting. Secondly, as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, multitask training helps learn face representations less sensitive to spoof patterns for face recognition. This mainly benefits from sharing parameters in the convolutional layers, giving more generic fusion features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel CNN model to jointly address face recognition and face anti-spoofing in a mutual boosting way. In order to learn more generalizable Presentation Attack (PA) representations for face anti-spoofing, we propose a novel Total Pairwise Confusion (TPC) loss to balance the contribution of each spoof pattern, preventing the PA representations from overfitting to dataset-specific spoof patterns. The Fast Domain Adaptation (FDA) is also incorporated into our framework to reduce distribution dissimilarity of face samples from different domains, further enhancing the robustness of PA representations. Extensive experiments on both face anti-spoofing and face recognition datasets show that our GFA-CNN achieves not only superior performance for face anti-spoofing on cross-tests, but also high accuracy for face recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of proposed GFA-CNN. The whole network contains two branches. The face anti-spoofing branch (upper) takes as input the domain-adaptive images transferred by FDA and optimized by TPC-loss and Anti-loss, while the face recognition branch (bottom) takes the cropped face images as input and is trained by minimizing Recog-loss. The structure settings are shown on top of each block, where "ID number" indicates the number of subjects involved in training. The two branches share parameters during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualized comparison of the learned feature distribution w/ and w/o L tpc . Without L tpc , the feature distribution is diverse and person-specific (left), while with L tpc , the feature distribution becomes compact and homogeneous (right). l is the classification hyperplane. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FCFigure 4 :</head><label>4</label><figDesc>The contribution-balanced process of SSFs. Darker color in the FC layer indicates a higher contribution to the classification while lighter color indicates lower. Each grid represents an SSF. The trade-off game between L tpc and L anti can balance the contributions of SSFs to the final decision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>shows some of our domain transferred samples. The target-domain image is sampled from the training data. Detailed analysis on the feature diversity between domains w/ and w/o FDA is provided in Sec. 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Example results by FDA. The upper left and bottom right images of the images in the middle column are the target-domain images expected to be transferred. Images of odd rows are from MSU-MFSD; images of even rows are from Replay-Attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Feature divergence comparison between MSU-MFSD and Replay-Attack. The numbers on x-axis correspond to the CNN layer of VGG16. Results transferred by FDA with different resolutions. The top left image is the target-domain image. For other images of each block, the left one is the original image, and the right is the transferred image. The green number located at the top left of each image indicates the resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of ROC curves of face verification on (a) LFW and (b) SiW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study (HTER %). +" means the corresponding component is used, while -" indicates removing the component. The numbers in bold are the best results.</figDesc><table><row><cell>Equal Error Rate (EER), Attack Presentation Classification</cell></row><row><cell>Error Rate (APCER), Bona Fide Presentation Classification</cell></row><row><cell>Error Rate (BPCER) and, ACER=(APCER+BPCER)/2.</cell></row><row><cell>Cross-test evaluation: HTER.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Intra-test results on MSU-MFSD. The numbers in bold are the best results.</figDesc><table><row><cell cols="2">Prot. Methods</cell><cell cols="3">APCER(%) BPCER (%) ACER (%)</cell></row><row><cell></cell><cell>CPqD</cell><cell>2.9</cell><cell>10.8</cell><cell>6.9</cell></row><row><cell>1</cell><cell>GRADIANT</cell><cell>1.3</cell><cell>12.5</cell><cell>6.9</cell></row><row><cell></cell><cell>GFA-CNN (ours)</cell><cell>2.5</cell><cell>8.9</cell><cell>5.7</cell></row><row><cell></cell><cell>MixedFASNet</cell><cell>9.7</cell><cell>2.5</cell><cell>6.1</cell></row><row><cell>2</cell><cell>GRADIANT</cell><cell>3.1</cell><cell>1.9</cell><cell>2.5</cell></row><row><cell></cell><cell>GFA-CNN (ours)</cell><cell>2.5</cell><cell>1.3</cell><cell>1.9</cell></row><row><cell></cell><cell>MixedFASNet</cell><cell>5.3</cell><cell>7.8</cell><cell>6.5</cell></row><row><cell>3</cell><cell>GRADIANT</cell><cell>2.6</cell><cell>5.0</cell><cell>3.8</cell></row><row><cell></cell><cell>GFA-CNN (ours)</cell><cell>4.3</cell><cell>7.1</cell><cell>5.7</cell></row><row><cell></cell><cell>Massy HNU</cell><cell>35.8</cell><cell>8.3</cell><cell>22.1</cell></row><row><cell>4</cell><cell>GRADIANT</cell><cell>5.0</cell><cell>15.0</cell><cell>10.0</cell></row><row><cell></cell><cell>GFA-CNN (ours)</cell><cell>7.4</cell><cell>10.4</cell><cell>8.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Intra-test results on the four protocols of Oulu-NPU. The numbers in bold are the best results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Cross-test results (HTER %) on CASIA-FASD, Replay-Attack, and MSU-MFSD. -" indicates the corresponding result is unavailable. The numbers in bold are the best results.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The acronym * → means training on database * " and testing on database ".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A competition on generalized software-based face presentation attack detection in mobile scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benlamoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Bekhouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ouafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taleb-Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="688" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face antispoofing based on color texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2636" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face spoofing detection using colour texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1818" to="1830" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face antispoofing using speeded-up robust features and fisher vector encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the generalization of color texture-based face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Oulu-npu: A mobile face presentation attack database with real-world variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the effectiveness of local binary patterns in face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIOSIG</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can face anti-spoofing countermeasures work in a real world scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Freitas Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>De Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<ptr target="https://github.com/lengstrom/fast-style-transfer/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Face de-spoofing: Anti-spoofing via noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09968</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Realtime face detection and motion analysis with application in liveness assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kollreider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fronthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Faraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="558" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context based face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning generalized deep feature representation for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2639" to="2652" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1794" to="1809" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep models for face anti-spoofing: Binary or auxiliary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Eyeblink-based antispoofing in face recognition from a generic webcamera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09441</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnnbased anti-spoofing two-tier multi-factor authentication system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sangaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castiglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face liveness detection from a single image with sparse low rank bilinear discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face spoof detection with image distortion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5601</idno>
		<title level="m">Learn convolutional neural network for face anti-spoofing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A face antispoofing database with diverse attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
