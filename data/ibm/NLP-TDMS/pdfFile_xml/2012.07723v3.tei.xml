<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolutionary learning of interpretable decision trees CC-BY-NC-ND 4.0</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-21">21 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">Lucio</forename><surname>Custode</surname></persName>
							<email>es:leonardo.custode@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Povo (Trento)</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Iacca</surname></persName>
							<email>giovanni.iacca@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Povo (Trento)</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolutionary learning of interpretable decision trees CC-BY-NC-ND 4.0</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-21">21 Apr 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement Learning</term>
					<term>Decision Tree</term>
					<term>Interpretability</term>
					<term>Evolutionary algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement learning techniques achieved human-level performance in several tasks in the last decade. However, in recent years, the need for interpretability emerged: we want to be able to understand how a system works and the reasons behind its decisions. Not only we need interpretability to assess the safety of the produced systems, we also need it to extract knowledge about unknown problems. While some techniques that optimize decision trees for reinforcement learning do exist, they usually employ greedy algorithms or they do not exploit the rewards given by the environment. This means that these techniques may easily get stuck in local optima. In this work, we propose a novel approach to interpretable reinforcement learning that uses decision trees. We present a two-level optimization scheme that combines the advantages of evolutionary algorithms with the advantages of Q-learning. This way we decompose the problem into two sub-problems: the problem of finding a meaningful and useful decomposition of the state space, and the problem of associating an action to each state. We test the proposed method on three well-known reinforcement learning benchmarks, on which it results competitive with respect to the stateof-the-art in both performance and interpretability. Finally, we perform an ablation study that confirms that using the two-level optimization scheme gives a boost in performance in non-trivial environments with respect to a one-layer optimization technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolutionary algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While machine learning is achieving very promising results in a variety of fields, there is an emergent need to understand what happens in the learned model, for testing, security and safety purposes.</p><p>There are mainly two approaches that try to address this problem: explainable AI (XAI) and interpretable AI (IAI) (which is actually a subset of XAI).</p><p>The field of explainable AI, in recent years, has seen a significant increase in the number of scientific contributions related to the topic. It is important to note, however, that these techniques are not applicable to all the tasks. In fact, as stated by [1], it is not safe to apply XAI techniques to safety-critical or high-stakes systems. This is due to the fact that explanations are usually approximations of the actual models and, as a consequence, do not represent exactly the models.</p><p>Interpretable AI techniques, instead, are based on the use of interpretable models, i.e. models that can be easily understood and inspected by an human operator [2]. These techniques, besides the ability to assess security and safety of the produced models, can also serve to better understand a problem. In fact, by looking at an interpretable model (with good performance), an human operator can extract knowledge about the problem faced.</p><p>However, interpretable AI techniques are not widely used in practice, due to their (usually) lower performance. In fact, it is widely accepted (although not proved) that a trade-off between interpretability and performance exists.</p><p>Recent work has addressed the problem of building interpretable reinforcement learning models. In <ref type="bibr" target="#b4">[3]</ref>, the authors implement a differentiable version of decision trees and optimize them by using backpropagation. Dhebar et al. [4]    propose nonlinear decision trees to approximate and refine an oracle policy.</p><p>While the results of these approaches seem very promising, the structure of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the tree must be defined a-priori. This requires us to either perform a trial-anderror search or to include prior knowledge.</p><p>In this work, we present a novel approach to the training of interpretable reinforcement learning agents that combines artificial evolution and lifelong reinforcement-learning. This two-level optimization algorithm allows us to decrease the amount of prior knowledge given to the algorithm. Our approach is able to generate agents in the form of decision trees that are able to learn both a decomposition of the space and the state-action mapping.</p><p>The contributions of this paper are the following:</p><p>• We propose a two-level optimization approach that optimizes both the topology of the tree and the decisions taken for each state • We interpret the solutions produced to understand how the agents work This paper is structured as follows. In Section 2 we give some background on the field and related work, while Section 3 describes the method used in our approach. Then, in Section 4 we present the results of our experiments. In Section 5 we will discuss our results by comparing them to the interpretable state-of-the-art, performing an ablation study and interpreting the produced solutions. Finally, in Section 6 we draw the conclusions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section we will give some background on the research problem being faced.</p><p>The use of decision trees to learn in reinforcement learning tasks has been explored in several previous work .</p><p>McCallum, in <ref type="bibr" target="#b6">[5]</ref>, proposes U-Trees: a kind of trees able to perform reinforcement learning that handle the following sub-problems: choice of the memories, selective perception and value function approximation. In <ref type="bibr" target="#b7">[6]</ref>, the authors extend U-Trees in order to make them able to cope with continuous environment.</p><p>They propose two novel tests that are used to create new conditions that split the state-space. They test the proposed approach in two environments, a continuous one and an ordered-discrete one, and their results show that their approach is competitive with respect to other approaches. <ref type="bibr">Pyeatt</ref> and Howe, in <ref type="bibr">[7]</ref>, propose a novel splitting criterion to build trees that are able to perform value function approximation. In their experiments, they compare the performance obtained by using their approach to the ones obtained using other splitting criteria, a table-lookup approach and a neural network. The results show that the proposed approach usually achieves better performance than all the other approaches.</p><p>In <ref type="bibr" target="#b9">[8]</ref> the authors propose a method that predicts the gain obtained by adding a split to the tree and select the best split to grow the tree. The experimental results show that this method is more effective than the method proposed in <ref type="bibr">[7]</ref> on the tested environment.</p><p>Silva et al. <ref type="bibr" target="#b4">[3]</ref> propose an approach to interpretable reinforcement learning that uses Proximal Policy Optimization on differentiable decision trees. Moreover, they provide an analysis of the learning process while using either Qlearning or PPO. The experimental results show that this approach is able to produce competitive solutions in some of the tasks. However, it is also shown that when discretizing the differentiable decision trees into typical decision trees, the performance may heavily decrease.</p><p>In <ref type="bibr" target="#b5">[4]</ref>, the authors used evolutionary algorithms to evolve non-linear decision trees. By non-linear, the authors mean that each split does not define a linear hyperplane in the feature-space. The experimental results show that this approach is able to obtain competitive performance with respect to a neural network based approach.</p><p>In <ref type="bibr" target="#b10">[9]</ref>, the authors use the grammatical evolution algorithm <ref type="bibr" target="#b11">[10]</ref> to evolve behavior trees (tree-based structures that allow more complex operations than a decision tree) for the Mario AI competitions. The proposed agent can perform basic actions or pre-determined combinations of basic actions. Their solution achieved the fourth place in the Mario AI competition. However, in this work the authors only evolve a controller, not exploiting the rewards given by the environment to increase the performance of the agent.</p><p>Hallawa et al., in <ref type="bibr" target="#b12">[11]</ref>, use behavior trees as evolved instinctive behaviors in agents that are then combined with a learned behavior. While behavior trees are usually interpretable, the authors did not take explicitly into account the interpretability of the whole model, which comprises both a behavior tree and either a neural network or a Q-learning table.</p><p>Several work applied the evolutionary computation paradigm to evolve treebased structures outside the reinforcement learning domain. <ref type="bibr">Krȩtowski</ref>, in <ref type="bibr" target="#b13">[12]</ref>, proposes a memetic algorithm based on genetic programming <ref type="bibr">[13]</ref> and local search to optimize decision trees. The results presented show that this approach is able to obtain performance that is comparable to the state-of-the-art, while keeping the size of the tree significantly lower.</p><p>In <ref type="bibr" target="#b15">[14]</ref>, the authors propose a multi-objective EA to evolve regression trees and model trees. They use a Pareto front to optimize RMSE, number of nodes, number of attributes. The experimental results show that this approach is able to obtain performance that are comparable or better than the state-of-the-art while using less nodes and less attributes.</p><p>In <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16]</ref>, the authors use the Genetic and Artificial Life Environment (GALE) to evolve decision trees. Their results show that GALE is able to produce decision trees that are competitive with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we aim to produce interpretable agents in the form of decision trees. Decision trees are trees (usually binary trees) where each inner node represents a "split" (i.e. a test on a condition) and each leaf node contains a decision. A representation of the proposed decision trees is shown in <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>When using decision trees for reinforcement learning tasks, there are two problems that need to be assessed:</p><p>1. How do we choose the splits? 2. Given a leaf, what action do we need to assign to this leaf?</p><p>Of course, there is an important relationship between splits and decisions taken in the leaves, so changing one of these without changing the other may lead to significant changes in performance.</p><p>Several works <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr">7]</ref> use greedy heuristics to induce the trees. However, this approaches have the following drawbacks:</p><p>• They use greedy rules to expand the trees: since inducing decision trees is an NP-complete problem <ref type="bibr" target="#b18">[17]</ref>, this may cause the induction of sub-optimal trees (i.e. stuck in local optima) of poor quality <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b19">18]</ref>.</p><p>• They use tests to expand the trees: this causes these algorithms to suffer from the curse of dimensionality because, for each expansion of the tree, all the input variables need to be tested <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b7">6]</ref>.</p><p>Other works <ref type="bibr" target="#b10">[9]</ref> (and <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b17">16]</ref>, even if they are not applied to reinforcement learning tasks) induce trees by means of evolutionary approaches.</p><p>However, these approaches only rely on the evolutionary algorithm. In RL tasks, not exploiting the reinforcement signals given by the environment may slow down the evolution and so result in a less-efficient process.</p><p>Our approach, instead, aims to combine artificial evolution and reinforcementlearning methods to take the best of both worlds. We propose a Baldwinianevolutionary approach to optimize simultaneously the structure of the tree and the state-action function. Baldwinian evolution is an evolutionary theory that,  knowledge acquired by the individual may be an evolutionary advantage that modifies the fitness landscape.</p><p>We do so by using an evolutionary algorithm to evolve the structure of the decision tree, while using Q-learning to learn the state-action function. This way, we search for trees that decompose the state-space in such a manner that, when taking optimal actions, maximize the reward of the agent.</p><p>The evolutionary algorithm we use is the Grammatical Evolution (GE) <ref type="bibr" target="#b11">[10]</ref>.</p><p>This evolutionary algorithm evolves (context-free) grammars in the Backus-Naur Form. <ref type="figure">Figure 2</ref> shows a block diagram that clarifies the inner working of the proposed algorithm. The blue-colored parts are the processes inherent the evolutionary part of our algorithm, while the red-colored ones are the processes inherent the reinforcement-learning part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evolutionary algorithm</head><p>To evolve decision trees, we evolve an associated grammar, similarly to the approach described in <ref type="bibr" target="#b11">[10]</ref>. In this subsection we will describe our algorithm design, highlighting the differences with the original Grammatical Evolution.  <ref type="figure">Figure 2</ref>: A scheme of the inner working of the proposed algorithm. The blue blocks are the ones that derive from the evolutionary part of our algorithm, while the red blocks are the ones that derive from the Q-learning part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Individual encoding</head><p>The genotype of an individual is encoded as a list of codons (represented as integers). However, differently from <ref type="bibr" target="#b11">[10]</ref>, the genotype has fixed length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Mutation operator</head><p>Instead of the mutation operator described in <ref type="bibr" target="#b11">[10]</ref>, we use a classical uniform mutation. This operator mutates each gene according to a probability. The new value of the gene is drawn uniformly from the range of variation of the variable.</p><p>However, since the grammar may have a different number of productions for each rule, we uniformly draw a random number between 0 and a number bigger than the maximum number of productions in the grammar. Then, by using the modulo operator, we choose the production from the production rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Crossover operator</head><p>As a crossover operator, we use the standard one-point crossover operator.</p><p>This operator simply sets a random cutting point and creates two individuals by mixing the two sub-strings of the genotype. This means that we do not prune the individuals that have genes that are not expressed in the phenotype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Replacement of the individuals</head><p>Instead of replacing all the individuals with their offspring (intended as the copies that undergo mutation/crossover), we replace a parent only when the fitness of its offspring is better than the fitness of the parent. Moreover, when an offspring has two parents (i.e. is a product of crossover), it replaces the parent with lowest fitness. In case two offspring have better fitness than only one of the parents, the best one replaces the worst parent.</p><p>This mechanism allows us to preserve diversity between the individuals and at the same time makes the fitness trend monotonically increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Fitness evaluation</head><p>The fitness evaluation process consists in the following steps. First of all, the genotype is translated to the corresponding phenotype. Then, for each timestep, the policy encoded by the tree is executed and the reward signals obtained from the environment are used to update the Q-values of the leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>To test our approach, we test it in the following OpenAI Gym <ref type="bibr" target="#b20">[19]</ref> environments:</p><p>• CartPole-v1</p><p>• MountainCar-v0</p><p>• LunarLander-v2</p><p>In this section, we will show the results obtained and compare them to the state-of-the-art using two metrics: the score given by the simulator and a metric of complexity (from the interpretability point of view).</p><p>We adopted the interpretability metric proposed in <ref type="bibr" target="#b21">[20]</ref>:</p><formula xml:id="formula_0">M orig = 79.1 − 0.2l − 0.5n o − 3.4n nao − 4.5n naoc</formula><p>where:</p><p>• l is the size of the formula (i.e. sum of constants, variables and operations)</p><p>• n o is the number of operations</p><p>• n nao is the number of non-arithmetical operations</p><p>• n naoc is the number of consecutive compositions of non-arithmetical operations.</p><p>However, this metric was meant to be bounded between 0 and 100, so we modified the metric in order to make it work as a complexity. For this purpose, the metric used is the following:</p><formula xml:id="formula_1">M = −0.2 + 0.2l + 0.5n o + 3.4n nao + 4.5n naoc</formula><p>The changes we made yield the following properties:</p><p>• By changing the sign of all the terms, we obtain that a model with an higher complexity is more hard to interpret</p><p>• We replaced the constant with -0.2, so that when we have a constant (best case from the point of view of the interpretability) its complexity becomes 0 Furthermore, it is important to note that this metric is in line with what Lipton states in <ref type="bibr" target="#b22">[21]</ref>. In fact, we can easily note that huge decision trees will be as interpretable as black-box methods, because the terms l, n o , n nao and n naoc will have a high magnitude. Also, M seems to be (loosely) in line with what the authors say in <ref type="bibr" target="#b23">[22]</ref>. In fact, by using the number of operations (although there are other variables in the metric we use) we loosely resemble the computational complexity of the model that we are executing.</p><p>To assess the statistical repeatability of our experiments, we perform 10 independent runs for each setting. For each run, as required by <ref type="bibr" target="#b20">[19]</ref>, we test the best model for each run over 100 independent episodes to assess its performance.</p><p>By "testing", we mean that the policy is executed in 100 unseen episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simplification mechanism</head><p>To make our solutions even more interpretable, we introduce a simplification mechanism that is executed on the final solutions. The simplification mechanism is the following. First of all, we execute the given policy for 100 episodes in a validation environment. Here, we keep a counter for each node of the tree that is increased each time the node is visited. Then, once this phase is finished, we remove all the nodes that have not been visited. Finally, we iteratively search for nodes in the tree whose leaves correspond to the same action. Each time such a node is found, it is replaced with a leaf that contains the action common to the two leaves. The iteration stops when the tree does not contain nodes of this type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Description of the environments</head><p>In this subsection we will describe the environments used and their properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">CartPole-v1</head><p>In this task the agent has to balance a pole that stands on top of a cart by moving the cart either to the left or to the right.</p><p>Observation space. The state of the environment is composed of the following features:</p><p>• • Push the cart to the left by applying a force of 10N (move lef t)</p><p>• Push the cart to the right by applying a force of 10N (move right)</p><p>Rewards. The agent receives a reward of +1 for each timestep.</p><p>Termination criterion. The simulation terminates if:</p><p>• The cart position lies outside the bounds for the x variable</p><p>• The angle of the pole lies outside the bounds for the θ variable Resolution criterion. This task is considered as solved if the agent receives a mean total reward R ≥ 475 on 100 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">MountainCar-v0</head><p>In this environment the agent has to drive a car, which is initially in a valley, up on a hill. However, the engine of the car is not powerful enough so the agent has to learn how to build momentum by exploiting the two hills.</p><p>Observation space. The state of the environment consists in the following variables:</p><p>• Horizontal position of the car: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">LunarLander-v2</head><p>In this task the agent has to land a lander on a landing pad.</p><p>Observation space. The state of the environment consists of 8 variables:   <ref type="table">Table 1</ref>: Grammar used to evolve orthogonal decision trees in the CartPole-v1 environment. The symbol "|" denotes the possibility to choose between different symbols. "comp op" is a short version of "comparison operator" and "lt" and "gt" are respectively the "less than" and "greater than" operators. input var represents one of the possible inputs in the given environment. Note that each input variable has a separate set of constants. <ref type="table">Table 2</ref>: Grammar used to evolve oblique decision trees in the CartPole-v1 environment. The symbol "|" denotes the possibility to choose between different symbols. "lt" refers to the "less than" operator.     The number of episodes used for Q-learning is quite low. This is because, since this is a "simple" environment, we want to lower the computational cost of the search by exploiting the randomness used to initialize the state-action function. This means that, in this case, Q-learning is used to "fine-tune" the state-action function instead of learning it from scratch.</p><formula xml:id="formula_2">Rule Production dt &lt; if &gt; if if &lt; condition &gt; then &lt; action &gt; else &lt; action &gt; condition lt(( n variables i=1 &lt; const &gt; input i ), &lt; const &gt;) action leaf | &lt; if &gt; const [−1, 1] with step 10 −3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Results</head><p>The results are shown in Tables 5 and 6. These tables show an interesting result. In fact, while the orthogonal grammar is able to solve the task in the 100% of the cases, the test score was the optimal one (500 ± 0) only in the 40% of the cases. On the other hand, the oblique grammar solves the task in the 90% of the cases, but achieves the optimal score in the 80% of the runs. This suggests us that, while the oblique grammar makes the search space more complex, it usually leads to more stable (as in Lyapunov's concept of stability) solutions.      In <ref type="figure" target="#fig_6">Figures 3 and 4</ref> we show the mean best fitness for each generation averaged across all the runs. We observe that, while both settings are able to converge towards the optimal fitness, the oblique grammar converges more quickly than the orthogonal one to the maximum fitness.</p><p>In <ref type="figure" target="#fig_9">Figure 5</ref>, a comparison of the solutions obtained by using the orthogonal and oblique grammars is shown. We can easily observe that, usually, the solutions obtained with the oblique grammar have a lower M than the ones obtained by using the orthogonal grammar. This is due to the fact that most of the produced oblique trees use only one split, resulting in a lower number of non-arithmetical operations.</p><p>To better assess the hypothesis made earlier, i.e. that oblique trees are more stable than orthogonal ones, in <ref type="table" target="#tab_10">Tables 7 and 8</ref> we compare all the trees produced by using the two grammars on a modified environment that has a maximum duration of 10 4 timesteps instead of 500. These results confirm our hypothesis, showing that all the oblique trees are able to obtain significantly better scores, often obtaining a perfect score (i.e. 10 4 ± 0) also in this setting. Moreover, we also tested the robustness of the produced agents with respect to noise on the inputs received by the sensors. In <ref type="figure" target="#fig_14">Figure 9</ref> we show how the performance of the two best agents vary with respect to additive input noise ( distributed as N (0, σ 2 )). The orthogonal tree was robust to noises with σ in the order of twice the sampling step used for the constants. On the other hand, the oblique tree proved to be significantly more robust, being able to cope with noises that have a σ about 50 times bigger than the sampling step used for the constants.       Finally, in <ref type="table">Table 9</ref> and <ref type="figure" target="#fig_2">Figure 10</ref> we compare our best solutions with other solutions found in literature. The complexities computed for the neural-network based approaches are approximations, i.e. we did not take into account all the details of the methods but only the network architectures, resulting in a slightly lower complexity. In our opinion, for the purpose of comparing our solution with the non-interpretable state-of-the-art, these small differences are negligible. Our solutions that have been used for the comparison are shown in <ref type="figure" target="#fig_2">Figures 11 and   12</ref>. The other produced solutions can be found in the repository of the project 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Score M Deep Q Network <ref type="bibr" target="#b24">[23]</ref> 327.30 1157.20 Tree-Backup(λ) <ref type="bibr" target="#b24">[23]</ref> 494.70 1157.20 Importance-Sampling <ref type="bibr" target="#b24">[23]</ref> 498.70 1157.20 Qπ <ref type="bibr" target="#b24">[23]</ref> 489.90 1157.20 Retrace(λ) <ref type="bibr" target="#b24">[23]</ref> 461  <ref type="table">Table 9</ref>: Comparison of the solutions obtained by using the proposed approach with respect to the state-of-the-art. The results from <ref type="bibr" target="#b24">[23]</ref> are averaged over ten independent runs. The results from <ref type="bibr" target="#b4">[3]</ref> regard the discretized tree shown in <ref type="figure" target="#fig_6">Figure 23</ref>  The tree has been simplified by using the technique used in our work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MountainCar</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Experimental setup</head><p>Also in this task, we tested both an orthogonal and an oblique grammar. The two grammars are shown in <ref type="table" target="#tab_13">Tables 10 and 11</ref>. Note that, in this environment we normalize the variables in the oblique case whereas in the others we do not. This is because the ranges of variation of the two variables are quite different.</p><p>Moreover, a preliminary experimental phase confirmed that it was hard to obtain good results by not normalizing the inputs.</p><p>The parameters used for the Grammatical Evolution are shown in <ref type="table" target="#tab_4">Tables   12 and 13</ref>. The settings for the Q-learning algorithm are shown in <ref type="table" target="#tab_5">Tables 14   and 15</ref>. Also in this case, we set the number of episodes to 10 to exploit the randomness of the initialization, since this is considered as a "simple" task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Results</head><p>The results obtained by the best solution for each run are shown in <ref type="table" target="#tab_8">Tables   16 and 17</ref>. In <ref type="table" target="#tab_10">Table 17</ref> there are some values in parenthesis. This is because,</p><p>given the difference in performance between training and testing scores, we proceeded with further investigation of the results. We deduced that in the latest steps of the training of such agents a change happened in the Q-values</p><formula xml:id="formula_3">Rule Production dt &lt; if &gt; if if &lt; condition &gt; then &lt; action &gt; else &lt; action &gt; condition input var &lt; comp o p &gt; &lt; const input var &gt; action leaf | &lt; if &gt; comp op lt | gt const x [-1.2, 0.6) with step 0.05 const v</formula><p>[-0.07, 0.07) with step 0.005 <ref type="table">Table 10</ref>: Grammar used to evolve orthogonal decision trees in the MountainCar-v0 environment. The symbol "|" denotes the possibility to choose between different symbols. "comp op" is a short version of "comparison operator" and "lt" and "gt" are respectively the "less than" and "greater than" operators. input var represents one of the possible inputs in the given environment. Note that each input variable has a separate set of constants.       As we can see from 16, the solutions obtained by using the orthogonal grammar solve the task in the 70% of the cases. On the other hand, as we can see from <ref type="table" target="#tab_10">Table 17</ref>, oblique trees perform poorly on this problem. This suggests us that this problem is harder to solve by using oblique trees than orthogonal ones.</p><formula xml:id="formula_4">Rule Production dt &lt; if &gt; if if &lt; condition &gt; then &lt; action &gt; else &lt; action &gt; condition lt(( n variables i=1 &lt; const &gt; input i , &lt; const &gt;) action leaf | &lt; if &gt; const [−1, 1] with step 10 −3</formula><p>While this may seem counter-intuitive, since oblique trees are a generalization of orthogonal trees, it may be because our grammar (the one used to produce oblique trees) makes it difficult to obtain an orthogonal decision tree.</p><p>The fitness trend for the best individual averaged on each run are shown in <ref type="figure" target="#fig_2">Figure 13</ref> and 14 for the orthogonal and oblique cases, respectively.</p><p>To compare the two approaches, we compare the robustness to input noise for both versions. The result is shown in <ref type="figure" target="#fig_2">Figure 15</ref>. In this case both approaches proved to be not so robust to noise. Surprisingly, we can observe that the orthogonal tree was not even robust to input noise that had σ &lt; min</p><formula xml:id="formula_5">i (step i )</formula><p>where step i is the sampling step for the constants of the i-th variable.</p><p>Finally, we perform a comparison of our solutions w.r.t. the state-of-the-art.</p><p>In <ref type="table" target="#tab_11">Table 18</ref> and <ref type="figure" target="#fig_2">Figure 16</ref> we          In this case, we were not able to find a configuration that gave satisfying results with orthogonal trees. For this reason, in this case we will show only the results obtained by using an oblique grammar.</p><p>The grammar used for this task is shown in <ref type="table">Table 19</ref>, while the parameters used for the grammatical evolution and the Q-learning are shown in Tables 20 and 21, respectively.</p><p>In this case, as shown in <ref type="table" target="#tab_26">Table 21</ref>, we significantly increased the number of episodes used for the training. This is due to the following reasons:</p><p>• The LunarLander-v2 environment is not as easy to solve as the previous environments.</p><p>• In this case, we did not use a random initialization of the leaves, to leverage only Q-learning to learn the state-action function.</p><p>Moreover, as shown in <ref type="table" target="#tab_26">Table 21</ref>, we used a slightly different Q-learning approach for this environment. In fact, in this case, we are using a decay for the ε parameter, in order to explore better the search space. The decay works as follows: in the k-th visit to the leaf, an ε = ε 0 · decay k is used. The learning rate has been set to 1 k , where k is the number of visits to the state-action pair. This guarantees that the state-action function converges to the optimum with k → ∞. Finally, to save computation time, we implemented an early stopping criterion such that if the mean score over the current period is smaller than the one obtained in the previous period, then the training is stopped. This is based on the following assumption: if the current mean score is worse than the previous one, then we can assume that the state-action function is converging to its optimum, so the small oscillations due to the randomness made it worse than the previous mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Results</head><p>The results obtained in this environment are summarized in <ref type="table" target="#tab_27">Table 22</ref> and plotted in <ref type="figure" target="#fig_2">Figure 19</ref>. We can easily observe that there is a local Pareto front  <ref type="table">Table 19</ref>: Grammar used to evolve oblique decision trees in the LunarLander-v2. The symbol "|" denotes the possibility to choose between different symbols. "lt" refers to the "less than" operator.     In this case, our approach is able to solve the task in the 100% of the cases.      and the state-of-the-art is shown in <ref type="table" target="#tab_4">Table 23</ref> and <ref type="figure" target="#fig_2">Figure 21</ref>. As we can observe, even though we do not achieve (in absolute) the best performance and the best M, our solutions represent the best compromise between the two metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Moreover, in <ref type="figure" target="#fig_2">Figure 21</ref> we can observe that a Pareto front that explains the trade-off between interpretability and performance seems to exist. However, our best solution achieves a comparable performance w.r.t. the best score of the state-of-the-art, while having a substantially smaller complexity. Our best solution is shown in <ref type="figure" target="#fig_30">Figure 22</ref> Source   <ref type="bibr" target="#b28">[27]</ref> are averaged on 5 runs. The results from <ref type="bibr" target="#b29">[28]</ref> and <ref type="bibr" target="#b30">[29]</ref> are averaged on 10 runs. </p><formula xml:id="formula_6">0.401p x − 0.104p y + +0.495v x − 0.055v y + −0.69θ − 0.845ω+ −0.2c l − 0.597c r &lt; 0 0.448p x − 0.366p y + +0.431v x − 0.462v y + −0.693θ − 0.821ω+ +0.461c l − 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section we briefly describe the interpretable techniques proposed in literature and we discuss our results in comparison to them. Then, we will perform an ablation study and, finally, we will interpret the produced trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CartPole 5.1.1. Differentiable Decision Trees</head><p>Silva et al. <ref type="bibr" target="#b4">[3]</ref> propose an approach based on differentiable decision trees, i.e. decision trees that replace hard splits with sigmoids. This means that they refactor the conditions from variable &gt; constant to σ(variable − constant). By replacing hard splits with sigmoids, the decision of the tree can be seen as the sum of all the leaves weighted by the product of the outputs of the sigmoids for that path (i.e. the product of all the σ(variable−threshold) for the true branch and (1−σ(variable−threshold)) for the false branch for each split encountered).</p><p>They optimize the splits of the tree and the actions taken by using PPO <ref type="bibr" target="#b31">[30]</ref> and backpropagation. The solution proposed for the CartPole-v1 environment is the decision tree shown in <ref type="figure" target="#fig_6">Figure 23</ref>. It is interesting to observe that their optimization process "selects" the same variables that have been selected in our case by artificial evolution.</p><p>Moreover, the tree proposed by them is slightly more complex than our best tree. In fact, while our best tree has a maximum depth of 2 conditions, their has a maximum depth of 3 conditions. This increase in complexity is reflected by the difference in the M measure.</p><p>Moreover, since the performance of this solution are not satisfactory, the authors gave us the solution coming from a follow-up work in a personal communication. This solution is shown in <ref type="figure" target="#fig_8">Figure 24</ref>. Besides the performance comparison performed in <ref type="table">Table 9</ref>, we compare here the robustness to noise, similarly to what we did in <ref type="figure" target="#fig_14">Figure 9</ref>.</p><p>As we can see in <ref type="figure" target="#fig_9">Figure 25</ref>, the orthogonal trees obtained by Silva et al.</p><p>have a robustness that is comparable to our orthogonal tree. This suggests us that orthogonal trees may be intrinsically less robust than oblique ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">MountainCar</head><p>Most of the approaches we used for the comparison in the MountainCar-v0 environments come from the OpenAI Gym leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Zhiqing Xiao</head><p>The system proposed by this entry <ref type="bibr" target="#b20">19</ref> consists in a closed-form policy. However, it is not clear whether the policy has been derived by a human or learned by a machine.</p><p>Anyway, this solution achieves the best performance (let alone our solutions) on this task while also having the best degree of interpretability (according to our modification of the metric proposed in <ref type="bibr" target="#b21">[20]</ref>). The policy is the following: While M is lower for this policy than for our best tree, it may be a bit harder to interpret this model. We think that this is due to the fact that the M metric has been proposed to evaluate the interpretability of mathematical formulae, while we are interested in interpreting hyperplanes. While hyperplanes are defined by mathematical formulae, the interpretability of an hyperplane may also depend on the number on non-linear operations that are used to determine the hyperplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Amit</head><p>This entry 20 uses SARSA to solve the task.</p><p>While tabular approaches like SARSA and Q-learning are transparent, their interpretability depends heavily on the number of states and actions. <ref type="table" target="#tab_11">Table 18</ref> shows that, even if this approach is transparent and easily interpretable, our solutions are able to achieve a better degree of interpretability. In our opinion, this is due to the fact that using decision trees as function approximators leads to the "grouping" of some states of the table used in tabular approaches. This is especially useful when we want to extract knowledge. In fact, by grouping some states, we take into account only the variables and the thresholds that have a big impact on the policy, discarding irrelevant details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Dhebar et al.</head><p>Dhebar et al., in <ref type="bibr" target="#b5">[4]</ref>, propose an approach to reinforcement learning that uses nonlinear decision trees. They first approximate an oracle policy and then they fine-tune it by using evolutionary algorithms. The policies obtained in these two phases are called "open-loop" and "closed-loop" policies.</p><p>In this case, we only had access to the open-loop policy for the MountainCar-v0 environment, which is shown in <ref type="figure" target="#fig_11">Figure 26</ref>. <ref type="bibr" target="#b21">20</ref> github.com/amitkvikram/rl-agent/blob/master/mountainCar-v0-sarsa.ipynb Also in this case, while M for this solution is better than our best solution (w.r.t. test score), it seems harder to interpret, due to the non-linearity of the hyperplanes. In fact, in our solution M is higher due to the higher number of splits in the tree, but that does not take into account the fact that in our case the hyperplanes that divide the feature space are simpler than the ones proposed in <ref type="bibr" target="#b5">[4]</ref>.</p><formula xml:id="formula_7">| −0.22xŷ+ 0.28ŷ − 1+ −0.63x − 2+ 0.96 | + −0.36 ≤ 0 | −0.30ŷ 2 + −0.28x 2 + 1.39 | + −</formula><p>Finally, we perform a comparison on the robustness to input noise with the solutions provided by "Zhiqing Xiao" and the one provided by Dhebar et al. In <ref type="bibr" target="#b4">[3]</ref> the authors, besides regular trees, use also decision lists. A decision list is a tree that is extremely unbalanced, i.e. il collapses to a list. <ref type="figure" target="#fig_10">Figure 28</ref> shows the solution obtained. However, as shown in <ref type="table" target="#tab_4">Table 23</ref>, it does not achieve satisfactory performance. This is due to the fact that, while the differentiable tree is able to achieve better performance (even though it does not solve the task), its discretization modifies the final distribution of the actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Malagon et al.</head><p>In <ref type="bibr" target="#b30">[29]</ref> the authors use the Univariate Marginal Distribution Algorithm to evolve a neural network without hidden layers in the LunarLander-v2 domain. Since the neural network has no hidden layers the whole system reduces to</p><formula xml:id="formula_8">a = argmax i (σ(w i T · x + b i ))</formula><p>where i refers to the output neurons.</p><p>This results in an easy-to-interpret system, according to both <ref type="bibr" target="#b22">[21]</ref> and the metric M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Dhebar et al.</head><p>In <ref type="bibr" target="#b5">[4]</ref> the authors propose a nonlinear decision tree that achieves a mean testing score of 234.98 points. However, the rules associated with this tree are not shown, so we only had access to the 3-levels-deep NLDT.</p><p>The tree is shown in <ref type="figure" target="#fig_14">Figure 29</ref>. It is important to note that even if the solution obtained is a tree, the interpretation is not easy, since the hyperplanes contained in each split are not linear.</p><p>Also in this case, we performed a comparison on the robustness to input noise, the result is shown in <ref type="figure" target="#fig_6">Figure 30</ref>. However, for this comparison we could not include the results from Malagon et al. since the weights were not publicly accessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation study</head><p>In order to assess whether our two-level optimization approach is convenient with respect to a single-level optimization approach, we perform an ablation study in which we use Grammatical Evolution alone to evolve decision trees.</p><p>Moreover, we perform statistical tests to test whether the difference are statistically significant by fixing a threshold for the p-value of α = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">CartPole</head><p>Orthogonal trees. To evolve orthogonal trees, we used the grammar shown in <ref type="table" target="#tab_5">Table 24</ref>, which has been evolved by using the same parameters described in <ref type="table" target="#tab_4">Table 3</ref>. Also in this case, the fitness was computed as the mean score on 10 episodes.</p><p>The results are shown in <ref type="table" target="#tab_7">Table 25</ref>. As we can observe, while in most cases the evolution is able to evolve agents that achieve a perfect training score, they have poor generalization capabilities. In our opinion, this is akin to overfitting.</p><formula xml:id="formula_9">| −0.39p x (v xĉlĉr ) −1 + −0.96p y −1ĉ l + −0.12(p xpy ) −1ĉ l 2 + 0.89 | + −0.8 ≤ 0 0.17v x −1 + −0.78v yĉr −1 + 0.9(p yĉl ) −2ĉ r −3 + 0.35 ≤ 0 True 0.82ĉ r −1 + 0.52(p xĉl ) −1θ + −0.59θ −1 + −0.</formula><p>In fact, in this case, the agents did not understand the "value" of going in a certain state, but just learned a rule that worked in the tested cases. Moreover, a two-tailed Mann-Whitney U-Test gives us a p-value of 9 · 10 −3 that allows us to reject the null hypothesis (i.e. that the mean testing score come from the same distribution) with threshold α = 0.05.</p><p>Oblique trees. We perform the same test also in the oblique setting. We use the grammar shown in <ref type="table" target="#tab_8">Table 26</ref> and the parameters used in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>The results are shown in <ref type="table" target="#tab_10">Table 27</ref>. We can easily observe that in this case  the results are similar to the ones shown in <ref type="table" target="#tab_8">Table 6</ref>. To check whether this similarity has a statistical significance, we perform a Two-tailed Mann-Whitney U-Test. The null hypothesis states that the results obtained by using the GE with Q-learning and GE alone come from the same statistical distribution. The p-value obtained with this test is 0.73, so we are not able to reject the null hypothesis with threshold α = 0.05. For this reason, we will assume that they come from the same distribution.</p><p>This suggests us that, since oblique trees seem to be both more robust to noise and more stable than orthogonal trees, an agent can learn good policies in simple environments without the need for Q-learning.   <ref type="table" target="#tab_7">Table 25</ref>: Results obtained by evolving orthogonal decision trees for the CartPole-v1 environment by using Grammatical Evolution alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">MountainCar</head><p>Orthogonal trees. We evolve orthogonal trees for the MountainCar-v0 environment by using the grammar shown in <ref type="table" target="#tab_11">Table 28</ref> and the parameters shown in <ref type="table">Table 10</ref>. Since in this case the number of episodes is low and the environment is harder to explore than CartPole, we expect GE alone to perform comparably with our approach.</p><p>The results are shown in <ref type="table" target="#tab_38">Table 29</ref>. As we expected, the performance are quite similar. To ensure that there are no statistical significant differences between the two approaches, we performed a Two-tailed Mann-Whitney U-Test on the testing mean score obtained by using the two approaches, which stated that the null hypothesis (i.e. the scores obtained come from the same distribution)   cannot be rejected with threshold α = 0.05.</p><p>Oblique trees. We perform the test also by using oblique trees. We use the grammar described in <ref type="table" target="#tab_4">Table 30</ref> with the parameters shown in <ref type="table" target="#tab_4">Table 13</ref>.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 31</ref>. While these results seem to be better than the ones shown in 17, they do not seem to be statistically significant according to a two-tailed Mann-Whitney U-Test (p-value 0.38). Thus, this seems to confirm our hypothesis that states that solving MountainCar-v0 with oblique trees seems to be harder than the case with orthogonal trees (with the proposed grammar).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">LunarLander</head><p>Finally, we perform the same test on LunarLander-v2, using only oblique trees. We use the grammar described in <ref type="table" target="#tab_4">Table 32</ref> and the parameters present in <ref type="table" target="#tab_25">Table 20</ref>. We expect that in this task, since it is harder than the previous   two, GE performs worse than our approach.</p><p>The results of this experiment are shown in <ref type="table" target="#tab_4">Table 33</ref>. According to our expectations, we are able to solve the task only in the 60% of the cases. Moreover, we perform a two-tailed Mann-Whitney U-Test to test the statistical significance of the differences between the two approaches (on the mean testing score). We obtain a p-value of 0.017 that allows us to reject the null hypothesis. We can thus hypothesize that the use of the two-level optimization technique gives us a boost in performance in complex environments such as LunarLander-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Interpretation of the solutions</head><p>In this subsection, we will look at the agents produced and try to interpret the policies.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1.">CartPole</head><p>Orthogonal tree. The tree shown in <ref type="figure" target="#fig_2">Figure 11</ref> is extremely easy to interpret. In fact, this agent moves the cart to the left if ω &lt; 0.074 ∧ θ &lt; 0.022 <ref type="bibr" target="#b2">(1)</ref> otherwise, it moves the cart to the right. Note that there is a case in which the pole is falling to the right but the agent moves the cart to the left: θ ∈ [0, 0.022)rad ∧ ω ∈ [0, 0.074)rad/s. This is not a problem because when the agent moves the cart to the right, it increases the velocity of the pole, resulting in a "move right" action in the subsequent steps.</p><p>Oblique tree. In this case, the interpretation of the policy is a bit harder. The condition used by the agent to discriminate between the two states is:   where k refers to the current timestep. To simplify the process, we write Equation 1 as the following:</p><formula xml:id="formula_10">− 0.274x k − 0.543v k − 0.904θ k − 0.559ω k &lt; −0.169<label>(2)</label></formula><formula xml:id="formula_11">− ax k − bv k − cθ k − dω k &lt; t<label>(3)</label></formula><p>First of all, we want to analyze the role of the constant t in the policy. By testing it with different values (i.e. t = −0.169, t = 0.169, t = −0.1, t = 0.1, t = 0) we observed that it holds that the final point in which the pole is balanced can be obtained as follows:</p><formula xml:id="formula_12">x n ≈ − t a<label>(4)</label></formula><p>where n is the index of the last timestep. For simplicity, let's assume that</p><p>x n = − t a . This means that we can rewrite Equation 3 as follows:</p><formula xml:id="formula_13">− x k − b a v k − c a θ k − d a ω k &lt; t a = −x n<label>(5)</label></formula><p>We can then perform other steps and obtain:</p><formula xml:id="formula_14">− x k − b v k − c θ k − d ω k &lt; −x n ⇒ (6) − b v k − c θ k − d ω k &lt; −x n + x k ⇒ (7) − b v k − c θ k − d ω k &lt; −x n + x n−1 − x n−1 + ... + x k = k+1 j=n −x j + x j−1 (8)</formula><p>Then, by noting that</p><formula xml:id="formula_15">x k − x k−1 τ = v k<label>(9)</label></formula><p>we can rewrite Equation 8 as:</p><formula xml:id="formula_16">− b v k − c θ k − d ω k &lt; − n j=k+1 v j τ (10) − c θ k − d ω k &lt; − n j=k g j v j τ<label>(11)</label></formula><p>where</p><formula xml:id="formula_17">g j =      −b τ if j == k 1 otherwise</formula><p>Now, by observing that</p><formula xml:id="formula_18">θ k − θ k−1 τ = ω<label>(12)</label></formula><p>we obtain</p><formula xml:id="formula_19">− c θ k − d θ k − θ k−1 τ &lt; − n j=k+1 g j v j τ (13) − (d + τ c )θ k + d θ k−1 &lt; −τ 2 n j=k+1 g j v j<label>(14)</label></formula><p>Finally, noting that after that usually, in the first 50 timesteps of the simulations the velocities are high (max k | v k |&lt; 1.5) and then the velocity become small (max k | v k |&lt; 0.55) because the pole is balanced, we can write that:</p><p>| n j=k+1 g j v j | b τ · 1.5 + 49 · 1.5 + 450 · 0.55 = 420 <ref type="bibr" target="#b16">(15)</ref> where the approximate equality holds in the worst case (i.e. k = 0 and all the velocities have the same sign). However, considering that in our observations the magnitude of the velocities was usually significantly smaller than the maximum and that the summation is multiplied by τ 2 (τ = 0.02 in this environment), we can safely consider only the term with the highest magnitude, i.e. b τ v k . Moreover, using only v k sets x n ≈ 0, which makes the system easier to understand intuitively.</p><p>Then, we obtain</p><formula xml:id="formula_20">− (d + τ c )θ k + d θ k−1 &lt; τ b v k (16) cθ k &gt; −(bv k + dω k )<label>(17)</label></formula><p>Approximating the constants, we set b = 0.543 ≈ 0.5, c = 0.904 ≈ 1, d = 0.559 ≈ 0.5, so the final policy is <ref type="bibr" target="#b22">21</ref> :</p><formula xml:id="formula_21">π(x, v, θ, ω) =      move right if θ k &gt; − 1 2 (v k + ω k )</formula><p>move lef t otherwise A dimensionally consistent policy is θ k + 1 2 (v k /l + ω) ntsτ τ &gt; 0, where l = 1 is the pole length and n ts is the number of steps that we are taking into consideration to balance the pole (in our case n ts = 1. This policy can be interpreted as follows. If the sum of the current angle and the mean angle given by the two contributions (i.e. linear velocity of the cart and angular velocity of the pole) are positive (it is a kind of "prediction" of the future angle), then move the cart to the right, because it is going to fall to the right. Otherwise, move the cart to the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.">MountainCar</head><p>Orthogonal tree. Also in this case, the orthogonal tree ( <ref type="figure" target="#fig_2">Figure 17</ref>) is easy to interpret. In fact, if we look at the leaves, we see that the agent accelerates to the left only in two cases:</p><formula xml:id="formula_22">(v &lt; 0∧x &gt; −0.9)∨(v ∈ [0, 0.035)∧x ∈ [−0.4, −0.3]).</formula><p>This means that the agents accelerates to the left when:</p><p>• it is going towards the hill on the left to build momentum and it is far from the border (x &gt; −0.9), so it tries to maximize the potential energy of the car</p><p>• velocity is positive but not enough (v &lt; 0.035) and it is near the valley In all the other cases, the agent accelerates to the right.</p><p>Oblique trees. In this case, the agent accelerates to the left when both conditions are false. This means that we have to solve the following system of two inequalities:</p><formula xml:id="formula_23">     0.717 x − 0.697 v ≥ −0.229 0.138 x − 0.883 v ≥ −0.389</formula><p>This means that the agent accelerates to the left when v ≤ 7.5799 · 10 −2 ·</p><p>x + 6.6955 ∧ v ≤ 1.1516 · 10 −2 · x + 5.495 · 10 −3 . This corresponds to the decision regions shown in <ref type="figure" target="#fig_2">Figure 31</ref>.</p><p>It is important to note that the lack of robustness for this solution does not allow us to further approximate the constants of the two hyperplanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3.">LunarLander</head><p>In this case, since the oblique tree ( <ref type="figure" target="#fig_30">Figure 22</ref>) has 4 conditions and 8 unknowns, it is a bit harder to interpret.</p><p>First condition. This condition, when it evaluates to False, turns on the right engine for a timestep. So, we turn on the right engine when</p><formula xml:id="formula_24">ap x − bp y + cv x − dv y − eθ − f ω − gc l − hc r ≥ 0<label>(18)</label></formula><p>where a, b, ..., h replace the constants shown in <ref type="figure" target="#fig_30">Figure 22</ref>.</p><p>To simplify the analysis, let's assume c l = c r = 0, since they can assume only two values: 0, 1. This simplification does not affect the generality of our analysis, since we are only assuming that there is no contact with the ground. We can simply say that, when contact with the ground happens, then the threshold is not 0 anymore, but it can take the following values: 0.2 (only right leg touches the ground), 0.597 (only right leg touches the ground), 0.797 (both legs touch the ground).</p><p>So, we can rewrite condition 18 as follows:</p><formula xml:id="formula_25">ap x + cv x − bp y − dv y − eθ − f ω ≥ 0 (19)</formula><p>By merging some terms we obtain:</p><formula xml:id="formula_26">a(p x + v x c ) − b(p y − v y d ) − e(θ − ωf ) ≥ 0<label>(20)</label></formula><p>We analyzed the terms in parenthesis and we discovered that they approximate the position (or the angle) in the following timestep. The constants c ≈ f ≈ x on the policy. Then, we can say that the agent turns on the right engine when θ ≤ a e p k+1</p><p>x so, when the agent is on the right part of the environment, the agent uses a linear threshold to activate the engine in order to avoid both high angles and high displacements from the landing pad location.</p><p>Similarly, when p k+1</p><p>x is negative, the threshold is negative so the agent tries both to compensate negative angles (that would move it farther on the left) and distance from the landing point.</p><p>Second condition. The second condition, when evaluates to True, leads to the firing of the left engine. Also in this case, let's neglect the terms c l and c r . We can write the condition as:</p><formula xml:id="formula_27">ap x − bp y + cv x − dv y − eθ − f ω &lt; 0 (22)</formula><p>Of course, in this case the coefficients a, ..., f are different from the previous ones. By grouping the terms as before we obtain</p><formula xml:id="formula_28">a(p x + v x c ) − b(p y + v y d ) − e(θ + ωf ) &lt; 0<label>(23)</label></formula><p>Also in this case, the constants seem to have the same role (i.e. some lead to overestimation of the next position and some to a better estimate) so we can write:</p><p>ap k+1</p><formula xml:id="formula_29">x − bp k+1 y &lt; eθ k+1<label>(24)</label></formula><p>This means that this condition is easy to understand given the previous one:</p><p>it is the opposite. This means that we can use the same reasoning used above to understand this condition.</p><p>Third condition. This condition handles the firing of the main engine. For this reason, we expect it to work differently from the previous two. In fact, we can easily observe that the signs of the terms in x and y are inverted. Moreover, the two angular terms do not have the same sign. Also in this case, let's use a,</p><p>..., f to rename the constants and ignore c l and c r . This leads to:</p><formula xml:id="formula_30">− ap x + bp y − cv x + dv y − eθ + f ω &lt; 0 (25)</formula><p>By performing a grouping of the variables similarly to the previous to conditions we obtain:</p><formula xml:id="formula_31">− a(p x + v x ) + b(p y + v y ) − (c − a)v x + (d − b)v y − eθ + f ω &lt; 0 (26)</formula><p>Then, by denoting with v k+1 and v k−1 the value of the variable v in the next and the previous timestep respectively, we can write: </p><p>Then, by noting that τ a ≈ 5·10 −3 , τ b ≈ 6.7·10 −3 , τ c ≈ 3.5·10 −2 , τ d ≈ 2.6·10 −2</p><p>and τ e ≈ 10 −2 , we can decide to neglect the effects of the first two terms. So we have: </p><p>By moving all the terms except the one in ω to the second member we get:</p><formula xml:id="formula_34">ω &lt; 1 f − τ e (c v x − d v y − τ eθ k−1 )<label>(31)</label></formula><p>Then, by noting that all the states that are tested in this condition have c | v x | ≈ 5d | v y | and c | v x | ≈ 120e| θ k−1 | (where v is the mean value of the variable v), we can neglect (as shown by experimental results) the effects of v y and θ k−1 .</p><p>Finally, the rule used to fire the main engine is:</p><formula xml:id="formula_35">ω &lt; c v x<label>(32)</label></formula><p>While we expected the main engine to depend on p y or v y , by analyzing the activation of the condition in several episodes we found that this rule represents the landing phase. In fact, the goal of this rule is to balance angular velocity and linear velocity to make the agent gently stop on the landing pad.</p><p>Fourth condition. This condition, when evaluates to True, does not fire any engine. On the other hand, when it evaluates to False, it fires the main engine.</p><p>The condition is the following (also in this case we replace the constants with letters): ap x − bp y − cv x − dv y − eθ + f ω &lt; 0 (33) By analyzing the mean values of the variables and their coefficients we obtain: a| p x | ≈ 8.5 · 10 3 , b| p y | ≈ 8.7 · 10 3 , c| v x | ≈ 7 · 10 2 , d| v y | ≈ 2.5 · 10 2 , e| θ | ≈ 1.3 · 10 2 , f | ω | ≈ 4.7 · 10 2 . This suggests that we can neglect the values of p x , p y , θ because their mean value is low w.r.t. the maximum. The experiments confirmed that these variables have a low impact on the performance of the agent.</p><p>So, the agent does not fire any engine when:</p><formula xml:id="formula_36">ω &lt; c f v x + d f v y<label>(34)</label></formula><p>This seems an extension of what we obtained in the previous condition, where we also have a dependency from v y . Moreover, it is important to note that this check is performed only when the third condition is not true. Finally, from experiments we observed that this condition is true usually when the agent has successfully landed. In this case, the terms in c l and c r can be seen as a further margin to the agent, so that when a leg touches the ground the agent is more likely to not fire any engine.</p><p>In the opposite case, i.e. when ω ≥ c v x + d v y , we the agent turns on the main engine to balance the high angular velocity of the agent. Note, again, that if the angular velocity is too low it is balanced by the previous condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4.">Considerations</head><p>In this subsection, we interpreted the policy produced in various settings.</p><p>We showed that the decision trees produced are interpretable and give an understanding about how the agent works. It is important to note that in several cases we performed approximations to ease the understanding process. However, this</p><p>is not a limitation of the method, because more exact interpretations can be obtained by not neglecting details. This is especially important in high-stakes or safety-critical settings, where humans need to have a thorough understanding to validate and trust the systems produced.</p><p>Finally, while some solutions may seem hard to interpret (i.e. oblique decision trees), it is important to see the them in a bigger context: while they may not be easy to interpret at a first sight, their analysis is pretty straightforward (as shown earlier). On the other hand, black-box models (such as deep neural networks) are way harder to inspect, due to the significantly bigger number of operations performed in the decision making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>While in recent years AI made a huge progress, the need of being able to understand how a model works is becoming more and more important. To overcome this issue, significant effort was put to advance the XAI field. However, XAI is not always a suitable solution. In fact, they suffer from some problems that make their use unsafe in safety-critical or high-stakes processes.</p><p>Interpretable AI, instead, consists in using transparent approaches in order to have a complete understanding of what happens in the model. However, these models are not widely used in practice because of their widely-thought lower performance.</p><p>In this paper, we propose a two-level optimization method that allows to induce decision trees that can perform reinforcement learning.</p><p>Our results show that the proposed approach is able to generate decision trees that are comparable or even better than the non-interpretable state-ofthe-art (from the performance point of view) while having significantly better interpretability. Furthermore, the results obtained in this work suggest that the widely though performance-interpretability trade-off does not always hold (as suggested by <ref type="bibr" target="#b2">[1]</ref>) and that interpretable models can be competitive with stateof-the-art techniques. For this reason, research in this field must be encouraged.</p><p>Moreover, we compared the solutions obtained to the state-of-the-art from the point of view of the interpretability. While the metric of interpretability does not perfectly suit our purpose, we can easily observe the difference in complexity with respect to black-box models. While we expect that changing the metric of interpretability does not significantly affect the difference w.r.t. black-box models, we think that future work should focus on the study of more tailored interpretability metrics (i.e. tailored on machine learning models).</p><p>Since it is important for practical applications, we also compared our solutions to the interpretable (and publicly available) state-of-the-art w.r.t. robustness to input noise. The results show that our approach is comparably or more robust than the other solutions.</p><p>Finally, we demonstrated that the produced agents can be interpreted, practically showing the advantage of interpretable models w.r.t. black boxes.</p><p>Other future developments include: experimental tests on more complex reinforcement learning domains; the extension of the proposed method to the imitation learning domain; the development of a method that can automatically tune the constants, reducing the prior knowledge that must be included in the grammar; a flexible grammar that easily allows oblique trees to become orthog-onal, to automatically choose the appropriate type of splits depending on the problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>We perform experimental tests on classic reinforcement learning problems: CartPole, MountainCar and LunarLander • We perform a comparison of the produced agents w.r.t. the interpretable and the non-interpretable state-of-the-art • We quantitatively measure the interpretability of our solutions and compare it to the state-of-the-art</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>opposed to Darwinian evolution and Lamarckian evolution, states that what an individual learns during his life is not passed to their offspring. However,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>A high-level representation of the proposed agent in the form of a decision tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•••••• 1 . 3 .</head><label>13</label><figDesc>Horizontal position: p x • Vertical position: p y Horizontal velocity: v x Vertical velocity: v y Angle w.r.t. the vertical axis: θ Angular velocity: ω Left leg contact: c l • Right leg contact: c r Action space. The agent can perform 4 actions: All engines disabled: nop 2. Enable left engine: lef t Enable main engine: main 4. Enable right engine: right Rewards. The reward for moving from the initial point to the landing pad with final velocity of zero varies between 100 and 140 points. If the lander crashes it receives a reward of -100 points. If the lander lands correctly it receives a reward of +100 points. For each leg contact the agent receives a reward of +10 points. Firing the main engine in a timestep gives a reward of -0.3 points, while firing a side-engine gives a reward of -0.03. Termination criterion. The simulator ends if either 1000 timesteps are passed, the lander crashes or it passes the bounds of the environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Fitness of the best solution on CartPole-v1, obtained by using the orthogonal grammar, at each generation averaged across 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Fitness of the best solution on CartPole-v1, obtained by using the oblique grammar, at each generation averaged across 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>A score-M plot of the solutions obtained by using the two types of grammars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8</head><label>8</label><figDesc>shows how the testing mean score varies by varying the number of maximum timesteps for the best agents.InFigures 6 and 7we show the mean distance from the point of equilibrium (p eq = [0, 0, 0, 0] T ) averaged over 100 episodes (of length 500 timesteps). In these figures we can easily observe that the oblique policy seems to be stable (according to the Lyapunov's concept of stability) while the orthogonal policy does not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Mean distance of the cart-pole system from the point of equilibrium when using the best orthogonal tree as policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Mean distance of the cart-pole system from the point of equilibrium when using the best oblique tree as policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Comparison between the best orthogonal tree with the best oblique at different maximum timesteps for the CartPole-v1 environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Performance of the two best agents on CartPole-v1 at the variation of the input noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(From Figure 3 -right in [3]) tested on the same episodes used for the evaluation of our solutions. (*): Result confirmed by personal communication with the first author of the study. (**):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :Figure 11 :Figure 12 :</head><label>101112</label><figDesc>. Xuan et al. Beltiukov Silva et al. Silva et al. (*) Silva et al. (**) Ours Score-M comparison of our solutions and the state-of-the-art in the CartPole-v1 environment. It is compared to the ones described in [23], [24] [3], and [25]. (*): Result confirmed by personal communication with the first author of the study. (**): The tree has been simplified by using the technique used in our work. Tree representation of one of the best individuals evolved in the CartPole-v1 environment by using the orthogonal grammar.−0.274x − 0.543v+ −0.904θ − 0.559ω &lt; −0.169 move right move left true false Tree representation of one of the best individuals evolved in the CartPole-v1 environment by using the orthogonal grammar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>show the results of our comparison. The best trees (on testing mean score) that have been used for the comparison are shown in Figures 17 and 18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :</head><label>13</label><figDesc>Fitness trend for the best individual in the MountainCar-v0 environment averaged on all the runs, when using orthogonal trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Fitness trend for the best individual in the MountainCar-v0 environment averaged on all the runs, when using oblique trees. Plot of the mean testing score with different input noises for the best orthogonal and oblique models on MountainCar-v0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 16 :</head><label>16</label><figDesc>Score-M comparison of our solutions and the state-of-the-art in the MountainCar-v0 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Best orthogonal decision tree (w.r.t. score) evolved in the MountainCar-v0 environment. 0.717 x − 0.697 v &lt; −0.229 Best oblique decision tree (w.r.t. score) evolved in the MountainCar-v0 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 19 :</head><label>19</label><figDesc>Score-M plot of the solutions obtained for the LunarLander-v2 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 20</head><label>20</label><figDesc>shows the average fitness trend for the best solution in each run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 20 :</head><label>20</label><figDesc>Mean fitness trend for the best individual in the population on the LunarLander-v2 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 21 :</head><label>21</label><figDesc>Plot of the score and M of our solutions compared to the state-of-the-art. Our solutions are the ones with a dashed circle around the symbol. The red line represents the "solved" threshold, fixed at 200 for the LunarLander-v2 task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 22 :</head><label>22</label><figDesc>132c r &lt; 0 True left True −0.101p x + 0.133p y + −0.791v x + 0.653v y + −0.207θ + 0.731ω+ +0.068c l + 0.525c r &lt; 0 False main True 0.12p x − 0.044p y + −0.772v x − 0.136v y + −0.169θ + 0.821ω+ −0.573c l − 0.251c r &lt; Best oblique decision tree (w.r.t. score) evolved in the LunarLander-v2 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 23 :</head><label>23</label><figDesc>Tree representation of the solution proposed in<ref type="bibr" target="#b4">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 24 :</head><label>24</label><figDesc>Tree representation of the solution obtained by private communication with the first author of<ref type="bibr" target="#b4">[3]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 25 :</head><label>25</label><figDesc>Robustness to input noise on the CartPole-v1 environment. (*): Private communication with the first author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>a</head><label></label><figDesc>= min(−0.09(x + 0.25) 2 + 0.03, 0.3(x + 0.9) 4 − 0.008) b = −0.07(x + 0.38) 2 + 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 26 :</head><label>26</label><figDesc>Tree representation of the solution proposed in [4] for the MountainCar-v0 environment. The variables with a hat are normalized by using this way: 1 + x−x min xmax−x min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 27 showsFigure 27 :</head><label>2727</label><figDesc>how performance vary by varying the standard deviation of the additive Gaussian noise. We observe that there is no significant difference between the solutions, meaning that all of them have high sensitivity to input noise. Comparison of the robustness to input noise between our solutions and the interpretable ones on the MountainCar-v0 environment.5.3. LunarLander-v2 5.3.1. Silva et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 28 :</head><label>28</label><figDesc>Tree representation of the solution proposed in<ref type="bibr" target="#b4">[3]</ref> for the LunarLander-v2 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 29 :</head><label>29</label><figDesc>Tree representation of the solution proposed in [4] for the LunarLander-v2 environment. The variables with a hat are normalized by using this way: 1 + x−x min xmax−x min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 30 :</head><label>30</label><figDesc>Comparison on the robustness to input noise w.r.t. other interpretable solutions on the LunarLander-v2 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 31 :</head><label>31</label><figDesc>Decision regions for the best oblique tree evolved in the MountainCar-v0 environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>1. 23</head><label>23</label><figDesc>lead to an overestimation of the magnitude of the future position (or angle), while the constant d ≈ 0.53 increases the precision of the approximation. By denoting the predictions of the next position on x, y and θ with p k+1 x , p k+1 y , θ k+1 respectively, we can write: this condition works, let's suppose that p k+1 x ≈ 0 (i.e. the lander is in the center of the environment). Then, if p k+1 y ≈ 1 (i.e. near the starting point), we will fire the right engine if θ k+1 ≤ −b/e ≈ −0.15rad, i.e. the angle of the lander is going to fall to the right. When p k+1 y ≈ 0 (i.e. near the landing pad), the agent will fire the right engine if θ k+1 ≤ 0, so we can say that the farther the lander is from the landing pad (vertically), the more margin we have on the threshold of the angle. Let's now suppose that p k+1 y = 0 to study the effect of p k+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>−</head><label></label><figDesc>c v x + d v y − eθ + f θ − θ k−1 τ &lt; 0(27)An experimental measurement of the τ variable led us to set τ = 0.05. By multiplying all the members by τ we obtain:− τ ap k+1 x + τ bp k+1 y − τ c v x + τ d v y − τ eθ + f (θ − θ k−1 ) &lt; 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>−</head><label></label><figDesc>τ c v x + τ d v y + (f − τ e)θ − f θ k−1 &lt; 0 (29)By merging the terms in θ and θ k−1 we obtain:− c v x + d v y + (f − τ e)ω + τ eθ k−1 &lt; 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Parameters used for the Grammatical Evolution with orthogonal grammar in the CartPole-v1 environment.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Population size</cell><cell>200</cell></row><row><cell>Generations</cell><cell>50</cell></row><row><cell>Genotype length</cell><cell>100</cell></row><row><cell>Crossover probability</cell><cell>0</cell></row><row><cell>Mutation probability</cell><cell>1</cell></row><row><cell>Mutation type</cell><cell>Uniform, with gene probability=0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Parameters used for the Grammatical Evolution with oblique grammar in the CartPole-v1 environment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Run Training mean Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R2</cell><cell>500.00</cell><cell>495.68</cell><cell>42.98</cell><cell>24.10</cell></row><row><cell>R3</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>48.20</cell></row><row><cell>R4</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R5</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R6</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R7</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R8</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R9</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R10</cell><cell>500.00</cell><cell>460.95</cell><cell>132.43</cell><cell>24.10</cell></row></table><note>Scores obtained by training interpretable agents on the CartPole-v1 environment by using the orthogonal grammar.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Scores obtained by training interpretable agents on the CartPole-v1 environment by using the oblique grammar.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Run Testing mean Testing std</cell></row><row><cell>R1</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R2</cell><cell>9900.68</cell><cell>988.22</cell></row><row><cell>R3</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R4</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R5</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R6</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R7</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R8</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R9</cell><cell>10000.00</cell><cell>0.00</cell></row><row><cell>R10</cell><cell>9200.95</cell><cell>2709.71</cell></row></table><note>Scores obtained by testing the solutions obtained by using an orthogonal grammar on a 10 4 -steps-long version of CartPole-v1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Scores obtained by testing the solutions obtained by using an oblique grammar on a 10 4 -steps-long version of CartPole-v1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Grammar used to evolve oblique decision trees in the MountainCar-v0 environment. The symbol "|" denotes the possibility to choose between different symbols. "lt" refers to the "less than" operator. input i refers to the normalized input i variable. For the normalization, the bounds [-1.2, 0.7] and [-0.07, 0.07] were used.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Population size</cell><cell>200</cell></row><row><cell>Generations</cell><cell>1000</cell></row><row><cell>Genotype length</cell><cell>1024</cell></row><row><cell>Crossover probability</cell><cell>0</cell></row><row><cell>Mutation probability</cell><cell>1</cell></row><row><cell>Mutation type</cell><cell>Uniform, with gene probability=0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Parameters used for the Grammatical Evolution with orthogonal grammar in the MountainCar-v0 environment.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Population size</cell><cell>200</cell></row><row><cell>Generations</cell><cell>2000</cell></row><row><cell>Genotype length</cell><cell>100</cell></row><row><cell>Crossover probability</cell><cell>0.1</cell></row><row><cell>Crossover operator</cell><cell>One-point crossover</cell></row><row><cell>Selection operator</cell><cell>Tournament selection with size 2</cell></row><row><cell>Mutation probability</cell><cell>1</cell></row><row><cell>Mutation type</cell><cell>Uniform, with gene probability=0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Parameters used for the Grammatical Evolution with oblique grammar in the MountainCar-v0 environment.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Algorithm</cell><cell>ε-greedy Q-learning</cell></row><row><cell>ε</cell><cell>0.05</cell></row><row><cell>Initialization strategy</cell><cell>Uniform ∈ [−1, 1]</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row><row><cell>Number of episodes</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Parameters used for the Q-learning algorithm in the CartPole-v1 and MountainCar-v0 (only with orthogonal trees) environments.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Algorithm</cell><cell>ε-greedy Q-learning</cell></row><row><cell>ε</cell><cell>0.01</cell></row><row><cell>Initialization strategy</cell><cell>Uniform ∈ [−1, 1]</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row><row><cell>Number of episodes</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Parameters used for the Q-learning algorithm in the MountainCar-v0 environment when evolving oblique trees.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>-109.3</cell><cell>-106.17</cell><cell>4.69</cell><cell>89</cell></row><row><cell>R2</cell><cell>-110.5</cell><cell>-108.62</cell><cell>16.72</cell><cell>124.6</cell></row><row><cell>R3</cell><cell>-105.6</cell><cell>-102.26</cell><cell>9.51</cell><cell>71.2</cell></row><row><cell>R4</cell><cell>-108.1</cell><cell>-101.72</cell><cell>3.14</cell><cell>106.8</cell></row><row><cell>R5</cell><cell>-112.9</cell><cell>-116.15</cell><cell>1.03</cell><cell>71.2</cell></row><row><cell>R6</cell><cell>-107.2</cell><cell>-101.72</cell><cell>3.14</cell><cell>106.8</cell></row><row><cell>R7</cell><cell>-120.5</cell><cell>-117.84</cell><cell>0.95</cell><cell>35.6</cell></row><row><cell>R8</cell><cell>-115.7</cell><cell>-115.51</cell><cell>1.18</cell><cell>35.6</cell></row><row><cell>R9</cell><cell>-109.3</cell><cell>-106.63</cell><cell>4.68</cell><cell>89</cell></row><row><cell>R10</cell><cell>-107.1</cell><cell>-104.94</cell><cell>3.56</cell><cell>53.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc></figDesc><table><row><cell cols="2">Run Training score</cell><cell>Testing mean</cell><cell>Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>-108.90</cell><cell>-106.66</cell><cell>9.30</cell><cell>70.00</cell></row><row><cell>R2</cell><cell>-106.50</cell><cell>-110.18</cell><cell>24.90</cell><cell>46.60</cell></row><row><cell>R3</cell><cell>-105.60</cell><cell>-106.50</cell><cell>15.27</cell><cell>23.40</cell></row><row><cell>R4</cell><cell>-109.10</cell><cell cols="3">-200.00 (-112.62) 0.00 (23.08) 0.00 (46.6)</cell></row><row><cell>R5</cell><cell>-106.10</cell><cell>-106.06</cell><cell>12</cell><cell>46.80</cell></row><row><cell>R6</cell><cell>-110.40</cell><cell>-116.66</cell><cell>16.01</cell><cell>46.60</cell></row><row><cell>R7</cell><cell>-112.80</cell><cell>-114.44</cell><cell>10.74</cell><cell>46.40</cell></row><row><cell>R8</cell><cell>-105.00</cell><cell>-200.00 (-107.5)</cell><cell cols="2">0.00 (13.46) 0.00 (23.4)</cell></row><row><cell>R9</cell><cell>-103.20</cell><cell>-106.02</cell><cell>15.41</cell><cell>46.80</cell></row><row><cell>R10</cell><cell>-111.40</cell><cell>-116.49</cell><cell>16.75</cell><cell>46.80</cell></row></table><note>Scores obtained by training interpretable agents on the MountainCar-v0 environ- ment when using the orthogonal grammar.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Scores obtained by training interpretable agents on the MountainCar-v0 environment when using the oblique grammar.</figDesc><table /><note>of a leaf. This change made the Q-values of the action taken with the current greedy policy have a value approximately equal to the another action. This caused a destructive change in the policy, so, in order to give more information, we included the test score of the solution by reverting the destructive change. Moreover, this change has only been used in this table. For the remainder of this work, we will assume that their test score is −200.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 18 :</head><label>18</label><figDesc></figDesc><table /><note>Comparison of the mean (testing) score of the solutions obtained by using the proposed approach versus the state-of-the-art.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 20 :</head><label>20</label><figDesc>Parameters used for the Grammatical Evolution with oblique grammar in the LunarLander-v2 environment.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Algorithm</cell><cell>ε-greedy Q-learning with ε-decay</cell></row><row><cell>ε 0</cell><cell>1</cell></row><row><cell>Decay multiplier</cell><cell>0.99</cell></row><row><cell>Initialization strategy</cell><cell>Constant, with value 0</cell></row><row><cell>Learning rate</cell><cell>1/k, k is the number of visits of the action</cell></row><row><cell>Number of episodes</cell><cell>1000 with early stopping</cell></row><row><cell>Early stopping period</cell><cell>30 episodes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 21 :</head><label>21</label><figDesc>Parameters used for the Q-learning algorithm in the LunarLander-v2 environment.between interpretability and performance composed of the solutions obtained in Runs 1 and 6.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>265.77</cell><cell>262.18</cell><cell>29.32</cell><cell>86.90</cell></row><row><cell>R2</cell><cell>256.07</cell><cell>252.40</cell><cell>21.80</cell><cell>146.70</cell></row><row><cell>R3</cell><cell>251.95</cell><cell>240.50</cell><cell>37.21</cell><cell>145.30</cell></row><row><cell>R4</cell><cell>248.19</cell><cell>234.45</cell><cell>79.07</cell><cell>117.50</cell></row><row><cell>R5</cell><cell>220.59</cell><cell>206.48</cell><cell>64.74</cell><cell>87.60</cell></row><row><cell>R6</cell><cell>274.77</cell><cell>272.14</cell><cell>28.31</cell><cell>118.90</cell></row><row><cell>R7</cell><cell>254.33</cell><cell>230.65</cell><cell>78.29</cell><cell>207.90</cell></row><row><cell>R8</cell><cell>256.09</cell><cell>251.21</cell><cell>32.81</cell><cell>87.60</cell></row><row><cell>R9</cell><cell>265.21</cell><cell>257.75</cell><cell>31.45</cell><cell>147.40</cell></row><row><cell>R10</cell><cell>262.86</cell><cell>252.70</cell><cell>43.11</cell><cell>87.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 22 :</head><label>22</label><figDesc></figDesc><table><row><cell></cell><cell>270</cell><cell>Run R1</cell></row><row><cell></cell><cell></cell><cell>R2</cell></row><row><cell></cell><cell>260</cell><cell>R3</cell></row><row><cell></cell><cell></cell><cell>R4</cell></row><row><cell>Testing mean</cell><cell>230 240 250</cell><cell>R5 R6 R7 R8 R9 R10</cell></row><row><cell></cell><cell>220</cell><cell></cell></row><row><cell></cell><cell>210</cell><cell></cell></row><row><cell></cell><cell>80</cell><cell>100 120 140 160 180 200 220 240 </cell></row></table><note>Summary of the best interpretable agents obtained for each run on the LunarLander- v2 environment. The task is considered solved when the mean score on 100 independent runs is greater or equal to 200.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 23 :</head><label>23</label><figDesc></figDesc><table /><note>Comparison of the proposed solution with respect to the state-of-the-art on the LunarLander-v2 environment. The results from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 24 :</head><label>24</label><figDesc>Grammar used to evolve orthogonal decision trees in the CartPole-v1 environment without Q-learning.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>500.00</cell><cell>500.00</cell><cell>0</cell><cell>53.40</cell></row><row><cell>R2</cell><cell>500.00</cell><cell>436.39</cell><cell>120.91</cell><cell>71.20</cell></row><row><cell>R3</cell><cell>500.00</cell><cell>473.32</cell><cell>80.42</cell><cell>35.60</cell></row><row><cell>R4</cell><cell>500.00</cell><cell>498.3</cell><cell>11.21</cell><cell>53.40</cell></row><row><cell>R5</cell><cell>500.00</cell><cell>418.30</cell><cell>136.11</cell><cell>35.60</cell></row><row><cell>R6</cell><cell>500.00</cell><cell>489.6</cell><cell>23.46</cell><cell>35.60</cell></row><row><cell>R7</cell><cell>500.00</cell><cell>486.63</cell><cell>45.32</cell><cell>35.60</cell></row><row><cell>R8</cell><cell>500.00</cell><cell>468.20</cell><cell>82.34</cell><cell>35.60</cell></row><row><cell>R9</cell><cell>500.00</cell><cell>455.57</cell><cell>126.74</cell><cell>71.20</cell></row><row><cell>R10</cell><cell>500.00</cell><cell>483.11</cell><cell>41.99</cell><cell>71.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 26 :</head><label>26</label><figDesc>Grammar used to evolve oblique decision trees in the CartPole-v1 environment without Q-learning.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R2</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R3</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R4</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R5</cell><cell>500.00</cell><cell>477.29</cell><cell>71.38</cell><cell>48.20</cell></row><row><cell>R6</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R7</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R8</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row><row><cell>R9</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>46.80</cell></row><row><cell>R10</cell><cell>500.00</cell><cell>500.00</cell><cell>0.00</cell><cell>24.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 27 :</head><label>27</label><figDesc>Results obtained by evolving oblique decision trees for the CartPole-v1 environment by using Grammatical Evolution alone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 28 :</head><label>28</label><figDesc>Grammar used to evolve orthogonal decision trees in the CartPole-v1 environment without Q-learning.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>-104.10</cell><cell>-107.44</cell><cell>16.02</cell><cell>106.8</cell></row><row><cell>R2</cell><cell>-115.60</cell><cell>-115.60</cell><cell>1.31</cell><cell>35.60</cell></row><row><cell>R3</cell><cell>-119.40</cell><cell>-119.34</cell><cell>3.66</cell><cell>17.80</cell></row><row><cell>R4</cell><cell>-118.70</cell><cell>-125.86</cell><cell>28.68</cell><cell>71.20</cell></row><row><cell>R5</cell><cell>-119.40</cell><cell>-119.34</cell><cell>3.66</cell><cell>35.60</cell></row><row><cell>R6</cell><cell>-103.00</cell><cell>-106.02</cell><cell>15.21</cell><cell>106.80</cell></row><row><cell>R7</cell><cell>-103.20</cell><cell>-108.31</cell><cell>18.53</cell><cell>124.60</cell></row><row><cell>R8</cell><cell>-103.00</cell><cell>-105.98</cell><cell>15.03</cell><cell>89.00</cell></row><row><cell>R9</cell><cell>-105.40</cell><cell>-104.71</cell><cell>3.66</cell><cell>106.80</cell></row><row><cell>R10</cell><cell>-101.80</cell><cell>-114.09</cell><cell>30.81</cell><cell>89.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 29 :</head><label>29</label><figDesc>Results obtained by evolving orthogonal decision trees for the MountainCar-v0 environment by using Grammatical Evolution alone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head>Table 30 :</head><label>30</label><figDesc>Grammar used to evolve oblique decision trees in the CartPole-v1 environment without Q-learning.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>-102.00</cell><cell>-105.83</cell><cell>16.49</cell><cell>139.80</cell></row><row><cell>R2</cell><cell>-97.10</cell><cell>-106.8</cell><cell>23.61</cell><cell>93.40</cell></row><row><cell>R3</cell><cell>-102.00</cell><cell>-107.74</cell><cell>20.33</cell><cell>70.20</cell></row><row><cell>R4</cell><cell>-101.40</cell><cell>-111.36</cell><cell>23.98</cell><cell>70.00</cell></row><row><cell>R5</cell><cell>-101.80</cell><cell>-109.71</cell><cell>23.12</cell><cell>93.40</cell></row><row><cell>R6</cell><cell>-101.90</cell><cell>-108.79</cell><cell>21.58</cell><cell>116.80</cell></row><row><cell>R7</cell><cell>-101.30</cell><cell>-110.56</cell><cell>25.63</cell><cell>93.40</cell></row><row><cell>R8</cell><cell>-97.20</cell><cell>-108.09</cell><cell>30.34</cell><cell>116.60</cell></row><row><cell>R9</cell><cell>-101.80</cell><cell>-107.48</cell><cell>20.00</cell><cell>93.20</cell></row><row><cell>R10</cell><cell>-105.80</cell><cell>-107.14</cell><cell>14.66</cell><cell>93.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 31 :</head><label>31</label><figDesc>Results obtained by evolving oblique decision trees for the MountainCar-v0 environment by using Grammatical Evolution alone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>Table 32 :</head><label>32</label><figDesc>Grammar used to evolve oblique decision trees in the LunarLander-v2 environment without Q-learning.</figDesc><table><row><cell cols="4">Run Training score Testing mean Testing std</cell><cell>M</cell></row><row><cell>R1</cell><cell>-88.83</cell><cell>-90.25</cell><cell>34.46</cell><cell>147.40</cell></row><row><cell>R2</cell><cell>216.45</cell><cell>172.83</cell><cell>76.83</cell><cell>60.50</cell></row><row><cell>R3</cell><cell>241.37</cell><cell>228.12</cell><cell>47.7</cell><cell>59.10</cell></row><row><cell>R4</cell><cell>272.25</cell><cell>252.88</cell><cell>54.27</cell><cell>115.40</cell></row><row><cell>R5</cell><cell>231.83</cell><cell>216.65</cell><cell>60.59</cell><cell>117.50</cell></row><row><cell>R6</cell><cell>103.36</cell><cell>49.15</cell><cell>127.21</cell><cell>89.00</cell></row><row><cell>R7</cell><cell>266.90</cell><cell>251.28</cell><cell>42.95</cell><cell>120.30</cell></row><row><cell>R8</cell><cell>247.40</cell><cell>205.25</cell><cell>73.41</cell><cell>58.40</cell></row><row><cell>R9</cell><cell>254.00</cell><cell>243.95</cell><cell>34.58</cell><cell>88.30</cell></row><row><cell>R10</cell><cell>5.47</cell><cell>-57.84</cell><cell>120.95</cell><cell>122.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head>Table 33 :</head><label>33</label><figDesc>Results obtained by evolving oblique decision trees for the LunarLander-v2 environment by using Grammatical Evolution alone.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://gitlab.com/leocus/ge q dts, accessed: 11 dec 2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">github.com/ZhiqingXiao/OpenAIGymSolution, accessed: 11 dec 2020. 3 github.com/StepNeverStop/RLs, accessed: 11 dec 2020. 4 github.com/harshitandro/Deep-Q-Network, accessed: 11 dec 2020. 5 github.com/CM-Data/Noisy-Dueling-Double-DQN-MountainCar, accessed: 11 dec 2020. 6 github.com/amitkvikram/rl-agent, accessed: 11 dec 2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">github.com/ZhiqingXiao/OpenAIGymSolution/tree/master/MountainCar-v0 close form</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">Implementing this policy by using the ω given by the environment may give slightly lower than perfect scores, in our opinion this is due to the error carried by the integration method used. On the other hand, using ω k = (θ k − θ k−1 )/τ gives the desired results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">StepNeverStop/RLs</title>
		<imprint>
			<date type="published" when="2020-12-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">14 github.com/RMiftakhov/LunarLander-v2-drlnd, accessed: 11 dec 2020. 15 github.com/Cozmo25/openai-lunar-lander-v2</title>
		<idno>2019/02/02</idno>
		<ptr target="17github.com/udacity/deep-reinforcement-learning" />
	</analytic>
	<monogr>
		<title level="m">16 github.com/nikhilbarhate99/Actor-Critic-PyTorch</title>
		<imprint>
			<date type="published" when="2020-12-11" />
		</imprint>
	</monogr>
	<note>/XinliYu/Reinforcement Learning-Projects</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gil-Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gombolay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Son</surname></persName>
		</author>
		<title level="m">Optimization Methods for Interpretable Differentiable Decision Trees Applied to Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1855" to="1865" />
		</imprint>
	</monogr>
	<note>International Conference on Artificial Intelligence and Statistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nageshrao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09521</idno>
		<title level="m">Interpretable-AI Policies using Evolutionary Nonlinear Decision Trees for Discrete Action Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reinforcement learning with selective perception and hidden state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>The University of Rochester, Eastman School of Music</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tree based discretization for continuous state space reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Uther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>AAAI/IAAI</publisher>
			<biblScope unit="page" from="769" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Decision tree function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Howe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Colorado State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01180</idno>
		<title level="m">Conservative qimprovement: Reinforcement learning for an interpretable decision-tree policy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evolving Behaviour Trees for the Mario AI Competition Using Grammatical Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brabazon</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1008/978-3-642-20525-5_13</idno>
	</analytic>
	<monogr>
		<title level="m">Applications of Evolutionary Computation</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grammatical evolution: Evolving programs for an arbitrary language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Neill</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/BFb0055930</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Genetic Programming</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="83" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hallawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmeink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Iacca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ascheid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evo-Rl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04725[cs,stat]ArXiv:2007.04725</idno>
		<title level="m">Evolutionary-Driven Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A memetic algorithm for global induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krȩtowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Current Trends in Theory and Practice of Computer Science</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Genetic programming: on the programming of computers by means of natural selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multi-objective evolutionary approach to Pareto-optimal model trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Czajkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krȩtowski</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-018-3646-3</idno>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1423" to="1437" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evolution of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Llora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Garrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Catalan Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge-independent data mining with finegrained parallel evolutionary algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Llorà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Garrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Annual Conference on Genetic and Evolutionary Computation</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="461" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constructing optimal binary decision trees is NPcomplete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hyafil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="17" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regression tree analysis using TARGET</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="218" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openai</forename><surname>Gym</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning a formula of interpretability to learn interpretable formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Virgolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Medvet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Randone</surname></persName>
		</author>
		<editor>T. Bäck, M. Preuss, A. Deutz, H. Wang, C. Doerr, M. Emmerich, H. Trautmann</editor>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="79" to="93" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Parallel Problem Solving from Nature -PPSN XVI</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236386.3241340</idno>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Model interpretability through the lens of computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barceló</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Subercaseaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Qualitative Measurements of Policy Discrepancy for Return-Based Deep Q-Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian Deep Reinforcement Learning via Deep Kernel Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beltiukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q-Learning With K-Fac</forename><surname>Optimizing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Algorithm</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-030-39575-9_1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Analysis of Images</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10891</idno>
		<idno>arXiv:1912.10891</idno>
		<title level="m">Soft Q-network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00177[cs,stat]ArXiv:1910.00177</idno>
		<title level="m">Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Reinforcement Learning with Adaptive Update Target Combination</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="995" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ceberio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10932[cs]ArXiv:1904.10932</idno>
		<title level="m">Evolving Neural Networks in Reinforcement Learning by means of UMDAc</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347[cs]ArXiv:1707.06347</idno>
		<title level="m">Proximal Policy Optimization Algorithms</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
