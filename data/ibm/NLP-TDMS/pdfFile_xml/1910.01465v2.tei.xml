<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ackermann</surname></persName>
							<email>johannes.ackermann@tum.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Gabler</surname></persName>
							<email>v.gabler@tum.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugiyama</forename><surname>Masashi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riken</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Takayuki Osa Kyushu Institute of Technology, RIKEN</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real world tasks require multiple agents to work together. Multi-agent reinforcement learning (RL) methods have been proposed in recent years to solve these tasks, but current methods often fail to efficiently learn policies. We thus investigate the presence of a common weakness in single-agent RL, namely value function overestimation bias, in the multi-agent setting. Based on our findings, we propose an approach that reduces this bias by using double centralized critics. We evaluate it on six mixed cooperative-competitive tasks, showing a significant advantage over current methods. Finally, we investigate the application of multiagent methods to high-dimensional robotic tasks and show that our approach can be used to learn decentralized policies in this domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, many real world problem settings have been modeled as multi-agent systems, be it smart-grid applications <ref type="bibr" target="#b9">[10]</ref>, package routing <ref type="bibr" target="#b27">[28]</ref>, or road transportation <ref type="bibr" target="#b0">[1]</ref>.</p><p>While it is possible to regard these problems as a centralized single agent, with a large state and action space, and apply methods from single-agent reinforcement learning (RL), this leads to an action space that increases exponentially with the number of agents <ref type="bibr" target="#b14">[15]</ref>. Another approach is to assume independent learners <ref type="bibr" target="#b23">[24]</ref>, in which agents regard the influence of other agents as part of the environment. However, due to the behavior of other agents changing over time, the transition probabilities change, leading to the Markov assumption being violated. Therefore, recent research has focused on decomposing these systems into individual, decentralized agents during execution, while updating them in a centralized training phase, allowing to maintain the Markov property during training <ref type="bibr" target="#b17">[18]</ref>. Although recent research has introduced powerful multi-agent reinforcement learning (MARL) techniques based on this principle, such as counterfactual multi-agent (COMA) policy gradients <ref type="bibr" target="#b3">[4]</ref>, or the multi-agent deep deterministic policy gradient (MADDPG) method <ref type="bibr" target="#b13">[14]</ref>, their performance has not been studied as thoroughly as approaches in single-agent RL. Motivated by findings in the single-agent case <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref>, which have shown it to generally suffer from an overestimation bias of the value function, we thus investigate this issue in MARL on the example of the popular MADDPG method.</p><p>Similarly to multi-agent tasks, complex robotic systems face the challenge of high-dimensional and continuous state-action spaces. A popular way to approach these tasks is decentralized control <ref type="bibr" target="#b6">[7]</ref>, which requires a high amount of model knowledge. Recently <ref type="bibr" target="#b18">[19]</ref> presented a method to learn decentralized policies with RL, but it still relies on a shared or centralized meta-policy. We propose a way to learn truly decentralized policies, by modeling robotic systems as multi-agent systems, eliminating the need for a centralized controller.</p><p>The main contribution of our work is a new method for MARL, that addresses overestimation bias and outperforms previous methods in most of the evaluated cooperative-competitive tasks. Furthermore, we provide an approach to learn decentralized policies for high-dimensional robotic tasks, based on MARL. We show that our method is able to learn decentralized policies on a simulated task and outperforms existing MARL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we explain the relevant background for our work, first explaining general methods in RL, then addressing more recent policy gradient algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Markov Games</head><p>In our work, we focus on Markov games <ref type="bibr" target="#b12">[13]</ref>, an extension of Markov decision processes to multiagent domains. A Markov game with N agents consists of a state set S, a collection of action sets, A 1 , . . . , A N , a transition function T : S × A 1 × · · · × A N → Dist(S), with Dist(S) being a distribution over states. Each agent has its own reward function r i : S × A 1 × · · · × A N → R, which depends on the actions of all agents. Since we regard decentralized agents, they each posses a different observation set O i , which is available to them during execution, and choose actions according to a policy π i :</p><formula xml:id="formula_0">O i → Dist(A i ).</formula><p>The agents each aim to maximize their own total expected return R i = t=T t=0 γ t r i , with a discount factor 0 &lt; γ ≤ 1 and time horizon T . If r i = kr j , i = j, the interaction is cooperative for k &gt; 0 and competitive for k &lt; 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Q-Learning</head><p>Q-learning <ref type="bibr" target="#b21">[22]</ref> is an off-policy algorithm that learns the value of executing action a in state s in form of the expected return Q π (s, a) = E[R|s t = s, a t = a], which can be recursively obtained as Q π (s, a) = E s [r(s, a) + γE a ∼π(s ) [Q π (s , a )]]. Assuming a greedy policy π(s) = arg max a (Q π (s, a)), Q-learning can be used to learn an optimal policy. Mnih et al. <ref type="bibr" target="#b15">[16]</ref> proposed an approach that approximates the Q-function with multi-layer perceptrons (MLPs), called deep Q-networks (DQN). It uses a target network Q π θ , whose parameters θ slowly follow the network parameters of Q π θ , to update the parameters of the Q-network. Additionally, transitions (s, a, r, s ) are stored in a replay buffer D.</p><p>Double Q-learning <ref type="bibr" target="#b24">[25]</ref> found that Q-learning often overestimates the Q-value in stochastic environments, leading to a failure to learn an efficient policy. To remove this positive bias, they proposed to learn two Q-functions Q 1 , Q 2 , which are updated using the value of the respective other function. For Q 1 , this resolves to Q 1 (s, a) = Q 1 (s, a) + α(r + γQ 2 (s , a ) − Q 1 (s, a)), with a = arg max a Q 1 (s , a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Policy Gradient Methods</head><p>Sutton et al. <ref type="bibr" target="#b22">[23]</ref> took a different approach to optimizing the behavior of the agent, by directly using gradient descent on the parameters of the policy. Their target J(θ) = E s∼p π ,a∼π θ [ ∞ t=0 γ t r t ] is defined as the expected total reward over a policy dependent state distribution p π and the gradient resolves to ∇ θ J(θ) = E s∼p π ,a∼π θ [∇ θ log π θ (a|s)Q π (s, a)] .</p><p>In 2014, Silver et al. <ref type="bibr" target="#b19">[20]</ref> derived a formulation of the policy gradient theorem for deterministic policies µ : S → A called deterministic policy gradient (DPG). They also showed that deterministic policies tend to learn significantly quicker than stochastic policies in some domains.</p><p>An algorithm for RL in continuous control problems, based on DPG, was presented in <ref type="bibr" target="#b11">[12]</ref>, called the deep deterministic policy gradient (DDPG). It performs off-policy updates using transitions from a replay buffer D and utilizes a target network, as in DQN. Using this, the gradient in (1) becomes</p><formula xml:id="formula_2">∇ θ J(θ) = E s∼D ∇ θ µ θ (s)∇ a Q µ (s, a)| a=µ θ (s) .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">TD3</head><p>The twin delayed deep deterministic policy gradient (TD3) <ref type="bibr" target="#b4">[5]</ref> improves on DDPG by addressing the overestimation bias of the Q-function, similarly to double Q-learning. They find that due to approximation errors of the MLP, combined with gradient descent, DDPG tends to overestimate the Q-value of state-action pairs, leading to a slower convergence. TD3 addresses this by using two Q-networks Q θ1 , Q θ2 , along with two target networks. The Q-functions are updated with the target y = r t + γ min 1,2 Q θ i (s , a ), while updating the policy with Q θ1 . Additionally, they introduce target policy smoothing by adding noise in the determination of the next action for the critic target a = µ θ π (s )+ , with being clipped Gaussian noise = clip(N (0, σ), −c, c), where c is a tunable parameter.</p><p>Additionally they use delayed upolicy updates, and only update the policy π and target network parameters θ π , θ Q once every d critic updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Multi-Agent Deep Deterministic Policy Gradient</head><p>MADDPG <ref type="bibr" target="#b13">[14]</ref> is an extension of DDPG to the multi-agent setting. It uses the decentralized execution with a centralized training setting, learning a centralized critic that has access to the policies of all agents. This centralized Q-function, representing the expected future reward of agent i, is then learned with</p><formula xml:id="formula_3">Q π i (x, a 1 , ..., a N ) = E r,x [r i + γQ π i (x , µ 1 (o 1 ), ..., µ N (o N )]<label>(3)</label></formula><p>Using this Q-function, the deterministic policy of agent i can be optimized by gradient descent:</p><formula xml:id="formula_4">∇ θi J(µ i ) = E x,a j =i ∼D ∇ θi µ i (a i |o i )∇ ai Q π i (x, a 1 , ..., a N )| ai=µi(oi) .<label>(4)</label></formula><p>In this work we denote the observation received at runtime by agent i as o i , and the full state information as x, from which the observations o i are derived. The replay buffer D here contains transitions (x, a 1 , ..., a N , r 1 , ..., r N , x ) of all agents. Motivated by related work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, that found an overestimation bias in other methods, we investigate whether this effect persists in the multiagent domain on the example of the popular MADDPG approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overestimation Bias in a Centralized Critic</head><p>As we are using deterministic policies, we can, in the short-term, approximate the environment as stationary from the view-point of each agent, so that the we can regard the transition probability as P (s |s, a 1 , ..., a N ) ≈ P (s |s, a i ).</p><p>Under this assumption, we can approximate the value of our centralized critic as Q π i (x, a 1 , ..., a N ) ≈ Q π i (x, a i ), reducing the setting to the one regarded in <ref type="bibr" target="#b4">[5]</ref>, in which they have shown that overestimation occurs in DDPG.</p><p>Empirical Evaluation: To test whether this overestimation also appears in practice, we evaluated MADDPG on the "Cooperative Navigation" task as outlined in <ref type="bibr" target="#b13">[14]</ref>. We increased the number of time-steps per episode to 200 and determine the true and estimated Q-values by sampling states and actions from the replay buffer, that were saved since the last evaluation time-step. From those states, we perform 200 rollouts, with 100 steps each, and save the discounted reward. We then compare the mean of the discounted rewards with the value of the Q-function approximator. The results are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. They show that MADDPG tends to overestimate the Q-values, especially during earlier episodes. Looking at single runs, we can see that this overestimation does not always occur, but when it happens it leads to a significantly worse final performance.</p><p>It should also be noted that the evaluated domains are deterministic. In contrast to that, most real world applications are stochastic. Stochasticity has been shown to lead to a higher value function overestimation, because it adds to the noise from function approximator errors <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Agent TD3</head><p>Our proposed approach, called multi-agent TD3 (MATD3), extends TD3 to the multi-agent domain in a similar manner to the extension of DDPG to MADDPG. We use the centralized training with decentralized execution setting, in which we assume that during training we have access to the past actions, observations and rewards, as well as policies, of all agents. We use this information to learn two centralized critics Q π i,θ1,2 (x, a 1 , ..., a N ) for each agent i. In order to reduce the overestimation bias, we update them with the minimum of both critics:</p><formula xml:id="formula_5">y i = r i + γ min j=1,2 Q π i,θj (x , a 1 , ..., a N )</formula><p>. This may lead to underestimation, however, this is preferable to overestimation: In the case of overestimation, actions with an overestimated value are chosen with a higher probability, due to the policy update. When then updating the critic, the overestimated value of the next action a is used Q π (x , µ (o )), which propagates the error to the update target y. If it is underestimated, the probability of choosing this action is reduced in the policy update. It is thus not used to update the Q-values and the error does not propagate further.</p><p>In addition, we use target policy smoothing, adding clipped Gaussian noise = clip <ref type="figure">(N (0, σ)</ref>, −c, c) to the actions of all agents in the critic update: a j = µ θ j (o j ) + . This serves as a regularization, based on the assumption that similar actions should have similar values. The complete target for the critic resolves to</p><formula xml:id="formula_6">y i = r i + γ min j=1,2 Q π i,θ j (x , µ 1 (o 1 ) + , ..., µ N (o N ) + ) ,<label>(5)</label></formula><p>with µ i being short for µ θ i . The policies are updated similar to (4), but using Q i,θ1 instead of Q i . We also employ delayed policy updates, only updating the target networks θ Q , θ π and policies π i after every d critic updates. This is motivated in <ref type="bibr" target="#b4">[5]</ref> by the need to have an accurate critic before using it to update the policy, thus updating the critic more often. This is especially crucial in multi-agent domains, as the change of the critic values has to reflect not only small changes in the own policy, but also in the policies other agents. However, in adversarial settings this is not always beneficial, as it can slow the adaption to the policy of an adversary. The full algorithm is shown in the Appendix. We evaluate the efficacy of our approach on the particle environments proposed by <ref type="bibr" target="#b16">[17]</ref> and used to evaluate MADDPG in <ref type="bibr" target="#b13">[14]</ref>. They are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The particle environments consist of twodimensional continuous state spaces, in which agents can exert a force on themselves. Additionally, the agents may have access to a discrete communication channel to each of the other agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation in Particle Environments</head><p>The particle environments consist of a set of six tasks: Two cooperative tasks called "Cooperative Navigation" and "Cooperative Communication", in addition to four adversarial tasks "Covert Communication", "Keep-Away", "Physical Deception" and "Predator-Prey". In all of the adversarial tasks, there is a team of "Agents" and a single "Adversary". Most of the tasks require a team of agents to learn a cooperative strategy, which can deal with most behaviors of the adversarial agent. An exception is the "Covert Communication" task, in which the adversary has to decode a message the agents are sending to each other. In this task a good result can be achieved by quickly changing the communication scheme, without learning a more complex behavior. <ref type="figure">Figure 3</ref>: Evaluation in the cooperative domains used in <ref type="bibr" target="#b13">[14]</ref>, "Cooperative Navigation" (left) and "Cooperative Communication" (right). We can see, that MATD3 significantly outperforms MADDPG. Shown is the mean episodic reward over the last 1000 episodes, shaded areas are the 95 % confidence intervals of the mean, averaged across 20 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We implement our approach, named MATD3, and compare its performance to MADDPG. <ref type="bibr" target="#b1">2</ref> Hyper-parameters are chosen by grid-search over learn rate α = [0.01, 0.003, 0.001], minibatch size b = [256, 1000] and policy update frequency d = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. The parameters we found to work best are α = 0.01, b = 1000, d = 2. We use the same set of parameters for all tasks, to ensure a fair comparison.</p><p>For MADDPG we use the hyper-parameters and implementation provided in <ref type="bibr" target="#b13">[14]</ref>, which were tuned for the same tasks. For both approaches we approximate the Q-functions and policies with MLPs with two hidden layers with 64 units each. As activation function we use the Rectified Linear Unit (ReLU) function, and as optimizer we use use Adam <ref type="bibr" target="#b8">[9]</ref> in all experiments, as well as the Gumbel-Softmax estimator <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cooperative Environments</head><p>The results in the cooperative tasks are shown in <ref type="figure">Figure 3</ref>. On the cooperative navigation task both achieve a similar final performance, while MATD3 learns a better policy significantly faster. On the cooperative communication task MATD3 achieves a significantly better final performance.</p><p>Competitive Environments Results in the competitive environments are shown in <ref type="figure" target="#fig_2">Figure 4</ref>, as 0-1 normalized, final rewards, averaged across 20 trials each. They show that, in direct comparison, MATD3 outperforms MADDPG in three out of four environments.</p><p>In the task where MADDPG significantly outperforms our proposed approach, 'Covert Communication', a team of agents has to learn a communication strategy, that the single adversary has to decode. Due to the delayed policy updates, MATD3 is slower at adapting to its opponent's behavior, thus <ref type="figure">Figure 5</ref>: Effect of the policy update rate d on performance in "Cooperative Navigation" (left) and "Covert Communication" (right). We can see, that a less frequent policy update is beneficial in the cooperative task, while in the adversarial task it leads to a better performance of the Adversary, i.e., it being better at decrypting the communication of the agent team. <ref type="figure">Figure 6</ref>: Evaluation of target policy smoothing on the "Cooperative Navigation" task. Shown is the mean reward for different values for , averaged across 10 runs each. Note the zoomed in axis. In our evaluation we did not find a significant advantage of target policy smoothing.</p><p>being outperformed by MADDPG. In the other tasks a consistent winning strategy, that can beat all behaviors of the single agent, can be learned, at which our proposed approach succeeds.</p><p>Delayed Policy Updates Delayed policy updates are intended to ensure a sufficiently converged critic before using it to update the policy. To investigate their effect in MARL, we evaluated different policy update rates and show the results in <ref type="figure">Figure 5</ref>. Less frequent policy updates showed to be beneficial in all tasks, leading to a lower variance in results and a higher final performance, with the exception being the "Covert Communication" task. In this task, the team of agents is made slower at changing it's communication policy, leading to the adversary being better at decrypting it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Policy Smoothing</head><p>We evaluate the effect of target policy smoothing in <ref type="figure">Figure 6</ref> for different levels of added noise . We do not find target policy smoothing to improve the performance unlike in <ref type="bibr" target="#b4">[5]</ref>. We assume that this is due to the policies of the other agents used in the critic target being updated frequently, and thus implicitly introducing a similar randomness.</p><p>We also evaluated the relative overestimation for different numbers of agents, but did not find a significant difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Learning Fully Decentralized Controllers for Robotic Systems</head><p>The particle environments regarded in the previous section are mostly fully observable, with the actions of one agent often not strongly affecting the other agents. Thus, in many tasks, a high reward can be achieved without much cooperation. We therefore also evaluate our approach in a new, challenging setting: Learning decentralized controllers for robotic systems. In high-dimensional robotic tasks decentralized control has been shown to be an effective method, enabling efficient locomotion <ref type="bibr" target="#b26">[27]</ref>. It commonly functions by virtually subdividing the robot into multiple parts. These parts are then coordinated by a central controller. Additionally, in many cases this kind of decentralization is required, for example due to band-width limitations or the structure of the robot.</p><p>We propose to eliminate the need for a centralized or shared control policy by regarding the robot as a multi-agent system. Furthermore, this does not require the additional model knowledge to design a decentralized controller. We thus partition the robotic system into multiple agents, which only have access to partial information about the state of the other agents. The agents learn to coordinate their actions based on the reward signal they receive and the centralized critic.</p><p>The reduction to a partial observation for each agent leads to the task becoming a partially observable stochastic game <ref type="bibr" target="#b5">[6]</ref>, that requires a large degree of cooperation between agents. We therefore do not aim to outperform the current state-of-the-art methods proposed for single-agent continuous tasks, as they have access to the full-state, but see this as a new, challenging task for MARL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Decomposition to Multiple Agents</head><p>We evaluate the functionality of our proposed approach on the OpenAI Gym [2] "Ant-v2" task. The ant consists of a spherical torso and four legs. The legs each have two actuated joints, one at the <ref type="figure">Figure 7</ref>: The "Ant-v2" task, split into two agents, visualized as the green and blue part. The observation of each agent consists of full information of its side, but only includes the joint positions of the other side, without acting forces or velocities. <ref type="figure">Figure 8</ref>: Performance on the "Ant-v2" task. Mean reward of deterministic evaluation episodes, averaged over 6 seeds per approach. The shaded area is a standard deviation. Shown are MATD3, MADDPG and independent learner (IL) TD3 on the decentralized task. For comparison we also show the performance of a single agent (SA) using TD3, with full state information.</p><p>Note that this is a significantly less difficult setting.</p><p>attachment to the torso and one at the knee. The state information provided in the standard Gym task consists of all joint positions and angular velocities, position and velocity of the torso as well as contact forces with the floor. This is then used to generate a reward consisting of the distance traveled in a set direction and a cost term for actuator force and contact with the surface. Additionally, a positive reward is obtained for every time-step that the agent does not reach a terminal position, which occurs when the torso falls below a threshold height.</p><p>We separate the ant into two halves, as shown in <ref type="figure">Figure 7</ref>. The action-space of one agent comprises the two left legs, and the action-space of the other consists of the other two legs. As observation each agent receives all information about their respective legs, that is provided in the "Ant-v2" taskposition, angular velocity and external forces -but only the position of the other legs. In addition, both agents receive the location and velocity of the torso of the ant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation</head><p>We implemented our approach, MATD3, for the ant experiments, along with a MADDPG and independent learner (IL) TD3 version. To our knowledge this is the first investigation of the efficacy of MADDPG in high-dimensional, continuous action spaces.</p><p>The hyperparameters for MATD3 and MADDPG were selected by grid-search over learn rate α = [0.01, 0.003, 0.001], batch-size b = [100, 300] and τ = [0.005, 0.01]. The hyper-parameters we found to be working best for both approaches are α = 0.001, b = 100 as batch-size and τ = 0.01, and d = 2 for MATD3. For single agent (SA) TD3 and IL TD3 we are using the hyper-parameters suggested in the original paper <ref type="bibr" target="#b4">[5]</ref>.</p><p>For all Q-functions and policies we use MLPs with two hidden layers with 400 and 300 units respectively. As activation function we use ReLU, except at the output, which uses a sigmoid activation. The output is then scaled linearly to the range of the respective action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>The results of our trials are shown in <ref type="figure">Figure 8</ref>. They show that MATD3 performs better than MADDPG. Both approaches usually first find a policy that remains in place, however, our approach, unlike MADDPG, usually recovers from this local optimum and achieves locomotion in the required direction. As a baseline we also show the performance of two independent learner (IL) TD3 agents, which receive the same observation as the MATD3 and MADDPG agents, but do not use a centralized critic. They failed to learn a successful policy in all trials.</p><p>Finally, we show the performance of SA TD3 in the standard, fully observable "Ant-v2" task. Unsurprisingly, due to the SA task being significantly easier, the final performance of SA TD3 outperforms all MA approaches. However it only starts to do so after a high number of time-steps, showing promise for further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Regarding the improvement of MADDPG, Minimax Multi-Agent DDPG (M3DDPG) <ref type="bibr" target="#b10">[11]</ref> has to be noted. They approximate a minimax training objective by adding adversarial perturbations to the actions of other agents when updating the critic and policy. However, their improvements are limited to adversarial tasks, while we also address cooperative ones. Additionally, it should be possible to combine their approach with ours. An approach that aims to reduce overestimation bias in MARL by using Double Deep Q Networks (DDQN) is presented in <ref type="bibr" target="#b20">[21]</ref>. However, they do not investigate whether overestimation does indeed occur in MARL and if their approach reduces it. Further, their work focuses on discrete state and action spaces in a grid-world, while our work focuses on more complex, continuous domains. Furthermore, DDQN has been shown to not be effective in the actor-critic setting <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the field of robotics, learning decentralized controllers via RL has been studied by <ref type="bibr" target="#b2">[3]</ref>. Instead of using a centralized critic, they use independent critics which are augmented by certain additional observations of the other agents. In addition, they use value iteration to learn the policies, which does not scale to high-dimensional tasks, and only study comparatively simple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We have shown, that overestimation occurs in multi-agent domains and significantly hinders convergence. We used this finding to propose a new approach for multi-agent reinforcement learning (MARL), called multi-agent TD3 (MATD3). It is based on the decentralized execution, centralized training setting and addresses the overestimation bias by using double centralized critics. We have shown that our proposed approach significantly outperforms MADDPG on most of the particledomain tasks. In addition, we propose a new method of learning decentralized controllers for robotic tasks, by regarding them as multi-agent systems and using methods from MARL. We showed that we can use this approach to learn decentralized policies for the popular "Ant" task, and that our proposed approach also outperforms MADDPG in this domain.</p><p>For future work, we plan to investigate a hybrid approach, that combines the initial benefits of multi-agent TD3 (MATD3) with the later performance of TD3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Multi-Agent TD3</head><p>Initialize replay buffer D and network parameters for t = 0 to T max do Select actions a i ∼ µ i (o i ) + Execute actions (a 1 , ..., a N ), observe r i , x Store transition (x, a 1 , ..., a N , r 1 , ..., r N , x ) in D x ← x for agent i = 1 to N do Sample a random minibatch of S samples ( Update the target networks θ ← τ θ + (1 − τ )θ</p><formula xml:id="formula_7">x b , a b , r b , x b ) from D y b ← r b i + γ min j=1,2 Q µ i,j (x b ,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Empirical evaluation of overestimation in MARL. The Q-values estimated by the Q-network and the true Q-values are shown. The results are averaged across 5 runs and 95 % CIs of the mean are shown for the estimated values. We can see, that MADDPG overestimates the Q-values, while MATD3 underestimates them and achieves higher real values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the particle environment tasks used in our evaluation. Left to right, top to bottom: "Cooperative Communication", "Cooperative Navigation", "Covert Communication", "Keep-away", "Physical Deception", "Predator-Prey". The figure is based on<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Evaluation in the adversarial domains (Team vs Single Agent). Shown is the 0-1 normalized final reward of the team, in all combinations of MATD3 and MADDPG, averaged across 20 trials each. In the domains where it is necessary to learn a stable winning strategy ("Keep away", "Physical-Deception", "Predator-Prey") MATD3 outperforms MADDPG in the direct comparison. However, in the "Covert Communication" domain, where quick adaption to the policy of the adversary is advantageous, MADDPG outperforms MATD3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a 1 , ..., a N ) | a k =µ k (o b k )+ Minimize Q-function loss for both critics j = 1, 2 L(θ j ) = 1 S b (Q µ i,j (x b , a b 1 , ..., a b N ) − y b ) 2 if t mod d = 0 thenUpdate policy µ i with gradient∇ θµ,i J ≈ 1 S b ∇ θ µ θµ,i (o b i )∇ ai Q µ i,1 (x b , a b 1 , ..., µ θµ,i (o i ), ...a b N )</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The source code will be available after the review period, to ensure anonymity.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multi-agent approach to cooperative traffic management and route guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Satapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Blue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="318" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Openai gym</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decentralized reinforcement learning control of a robotic manipulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buşoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuška</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICARCV-06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1347" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counterfactual Multi-Agent Policy Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Addressing Function Approximation Error in Actor-Critic Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic Programming For Partially Observable Stochastic Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Central pattern generators for locomotion control in animals and robots: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="642" to="653" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal control in microgrid using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISA Trans</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="743" to="751" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6379" to="6390" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-Agent Shared Hierarchy Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Science</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Work. Richer Represent. Reinf. Learn</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emergence of Grounded Compositional Language in Multi-Agent Populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1495" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal and approximate Q-value functions for decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed learning for the decentralized control of articulated mobile robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sartoretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Paivine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Travers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICRA</title>
		<imprint>
			<date type="published" when="2018-05" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Deterministic Policy Gradient Algorithms. ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-agent Double Deep Q-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Artificial Intelligence, EPIA 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction. Number 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1057" to="1063" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Double Q-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shape-based compliant control with variable coordination centralization on a snake robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruscelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Travers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CDC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5165" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multi-agent framework for packet routing in wireless sensor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="10026" to="10047" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
