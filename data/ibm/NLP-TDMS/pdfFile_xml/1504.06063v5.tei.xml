<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Convolutional Neural Networks for Matching Image and Sentence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Convolutional Neural Networks for Matching Image and Sentence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Associating image with natural language sentence plays the essential role in many applications. Describing the image with natural sentences is useful for image annotation and caption <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>, while retrieval image with query sentences is more convenient and helpful for the natural image search applications <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. The association between image and sentence can be formalized as a multimodal matching problem, where the semantically correlated image and sentence pairs should produce higher matching scores than uncorrelated ones.</p><p>The multimodal matching relations between image and sentence are complicated, which happen at different levels as shown in <ref type="figure">Figure 1</ref>. The words in the sentence, such as "grass", "dog", and "ball", denote the objects in the image. The phrases describing the objects and their attributes or activities, such as "black and brown dog", and "small black and grass black and brown dog dog small black and brown dog play with a red ball dog play with a red ball in the grass small black and brown dog play with a red ball in the grass ball a red ball <ref type="figure">Figure 1</ref>. The multimodal matching relations between image and sentence. The words and phrases, such as "grass", "a red ball", and "small black and brown dog play with a red ball", correspond to the image areas of their grounding meanings. The global sentence "small black and brown dog play with a red ball in the grass" expresses the whole semantic meaning of the image content.</p><p>brown dog play with a red ball", correspond to the image areas of their grounding meanings. The whole sentence "small black and brown dog play with a red ball in the grass", expressing a complete semantic meaning, associates with the whole image content. These matching relations should be all taken into consideration for an accurate inter-modal matching between image and sentence. Recently, much research work focuses on modeling the image and sentence matching relation at the specific level, namely the word level <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b6">7]</ref>, phrase level <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b33">34]</ref>, and sentence level <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>. However, to the best of our knowledge, there are no models to fully exploit the matching relations between image and sentence by considering the word, phrase, and sentence level intermodal correspondences together.</p><p>The multimodal matching between image and sentence requires good representations of the image and sentence. Recently, deep neural networks have been employed to learn better image and sentence representations. Specifically, convolutional neural networks (CNNs) have shown their powerful abilities on image representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12]</ref> and sentence representation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. However, the abil-ity of CNN on multimodal matching, specifically the image and sentence matching problem, has not been studied.</p><p>In this paper, we propose a novel multimodal convolutional neural network (m-CNN) framework for the image and sentence matching problem. By training on a set of image and sentence pairs, the proposed m-CNNs are able to retrieve and rank the images given a natural sentence query, and vice versa. Our core contributions are:</p><p>1. CNN is firstly studied for the image and sentence matching problem. We employ convolutional architectures to summarize the image, compose words of the sentence into different semantic fragments, and learn the matching relations and interactions between image and the composed fragments.</p><p>2. The complicated matching relations between image and sentence are fully studied in our proposed m-CNN by letting image and the composed fragments of the sentence meet and interact at different levels. We validate the effectiveness of m-CNNs on bidirectional image and sentence retrieval experiments, in which we achieve performances superior to the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Association between Image and Text</head><p>There is a long thread of work on the association between image and text. Early work usually focuses on modeling the correlation between image and the annotating words <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43]</ref> or phrases <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">46]</ref>. These models cannot well capture the complicated matching relations between image and the natural sentence. Recently, the association between image and sentence has been studied for bidirectional image and sentence retrieval <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> and automatic image captioning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>For bidirectional image and sentence retrieval, Hodosh et al. <ref type="bibr" target="#b13">[14]</ref> proposed KCCA to discover the shared feature space between image and sentence. However, the highly non-linear inter-modal relations cannot be well exploited based on the shallow representations of image and sentence. Recent papers seek better representations of image and sentence from deep architectures. Socher et al. <ref type="bibr" target="#b36">[37]</ref> proposed to employ the semantic dependency-tree recursive neural network (SDT-RNN) to map the sentence into the same semantic space as the image representation, and the association is then measured as the distance in that space. Yan et al. <ref type="bibr" target="#b43">[44]</ref> stacked fully connected layers together to represent the sentence and used deep canonical correlation analysis (DCCA) for matching images and text. Klein et al. <ref type="bibr" target="#b24">[25]</ref> used the Fisher vector (FV) for the sentence representation. Kiros et. al <ref type="bibr" target="#b23">[24]</ref> proposed skip-thought vector (STV) to encode the sentence for matching the image. As such, the global level matching relations between image and sentence are studied by representing the sentence as a global vector. However, they neglect the local fragments of the sentence and their correspondences to the image content. Compared with <ref type="bibr" target="#b36">[37]</ref>, Karpathy et al. <ref type="bibr" target="#b18">[19]</ref> work on a finer level by aligning the fragments of sentence and regions of image. Plummer et.al <ref type="bibr" target="#b32">[33]</ref> used the entities to collect region-tophrase (RTP) correspondences for richer image-to-sentence models. The local inter-modal correspondences between image and sentence fragments are thus studied, where the global matching relations are not considered. As illustrated in <ref type="figure">Figure 1</ref>, the image content corresponds to different fragments of sentence from local words to the global sentence. To fully exploit the inter-modal matching relations, we propose m-CNNs to compose words of sentence to different fragments, let the fragments meet image at different levels, and learn their matching relations.</p><p>For automatic image captioning, the authors use recurrent visual representation (RVP) <ref type="bibr" target="#b2">[3]</ref>, multimodal recurrent neural network (m-RNN) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>, multimodal neural language model (MNLM) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, neural image caption (NIC) <ref type="bibr" target="#b41">[42]</ref>, deep visual-semantic alignments (DVSA) <ref type="bibr" target="#b17">[18]</ref>, and long-term recurrent convolution networks (LRCN) <ref type="bibr" target="#b5">[6]</ref> to learn the relation between image and sentence and generate the caption for a given image. Please note that those models naturally produce scores for image-sentence association (e.g., the likelihood of a sentence as the caption for a given image). It can thus be readily used for bidirectional retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image and Sentence Representation</head><p>For image, CNNs have demonstrated their powerful abilities to learn the image representation from image pixels, which achieved the state-of-the-art performances on image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12]</ref> and object detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. For sentence, there is a thread of neural networks for the sentence representation, such as CNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, time-delay neural network <ref type="bibr" target="#b3">[4]</ref>, recursive neural network <ref type="bibr" target="#b14">[15]</ref>, and recurrent neural network <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. The obtained sentence representation can be used for the sentence classification <ref type="bibr" target="#b19">[20]</ref>, image and sentence retrieval <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29]</ref>, language modeling <ref type="bibr" target="#b3">[4]</ref>, text generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40]</ref>, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">m-CNNs for Matching Image and Sentence</head><p>As illustrated in <ref type="figure">Figure 2</ref>, m-CNN takes the image and sentence as the inputs and generates the matching score between them. More specifically, m-CNN consists of three components.</p><p>• Image CNN: The image CNN is used to generate the image representation for matching the fragments composed from words, which is computed as follows: where σ(·) is the activation function (e.g., Sigmoid or ReLU <ref type="bibr" target="#b4">[5]</ref>). CN N im is an image CNN which takes the image as the input and generates a fixed length image representation. The successful image CNNs for image recognition, such as <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, can be used to initialize the image CNN, which returns the 4096-dimensional activations of the fully connected layer immediately before the last ReLU layer. The matrix w im is of the dimension d × 4096, where d is set as 256 in our experiments. Each image is thus represented as one ddimension vector ν im .</p><formula xml:id="formula_0">ν im = σ(w im (CN N im (I)) + b im ),<label>(1)</label></formula><p>• Matching CNN The matching CNN takes the encoded image representation ν im and word representations ν i wd as the input and produces the joint representation ν JR . As illustrated in <ref type="figure">Figure 1</ref>, the image content may correspond to sentence fragments with varying scales, which will be adequately considered in the learnt joint representation of image and sentence. Targeting at fully exploiting the inter-modal matching relation, our proposed matching CNNs firstly compose words to different semantic fragments and then let the image meet these fragments to learn their inter-modal structures and interactions. More specifically, different matching CNNs are designed to make the image interact with the composed fragments at different levels to generate the joint representation, from the word and phrase level to the sentence level. Detailed information of the matching CNNs at different levels will be introduced in the following subsections.</p><p>• MLP Multilayer perceptron (MLP) takes the joint representation ν JR as the input and produces the final matching score between image and sentence, which is calculated as follows.</p><formula xml:id="formula_1">s match = w s σ(w h (ν JR ) + b h ) + b s .<label>(2)</label></formula><p>where σ(·) is the nonlinear activation function. w h and b h are used to map ν JR to the representation in the hidden layer. w s and b s are used to compute the matching score between image and sentence.</p><p>The three components of our proposed m-CNN are fully coupled in the end-to-end image and sentence matching framework, with all the parameters (e.g., those for image CNN, matching CNN, MLP, w im and b im in Eq. (1), and word representations) can be jointly learned under the supervision from matching instances. Threefold benefits are provided. Firstly, the image CNN can be tuned to generate a better image representation for matching. Secondly, word representations can be tuned for further composition and matching processes. Thirdly, the matching CNN (as detailed in the following) composes word representations to different fragments and lets the image representation meet these fragments at different levels, which can fully exploit the inter-modal matching correspondences between image and sentence. With the nonlinear projection in Eq. (1), the image representations ν im for different matching CNNs are expected to encode the image content for matching the composed semantic fragments of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Different Variants of Matching CNN</head><p>To fully exploit the matching relations of image and sentence, we let the image representation meet and interact with different composed fragments of the sentence (roughly the word, phrase, and sentence) to generate the joint representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Word-level Matching CNN</head><p>In order to find the word-level matching relation, we let the image meet with the word-level fragments of sentence and learn their interactions and relations. Moreover, as most convolutional models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>, we consider the convolution units with a local "receptive field" and shared weights to adequately model the rich structures for word composition and inter-modal interaction. The word-level matching CNN, denoted as MatchCNN wd , is designed as in <ref type="figure" target="#fig_0">Figure 3</ref> (a). After sequential layers of convolution and pooling, the joint representation of image and sentence is generated as the input of MLP for calculating the matching score.</p><p>Convolution Generally, with a sequential input ν, the convolution unit for feature map of type-f (among F of them) on the th layer is  where w ( ,f ) are the parameters for the f feature map on th layer, σ(·) is the activation function, and ν i ( −1) denotes the segment of ( −1) th layer for the convolution at location i , which is defined as follows</p><formula xml:id="formula_2">ν i ( ,f ) def = σ(w ( ,f ) ν i ( −1) + b ( ,f ) ),<label>(3)</label></formula><formula xml:id="formula_3">ν i ( −1) def = ν i ( −1) ν i+1 ( −1) · · · ν i+krp−1 ( −1)</formula><p>.</p><p>(4)</p><p>k rp defines the size of local "receptive field" for convolution. " " concatenates the neighboring k rp word vectors into a long vector. In this paper, k rp is chosen as 3 for the convolution process. As MatchCNN wd targets at exploring word-level matching relation, the multimodal convolution layer is introduced by letting the image meet the word-level fragments of sentence. The convolution unit of the multimodal convolution layer is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref> (b). The input of the multimodal convolution unit is denoted as:</p><formula xml:id="formula_4">ν i (0) def = ν i wd ν i+1 wd · · · ν i+krp−1 wd ν im ,<label>(5)</label></formula><p>where ν i wd is the vector representation of word i of the sentence, and ν im is the encoded image feature for matching word-level fragments of sentence. It is not hard to see that this input will lead the "interaction" between words and image representation at the first convolution layer, which provides the local matching signal at word level. From the sentence perspective, the multimodal convolution on ν i (0) composes the words ν i wd , · · · , ν i+krp−1 wd in local "receptive field" to a higher semantic representation, such as the phrase "a white ball". From the matching perspective, the multimodal convolution on ν i (0) captures and learns the inter-modal correspondence between image representation and the word-level fragments of sentence. The meanings of the word "ball" and the composed phrase "a white ball" are grounded in the image to make the inter-modal matching relations.</p><p>Moreover, in order to handle natural sentences of variable lengthes, the maximum length of sentence is fixed for MatchCNN wd . Zero vectors are padded for the image and word representation, as the dashed ones in <ref type="figure" target="#fig_0">Figure 3</ref> (a). The output of the convolution process on zero vectors is gated to be zero. The convolution process in Eq. <ref type="formula" target="#formula_2">(3)</ref> is further formulated as:</p><formula xml:id="formula_5">ν i ( ,f ) = g( ν i ( −1) ) · σ(w ( ,f ) ν i ( −1) + b ( ,f ) ) where, g(x) = 0, x == 0 1, otherwise<label>(6)</label></formula><p>The gating function can eliminate the unexpected matching noise composed from the convolution process.</p><p>Max-pooling After each convolution layer, a maxpooling layer is followed. Taking a two-unit window maxpooling as an example, the pooled feature is obtained by:</p><formula xml:id="formula_6">ν i ( +1,f ) = max(ν 2i ( ,f ) , ν 2i+1 ( ,f ) )<label>(7)</label></formula><p>The effects of max-pooling are two-fold. 1) Together with the stride as two, the max-pooling process lowers the dimensionality of the representation by half, thus quickly making the final joint representation of the image and sentence.</p><p>2) It helps filter out the undesired interaction and relation between image and fragments of sentence. Take the sentence in <ref type="figure" target="#fig_0">Figure 3</ref> (a) as an example, the composed phrase "dog chase a" matches more closely to the image than "chase a white". Therefore, we can imagine that a well-trained multimodal convolution unit will generate better matching representation of "dog chase a" and image. The max-pooling process will pool the matching representation out for further convolution and pooling processes. The convolution and pooling processes explore and summarize the local matching signals explored at the word level. More layers of convolution and pooling can be further employed to form matching decisions at larger scales and finally reach a global joint representation. Specifically, in this paper another two more convolution and max-pooling layers alternate to summarize the local matching decisions and finally produce the global joint representation of matching, which reflects the inter-modal correspondence between image and word-level fragments of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Phrase-level Matching CNN</head><p>Different from matching CNN at word-level, we let CNN work solely on words to certain levels before interacting with the image. Without seeing the image feature, the convolution process will compose the words in the "receptive field" into a higher semantic representation, while the maxpooling process will filter out the undesired compositions. These composed representations are named as phrase from the language perspective. We let image meet the composed phrases to reason their inter-modal matching relations.</p><p>As illustrated in <ref type="figure" target="#fig_1">Figure 4</ref> (a), after one layer of convolution and max-pooling process, short phrases (denoted as ν i (2) ) are composed from four words, such as "a woman in jean". These composed short phrases present richer and specific descriptions about the objects and their relationships compared with single words, such as "woman" and "jean". With an additional layer of convolution and maxpooling process on short phrases, long phrases (denoted as ν i (4) ) are composed from four short phrases (also from ten words), such as "a black dog be in the grass with a woman" in <ref type="figure" target="#fig_1">Figure 4 (b)</ref>. Compared with the composed short phrases and single words, the long phrases present even richer and higher semantic meanings about the specific description of the objects, their activities, and their relative positions.</p><p>In order to reason the inter-modal relations between image and the composed phrases, a multimodal convolution layer is introduced by performing convolution on the image and phrase representations. The input of the multimodal convolution unit is:</p><formula xml:id="formula_7">ν i ph def = ν i ph ν i+1 ph · · · ν i+krp−1 ph ν im .<label>(8)</label></formula><p>where ν i ph is the composed phrase representation, which can be either short phrases ν i <ref type="bibr" target="#b1">(2)</ref> or long phrases ν i (4) . The multimodal convolution process produces the phrase-level matching decisions. Then the layers after that (namely the max-pooling layer or convolution layer) can be viewed as further fusion of these local phrase-level matching decisions to a joint representation, which captures the local matching relations between image and composed phrase fragments. Specifically, for short phrases, two sequential layers of convolution and pooling are followed to generate the joint rep- resentation. We name the matching CNN for short phrases and image as MatchCNN phs . For long phrases, only one sequential layer of convolution and pooling is used to summarize the local matching to the joint representation. The matching CNN for long phrases and image is named as MatchCNN phl .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Sentence-level Matching CNN</head><p>The sentence-level convolutional matching CNN, denoted as MatchCNN st , goes one step further in the composition and defers the matching until the sentence is fully represented, as illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. More specifically, one image CNN encodes the image into a feature vector. One sentence CNN, consisting of three sequential layers of convolution and pooling, represents the whole sentence as a feature vector. The multimodal layer concatenates the image and sentence representation together as their joint representation:</p><formula xml:id="formula_8">ν JR = ν im ν st ,<label>(9)</label></formula><p>where ν st denotes the sentence representation by vectorizing the features in the last layer of the sentence CNN. For the sentence "a little boy in a bright green field have kick a soccer ball very high in the air" illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>, although word-level and phrase-level fragments, such as "boy", "kick a soccer ball", correspond to the objects as well as their activities in the image, the whole sentence needs to be fully represented to make a reliable association with the image. The sentence CNN with layers of convolution and pooling is used to encode the whole sentence as a feature vector representing its semantic meaning.</p><p>Concatenating the image and sentence representation together, MatchCNN st does no non-trivial matching, but transfer the representations of the two modalities to the later MLP for fusing and matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">m-CNNs with Different Matching CNNs</head><p>We can get different m-CNNs with different variants of Matching CNNs, namely m-CNN wd , m-CNN phs , m- CNN phl , and m-CNN st . To fully exploit the inter-modal matching relations between image and sentence at different levels, we use an ensemble m-CNN EN S of the four variants by summing the matching scores generated from these m-CNNs together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>In this section, we describe the detailed configurations of our proposed m-CNN models and how we train the proposed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Configurations</head><p>We use two different image CNNs, OverFeat <ref type="bibr" target="#b34">[35]</ref> (the "fast" network) and VGG <ref type="bibr" target="#b35">[36]</ref> (with 19 weight layers), with which we take not only the architecture but also the original parameters (learnt on ImageNet dataset) for initialization. By chopping the top softmax layer and the last ReLU layer, the output of the last fully-connected layer is deemed as image representation, denoted as CN N im (I) in Eq. (1).</p><p>The configurations of MatchCNN wd , MatchCNN phs , MatchCNN phl , and MatchCNN st are outlined in <ref type="table">Table 1</ref>. We use three convolution layers, three max pooling layers, and an MLP with two fully connected layers for all these four networks. The first convolution layer of MatchCNN wd , second convolution layer of MatchCNN phs , and third convolution layer of MatchCNN phl are the multimodal convolution layers, which blend the image representation and fragments of the sentence together to compose a higher level semantic representation. The MatchCNN st concatenates the image and sentence representation together and leave the interaction to the final MLP. The matching CNNs are designed on fixed architectures, which need to be set to accommodate the maximum length of the input sentences. During our evaluations, the maximum length is set as 30.</p><p>The word representations are initialized by the skip-gram model <ref type="bibr" target="#b29">[30]</ref> with dimension 50. The joint representation ob-tained from the matching CNNs is fed into MLP with one hidden layer with size 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning</head><p>The m-CNN models can be trained with contrastive sampling using a ranking loss function. More specifically, for the score function s match (·) as in Eq. (2), the objective function is defined as: e θ (x n , y n , y m ) = max 0, µ−s match (x n , y n )+s match (x n , y m )</p><p>where θ denotes the parameters, (x n , y n ) denotes the correlated image-sentence pair, and (x n , y m ) is the randomly sampled uncorrelated image-sentence pair (n = m). The notational meaning of x and y varies with the matching task: for image retrieval from query sentence, x denotes the natural sentence and y denotes the image; for sentence retrieval from query image, it is just the opposite. The object is to force the matching score of the correlated pair (x n , y n ) to be greater than the uncorrelated pair (x n , y m ) by a margin µ, which is simply set as 0.5 for our training process.</p><p>We use stochastic gradient descent (SGD) with minibatches of 100∼150 for optimization. In order to avoid overfitting, early-stopping <ref type="bibr" target="#b1">[2]</ref> and dropout (with probability 0.1) <ref type="bibr" target="#b12">[13]</ref> are used. ReLU is used as the activation function throughout m-CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the effectiveness of our m-CNNs on bidirectional image and sentence retrieval. We begin by describing the datasets used for evaluation, followed by a brief description of competitor models. As our m-CNNs are bidirectional, we evaluate the performances on both image retrieval and sentence retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We test our matching models on the public imagesentence datasets, with varying sizes and characteristics.</p><p>Flickr8K <ref type="bibr" target="#b13">[14]</ref> This dataset consists of 8,000 images collected from Flickr. Each image is accompanied with 5 sentences describing the image content. This database provides the standard training, validation, and testing split.</p><p>Flickr30K <ref type="bibr" target="#b44">[45]</ref> This dataset consists of 31,783 images collected from Flickr. Each image is also accompanied with 5 sentences describing the content of the image. Most of the images depict varying human activities. We used the public split as in <ref type="bibr" target="#b28">[29]</ref> for training, validation, and testing. associated with 5 sentences describing the content of the image. We used the public split as in <ref type="bibr" target="#b27">[28]</ref> for training, validation, and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Competitor Models</head><p>We compared our models with recently developed models on the performances of the bidirectional image and sentence retrieval, specifically DeViSE <ref type="bibr" target="#b6">[7]</ref>, SDT-RNN <ref type="bibr" target="#b36">[37]</ref>, DCCA <ref type="bibr" target="#b43">[44]</ref>, FV <ref type="bibr" target="#b24">[25]</ref>, STV <ref type="bibr" target="#b23">[24]</ref>, RTP <ref type="bibr" target="#b32">[33]</ref>, Deep Fragment <ref type="bibr" target="#b18">[19]</ref>, m-RNN <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, MNLM <ref type="bibr" target="#b21">[22]</ref>, RVP <ref type="bibr" target="#b2">[3]</ref>, DVSA <ref type="bibr" target="#b17">[18]</ref>, NIC <ref type="bibr" target="#b41">[42]</ref>, and LRCN <ref type="bibr" target="#b5">[6]</ref>. DeViSE and Deep Fragment are regarded as working on word-level and phrase-level, respectively. SDT-RNN, DCCA, and FV are all regarded as working on the sentence-level, which embed the image and sentence into the same semantic space. The other models, namely MNLM, m-RNN, RVP, DVSA, NIC, and LRCN, which are originally proposed for automatic image captioning, can also be used for retrieval in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Bidirectional Image and Sentence Retrieval</head><p>We adopt the evaluation metrics <ref type="bibr" target="#b18">[19]</ref> for a fair comparison. More specifically, for bidirectional retrieval, we report the median rank (Med r ) of the closest ground truth result in the list, as well as the R@K (with K = 1, 5, 10) which computes the fraction of times the correct result was found among the top K items. The performances of the proposed m-CNNs on bidirectional image and sentence retrieval of Flickr8K, Flickr30K, Microsoft COCO are illustrated in <ref type="table" target="#tab_3">Table 2</ref>, 3, and 4. We highlight the best performance of each evaluation metric.</p><p>On Flickr8K, FV performs the best, suggesting the strong and beneficial bias of Fisher vector on modeling sentences, which is most obvious when the training data are relatively scarce. Our proposed m-CNN performs inferiorly to FV, but still superior to other methods. The reason, as suggested by the results of larger datasets (Flickr30K On Flickr30K, with more training instances (30,000 im-ages), the best performing competitor model becomes the RTP on both tasks. Only m-RNN-vgg, FV, and RTP outperform m-CNN EN S (with VGG) on sentence retrieval task measured by R@1. When it comes to image retrieval, m-CNN EN S (with VGG) is consistently better than all competitor models. One possible reason may be that m-RNNvgg is designed for caption generation and is particularly good at finding the suitable sentence for any given image. One possible reason for RTP may be that the Flickr30K entities are specifically presented, where the bounding boxes corresponding to each entity are manually labeled. As such, much more information are available for image retrieval.</p><p>On Microsoft COCO, with more training instances (over 110,000 images), the performances of our proposed m-CNN in terms of all the evaluation metrics have been sig-  nificantly improved, compared with those on Flickr8k and Flickr30K. Firstly, it demonstrates that with sufficient training samples, the parameters of the convolutional architecture in m-CNN can be more adequately tuned. Secondly, only DVSA outperforms the proposed m-CNN EN S (with VGG) on sentence retrieval in terms of Med r. On image retrieval, m-CNN EN S significantly and consistently outperforms all the competitor models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Performances of Different m-CNNs</head><p>The proposed m-CNN wd and DeViSE <ref type="bibr" target="#b6">[7]</ref> both target at exploiting word-level inter-modal correspondences between image and sentence. However, DeViSE treats each word equally and average their word vectors as the representation of the sentence, while our m-CNN wd let image interact with each word and compose them to higher semantic representations, which significantly outperforms DeViSE. On the other end, both SDT-RNN <ref type="bibr" target="#b36">[37]</ref> and the proposed m-CNN st exploit the matching between image and sentence at the sentence level. However, SDT-RNN encodes each sentence recursively into a feature vector based on a pre-given dependency tree, while m-CNN st works on a more flexible manner with sliding window on the sentence to finally generate the sentence representation. Therefore, a better performance is obtained by m-CNN st .</p><p>Deep Fragment <ref type="bibr" target="#b18">[19]</ref> and the proposed m-CNN phs and m-CNN phl match the image and sentence fragments at phrase levels. However, Deep Fragment uses edges of dependency tree to model the sentence fragments, making it unable to describe more complex relations in sentence. For example, Deep Fragment parses a relative complex phrase "black and brown dog" to two relations "(CONJ, black, brown)" and "(AMOD, brown, dog)", while m-CNN phs handles the same phrase as a whole to compose them to a higher semantic representation. Moreover, m-CNN phl can readily handle longer phrases and reason their grounding meanings in the image. Consequently, better performances of m-CNN phs and m-CNN phl (with VGG) are obtained compared with Deep Fragment.</p><p>Moreover, it can be observed that m-CNN st consistently outperform other m-CNNs. The sentence CNN can well summarize the natural sentence and make a better sentencelevel association with image in m-CNN st . Other m-CNNs captures the matching relations at word and phrase levels. The matching relations should be considered together to fully depict the inter-modal correspondences between image and sentence. Thus m-CNN EN S achieves the best performances, which indicates that m-CNNs at different levels are complementary with each other to capture the complicated image and sentence matching relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Influence of Image CNN</head><p>We use OverFeat and VGG to initialize the image CNN in m-CNN for the retrieval tasks. It can be observed that m-CNNs with VGG significantly outperform that with Over-Feat by a large margin, which is consistent with their performance on classification on ImageNet (14% and 7% top-5 classification errors for OverFeat and VGG, respectively). Clearly the retrieval performance depends heavily on the efficacy of the image CNN, which might explain the good performance of NIC on Flickr8K. Moreover, region with CNN features <ref type="bibr" target="#b7">[8]</ref> are used for encoding image regions to feature vectors, which are used as the image fragments in Deep Fragment and DVSA. In the future, we will consider to incorporate these image CNNs into our m-CNNs to make more accurate inter-modal matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Composition Abilities of m-CNNs</head><p>m-CNNs can compose words to different semantic fragments of the sentence for the inter-modal matching at different levels, and therefore posses the ability of word composition. More specifically, we want to check whether the m-CNNs can compose words of random orders into semantic fragments for matching the image content. As demonstrated in <ref type="table" target="#tab_6">Table 5</ref>, the matching scores between an image and its accompanied sentence (from different m-CNNs) greatly decrease after the random reshuffle of words. It is a fairly strong evidence that m-CNNs will compose words in natural sequential order into high semantic representations and thus make the inter-modal matching relations between image and sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed multimodal convolutional neural networks (m-CNNs) for matching image and sentence. The proposed m-CNNs rely on convolution architectures to compose different semantic fragments of the sentence and learn the interaction between image and the composed fragments at different levels, therefore fully exploit the inter-modal matching relations. Experimental results on bidirectional image and sentence retrieval demonstrate the consistent state-ofthe-art performances of our proposed models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>The word-level matching CNN. (a) The word-level matching CNN architecture. (b) The convolution units of multimodal convolution layer of MatchCNN wd . The dashed ones indicate the zero padded word and image representations, which are gated out after convolution process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The phrase-level matching CNN and composed phrases. (a): The short phrase is composed by one layer convolution and pooling. (b): The long phrase is composed by two sequential layers of convolution and pooling. (c): The phase-level matching CNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>The sentence-level matching CNN. The joint representation is obtained by concatenating the image and sentence representations together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Microsoft COCO<ref type="bibr" target="#b26">[27]</ref> This dataset consists of 82,783 training and 40,504 validation images with 80 categories labeled for a total of 886,284 instances. Each image is also Bidirectional image and sentence retrieval results on Flickr8K.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Sentence Retrieval</cell><cell></cell><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell></row><row><cell></cell><cell cols="8">R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r</cell></row><row><cell>Random Ranking</cell><cell>0.1</cell><cell>0.6</cell><cell>1.1</cell><cell>631</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500</cell></row><row><cell>DeViSE [7]</cell><cell>4.8</cell><cell>16.5</cell><cell>27.3</cell><cell>28.0</cell><cell>5.9</cell><cell>20.1</cell><cell>29.6</cell><cell>29</cell></row><row><cell>SDT-RNN [37]</cell><cell>6.0</cell><cell>22.7</cell><cell>34.0</cell><cell>23.0</cell><cell>6.6</cell><cell>21.6</cell><cell>31.7</cell><cell>25</cell></row><row><cell>MNLM [22]</cell><cell>13.5</cell><cell>36.2</cell><cell>45.7</cell><cell>13</cell><cell>10.4</cell><cell>31.0</cell><cell>43.7</cell><cell>14</cell></row><row><cell>MNLM-vgg [22]</cell><cell>18.0</cell><cell>40.9</cell><cell>55.0</cell><cell>8</cell><cell>12.5</cell><cell>37.0</cell><cell>51.5</cell><cell>10</cell></row><row><cell>m-RNN [29]</cell><cell>14.5</cell><cell>37.2</cell><cell>48.5</cell><cell>11</cell><cell>11.5</cell><cell>31.0</cell><cell>42.4</cell><cell>15</cell></row><row><cell>Deep Fragment [19]</cell><cell>12.6</cell><cell>32.9</cell><cell>44.0</cell><cell>14</cell><cell>9.7</cell><cell>29.6</cell><cell>42.5</cell><cell>15</cell></row><row><cell>RVP (T) [3]</cell><cell>11.6</cell><cell>33.8</cell><cell>47.3</cell><cell>11.5</cell><cell>11.4</cell><cell>31.8</cell><cell>45.8</cell><cell>12.5</cell></row><row><cell>RVP (T+I) [3]</cell><cell>11.7</cell><cell>34.8</cell><cell>48.6</cell><cell>11.2</cell><cell>11.4</cell><cell>32.0</cell><cell>46.2</cell><cell>11</cell></row><row><cell>DVSA (DepTree) [18]</cell><cell>14.8</cell><cell>37.9</cell><cell>50.0</cell><cell>9.4</cell><cell>11.6</cell><cell>31.4</cell><cell>43.8</cell><cell>13.2</cell></row><row><cell>DVSA (BRNN) [18]</cell><cell>16.5</cell><cell>40.6</cell><cell>54.2</cell><cell>7.6</cell><cell>11.8</cell><cell>32.1</cell><cell>44.7</cell><cell>12.4</cell></row><row><cell>DCCA [44]</cell><cell>17.9</cell><cell>40.3</cell><cell>51.9</cell><cell>9</cell><cell>12.7</cell><cell>31.2</cell><cell>44.1</cell><cell>13</cell></row><row><cell>NIC [42]</cell><cell>20.0</cell><cell>*</cell><cell>61.0</cell><cell>6</cell><cell>19.0</cell><cell>*</cell><cell>64.0</cell><cell>5</cell></row><row><cell>FV (Mean Vec) [25]</cell><cell>22.6</cell><cell>48.8</cell><cell>61.2</cell><cell>6</cell><cell>19.1</cell><cell>45.3</cell><cell>60.4</cell><cell>7</cell></row><row><cell>FV (GMM) [25]</cell><cell>28.4</cell><cell>57.7</cell><cell>70.1</cell><cell>4</cell><cell>20.6</cell><cell>48.5</cell><cell>64.1</cell><cell>6</cell></row><row><cell>FV (LMM) [25]</cell><cell>27.7</cell><cell>56.6</cell><cell>69.0</cell><cell>4</cell><cell>19.8</cell><cell>47.6</cell><cell>62.7</cell><cell>6</cell></row><row><cell>FV (HGLMM) [25]</cell><cell>28.5</cell><cell>58.4</cell><cell>71.7</cell><cell>4</cell><cell>20.6</cell><cell>49.4</cell><cell>64</cell><cell>6</cell></row><row><cell>FV (GMM+HGLMM) [25]</cell><cell>31.0</cell><cell>59.3</cell><cell>73.7</cell><cell>4</cell><cell>21.3</cell><cell>50.0</cell><cell>64.8</cell><cell>5</cell></row><row><cell>OverFeat [35]:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m-CNN wd</cell><cell>8.6</cell><cell>26.8</cell><cell>38.8</cell><cell>18.5</cell><cell>8.1</cell><cell>24.7</cell><cell>36.1</cell><cell>20</cell></row><row><cell>m-CNN phs</cell><cell>10.5</cell><cell>29.4</cell><cell>41.7</cell><cell>15</cell><cell>9.3</cell><cell>27.9</cell><cell>39.6</cell><cell>17</cell></row><row><cell>m-CNN phl</cell><cell>10.7</cell><cell>26.5</cell><cell>38.7</cell><cell>18</cell><cell>8.1</cell><cell>26.6</cell><cell>37.8</cell><cell>18</cell></row><row><cell>m-CNNst</cell><cell>10.6</cell><cell>32.5</cell><cell>43.6</cell><cell>14</cell><cell>8.5</cell><cell>27.0</cell><cell>39.1</cell><cell>18</cell></row><row><cell>m-CNNENS</cell><cell>14.9</cell><cell>35.9</cell><cell>49.0</cell><cell>11.0</cell><cell>11.8</cell><cell>34.5</cell><cell>48.0</cell><cell>11.0</cell></row><row><cell>VGG [36]:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m-CNN wd</cell><cell>15.6</cell><cell>40.1</cell><cell>55.7</cell><cell>8</cell><cell>14.5</cell><cell>38.2</cell><cell>52.6</cell><cell>9</cell></row><row><cell>m-CNN phs</cell><cell>18.0</cell><cell>43.5</cell><cell>57.2</cell><cell>8</cell><cell>14.6</cell><cell>39.5</cell><cell>53.8</cell><cell>9</cell></row><row><cell>m-CNN phl</cell><cell>16.7</cell><cell>43.0</cell><cell>56.7</cell><cell>7</cell><cell>14.4</cell><cell>38.6</cell><cell>52.2</cell><cell>9</cell></row><row><cell>m-CNNst</cell><cell>18.1</cell><cell>44.1</cell><cell>57.9</cell><cell>7</cell><cell>14.6</cell><cell>38.5</cell><cell>53.5</cell><cell>9</cell></row><row><cell>m-CNNENS</cell><cell>24.8</cell><cell>53.7</cell><cell>67.1</cell><cell>5</cell><cell>20.3</cell><cell>47.6</cell><cell>61.7</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Bidirectional image and sentence retrieval results on Flickr30K.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Sentence Retrieval</cell><cell></cell><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell></row><row><cell></cell><cell cols="8">R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r</cell></row><row><cell>Random Ranking</cell><cell>0.1</cell><cell>0.6</cell><cell>1.1</cell><cell>631</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500</cell></row><row><cell>DeViSE [7]</cell><cell>4.5</cell><cell>18.1</cell><cell>29.2</cell><cell>26</cell><cell>6.7</cell><cell>21.9</cell><cell>32.7</cell><cell>25</cell></row><row><cell>SDT-RNN [37]</cell><cell>9.6</cell><cell>29.8</cell><cell>41.1</cell><cell>16</cell><cell>8.9</cell><cell>29.8</cell><cell>41.1</cell><cell>16</cell></row><row><cell>MNLM [22]</cell><cell>14.8</cell><cell>39.2</cell><cell>50.9</cell><cell>10</cell><cell>11.8</cell><cell>34.0</cell><cell>46.3</cell><cell>13</cell></row><row><cell>MNLM-vgg [22]</cell><cell>23.0</cell><cell>50.7</cell><cell>62.9</cell><cell>5</cell><cell>16.8</cell><cell>42.0</cell><cell>56.5</cell><cell>8</cell></row><row><cell>m-RNN [29]</cell><cell>18.4</cell><cell>40.2</cell><cell>50.9</cell><cell>10</cell><cell>12.6</cell><cell>31.2</cell><cell>41.5</cell><cell>16</cell></row><row><cell>m-RNN-vgg [28]</cell><cell>35.4</cell><cell>63.8</cell><cell>73.7</cell><cell>3</cell><cell>22.8</cell><cell>50.7</cell><cell>63.1</cell><cell>5</cell></row><row><cell>Deep Fragment [19]</cell><cell>14.2</cell><cell>37.7</cell><cell>51.3</cell><cell>10</cell><cell>10.2</cell><cell>30.8</cell><cell>44.2</cell><cell>14</cell></row><row><cell>RVP (T) [3]</cell><cell>11.9</cell><cell>25.0</cell><cell>47.7</cell><cell>12</cell><cell>12.8</cell><cell>32.9</cell><cell>44.5</cell><cell>13</cell></row><row><cell>RVP (T+I) [3]</cell><cell>12.1</cell><cell>27.8</cell><cell>47.8</cell><cell>11</cell><cell>12.7</cell><cell>33.1</cell><cell>44.9</cell><cell>12.5</cell></row><row><cell>DVSA (DepTree) [18]</cell><cell>20.0</cell><cell>46.6</cell><cell>59.4</cell><cell>5.4</cell><cell>15.0</cell><cell>36.5</cell><cell>48.2</cell><cell>10.4</cell></row><row><cell>DVSA (BRNN) [18]</cell><cell>22.2</cell><cell>48.2</cell><cell>61.4</cell><cell>4.8</cell><cell>15.2</cell><cell>37.7</cell><cell>50.5</cell><cell>9.2</cell></row><row><cell>DCCA [44]</cell><cell>16.7</cell><cell>39.3</cell><cell>52.9</cell><cell>8</cell><cell>12.6</cell><cell>31.0</cell><cell>43.0</cell><cell>15</cell></row><row><cell>NIC [42]</cell><cell>17.0</cell><cell>*</cell><cell>56.0</cell><cell>7</cell><cell>17.0</cell><cell>*</cell><cell>57.0</cell><cell>7</cell></row><row><cell>LRCN [6]</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>17.5</cell><cell>40.3</cell><cell>50.8</cell><cell>9</cell></row><row><cell>RTP (joint training) [33]</cell><cell>31.0</cell><cell>58.6</cell><cell>67.9</cell><cell>*</cell><cell>22.0</cell><cell>50.7</cell><cell>62.0</cell><cell>*</cell></row><row><cell>RTP (SAE) [33]</cell><cell>36.7</cell><cell>61.9</cell><cell>73.6</cell><cell>*</cell><cell>25.4</cell><cell>55.2</cell><cell>68.6</cell><cell>*</cell></row><row><cell>RTP (weighted distance) [33]</cell><cell>37.4</cell><cell>63.1</cell><cell>74.3</cell><cell>*</cell><cell>26.0</cell><cell>56.0</cell><cell>69.3</cell><cell>*</cell></row><row><cell>FV (Mean Vec) [25]</cell><cell>24.8</cell><cell>52.5</cell><cell>64.3</cell><cell>5</cell><cell>20.5</cell><cell>46.3</cell><cell>59.3</cell><cell>6.8</cell></row><row><cell>FV (GMM) [25]</cell><cell>33.0</cell><cell>60.7</cell><cell>71.9</cell><cell>3</cell><cell>23.9</cell><cell>51.6</cell><cell>64.9</cell><cell>5</cell></row><row><cell>FV (LMM) [25]</cell><cell>32.5</cell><cell>59.9</cell><cell>71.5</cell><cell>3.2</cell><cell>23.6</cell><cell>51.2</cell><cell>64.4</cell><cell>5</cell></row><row><cell>FV (HGLMM) [25]</cell><cell>34.4</cell><cell>61.0</cell><cell>72.3</cell><cell>3</cell><cell>24.4</cell><cell>52.1</cell><cell>65.6</cell><cell>5</cell></row><row><cell>FV (GMM+HGLMM) [25]</cell><cell>35.0</cell><cell>62.0</cell><cell>73.8</cell><cell>3</cell><cell>25.0</cell><cell>52.7</cell><cell>66.0</cell><cell>5</cell></row><row><cell>OverFeat [35]:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m-CNN wd</cell><cell>12.7</cell><cell>30.2</cell><cell>44.5</cell><cell>14</cell><cell>11.6</cell><cell>32.1</cell><cell>44.2</cell><cell>14</cell></row><row><cell>m-CNN phs</cell><cell>14.4</cell><cell>38.6</cell><cell>49.6</cell><cell>11</cell><cell>12.4</cell><cell>33.3</cell><cell>44.7</cell><cell>14</cell></row><row><cell>m-CNN phl</cell><cell>13.8</cell><cell>38.1</cell><cell>48.5</cell><cell>11.5</cell><cell>11.6</cell><cell>32.7</cell><cell>44.1</cell><cell>14</cell></row><row><cell>m-CNNst</cell><cell>14.8</cell><cell>37.9</cell><cell>49.8</cell><cell>11</cell><cell>12.5</cell><cell>32.8</cell><cell>44.2</cell><cell>14</cell></row><row><cell>m-CNNENS</cell><cell>20.1</cell><cell>44.2</cell><cell>56.3</cell><cell>8</cell><cell>15.9</cell><cell>40.3</cell><cell>51.9</cell><cell>9.5</cell></row><row><cell>VGG [36]:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m-CNN wd</cell><cell>21.3</cell><cell>53.2</cell><cell>66.1</cell><cell>5</cell><cell>18.2</cell><cell>47.2</cell><cell>60.9</cell><cell>6</cell></row><row><cell>m-CNN phs</cell><cell>25.0</cell><cell>54.8</cell><cell>66.8</cell><cell>4.5</cell><cell>19.7</cell><cell>48.2</cell><cell>62.2</cell><cell>6</cell></row><row><cell>m-CNN phl</cell><cell>23.9</cell><cell>54.2</cell><cell>66.0</cell><cell>5</cell><cell>19.4</cell><cell>49.3</cell><cell>62.4</cell><cell>6</cell></row><row><cell>m-CNNst</cell><cell>27.0</cell><cell>56.4</cell><cell>70.1</cell><cell>4</cell><cell>19.7</cell><cell>48.4</cell><cell>62.3</cell><cell>6</cell></row><row><cell>m-CNNENS</cell><cell>33.6</cell><cell>64.1</cell><cell>74.9</cell><cell>3</cell><cell>26.2</cell><cell>56.3</cell><cell>69.6</cell><cell>4</cell></row><row><cell cols="3">and Microsoft COCO), is mainly the insufficient training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">samples. Flickr8K consists of only 8,000 images, which</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">are insufficient for adequately tuning the parameters of the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">convolutional architectures in m-CNNs. On Flickr30K and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Microsoft COCO datasets, with more training samples, m-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CNN EN S (with VGG) outperforms all the competitor mod-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">els in terms of most metrics, as illustrated in Table. 3 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">4. Moreover, except FV, only NIC slightly outperforms m-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CNN EN S (with VGG) on image retrieval task measured by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">R@10. Except the lack of training samples, another possi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ble reason is that NIC uses a better image CNN [41], com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">pared with VGG. As discussed in Section 5.3.3, the perfor-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mance of image CNN greatly affects the performance of the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bidirectional image and sentence retrieval.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Bidirectional image and sentence retrieval results on Microsoft COCO.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Sentence Retrieval</cell><cell></cell><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell></row><row><cell></cell><cell cols="8">R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r</cell></row><row><cell>Random Ranking</cell><cell>0.1</cell><cell>0.6</cell><cell>1.1</cell><cell>631</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500</cell></row><row><cell>m-RNN-vgg [28]</cell><cell>41.0</cell><cell>73.0</cell><cell>83.5</cell><cell>2</cell><cell>29.0</cell><cell>42.2</cell><cell>77.0</cell><cell>3</cell></row><row><cell>DVSA[18]</cell><cell>38.4</cell><cell>69.9</cell><cell>80.5</cell><cell>1</cell><cell>27.4</cell><cell>60.2</cell><cell>74.8</cell><cell>3</cell></row><row><cell>STV (uni-skip) [24]</cell><cell>30.6</cell><cell>64.5</cell><cell>79.8</cell><cell>3</cell><cell>22.7</cell><cell>56.4</cell><cell>71.7</cell><cell>4</cell></row><row><cell>STV (bi-skip) [24]</cell><cell>32.7</cell><cell>67.3</cell><cell>79.6</cell><cell>3</cell><cell>24.2</cell><cell>57.1</cell><cell>73.2</cell><cell>4</cell></row><row><cell>STV (combine-skip) [24]</cell><cell>33.8</cell><cell>67.7</cell><cell>82.1</cell><cell>3</cell><cell>25.9</cell><cell>60.0</cell><cell>74.6</cell><cell>4</cell></row><row><cell>FV (Mean Vec) [25]</cell><cell>33.2</cell><cell>61.8</cell><cell>75.1</cell><cell>3</cell><cell>24.2</cell><cell>56.4</cell><cell>72.4</cell><cell>4</cell></row><row><cell>FV (GMM) [25]</cell><cell>39.0</cell><cell>67.0</cell><cell>80.3</cell><cell>3</cell><cell>24.2</cell><cell>59.2</cell><cell>76.0</cell><cell>4</cell></row><row><cell>FV (LMM) [25]</cell><cell>38.6</cell><cell>67.8</cell><cell>79.8</cell><cell>3</cell><cell>25.0</cell><cell>59.5</cell><cell>76.1</cell><cell>4</cell></row><row><cell>FV (HGLMM) [25]</cell><cell>37.7</cell><cell>66.6</cell><cell>79.1</cell><cell>3</cell><cell>24.9</cell><cell>58.8</cell><cell>76.5</cell><cell>4</cell></row><row><cell>FV (GMM+HGLMM) [25]</cell><cell>39.4</cell><cell>67.9</cell><cell>80.9</cell><cell>2</cell><cell>25.1</cell><cell>59.8</cell><cell>76.6</cell><cell>4</cell></row><row><cell>VGG [36]:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m-CNN wd</cell><cell>34.1</cell><cell>66.9</cell><cell>79.7</cell><cell>3</cell><cell>27.9</cell><cell>64.7</cell><cell>80.4</cell><cell>3</cell></row><row><cell>m-CNN phs</cell><cell>34.6</cell><cell>67.5</cell><cell>81.4</cell><cell>3</cell><cell>27.6</cell><cell>64.4</cell><cell>79.5</cell><cell>3</cell></row><row><cell>m-CNN phl</cell><cell>35.1</cell><cell>67.3</cell><cell>81.6</cell><cell>2</cell><cell>27.1</cell><cell>62.8</cell><cell>79.3</cell><cell>3</cell></row><row><cell>m-CNNst</cell><cell>38.3</cell><cell>69.6</cell><cell>81.0</cell><cell>2</cell><cell>27.4</cell><cell>63.4</cell><cell>79.5</cell><cell>3</cell></row><row><cell>m-CNNENS</cell><cell>42.8</cell><cell>73.1</cell><cell>84.1</cell><cell>2</cell><cell>32.6</cell><cell>68.6</cell><cell>82.8</cell><cell>3</cell></row><row><cell>image</cell><cell></cell><cell>sentence</cell><cell></cell><cell></cell><cell cols="2">m-CNN wd</cell><cell>m-CNN phs</cell><cell>m-CNN phl</cell><cell>m-CNNst</cell></row><row><cell cols="5">three person sit at an outdoor table in front of a building paint like the union jack .</cell><cell></cell><cell>-0.87</cell><cell>1.91</cell><cell>-1.84</cell><cell>2.93</cell></row><row><cell cols="5">like union at in sit three jack the person a paint building table outdoor of front an .</cell><cell></cell><cell>-1.49</cell><cell>1.66</cell><cell>-3.00</cell><cell>2.37</cell></row><row><cell cols="5">sit union a jack three like in of paint the person table outdoor building front at an .</cell><cell></cell><cell>-2.44</cell><cell>1.55</cell><cell>-3.90</cell><cell>2.53</cell></row><row><cell cols="5">table sit three paint at a building of like the an person front outdoor jack union in .</cell><cell></cell><cell>-1.93</cell><cell>1.64</cell><cell>-3.81</cell><cell>2.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>The matching scores of the image and sentence. The natural sentence (in bold) is the true caption of the image, while the other three sentences are generated by random reshuffle of words.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5654</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Colllobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darreell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierachies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural network to retrieve images from text queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICANN</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by proventing co-adaptation of feature detecters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.3584</idno>
		<title level="m">Recurrent convolutional neural networks for discourse compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep fragment embeddings for bidirectional image sentence mapping. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural network for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Multimodal neural language model. ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep representations and codes for image auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech and time series. The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Dep captioning with multimodal recurrent neural networks (mrnn)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1090</idno>
		<title level="m">Explain images with multimodal recurrent neural networks</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Im2txt: Describing images using 1 million captioned photogrphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3505</idno>
		<title level="m">Deepid-net: Multi-stage and deformable deep convolutional neural networks for object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosnetence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04870</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recognition using visual phrases. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Overfeat: Intergrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V L A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations for multimodal data with deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermannet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Show and tell: a neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4556</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: new similarity metrics for semantic inference over event descriptions. TACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning the visual interpretation of sentences. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
