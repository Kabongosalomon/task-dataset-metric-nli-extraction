<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Songs</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
							<email>jaschasd@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
							<email>pooleb@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">tanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Two successful classes of probabilistic generative models involve sequentially corrupting training data with slowly increasing noise, and then learning to reverse this corruption in order to form a generative model of the data. Score matching with Langevin dynamics (SMLD)  estimates the score (i.e., the gradient of the log probability density with respect to data) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) <ref type="bibr" target="#b41">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b18">Ho et al., 2020)</ref> trains a sequence of probabilistic models to reverse each step of the noise corruption, using knowledge of the functional form of the reverse distributions to make training tractable. For continuous state spaces, the DDPM training objective implicitly computes scores at each noise scale. We therefore refer to these two model classes together as score-based generative models.</p><p>Score-based generative models, and related techniques <ref type="bibr" target="#b3">(Bordes et al., 2017;</ref><ref type="bibr" target="#b14">Goyal et al., 2017;</ref><ref type="bibr" target="#b11">Du &amp; Mordatch, 2019)</ref>, have proven effective at generation of images <ref type="bibr" target="#b18">Ho et al., 2020)</ref>, audio <ref type="bibr" target="#b6">(Chen et al., 2020;</ref><ref type="bibr" target="#b28">Kong et al., 2020)</ref>, graphs <ref type="bibr" target="#b33">(Niu et al., 2020)</ref>, and shapes (Cai Work partially done during an internship at Google Brain.  <ref type="bibr">et al., 2020)</ref>. To enable new sampling methods and further extend the capabilities of score-based generative models, we propose a unified framework that generalizes previous approaches through the lens of stochastic differential equations (SDEs).</p><p>Specifically, instead of perturbing data with a finite number of noise distributions, we consider a continuum of distributions that evolve over time according to a diffusion process. This process progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not depend on the data and has no trainable parameters. By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE <ref type="bibr" target="#b1">(Anderson, 1982)</ref>, which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximate the reverse-time SDE by training a time-dependent neural network to estimate the scores, and then produce samples using numerical SDE solvers. Our key idea is summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Our proposed framework has several theoretical and practical contributions:</p><p>Flexible sampling and likelihood computation: We can employ any general-purpose SDE solver to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC <ref type="bibr" target="#b36">(Parisi, 1981)</ref> and <ref type="bibr">HMC (Neal et al., 2011)</ref>; and (ii) deterministic samplers based on the probability flow ordinary differential equation <ref type="bibr">(ODE)</ref>. The former unifies and improves over existing sampling methods for score-based models. The latter allows for fast adaptive sampling via black-box ODE solvers, flexible data manipulation via latent codes, a uniquely identifiable encoding, and notably, exact likelihood computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controllable generation:</head><p>We can modulate the generation process by conditioning on information not available during training, because the conditional reverse-time SDE can be efficiently estimated from unconditional scores. This enables applications such as class-conditional generation, image inpainting, colorization and other inverse problems, all achievable using a single unconditional score-based model without re-training.</p><p>Unified framework: Our framework provides a unified way to explore and tune various SDEs for improving score-based generative models. The methods of SMLD and DDPM can be amalgamated into our framework as discretizations of two separate SDEs. Although DDPM <ref type="bibr" target="#b18">(Ho et al., 2020)</ref> was recently reported to achieve higher sample quality than SMLD , we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch up-it achieves new state-of-the-art Inception score (9.89) and FID score <ref type="bibr">(2.20)</ref> on CIFAR-10, as well as high-fidelity generation of 1024ˆ1024 images for the first time from a score-based model. In addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99 bits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DENOISING SCORE MATCHING WITH LANGEVIN DYNAMICS (SMLD)</head><p>Let p σ px | xq :" N px; x, σ 2 Iq be a perturbation kernel, and p σ pxq :" ş p data pxqp σ px | xqdx, where p data pxq denotes the data distribution. Consider a sequence of positive noise scales σ min " σ 1 ă σ 2 ă¨¨¨ă σ N " σ max . Typically, σ min is small enough such that p σmin pxq « p data pxq, and σ max is large enough such that p σmax pxq « N px; 0, σ 2 max Iq.  propose to train a Noise Conditional Score Network (NCSN), denoted by s θ px, σq, with a weighted sum of denoising score matching <ref type="bibr" target="#b47">(Vincent, 2011)</ref> objectives:</p><formula xml:id="formula_0">θ˚" arg min θ N ÿ i"1 σ 2 i E p data pxq E pσ i px|xq " s θ px, σ i q´∇x log p σi px | xq 2 2 ‰ .<label>(1)</label></formula><p>Given sufficient data and model capacity, the optimal score-based model s θ˚p x, σq matches ∇ x log p σ pxq almost everywhere for σ P tσ i u N i"1 . For sampling,  run M steps of Langevin MCMC to get a sample for each p σi pxq sequentially:</p><formula xml:id="formula_1">x m i " x m´1 i` i s θ˚p x m´1 i , σ i q`?2 i z m i , m " 1, 2,¨¨¨, M,<label>(2)</label></formula><p>where i ą 0 is the step size, and z m i is standard normal. The above is repeated for i " N, N1 ,¨¨¨, 1 in turn with x 0 N " N px | 0, σ 2 max Iq and x 0 i " x M i`1 when i ă N . As M Ñ 8 and i Ñ 0 for all i, x M 1 becomes an exact sample from p σmin pxq « p data pxq under some regularity conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DENOISING DIFFUSION PROBABILISTIC MODELS (DDPM)</head><p>Sohl-Dickstein et al. <ref type="formula" target="#formula_0">(2015)</ref>; <ref type="bibr" target="#b18">Ho et al. (2020)</ref> consider a sequence of positive noise scales 0 ă β 1 , β 2 ,¨¨¨, β N ă 1. For each training data point x 0 " p data pxq, a discrete Markov chain</p><formula xml:id="formula_2">tx 0 , x 1 ,¨¨¨, x N u is constructed such that ppx i | x i´1 q " N px i ; ? 1´β i x i´1 , β i Iq, and therefore p αi px i | x 0 q " N px i ; ? α i x 0 , p1´α i qIq, where α i :" ś i j"1 p1´β j q.</formula><p>Similar to SMLD, we can denote the perturbed data distribution as p αi pxq :" ş p data pxqp αi px | xqdx. The noise scales are prescribed such that x N is approximately distributed according to N p0, Iq. A variational Markov chain in the reverse direction is parameterized with p θ px i´1 |x i q " N px i´1 ; 1 ? 1´βi px i`βi s θ px i , iqq, β i Iq, and trained with a re-weighted variant of the evidence lower bound (ELBO):</p><formula xml:id="formula_3">θ˚" arg min θ N ÿ i"1 p1´α i qE p data pxq E pα i px|xq r s θ px, iq´∇x log p αi px | xq 2 2 s.<label>(3)</label></formula><p>After solving Eq. <ref type="formula" target="#formula_3">(3)</ref> to get the optimal model s θ˚p x, iq, samples can be generated by starting from x N " N p0, Iq and following the estimated reverse Markov chain as below</p><formula xml:id="formula_4">x i´1 " 1 ? 1´β i px i`βi s θ˚p x i , iqq`aβ i z i , i " N, N´1,¨¨¨, 1.<label>(4)</label></formula><p>We call this method ancestral sampling, since it amounts to performing ancestral sampling from the graphical model ś N i"1 p θ px i´1 | x i q. The objective Eq. (3) described here is L simple in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>, written in a form to expose more similarity to Eq. (1). Like Eq. (1), Eq. <ref type="formula" target="#formula_3">(3)</ref> is also a weighted sum of denoising score matching objectives, which implies that the optimal model, s θ˚px , iq, matches the score of the perturbed data distribution, ∇ x log p αi pxq. Notably, the weights of the i-th summand in Eq. (1) and Eq. <ref type="formula" target="#formula_3">(3)</ref>, namely σ 2 i and p1´α i q, are related to corresponding perturbation kernels in the same functional form:</p><formula xml:id="formula_5">σ 2 i 91{Er ∇ x log p σi px | xq 2 2 s and p1´α i q91{Er ∇ x log p αi px | xq 2 2 s.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCORE-BASED GENERATIVE MODELING WITH SDES</head><p>Perturbing data with multiple noise scales is key to the success of previous methods. We propose to generalize this idea further to an infinite number of noise scales, such that perturbed data distributions evolve according to an SDE as the noise intensifies. An overview of our framework is given in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PERTURBING DATA WITH SDES</head><p>Our goal is to construct a diffusion process txptqu T t"0 indexed by a continuous time variable t P r0, T s, such that xp0q " p 0 , for which we have a dataset of i.i.d. samples, and xpT q " p T , for which we have a tractable form to generate samples efficiently. In other words, p 0 is the data distribution and p T is the prior distribution. This diffusion process can be modeled as the solution to an Itô SDE:</p><formula xml:id="formula_6">dx " f px, tqdt`gptqdw,<label>(5)</label></formula><p>Forward SDE Data Prior Data Reverse SDE <ref type="figure">Figure 2</ref>: Overview of score-based generative modeling through SDEs. We can map data to a noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling (Section 3.2). We can also reverse the associated probability flow ODE (Section 4.3), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score ∇ x log p t pxq (Section 3.3).</p><p>where w is the standard Wiener process (a.k.a., Brownian motion), f p¨, tq : R d Ñ R d is a vectorvalued function called the drift coefficient of xptq, and gp¨q : R Ñ R is a scalar function known as the diffusion coefficient of xptq. For ease of presentation we assume the diffusion coefficient is a scalar (instead of a dˆd matrix) and does not depend on x, but our theory can be generalized to hold in those cases (see Appendix A). The SDE has a unique strong solution as long as the coefficients are globally Lipschitz in both state and time <ref type="bibr">(Øksendal, 2003)</ref>. We hereafter denote by p t pxq the probability density of xptq, and use p st pxptq | xpsqq to denote the transition kernel from xpsq to xptq, where 0 ď s ă t ď T .</p><p>Typically, p T is an unstructured prior distribution that contains no information of p 0 , such as a Gaussian distribution with fixed mean and variance. There are various ways of designing the SDE in Eq. (5) such that it diffuses the data distribution into a fixed prior distribution. We provide several examples later in Section 3.4 that are derived from continuous generalizations of SMLD and DDPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GENERATING SAMPLES BY REVERSING THE SDE</head><p>By starting from samples of xpT q " p T and reversing the process, we can obtain samples xp0q " p 0 . A remarkable result from <ref type="bibr" target="#b1">Anderson (1982)</ref> states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:</p><formula xml:id="formula_7">dx " rf px, tq´gptq 2 ∇ x log p t pxqsdt`gptqdw,<label>(6)</label></formula><p>wherew is a standard Wiener process when time flows backwards from T to 0, and dt is an infinitesimal negative timestep. Once the score of each marginal distribution, ∇ x log p t pxq, is known for all t, we can derive the reverse diffusion process from Eq. (6) and simulate it to sample from p 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ESTIMATING SCORES FOR THE SDE</head><p>The score of a distribution can be estimated by training a score-based model on samples with score matching <ref type="bibr" target="#b20">(Hyvärinen, 2005;</ref><ref type="bibr" target="#b44">Song et al., 2019a)</ref>. To estimate ∇ x log p t pxq, we can train a time-dependent score-based model s θ px, tq via a continuous generalization to Eqs. (1) and <ref type="formula" target="#formula_3">(3)</ref>:</p><formula xml:id="formula_8">θ˚" arg min θ E t ! λptqE xp0q E xptq|xp0q " s θ pxptq, tq´∇ xptq log p 0t pxptq | xp0qq 2 2 ‰ ) .<label>(7)</label></formula><p>Here λ : r0, T s Ñ R ą0 is a positive weighting function, t is uniformly sampled over r0, T s, xp0q " p 0 pxq and xptq " p 0t pxptq | xp0qq. With sufficient data and model capacity, score matching ensures that the optimal solution to Eq. <ref type="formula" target="#formula_8">(7)</ref>, denoted by s θ˚p x, tq, equals ∇ x log p t pxq for almost all x and t. As in SMLD and DDPM, we can typically choose λ91{E " ∇ xptq log p 0t pxptq | xp0qq <ref type="bibr">2 2</ref> ‰ . Note that Eq. (7) uses denoising score matching, but other score matching objectives, such as sliced score matching <ref type="bibr" target="#b44">(Song et al., 2019a</ref>) and finite-difference score matching <ref type="bibr" target="#b35">(Pang et al., 2020)</ref> are also applicable here.</p><p>We typically need to know the transition kernel p 0t pxptq | xp0qq to efficiently solve Eq. (7). When f p¨, tq is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in <ref type="bibr" target="#b39">Särkkä &amp; Solin (2019)</ref>). For more general SDEs, we may solve Kolmogorov's forward equation <ref type="bibr">(Øksendal, 2003)</ref> to obtain p 0t pxptq | xp0qq. Alternatively, we can simulate the SDE to sample from p 0t pxptq | xp0qq and replace denoising score matching in Eq. (7) with sliced score matching for model training, which bypasses the computation of ∇ xptq log p 0t pxptq | xp0qq (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">EXAMPLES: VE, VP SDES AND BEYOND</head><p>The noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different SDEs. Below we provide a brief discussion and relegate more details to Appendix B.</p><p>When using a total of N noise scales, each perturbation kernel p σi px | x 0 q of SMLD corresponds to the distribution of x i in the following Markov chain:</p><formula xml:id="formula_9">x i " x i´1`b σ 2 i´σ 2 i´1 z i´1 , i " 1,¨¨¨, N,<label>(8)</label></formula><p>where z i´1 " N p0, Iq, and we have introduced σ 0 " 0 to simplify the notation. In the limit of N Ñ 8, tσ i u N i"1 becomes a function σptq, z i becomes zptq, and the Markov chain tx i u N i"1 becomes a continuous stochastic process txptqu 1 t"0 , where we have used a continuous time variable t P r0, 1s for indexing, rather than an integer i. The process txptqu 1 t"0 is given by the following SDE dx "</p><formula xml:id="formula_10">c d rσ 2 ptqs dt dw.<label>(9)</label></formula><p>Likewise for the perturbation kernels tp αi px | x 0 qu N i"1 of DDPM, the discrete Markov chain is</p><formula xml:id="formula_11">x i " a 1´β i x i´1`a β i z i´1 , i " 1,¨¨¨, N.<label>(10)</label></formula><p>As N Ñ 8, Eq. (10) converges to the following SDE,</p><formula xml:id="formula_12">dx "´1 2 βptqx dt`aβptq dw.<label>(11)</label></formula><p>Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs Eqs. (9) and (11). Interestingly, the SDE of Eq. (9) always gives a process with exploding variance when t Ñ 8, whilst the SDE of Eq. (11) yields a process with a fixed variance of one when the initial distribution has unit variance (proof in Appendix B). Due to this difference, we hereafter refer to Eq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE.</p><p>Inspired by the VP SDE, we propose a new type of SDEs which perform particularly well on likelihoods (see Section 4.3), given by</p><formula xml:id="formula_13">dx "´1 2 βptqx dt`bβptqp1´e´2 ş t 0 βpsqds qdw.<label>(12)</label></formula><p>When using the same βptq and starting from the same initial distribution, the variance of the stochastic process induced by Eq. (12) is always bounded by the VP SDE at every intermediate time step <ref type="bibr">(proof in Appendix B)</ref>. For this reason, we name Eq. (12) the sub-VP SDE.</p><p>Since VE, VP and sub-VP SDEs all have affine drift coefficients, their perturbation kernels p 0t pxptq | xp0qq are all Gaussian and can be computed in closed-forms, as discussed in Section 3.3. This makes training with Eq. (7) particularly efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SOLVING THE REVERSE SDE</head><p>After training a time-dependent score-based model s θ , we can use it to construct the reverse-time SDE and then simulate it with numerical approaches to generate samples from p 0 .  <ref type="bibr">den &amp; Platen, 2013)</ref>, which correspond to different discretizations of the stochastic dynamics. We can apply any of them to the reverse-time SDE for sample generation.</p><p>Ancestral sampling, the sampling method of DDPM (Eq. <ref type="formula" target="#formula_4">(4)</ref>), actually corresponds to one special discretization of the reverse-time VP SDE (Eq. (11)) (see Appendix E). Deriving the ancestral sampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose reverse diffusion samplers (details in Appendix E), which discretize the reverse-time SDE in the same way as the forward one, and thus can be readily derived given the forward discretization. As shown in <ref type="table" target="#tab_0">Table 1</ref>, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and DDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models, see Appendix F.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PREDICTOR-CORRECTOR SAMPLERS</head><p>Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we have a score-based model s θ˚p x, tq « ∇ x log p t pxq, we can employ score-based MCMC approaches, such as Langevin MCMC <ref type="bibr" target="#b36">(Parisi, 1981;</ref><ref type="bibr" target="#b16">Grenander &amp; Miller, 1994)</ref> or <ref type="bibr">HMC (Neal et al., 2011)</ref> to sample from p t directly, and correct the solution of a numerical SDE solver.</p><p>Specifically, at each time step, the numerical SDE solver first gives an estimate of the sample at the next time step, playing the role of a "predictor". Then, the score-based MCMC approach corrects the marginal distribution of the estimated sample, playing the role of a "corrector". The idea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for solving systems of equations (Allgower &amp; Georg, 2012), and we similarly name our hybrid sampling algorithms Predictor-Corrector (PC) samplers. Please find pseudo-code and a complete description in Appendix G. PC samplers generalize the original sampling methods of SMLD and DDPM: the former uses an identity function as the predictor and annealed Langevin dynamics as the corrector, while the latter uses ancestral sampling as the predictor and identity as the corrector.</p><p>We test PC samplers on SMLD and DDPM models (see Algorithms 2 and 3 in Appendix G) trained with original discrete objectives given by Eqs.</p><p>(1) and <ref type="bibr" target="#b52">(3)</ref>. This exhibits the compatibility of PC samplers to score-based models trained with a fixed number of noise scales. We summarize the performance of different samplers in <ref type="table" target="#tab_0">Table 1</ref>, where probability flow is a predictor to be discussed in Section 4.3. Detailed experimental settings and additional results are given in Appendix G. We observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation (In fact, we need way more corrector steps per noise scale, and thus more computation, to match the performance of other samplers.) For all predictors, adding one corrector step for each predictor step (PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it is typically better than doubling the number of predictor steps without adding a corrector (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for SMLD/DDPM models. In <ref type="figure" target="#fig_7">Fig. 9</ref> (Appendix G), we additionally provide qualitative comparison for   3.45 -Glow <ref type="bibr" target="#b26">(Kingma &amp; Dhariwal, 2018)</ref> 3.35 -MintNet <ref type="bibr" target="#b45">(Song et al., 2019b)</ref> 3.32 -Residual Flow  3.28 46.37 FFJORD <ref type="bibr" target="#b15">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type="bibr" target="#b17">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type="bibr" target="#b18">(Ho et al., 2020)</ref> ď 3.70 * 13.51 DDPM (Lsimple) <ref type="bibr" target="#b18">(Ho et al., 2020)</ref> ď    25.32 8.87˘.12 NCSNv2  10.87 8.40˘.07 DDPM <ref type="bibr" target="#b18">(Ho et al., 2020)</ref> 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PROBABILITY FLOW AND CONNECTION TO NEURAL ODES</head><p>Score-based models enable another numerical method for solving the reverse-time SDE. For all diffusion processes, there exists a corresponding deterministic process whose trajectories share the same marginal probability densities tp t pxqu T t"0 as the SDE. This deterministic process satisfies an ODE (more details in Appendix D.1):</p><p>dx "</p><formula xml:id="formula_14">" f px, tq´1 2 gptq 2 ∇ x log p t pxq ı dt,<label>(13)</label></formula><p>which can be determined from the SDE once scores are known. We name the ODE in Eq. (13) the probability flow ODE. When the score function is approximated by the time-dependent score-based model, which is typically a neural network, this is an example of a neural ODE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exact likelihood computation</head><p>Leveraging the connection to neural ODEs, we can compute the density defined by Eq. (13) via the instantaneous change of variables formula . This allows us to compute the exact likelihood on any input data (details in Appendix D.2). As an example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset in <ref type="table" target="#tab_1">Table 2</ref>. We compute log-likelihoods on uniformly dequantized data, and only compare to models evaluated in the same way (omitting models evaluated with variational dequantization <ref type="bibr" target="#b17">(Ho et al., 2019)</ref> or discrete data), except for DDPM (L/L simple ) whose ELBO values (annotated with *) are reported on discrete data. Main results: (i) For the same DDPM model in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>, we obtain better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with the continuous objective in Eq. (7) (i.e., DDPM cont.), which further improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared to VP SDEs; (iv) With improved architecture (i.e., DDPM++ cont., details in Section 4.4) and the sub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even without maximum likelihood training.</p><p>Manipulating latent representations By integrating Eq. (13), we can encode any datapoint xp0q into a latent space xpT q. Decoding can be achieved by integrating a corresponding ODE for the reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing flows <ref type="bibr" target="#b9">(Dinh et al., 2016;</ref><ref type="bibr" target="#b26">Kingma &amp; Dhariwal, 2018)</ref>, we can manipulate this latent representation for image editing, such as interpolation, and temperature scaling (see <ref type="figure" target="#fig_1">Fig. 3</ref> and Appendix D.4).</p><p>Uniquely identifiable encoding Unlike most current invertible models, our encoding is uniquely identifiable, meaning that with sufficient training data, model capacity, and optimization accuracy, the encoding for an input is uniquely determined by the data distribution <ref type="bibr" target="#b38">(Roeder et al., 2020)</ref>. This is because our forward SDE, Eq. (5), has no trainable parameters, and its associated probability flow ODE, Eq. (13), provides the same trajectories given perfectly estimated scores. We provide additional empirical verification on this property in Appendix D.5.</p><p>Efficient sampling As with neural ODEs, we can sample xp0q " p 0 by solving Eq. (13) from different final conditions xpT q " p T . Using a fixed discretization strategy we can generate competitive samples, especially when used in conjuction with correctors (  <ref type="table" target="#tab_1">Table 2</ref>, details in Appendix D.4), but also allows us to explicitly trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations can be reduced by over 90% without affecting the visual quality of samples ( <ref type="figure" target="#fig_1">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ARCHITECTURE IMPROVEMENTS</head><p>We explore several new architecture designs for score-based models using both VE and VP SDEs (details in Appendix H), where we train models with the same discrete objectives as in SMLD/DDPM. We directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our optimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with PC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.</p><p>By switching to the continuous training objective in Eq. <ref type="formula" target="#formula_8">(7)</ref>, and increasing the network depth, we can further improve sample quality for all models. The resulting architectures are denoted as NCSN++ cont. and DDPM++ cont. in <ref type="table" target="#tab_3">Table 3</ref> for VE and VP/sub-VP SDEs respectively. Results reported in <ref type="table" target="#tab_3">Table 3</ref> are for the checkpoint with the smallest FID over the course of training, where samples are generated with PC samplers. In contrast, FID scores and NLL values in <ref type="table" target="#tab_1">Table 2</ref> are reported for the last training checkpoint, and samples are obtained with black-box ODE solvers. As shown in <ref type="table" target="#tab_3">Table 3</ref>, VE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically observe that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that practitioners likely need to experiment with different SDEs for varying domains and architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONTROLLABLE GENERATION</head><p>The continuous structure of our framework allows us to not only produce data samples from p 0 , but also from p 0 pxp0q | yq if p t py | xptqq is known. Given a forward SDE as in Eq. <ref type="formula" target="#formula_6">(5)</ref>, we can sample from p t pxptq | yq by starting from p T pxpT q | yq and solving a conditional reverse-time SDE:</p><formula xml:id="formula_15">dx " tf px, tq´gptq 2 r∇ x log p t pxq`∇ x log p t py | xqsudt`gptqdw.<label>(14)</label></formula><p>In general, we can use Eq. <ref type="formula" target="#formula_0">(14)</ref> to solve a large family of inverse problems with score-based generative models, once given an estimate of the gradient of the forward process, ∇ x log p t py | xptqq. In some cases, it is possible to train a separate model to learn the forward process log p t py | xptqq and compute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge. In Appendix I.4, we provide a broadly applicable method for obtaining such an estimate without the need of training auxiliary models.</p><p>We consider three applications of controllable generation with this approach: class-conditional generation, image imputation and colorization. When y represents class labels, we can train a time-dependent classifier p t py | xptqq for class-conditional sampling. Since the forward SDE is tractable, we can easily create training data pxptq, yq for the time-dependent classifier by first sampling pxp0q, yq from a dataset, and then sampling xptq " p 0t pxptq | xp0qq. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. <ref type="formula" target="#formula_8">(7)</ref>, to train the time-dependent classifier p t py | xptqq. We provide class-conditional CIFAR-10 samples in <ref type="figure" target="#fig_2">Fig. 4</ref> (left), and relegate more details and results to Appendix I.</p><p>Imputation is a special case of conditional sampling. Suppose we have an incomplete data point y where only some subset, Ωpyq is known. Imputation amounts to sampling from ppxp0q | Ωpyqq, which we can accomplish using an unconditional model (see Appendix I.2). Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions with an orthogonal linear transformation, and perform imputation in the transformed space (details in Appendix I.3). <ref type="figure" target="#fig_2">Fig. 4 (right)</ref> shows results for inpainting and colorization achieved with unconditional time-dependent score-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented a framework for score-based generative modeling based on SDEs. Our work enables a better understanding of existing approaches, new sampling algorithms, exact likelihood computation, uniquely identifiable encoding, latent code manipulation, and brings new conditional generation abilities to the family of score-based generative models.</p><p>While our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref> on the same datasets. Identifying ways of combining the stable learning of score-based generative models with the fast sampling of implicit models like GANs remains an important research direction. Additionally, the breadth of samplers one can use when given access to score functions introduces a number of hyper-parameters. Future work would benefit from improved methods to automatically select and tune these hyperparameters, as well as more extensive investigation on the merits and limitations of various samplers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We include several appendices with additional details, derivations, and results. Our framework allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which we provide a detailed discussion in Appendix A. We give a full derivation of VE, VP and sub-VP SDEs in Appendix B, and discuss how to use them from a practitioner's perspective in Appendix C. We elaborate on the probability flow formulation of our framework in Appendix D, including a derivation of the probability flow ODE (Appendix D.1), exact likelihood computation (Appendix D.2), probability flow sampling with a fixed discretization strategy (Appendix D.3), sampling with blackbox ODE solvers (Appendix D.4), and experimental verification on uniquely identifiable encoding (Appendix D.5). We give a full description of the reverse diffusion sampler in Appendix E, the DDPM-type ancestral sampler for SMLD models in Appendix F, and Predictor-Corrector samplers in Appendix G. We explain our model architectures and detailed experimental settings in Appendix H, with 1024ˆ1024 CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable generation in Appendix I, and include extended results for class-conditional generation (Appendix I.1), image inpainting (Appendix I.2), colorization (Appendix I.3), and a strategy for solving general inverse problems (Appendix I.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THE FRAMEWORK FOR MORE GENERAL SDES</head><p>In the main text, we introduced our framework based on a simplified SDE Eq. <ref type="formula" target="#formula_6">(5)</ref> where the diffusion coefficient is independent of xptq. It turns out that our framework can be extended to hold for more general diffusion coefficients. We can consider SDEs in the following form:</p><formula xml:id="formula_16">dx " f px, tqdt`Gpx, tqdw,<label>(15)</label></formula><p>where f p¨, tq : R d Ñ R d and Gp¨, tq : R d Ñ R dˆd . We follow the Itô interpretation of SDEs throughout this paper.</p><p>According to <ref type="bibr" target="#b1">(Anderson, 1982)</ref>, the reverse-time SDE is given by (cf ., Eq. <ref type="formula" target="#formula_7">(6))</ref> dx " tf px, tq´∇¨rGpx, tqGpx, tq T s´Gpx, tqGpx, tq T ∇ x log p t pxqudt`Gpx, tqdw, <ref type="formula" target="#formula_0">(16)</ref> where we define ∇¨Fpxq :" p∇¨f 1 pxq, ∇¨f 2 pxq,¨¨¨, ∇¨f d pxqq T for a matrix-valued function Fpxq :" pf 1 pxq, f 2 pxq,¨¨¨, f d pxqq T throughout the paper.</p><p>The probability flow ODE corresponding to Eq. (15) has the following form (cf ., Eq. (13), see a detailed derivation in Appendix D.1):</p><p>dx "</p><formula xml:id="formula_17">" f px, tq´1 2 ∇¨rGpx, tqGpx, tq T s´1 2 Gpx, tqGpx, tq T ∇ x log p t pxq * dt.<label>(17)</label></formula><p>Finally for conditional generation with the general SDE Eq. (15), we can solve the conditional reverse-time SDE below (cf ., Eq. <ref type="formula" target="#formula_0">(14)</ref>, details in Appendix I):</p><formula xml:id="formula_18">dx " tf px, tq´∇¨rGpx, tqGpx, tq T s´Gpx, tqGpx, tq T ∇ x log p t pxq Gpx, tqGpx, tq T ∇ x log p t py | xqudt`Gpx, tqdw.<label>(18)</label></formula><p>When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the transition kernel p 0t pxptq | xp0qq in closed form. This hinders the training of score-based models, because Eq. (7) requires knowing ∇ xptq log p 0t pxptq | xp0qq. To overcome this difficulty, we can replace denoising score matching in Eq. (7) with other efficient variants of score matching that do not require computing ∇ xptq log p 0t pxptq | xp0qq. For example, when using sliced score matching <ref type="bibr" target="#b44">(Song et al., 2019a)</ref>, our training objective Eq. (7) becomes θ˚" arg min</p><formula xml:id="formula_19">θ E t " λptqE xp0q E xptq E v"pv " 1 2 s θ pxptq, tq 2 2`v T s θ pxptq, tqv * ,<label>(19)</label></formula><p>where λ : r0, T s Ñ R`is a positive weighting function, t " Up0, T q, Ervs " 0, and Covrvs " I. We can always simulate the SDE to sample from p 0t pxptq | xp0qq, and solve Eq. (19) to train the time-dependent score-based model s θ px, tq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B VE, VP AND SUB-VP SDES</head><p>Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively. We additionally introduce sub-VP SDEs, a modification to VP SDEs that often achieves better performance in both sample quality and likelihoods.</p><p>First, when using a total of N noise scales, each perturbation kernel p σi px | x 0 q of SMLD can be derived from the following Markov chain:</p><formula xml:id="formula_20">x i " x i´1`b σ 2 i´σ 2 i´1 z i´1 , i " 1,¨¨¨, N,<label>(20)</label></formula><p>where z i´1 " N p0, Iq, x 0 " p data , and we have introduced σ 0 " 0 to simplify the notation. In the limit of N Ñ 8, the Markov chain tx i u N i"1 becomes a continuous stochastic process txptqu 1 t"0 , tσ i u N i"1 becomes a function σptq, and z i becomes zptq, where we have used a continuous time variable t P r0, 1s for indexing, rather than an integer i P t1, 2,¨¨¨, N u. Let x`i N˘" x i , σ`i N˘" σ i , and z`i N˘" z i for i " 1, 2,¨¨¨, N . We can rewrite Eq. (20) as follows with ∆t " 1 N and</p><formula xml:id="formula_21">t P 0, 1 N ,¨¨¨, N´1 N ( : xpt`∆tq " xptq`aσ 2 pt`∆tq´σ 2 ptq zptq « xptq`c d rσ 2 ptqs dt ∆t zptq,</formula><p>where the approximate equality holds when ∆t ! 1. In the limit of ∆t Ñ 0, this converges to</p><formula xml:id="formula_22">dx " c d rσ 2 ptqs dt dw,<label>(21)</label></formula><p>which is the VE SDE.</p><p>For the perturbation kernels tp αi px | x 0 qu N i"1 used in DDPM, the discrete Markov chain is</p><formula xml:id="formula_23">x i " a 1´β i x i´1`a β i z i´1 , i " 1,¨¨¨, N,<label>(22)</label></formula><p>where z i´1 " N p0, Iq. To obtain the limit of this Markov chain when N Ñ 8, we define an auxiliary set of noise scales tβ i " N β i u N i"1 , and re-write Eq. <ref type="formula" target="#formula_1">(22)</ref> as below</p><formula xml:id="formula_24">x i " c 1´β i N x i´1`cβ i N z i´1 , i " 1,¨¨¨, N.<label>(23)</label></formula><p>In the limit of N Ñ 8, tβ i u N i"1 becomes a function βptq indexed by t P r0, 1s. Let β`i N˘"β i , xp i N q " x i , zp i N q " z i . We can rewrite the Markov chain Eq. (23) as the following with ∆t " 1 N and t P t0, 1,¨¨¨, N´1 N u:</p><formula xml:id="formula_25">xpt`∆tq " a 1´βpt`∆tq∆t xptq`aβpt`∆tq∆t zptq « xptq´1 2 βpt`∆tq∆t xptq`aβpt`∆tq∆t zptq « xptq´1 2 βptq∆t xptq`aβptq∆t zptq,<label>(24)</label></formula><p>where the approximate equality holds when ∆t ! 1. Therefore, in the limit of ∆t Ñ 0, Eq. (24) converges to the following VP SDE:</p><formula xml:id="formula_26">dx "´1 2 βptqx dt`aβptq dw.<label>(25)</label></formula><p>So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding variance when t Ñ 8. In contrast, the VP SDE yields a process with bounded variance. In addition, the process has a constant unit variance for all t P r0, 8q when ppxp0qq has a unit variance. Since the VP SDE has affine drift and diffusion coefficients, we can use Eq. (5.51) in <ref type="bibr" target="#b39">Särkkä &amp; Solin (2019)</ref> to obtain an ODE that governs the evolution of variance</p><formula xml:id="formula_27">dΣ VP ptq dt " βptqpI´Σ VP ptqq,</formula><p>where Σ VP ptq :" Covrxptqs for txptqu 1 t"0 obeying a VP SDE. Solving this ODE, we obtain</p><formula xml:id="formula_28">Σ VP ptq " I`e ş t 0´β psqds pΣ VP p0q´Iq,<label>(26)</label></formula><p>from which it is clear that the variance Σ VP ptq is always bounded given Σ VP p0q. Moreover, Σ VP ptq " I if Σ VP p0q " I. Due to this difference, we name Eq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE.</p><p>Inspired by the VP SDE, we propose a new SDE called the sub-VP SDE, namely</p><formula xml:id="formula_29">dx "´1 2 βptqx dt`bβptqp1´e´2 ş t 0 βpsqds qdw.<label>(27)</label></formula><p>Following standard derivations, it is straightforward to show that Erxptqs is the same for both VP and sub-VP SDEs; the variance function of sub-VP SDEs is different, given by</p><formula xml:id="formula_30">Σ sub-VP ptq " I`e´2 ş t 0 βpsqds I`e´ş t 0 βpsqds pΣ sub-VP p0q´2Iq,<label>(28)</label></formula><p>where Σ sub-VP ptq :" Covrxptqs for a process txptqu 1 t"0 obtained by solving Eq. <ref type="formula" target="#formula_1">(27)</ref>. In addition, we observe that (i) Σ sub-VP ptq ď Σ VP ptq for all t ě 0 with Σ sub-VP p0q " Σ VP p0q and shared βpsq; and (ii) lim tÑ8 Σ sub-VP ptq " lim tÑ8 Σ VP ptq " I if lim tÑ8 ş t 0 βpsqds " 8. The former is why we name Eq. (27) the sub-VP SDE-its variance is always upper bounded by the corresponding VP SDE. The latter justifies the use of sub-VP SDEs for score-based generative modeling, since they can perturb any data distribution to standard Gaussian under suitable conditions, just like VP SDEs.</p><p>VE, VP and sub-VP SDEs all have affine drift coefficients. Therefore, their perturbation kernels p 0t pxptq | xp0qq are all Gaussian and can be computed with Eqs. (5.50) and (5.51) in <ref type="bibr" target="#b39">Särkkä &amp; Solin (2019)</ref>: . <ref type="formula" target="#formula_1">(29)</ref> As a result, all SDEs introduced here can be efficiently trained with the objective in Eq. (7).</p><formula xml:id="formula_31">p 0t pxptq | xp0qq " $ ' &amp; ' % N`xptq; xp0q, rσ 2 ptq´σ 2 p0qsI˘, (VE SDE) N`xptq; xp0qe´1 2 ş t 0 βpsqds , I´Ie´ş</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SDES IN THE WILD</head><p>Below we discuss concrete instantiations of VE and VP SDEs whose discretizations yield SMLD and DDPM models, and the specific sub-VP SDE used in our experiments. In SMLD, the noise scales tσ i u N i"1 is typically a geometric sequence where σ min is fixed to 0.01 and σ max is chosen according to Technique 1 in . Usually, SMLD models normalize image inputs to the range r0, 1s. Since tσ i u N i"1 is a geometric sequence, we have σp i N q " σ i " σ min´σ max σmin¯i´1 N´1 for i " 1, 2,¨¨¨, N . In the limit of N Ñ 8, we have σptq " σ min´σ max σmin¯t for t P p0, 1s. The corresponding VE SDE is</p><formula xml:id="formula_32">dx " σ minˆσ max σ min˙t c 2 log σ max σ min dw, t P p0, 1s,<label>(30)</label></formula><p>and the perturbation kernel can be derived via Eq. (29):</p><formula xml:id="formula_33">p 0t pxptq | xp0qq " Nˆxptq; xp0q, σ 2 min´σ max σ min¯2 t I˙, t P p0, 1s.<label>(31)</label></formula><p>There is one subtlety when t " 0: by definition, σp0q " σ 0 " 0 (following the convention in Eq. <ref type="formula" target="#formula_1">(20)</ref>), but σp0`q :" lim tÑ0`σ ptq " σ min ‰ 0. In other words, σptq for SMLD is not differentiable since σp0q ‰ σp0`q, causing the VE SDE in Eq. (21) undefined for t " 0. In practice, we bypass this issue by always solving the SDE and its associated probability flow ODE in the range t P r , 1s for some small constant ą 0, and we use " 10´5 in our VE SDE experiments. For DDPM models, tβ i u N i"1 is typically an arithmetic sequence where β i "β min N`i´1 N pN´1q pβ maxβ min q for i " 1, 2,¨¨¨, N . Therefore, βptq "β min`t pβ max´βmin q for t P r0, 1s in the limit of N Ñ 8. This corresponds to the following instantiation of the VP SDE:</p><p>dx "´1 2 pβ min`t pβ max´βmin qqxdt`bβ min`t pβ max´βmin qdw, t P r0, 1s,</p><p>where xp0q " p data pxq. In our experiments, we letβ min " 0.1 andβ max " 20 to match the settings in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formula xml:id="formula_35">p 0t pxptq | xp0qq " N´xptq; e´1 4 t 2 pβmax´βminq´1 2 tβmin xp0q, I´Ie´1 2 t 2 pβmax´βminq´tβmin¯, t P r0, 1s. (33)</formula><p>For DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical instability issues for training and sampling at t " 0, due to the vanishing variance of xptq as t Ñ 0. Therefore, same as the VE SDE, we restrict computation to t P r , 1s for a small ą 0. For sampling, we choose " 10´3 so that the variance of xp q in VP SDE matches the variance of x 1 in DDPM; for training and likelihood computation, we adopt " 10´5 which empirically gives better results.</p><p>As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation kernels of SDEs and original discrete Markov chains in <ref type="figure" target="#fig_4">Fig. 5</ref>. The SMLD and DDPM models both use N " 1000 noise scales. For SMLD, we only need to compare the variances of perturbation kernels since means are the same by definition. For DDPM, we compare the scaling factors of means and the variances. As demonstrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, the discrete perturbation kernels of original SMLD and DDPM models align well with perturbation kernels derived from VE and VP SDEs.</p><p>For sub-VP SDEs, we use exactly the same βptq as VP SDEs. This leads to the following perturbation kernel p 0t pxptq | xp0qq " N´xptq; e´1 4 t 2 pβmax´βminq´1 2 tβmin xp0q, r1´e´1 2 t 2 pβmax´βminq´tβmin s 2 I¯, t P r0, 1s. (34)</p><p>We also restrict numerical computation to the same interval of r , 1s as VP SDEs.</p><p>Empirically, we observe that smaller generally yields better likelihood values for all SDEs. For sampling, it is important to use an appropriate for better Inception scores and FIDs, although samples across different look visually the same to human eyes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROBABILITY FLOW ODE D.1 DERIVATION</head><p>The idea of probability flow ODE is inspired by <ref type="bibr" target="#b31">Maoutsa et al. (2020)</ref>, and one can find the derivation of a simplified case therein. Below we provide a derivation for the fully general ODE in Eq. (17). We consider the SDE in Eq. (15), which possesses the following form:</p><p>dx " f px, tqdt`Gpx, tqdw,</p><p>where f p¨, tq : R d Ñ R d and Gp¨, tq : R d Ñ R dˆd . The marginal probability density p t pxptqq evolves according to Kolmogorov's forward equation (Fokker-Planck equation) <ref type="bibr">(Øksendal, 2003)</ref> Bp t pxq Bt "´d</p><formula xml:id="formula_36">ÿ i"1 B Bx i rf i px, tqp t pxqs`1 2 d ÿ i"1 d ÿ j"1 B 2 Bx i Bx j " d ÿ k"1 G ik px, tqG jk px, tqp t pxq ı .<label>(35)</label></formula><p>We can easily rewrite Eq. (35) to obtain</p><formula xml:id="formula_37">Bp t pxq Bt "´d ÿ i"1 B Bx i rf i px, tqp t pxqs`1 2 d ÿ i"1 d ÿ j"1 B 2 Bx i Bx j " d ÿ k"1 G ik px, tqG jk px, tqp t pxq ı "´d ÿ i"1 B Bx i rf i px, tqp t pxqs`1 2 d ÿ i"1 B Bx i " d ÿ j"1 B Bx j " d ÿ k"1 G ik px, tqG jk px, tqp t pxq ıı . (36) Note that d ÿ j"1 B Bx j " d ÿ k"1 G ik px, tqG jk px, tqp t pxq ı " d ÿ j"1 B Bx j " d ÿ k"1 G ik px, tqG jk px, tq ı p t pxq`d ÿ j"1 d ÿ k"1 G ik px, tqG jk px, tqp t pxq B Bx j log p t pxq "p t pxq∇¨rGpx, tqGpx, tq T s`p t pxqGpx, tqGpx, tq T ∇ x log p t pxq,</formula><p>based on which we can continue the rewriting of Eq. (36) to obtain</p><formula xml:id="formula_38">Bp t pxq Bt "´d ÿ i"1 B Bx i rf i px, tqp t pxqs`1 2 d ÿ i"1 B Bx i " d ÿ j"1 B Bx j " d ÿ k"1 G ik px, tqG jk px, tqp t pxq ıı "´d ÿ i"1 B Bx i rf i px, tqp t pxqs 1 2 d ÿ i"1 B Bx i " p t pxq∇¨rGpx, tqGpx, tq T s`p t pxqGpx, tqGpx, tq T ∇ x log p t pxq ı "´d ÿ i"1 B Bx i ! f i px, tqp t pxq 1 2 " ∇¨rGpx, tqGpx, tq T s`Gpx, tqGpx, tq T ∇ x log p t pxq ı p t pxq ) "´d ÿ i"1 B Bx i rf i px, tqp t pxqs,<label>(37)</label></formula><p>where we definẽ</p><formula xml:id="formula_39">f px, tq :" f px, tq´1 2 ∇¨rGpx, tqGpx, tq T s´1 2 Gpx, tqGpx, tq T ∇ x log p t pxq.</formula><p>Inspecting Eq. (37), we observe that it equals Kolmogorov's forward equation of the following SDE withGpx, tq :" 0 (Kolmogorov's forward equation in this case is also known as the Liouville equation.)</p><p>dx "f px, tqdt`Gpx, tqdw, which is essentially an ODE:</p><p>dx "f px, tqdt, same as the probability flow ODE given by Eq. <ref type="figure" target="#fig_0">(17)</ref>. Therefore, we have shown that the probability flow ODE Eq. (17) induces the same marginal probability density p t pxq as the SDE in Eq. (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 LIKELIHOOD COMPUTATION</head><p>The probability flow ODE in Eq. (17) has the following form when we replace the score ∇ x log p t pxq with the time-dependent score-based model s θ px, tq:</p><p>dx "</p><formula xml:id="formula_40">" f px, tq´1 2 ∇¨rGpx, tqGpx, tq T s´1 2 Gpx, tqGpx, tq T s θ px, tq * loooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon ":f θ px,tq dt.<label>(38)</label></formula><p>With the instantaneous change of variables formula , we can compute the loglikelihood of p 0 pxq using</p><formula xml:id="formula_41">log p 0 pxp0qq " log p T pxpT qq`ż T 0 ∇¨f θ pxptq, tqdt,<label>(39)</label></formula><p>where the random variable xptq as a function of t can be obtained by solving the probability flow ODE in Eq. (38). In many cases computing ∇¨f θ px, tq is expensive, so we follow <ref type="bibr" target="#b15">Grathwohl et al. (2018)</ref> to estimate it with the Skilling-Hutchinson trace estimator <ref type="bibr" target="#b40">(Skilling, 1989;</ref><ref type="bibr" target="#b19">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml:id="formula_42">∇¨f θ px, tq " E pp q r T ∇f θ px, tq s,<label>(40)</label></formula><p>where ∇f θ denotes the Jacobian off θ p¨, tq, and the random variable satisfies E pp q r s " 0 and Cov pp q r s " I. The vector-Jacobian product T ∇f θ px, tq can be efficiently computed using reversemode automatic differentiation, at approximately the same cost as evaluatingf θ px, tq. As a result, we can sample " pp q and then compute an efficient unbiased estimate to ∇¨f θ px, tq using T ∇f θ px, tq . Since this estimator is unbiased, we can attain an arbitrarily small error by averaging over a sufficient number of runs. Therefore, by applying the  to Eq. (39), we can compute the log-likelihood to any accuracy.</p><p>In our experiments, we use the RK45 ODE solver <ref type="bibr" target="#b10">(Dormand &amp; Prince, 1980)</ref> provided by scipy.integrate.solve_ivp in all cases. The bits/dim values in <ref type="table" target="#tab_1">Table 2</ref> are computed with atol=1e-5 and rtol=1e-5, same as <ref type="bibr" target="#b15">Grathwohl et al. (2018)</ref>. To give the likelihood results of our models in <ref type="table" target="#tab_1">Table 2</ref>, we average the bits/dim obtained on the test dataset over five different runs with " 10´5 (see definition of in Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 PROBABILITY FLOW SAMPLING</head><p>Suppose we have a forward SDE dx " f px, tqdt`Gptqdw, and one of its discretization</p><formula xml:id="formula_43">x i`1 " x i`fi px i q`G i z i , i " 0, 1,¨¨¨, N´1,<label>(41)</label></formula><p>where z i " N p0, Iq. We assume the discretization schedule of time is fixed beforehand, and thus we absorb the dependency on ∆t into the notations of f i and G i . Using Eq. (17), we can obtain the following probability flow ODE:</p><p>dx "</p><formula xml:id="formula_44">" f px, tq´1 2 GptqGptq T ∇ x log p t pxq * dt.<label>(42)</label></formula><p>We may employ any numerical method to integrate the probability flow ODE backwards in time for sample generation. In particular, we propose a discretization in a similar functional form to Eq. (41):</p><formula xml:id="formula_45">x i " x i`1´fi`1 px i`1 q`1 2 G i`1 G T i`1 s θ˚p x i`1 , i`1q, i " 0, 1,¨¨¨, N´1</formula><p>, where the score-based model s θ˚p x i , iq is conditioned on the iteration number i. This is a deterministic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional randomness once the initial sample x N is obtained from the prior distribution. When applied to SMLD models, we can get the following iteration rule for probability flow sampling:</p><formula xml:id="formula_46">x i " x i`1`1 2 pσ 2 i`1´σ 2 i qs θ˚p x i`1 , σ i`1 q, i " 0, 1,¨¨¨, N´1.<label>(43)</label></formula><p>Similarly, for DDPM models, we have</p><formula xml:id="formula_47">x i " p2´a1´β i`1 qx i`1`1 2 β i`1 s θ˚p x i`1 , i`1q, i " 0, 1,¨¨¨, N´1.<label>(44)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 SAMPLING WITH BLACK-BOX ODE SOLVERS</head><p>For producing figures in <ref type="figure" target="#fig_1">Fig. 3</ref>, we use a DDPM model trained on 256ˆ256 CelebA-HQ with the same settings in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>. All FID scores of our models in <ref type="table" target="#tab_1">Table 2</ref> are computed on samples from the RK45 ODE solver implemented in scipy.integrate.solve_ivp with atol=1e-5 and rtol=1e-5. We use " 10´5 for VE SDEs and " 10´3 for VP SDEs (see also Appendix C).</p><p>Aside from the interpolation results in <ref type="figure" target="#fig_1">Fig. 3</ref>, we demonstrate more examples of latent space manipulation in <ref type="figure" target="#fig_5">Fig. 6</ref>, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>.</p><p>Although solvers for the probability flow ODE allow fast sampling, their samples typically have higher (worse) FID scores than those from SDE solvers if no corrector is used. We have this empirical observation for both the discretization strategy in Appendix D.3, and black-box ODE solvers introduced above. Moreover, the performance of probability flow ODE samplers depends on the choice of the SDE-their sample quality for VE SDEs is much worse than VP SDEs especially for high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 UNIQUELY IDENTIFIABLE ENCODING</head><p>As a sanity check, we train two models (denoted as "Model A" and "Model B") with different architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per resolution trained using the continuous objective in Eq. <ref type="formula" target="#formula_8">(7)</ref>, and Model B is all the same except that it uses 8 layers per resolution. Model definitions are in Appendix H.</p><p>We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in <ref type="figure" target="#fig_6">Fig. 7</ref>. In <ref type="figure">Fig. 8, we</ref> show the dimension-wise differences and correlation coefficients between latent encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model A and Model B provide encodings that are close in every dimension, despite having different model architectures and training runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E REVERSE DIFFUSION SAMPLING</head><p>Given a forward SDE dx " f px, tqdt`Gptqdw, and suppose the following iteration rule is a discretization of it:</p><formula xml:id="formula_48">x i`1 " x i`fi px i q`G i z i , i " 0, 1,¨¨¨, N´1<label>(45)</label></formula><p>where z i " N p0, Iq. Here we assume the discretization schedule of time is fixed beforehand, and thus we can absorb it into the notations of f i and G i .</p><p>Based on Eq. (45), we propose to discretize the reverse-time SDE dx " rf px, tq´GptqGptq T ∇ x log p t pxqsdt`Gptqdw, with a similar functional form, which gives the following iteration rule for i P t0, 1,¨¨¨, N´1u:</p><formula xml:id="formula_49">x i " x i`1´fi`1 px i`1 q`G i`1 G T i`1 s θ˚p x i`1 , i`1q`G i`1 z i`1 ,<label>(46)</label></formula><p>where our trained score-based model s θ˚p x i , iq is conditioned on iteration number i.</p><p>When applying Eq. (46) to Eqs. <ref type="formula" target="#formula_0">(10)</ref> and <ref type="formula" target="#formula_1">(20)</ref>, we obtain a new set of numerical solvers for the reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the "predictor" part of Algorithms 2 and 3. We name these sampling methods (that are based on the discretization strategy in Eq. (46)) reverse diffusion samplers.</p><p>As expected, the ancestral sampling of DDPM <ref type="bibr" target="#b18">(Ho et al., 2020</ref>) (Eq. <ref type="formula" target="#formula_4">(4)</ref>) matches its reverse diffusion counterpart when β i Ñ 0 for all i (which happens when ∆t Ñ 0 since β i "β i ∆t, see Appendix B),   x 1 (T) <ref type="figure">Figure 8</ref>: Left: The dimension-wise difference between encodings obtained by Model A and B. As a baseline, we also report the difference between shuffled representations of these two models. Right:</p><p>The dimension-wise correlation coefficients of encodings obtained by Model A and Model B. because</p><formula xml:id="formula_50">x i " 1 a 1´β i`1 px i`1`βi`1 s θ˚p x i`1 , i`1qq`aβ i`1 z i`1 "ˆ1`1 2 β i`1`o pβ i`1 q˙px i`1`βi`1 s θ˚p x i`1 , i`1qq`aβ i`1 z i`1 «ˆ1`1 2 β i`1˙p x i`1`βi`1 s θ˚p x i`1 , i`1qq`aβ i`1 z i`1 "ˆ1`1 2 β i`1˙xi`1`βi`1 s θ˚p x i`1 , i`1q`1 2 β 2 i`1 s θ˚p x i`1 , i`1q`aβ i`1 z i`1 «ˆ1`1 2 β i`1˙xi`1`βi`1 s θ˚p x i`1 , i`1q`aβ i`1 z i`1 " " 2´ˆ1´1 2 β i`1˙ x i`1`βi`1 s θ˚p x i`1 , i`1q`aβ i`1 z i`1 « " 2´ˆ1´1 2 β i`1˙`o pβ i`1 q  x i`1`βi`1 s θ˚p x i`1 , i`1q`aβ i`1 z i`1</formula><p>"p2´a1´β i`1 qx i`1`βi`1 s θ˚p x i`1 , i`1q`aβ i`1 z i`1 . Therefore, the original ancestral sampler of Eq. (4) is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type="bibr" target="#b18">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP SDE in our continuous framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ANCESTRAL SAMPLING FOR SMLD MODELS</head><p>The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a sequence of noise scales σ 1 ă σ 2 ă¨¨¨ă σ N as in SMLD. By perturbing a data point x 0 with these noise scales sequentially, we obtain a Markov chain</p><formula xml:id="formula_51">x 0 Ñ x 1 Ñ¨¨¨Ñ x N , where ppx i | x i´1 q " N px i ; x i´1 , pσ 2 i´σ 2 i´1 qIq, i " 1, 2,¨¨¨, N.</formula><p>Algorithm 1 Predictor-Corrector (PC) sampling Require: N : Number of discretization steps for the reverse-time SDE M : Number of corrector steps 1: Initialize x N " p T pxq 2: for i " N´1 to 0 do 3:</p><p>x i Ð Predictorpx i`1 q 4:</p><p>for j " 1 to M do 5:</p><formula xml:id="formula_52">x i Ð Correctorpx i q 6: return x 0</formula><p>Here we assume σ 0 " 0 to simplify notations. Following <ref type="bibr" target="#b18">Ho et al. (2020)</ref>, we can compute</p><formula xml:id="formula_53">qpx i´1 | x i , x 0 q " Nˆx i´1 ; σ 2</formula><p>where L t´1 is one representative term in the ELBO objective (see Eq. <ref type="formula" target="#formula_9">(8)</ref> in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>), C is a constant that does not depend on θ, z " N p0, Iq, and x i px 0 , zq " x 0`σi z. We can therefore parameterize µ θ px i , iq via</p><formula xml:id="formula_54">µ θ px i , iq " x i`p σ 2 i´σ 2 i´1 qs θ px i , iq,</formula><p>where s θ px i , iq is to estimate z{σ i . As in <ref type="bibr" target="#b18">Ho et al. (2020)</ref>, we let τ i "</p><formula xml:id="formula_55">c σ 2 i´1 pσ 2 i´σ 2 i´1 q σ 2 i</formula><p>. Through ancestral sampling on ś N i"1 p θ px i´1 | x i q, we obtain the following iteration rule</p><formula xml:id="formula_56">x i´1 " x i`p σ 2 i´σ 2 i´1 qs θ˚p x i , iq`d σ 2 i´1 pσ 2 i´σ 2 i´1 q σ 2 i z i , i " 1, 2,¨¨¨, N,<label>(47)</label></formula><p>where x N " N p0, σ 2 N Iq, θ˚denotes the optimal parameter of s θ , and z i " N p0, Iq. We call Eq. (47) the ancestral sampling method for SMLD models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PREDICTOR-CORRECTOR SAMPLERS</head><p>Predictor-Corrector (PC) sampling The predictor can be any numerical solver for the reversetime SDE with a fixed discretization strategy. The corrector can be any score-based MCMC approach. In PC sampling, we alternate between the predictor and corrector, as described in Algorithm 1. For example, when using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed Langevin dynamics  as the corrector, we have Algorithms 2 and 3 for VE and VP SDEs respectively, where t i u N´1 i"0 are step sizes for Langevin dynamics as specified below.</p><p>The corrector algorithms We take the schedule of annealed Langevin dynamics in , but re-frame it with slight modifications in order to get better interpretability and empirical performance. We provide the corrector algorithms in Algorithms 4 and 5 respectively, where we call r the "signal-to-noise" ratio. We determine the step size using the norm of the Gaussian noise z 2 , norm of the score-based model s θ˚ 2 and the signal-to-noise ratio r. When sampling a large batch of samples together, we replace the norm ¨ 2 with the average norm across the mini-batch. When the batch size is small, we suggest replacing z 2 with ? d, where d is the dimensionality of z.</p><p>Denoising For both SMLD and DDPM models, the generated samples typically contain small noise that is hard to detect by humans. As noted by <ref type="bibr" target="#b21">Jolicoeur-Martineau et al. (2020)</ref>, FIDs can be significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of the reason why NCSN models trained with SMLD has been performing worse than DDPM models in terms of FID, because the former does not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedie's formula <ref type="bibr" target="#b12">(Efron, 2011)</ref>. Samples are the best when computation is split between the predictor and corrector.</p><p>Training We use the same architecture in <ref type="bibr" target="#b18">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we train a model with the original SMLD objective in Eq. (1); similarly for the VP SDE, we use the original DDPM objective in Eq. <ref type="formula" target="#formula_3">(3)</ref>. We apply a total number of 1000 noise scales for training both models. For results in <ref type="figure" target="#fig_7">Fig. 9</ref>, we train an NCSN++ model (definition in Appendix H) on  256ˆ256 LSUN bedroom and church outdoor <ref type="bibr" target="#b48">(Yu et al., 2015)</ref> datasets with the VE SDE and our continuous objective Eq. <ref type="formula" target="#formula_8">(7)</ref>. The batch size is fixed to 128 on CIFAR-10 and 64 on LSUN.</p><p>Ad-hoc interpolation methods for noise scales Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type="bibr" target="#b18">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in ). Specifically, for SMLD models, we keep σ min and σ max fixed and double the number of time steps. For DDPM models, we halve β min and β max before doubling the number of time steps. Suppose ts θ px, iqu N´1 i"0 is a score-based model trained on N time steps, and let ts 1 θ px, iqu 2N´1</p><p>i"0 denote the corresponding interpolated score-based model at 2N time steps. We test two different interpolation strategies for time steps: linear interpolation where s 1 θ px, iq " s θ px, i{2q and rounding interpolation where s 1 θ px, iq " s θ px, ti{2uq. We provide results with linear interpolation in <ref type="table" target="#tab_0">Table 1</ref>, and give results of rounding interpolation in <ref type="table" target="#tab_8">Table 4</ref>. We observe that different interpolation methods result in performance differences but maintain the general trend of predictor-corrector methods performing on par or better than predictor-only or corrector-only samplers.</p><p>Hyper-parameters of the samplers For Predictor-Corrector and corrector-only samplers on CIFAR-10, we search for the best signal-to-noise ratio (r) over a grid that increments at 0.01. We report the best r in <ref type="table" target="#tab_9">Table 5</ref>. For LSUN bedroom/church outdoor, we fix r to 0.075. Unless otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector steps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size is 1024 on CIFAR-10 and 8 on LSUN bedroom/church outdoor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ARCHITECTURE IMPROVEMENTS</head><p>We explored several architecture designs to improve score-based models for both VE and VP SDEs. Our endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art likelihood on uniformly dequantized CIFAR-10, and enables the first high-fidelity image samples of resolution 1024ˆ1024 from score-based generative models. Code and checkpoints are open-sourced at https://github.com/yang-song/score sde.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 SETTINGS FOR ARCHITECTURE EXPLORATION</head><p>Unless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per 50k iterations. For VE SDEs, we consider two datasets: 32ˆ32 CIFAR-10 <ref type="bibr" target="#b29">(Krizhevsky et al., 2009)</ref> and 64ˆ64 CelebA <ref type="bibr" target="#b30">(Liu et al., 2015)</ref>, pre-processed following . We compare different configurations based on their FID scores averaged over checkpoints after 0.5M iterations. For VP SDEs, we only consider the CIFAR-10 dataset to save computation, and compare models based on the average FID scores over checkpoints obtained between 0.25M and 0.5M iterations, because FIDs turn to increase after 0.5M iterations for VP SDEs.</p><p>All FIDs are computed on 50k samples with tensorflow gan. For sampling, we use the PC sampler discretized at 1000 time steps. We choose reverse diffusion (see Appendix E) as the predictor. We use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of 0.16, but save the corrector step for VP SDEs since correctors there only give slightly better results but require double computation. We follow <ref type="bibr" target="#b18">Ho et al. (2020)</ref> for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are trained with the original discrete SMLD and DDPM objectives in Eqs. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_3">(3)</ref> and use a batch size of 128. The optimal architectures found under these settings are subsequently transferred to continuous objectives and deeper models. We also directly transfer the best architecture for VP SDEs to sub-VP SDEs, given the similarity of these two SDEs.</p><p>CIFAR-10 CelebA dataset 2.5 3.0 5. Incorporating progressive growing architectures. We consider two progressive architectures for input: "input skip" and "residual", and two progressive architectures for output: "output skip" and "residual". These progressive architectures are defined and implemented according to  We also tested equalized learning rates, a trick used in very successful models like Progressive-GAN  and StyleGAN <ref type="bibr" target="#b23">(Karras et al., 2019)</ref>. However, we found it harmful at an early stage of our experiments, and therefore decided not to explore more on it.</p><p>The exponential moving average (EMA) rate has a significant impact on performance. For models trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999 for VE and VP models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 RESULTS ON CIFAR-10</head><p>All architecture components introduced above can improve the performance of score-based models trained with VE SDEs, as shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The box plots demonstrate the importance of each component when other components can vary freely. On both CIFAR-10 and CelebA, the additional components that we explored always improve the performance on average for VE SDEs. For progressive growing, it is not clear which combination of configurations consistently performs the best, but the results are typically better than when no progressive growing architecture is used. Our best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip connections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution instead of 2, and 5) uses "residual" for input and no progressive growing architecture for output. We name this model "NCSN++", following the naming convention of previous SMLD models .</p><p>We followed a similar procedure to examine these architecture components for VP SDEs, except that we skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture worked decently well for VP SDEs, ranked 4th place over all 144 possible configurations. The top configuration, however, has a slightly different structure, which uses no FIR upsampling/downsampling and no progressive growing architecture compared to NCSN++. We name this model "DDPM++", following the naming convention of <ref type="bibr" target="#b18">Ho et al. (2020)</ref>.</p><p>The basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10, whereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention used in ;  and <ref type="bibr" target="#b18">Ho et al. (2020)</ref>, we report the lowest FID value over the course of training, rather than the average FID value over checkpoints after 0.5M iterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations (used for comparing VP SDE models) in our architecture exploration.</p><p>Switching from discrete training objectives to continuous ones in Eq. <ref type="formula" target="#formula_8">(7)</ref> further improves the FID values for all SDEs. To condition the NCSN++ model on continuous time variables, we change positional embeddings, the layers in <ref type="bibr" target="#b18">Ho et al. (2020)</ref> for conditioning on discrete time steps, to random Fourier feature embeddings <ref type="bibr" target="#b46">(Tancik et al., 2020)</ref>. The scale parameter of these random Fourier feature embeddings is fixed to 16. We also reduce the number of training iterations to 0.95M to suppress overfitting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++ trained with the VE SDE, resulting in a model called "NCSN++ cont.". In addition, we can further improve the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for NCSN++ cont., resulting in the model denoted as "NCSN++ cont. (deep)". All quantitative results are summarized in <ref type="table" target="#tab_3">Table 3</ref>, and we provide random samples from our best model in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p><p>Similarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model "DDPM++ cont.". When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to 2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance, we used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the ancestral sampling predictor or the reverse diffusion predictor. This is because the discretization strategy of the original DDPM method does not match the variance of the continuous process well when t Ñ 0, which significantly hurts FID scores. As shown in Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 1024ˆ1024 CelebA-HQ , a task that was previously only achievable by some GAN models and VQ-VAE-2 <ref type="bibr" target="#b37">(Razavi et al., 2019)</ref>. We used a batch size of 8, increased the EMA rate to 0.9999, and trained a model similar to NCSN++ with the continuous objective (Eq. <ref type="formula" target="#formula_8">(7)</ref>) for around 2.4M iterations (please find the detailed architecture in our code release.) We use the PC sampler discretized at 2000 steps with the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 16. We use the "input skip" progressive architecture for the input, and "output skip" progressive architecture for the output. We provide samples in <ref type="figure" target="#fig_0">Fig. 12</ref>. Although these samples are not perfect (e.g., there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I CONTROLLABLE GENERATION</head><p>Consider a forward SDE with the following general form dx " f px, tqdt`Gpx, tqdw, and suppose the initial state distribution is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using <ref type="bibr" target="#b1">Anderson (1982)</ref>, the reverse-time SDE is given by dx " tf px, tq´∇¨rGpx, tqGpx, tq T s´Gpx, tqGpx, tq T ∇ x log p t px | yqudt`Gpx, tqdw. <ref type="formula" target="#formula_4">(48)</ref> Since p t pxptq | yq9p t pxptqqppy | xptqq, the score ∇ x log p t pxptq | yq can be computed easily by ∇ x log p t pxptq | yq " ∇ x log p t pxptqq`∇ x log ppy | xptqq.</p><p>This subsumes the conditional reverse-time SDE in Eq. (14) as a special case. All sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 CLASS-CONDITIONAL SAMPLING</head><p>When y represents class labels, we can train a time-dependent classifier p t py | xptqq for classconditional sampling. Since the forward SDE is tractable, we can easily create a pair of training data pxptq, yq by first sampling pxp0q, yq from a dataset and then obtaining xptq " p 0t pxptq | xp0qq. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. <ref type="formula" target="#formula_8">(7)</ref>, to train the time-dependent classifier p t py | xptqq.</p><p>To test this idea, we trained a Wide ResNet <ref type="bibr" target="#b49">(Zagoruyko &amp; Komodakis, 2016)</ref> (Wide-ResNet-28-10) on CIFAR-10 with VE perturbations. The classifier is conditioned on log σ i using random Fourier features <ref type="bibr" target="#b46">(Tancik et al., 2020)</ref>, and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in <ref type="figure" target="#fig_0">Fig. 13</ref>. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in <ref type="table" target="#tab_3">Table 3</ref>, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in <ref type="figure" target="#fig_2">Fig. 4</ref>, and an extended set of conditional samples is given in <ref type="figure" target="#fig_0">Fig. 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 IMPUTATION</head><p>Imputation is a special case of conditional sampling. Denote by Ωpxq andΩpxq the known and unknown dimensions of x respectively, and let fΩp¨, tq and GΩp¨, tq denote f p¨, tq and Gp¨, tq restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient f p¨, tq is element-wise, and the diffusion coefficient Gp¨, tq is diagonal. When f p¨, tq is element-wise, fΩp¨, tq denotes the same element-wise function applied only to the unknown dimensions. When Gp¨, tq is diagonal, GΩp¨, tq denotes the sub-matrix restricted to unknown dimensions.</p><p>For imputation, our goal is to sample from ppΩpxp0qq | Ωpxp0qq " yq. Define a new diffusion process zptq "Ωpxptqq, and note that the SDE for zptq can be written as dz " fΩpz, tqdt`GΩpz, tqdw.</p><p>The reverse-time SDE, conditioned on Ωpxp0qq " y, is given by dz " fΩpz, tq´∇¨rGΩpz, tqGΩpz, tq T ś GΩpz, tqGΩpz, tq T ∇ z log p t pz | Ωpzp0qq " yq ( dt`GΩpz, tqdw.</p><p>Although p t pzptq | Ωpxp0qq " yq is in general intractable, it can be approximated. Let A denote the event Ωpxp0qq " y. We have p t pzptq | Ωpxp0qq " yq " p t pzptq | Aq " ż p t pzptq | Ωpxptqq, Aqp t pΩpxptqq | AqdΩpxptqq " E ptpΩpxptqq|Aq rp t pzptq | Ωpxptqq, Aqs « E ptpΩpxptqq|Aq rp t pzptq | Ωpxptqqqs</p><formula xml:id="formula_58">« p t pzptq |Ωpxptqqq,</formula><p>whereΩpxptqq is a random sample from p t pΩpxptqq | Aq, which is typically a tractable distribution. Therefore, ∇ z log p t pzptq | Ωpxp0qq " yq « ∇ z log p t pzptq |Ωpxptqqq " ∇ z log p t przptq;Ωpxptqqsq, where rzptq;Ωpxptqqs denotes a vector uptq such that Ωpuptqq "Ωpxptqq andΩpuptqq " zptq, and the identity holds because ∇ z log p t przptq;Ωpxptqqsq " ∇ z log p t pzptq |Ωpxptqqq∇ z log ppΩpxptqqq " ∇ z log p t pzptq |Ωpxptqqq. We provided an extended set of inpainting results in Figs. 14 and 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 COLORIZATION</head><p>Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions by using an orthogonal linear transformation to map the gray-scale image to a separate channel in a different space, and then perform imputation to complete the other channels before transforming everything back to the original image space. The orthogonal matrix we used to decouple color channels is 0.577´0.816 0 0.577 0.408 0.707 0.577 0.408´0.707¸.</p><p>Because the transformations are all orthogonal matrices, the standard Wiener process wptq will still be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same imputation method in Appendix I.2. We provide an extended set of colorization results in Figs. 16 and 17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.4 SOLVING GENERAL INVERSE PROBLEMS</head><p>Suppose we have two random variables x and y, and we know the forward process of generating y from x, given by ppy | xq. The inverse problem is to obtain x from y, that is, generating samples from ppx | yq. In principle, we can estimate the prior distribution ppxq and obtain ppx | yq using Bayes' rule: ppx | yq " ppxqppy | xq{ppyq. In practice, however, both estimating the prior and performing Bayesian inference are non-trivial.</p><p>Leveraging Eq. (48), score-based generative models provide one way to solve the inverse problem. Suppose we have a diffusion process txptqu T t"0 generated by perturbing x with an SDE, and a time-dependent score-based model s θ˚p xptq, tq trained to approximate ∇ x log p t pxptqq. Once we have an estimate of ∇ x log p t pxptq | yq, we can simulate the reverse-time SDE in Eq. (48) to sample from p 0 pxp0q | yq " ppx | yq. To obtain this estimate, we first observe that ∇ x log p t pxptq | yq " ∇ x log ż p t pxptq | yptq, yqppyptq | yqdyptq,</p><p>where yptq is defined via xptq and the forward process ppyptq | xptqq. Now assume two conditions:</p><p>• ppyptq | yq is tractable. We can often derive this distribution from the interaction between the forward process and the SDE, like in the case of image imputation and colorization. • p t pxptq | yptq, yq « p t pxptq | yptqq. For small t, yptq is almost the same as y so the approximation holds. For large t, y becomes further away from xptq in the Markov chain, and thus have smaller impact on xptq. Moreover, the approximation error for large t matter less for the final sample, since it is used early in the sampling process.</p><p>Given these two assumptions, we have</p><formula xml:id="formula_59">∇ x log p t pxptq | yq « ∇ x log ż p t pxptq | yptqqppyptq | yqdy « ∇ x log p t pxptq |ŷptqq " ∇ x log p t pxptqq`∇ x log p t pŷptq | xptqq « s θ˚p xptq, tq`∇ x log p t pŷptq | xptqq,<label>(50)</label></formula><p>whereŷptq is a sample from ppyptq | yq. Now we can plug Eq. (50) into Eq. (48) and solve the resulting reverse-time SDE to generate samples from ppx | yq. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Solving a reversetime SDE yields a score-based generative model. Transforming data to a simple noise distribution can be accomplished with a continuous-time SDE. This SDE can be reversed if we know the score of the distribution at each intermediate time step, ∇ x log p t pxq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Probability flow ODE enables fast sampling with adaptive stepsizes as the numerical precision is varied (left), and reduces the number of score function evaluations (NFE) without harming quality (middle). The invertible mapping from latents to images allows for interpolations (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Left: Class-conditional samples on 32ˆ32 CIFAR-10. Top four rows are automobiles and bottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows) results on 256ˆ256 LSUN. First column is the original image, second column is the masked/grayscale image, remaining columns are sampled image completions or colorizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>s 2 I˘(sub-VP SDE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Discrete-time perturbation kernels and our continuous generalizations match each other almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b) compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c) compares the variance of perturbation kernels for DDPM and VP SDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Samples from the probability flow ODE for VP SDE on 256ˆ256 CelebA-HQ. Top: spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of embedding).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10 image. "Model A" and "Model B" are separately trained with different architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total computation, and the horizontal axis represents the amount of computation allocated to the corrector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Class-conditional image generation by solving the conditional reverse-time SDE with PC. The curve shows the accuracy of our noise-conditional classifier over different noise scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 17 :</head><label>17</label><figDesc>Extended colorization results for 256ˆ256 church images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs. "P1000" or "P2000": predictor-only samplers using 1000 or 2000 steps. "C2000": corrector-only samplers using 2000 steps. "PC1000": Predictor-Corrector (PC) samplers using 1000 predictor and 1000 corrector steps.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Variance Exploding SDE (SMLD)</cell><cell cols="4">Variance Preserving SDE (DDPM)</cell></row><row><cell>FIDÓ</cell><cell>Sampler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell></cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell></row><row><cell cols="3">ancestral sampling 4.98˘.06</cell><cell>4.88˘.06</cell><cell></cell><cell cols="3">3.62˘.03 3.24˘.02 3.24˘.02</cell><cell></cell><cell>3.21˘.02</cell></row><row><cell cols="2">reverse diffusion</cell><cell>4.79˘.07</cell><cell>4.74˘.08</cell><cell>20.43˘.07</cell><cell cols="3">3.60˘.02 3.21˘.02 3.19˘.02</cell><cell>19.06˘.06</cell><cell>3.18˘.01</cell></row><row><cell cols="2">probability flow</cell><cell cols="2">15.41˘.15 10.54˘.08</cell><cell></cell><cell cols="3">3.51˘.04 3.59˘.04 3.23˘.03</cell><cell></cell><cell>3.06˘.03</cell></row><row><cell cols="6">4.1 GENERAL-PURPOSE NUMERICAL SDE SOLVERS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Numerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical</cell></row><row><cell cols="10">methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods (Kloe</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>NLLs and FIDs (ODE) on CIFAR-10.</figDesc><table><row><cell>Model</cell><cell cols="2">NLL Test Ó FID Ó</cell></row><row><cell>RealNVP (Dinh et al., 2016)</cell><cell>3.49</cell><cell>-</cell></row><row><cell>iResNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CIFAR-10 sample quality.</figDesc><table><row><cell>Model</cell><cell>FIDÓ</cell><cell>ISÒ</cell></row><row><cell>Conditional</cell><cell></cell><cell></cell></row><row><cell>BigGAN (Brock et al., 2018)</cell><cell>14.73</cell><cell>9.22</cell></row><row><cell cols="2">StyleGAN2-ADA (Karras et al., 2020a) 2.42</cell><cell>10.14</cell></row><row><cell>Unconditional</cell><cell></cell><cell></cell></row><row><cell cols="2">StyleGAN2-ADA (Karras et al., 2020a) 2.92</cell><cell>9.83</cell></row><row><cell>NCSN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Our best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly, we can achieve better FID than the previous best conditional generative model without requiring labeled data. With all improvements together, we also obtain the first set of high-fidelity samples on CelebA-HQ 1024ˆ1024 from score-based models (see Appendix H.3). Our best model for likelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a log-likelihood of 2.99 bits/dim with the continuous objective in Eq. (7). To our best knowledge, this is the highest likelihood on uniformly dequantized CIFAR-10.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparing different samplers on CIFAR-10, where "P2000" uses the rounding interpolation between noise scales. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Variance Exploding SDE (SMLD)</cell><cell cols="4">Variance Preserving SDE (DDPM)</cell></row><row><cell>FIDÓ</cell><cell>Sampler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell></cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell></row><row><cell cols="3">ancestral sampling 4.98˘.06</cell><cell>4.92˘.02</cell><cell></cell><cell cols="3">3.62˘.03 3.24˘.02 3.11˘.03</cell><cell></cell><cell>3.21˘.02</cell></row><row><cell cols="2">reverse diffusion</cell><cell>4.79˘.07</cell><cell>4.72˘.07</cell><cell>20.43˘.07</cell><cell cols="3">3.60˘.02 3.21˘.02 3.10˘.03</cell><cell>19.06˘.06</cell><cell>3.18˘.01</cell></row><row><cell cols="2">probability flow</cell><cell cols="2">15.41˘.15 12.87˘.09</cell><cell></cell><cell cols="3">3.51˘.04 3.59˘.04 3.25˘.04</cell><cell></cell><cell>3.06˘.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="10">: Optimal signal-to-noise ratios of different samplers. "P1000" or "P2000": predictor-only</cell></row><row><cell cols="10">samplers using 1000 or 2000 steps. "C2000": corrector-only samplers using 2000 steps. "PC1000":</cell></row><row><cell cols="6">PC samplers using 1000 predictor and 1000 corrector steps.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">VE SDE (SMLD)</cell><cell></cell><cell></cell><cell cols="2">VP SDE (DDPM)</cell></row><row><cell>r</cell><cell>Sampler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell></cell><cell cols="8">P1000 P2000 C2000 PC1000 P1000 P2000 C2000 PC1000</cell></row><row><cell cols="2">ancestral sampling</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.17</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.01</cell></row><row><cell cols="2">reverse diffusion</cell><cell>-</cell><cell>-</cell><cell>0.22</cell><cell>0.16</cell><cell>-</cell><cell>-</cell><cell>0.27</cell><cell>0.01</cell></row><row><cell cols="2">probability flow</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.17</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2</head><label>2</label><figDesc>, the likelihood values are 3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin withFigure 12: Samples on 1024ˆ1024 CelebA-HQ from a modified NCSN++ model trained with the VE SDE.Published as a conference paper at ICLR 2021 0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a model "DDPM++ cont. (deep)". Its FID score is 2.41, same for both VP and sub-VP SDEs. When trained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values are reported for the last checkpoint during training.</figDesc><table /><note>H.3 HIGH RESOLUTION IMAGES</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and Han Zhang for their insightful discussions during the course of this project. This research was partially supported by NSF (#1651565, #1522054, #1733686), ONR (N000141912145), AFOSR (FA95501910024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple PhD Fellowship in AI/ML.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 2 PC sampling (VE SDE) 1: xN " N p0, σ 2 max Iq 2: for i " N´1 to 0 do 3: x 1 i Ð xi`1`pσ 2 i`1´σ 2 i qs θ˚p xi`1, σi`1q 4: z " N p0, Iq</p><p>z " N p0, Iq 8:</p><p>xi Ð xi` isθ˚pxi, σiq`?2 iz 9: return x0 Algorithm 3 PC sampling (VP SDE)</p><p>Algorithm 4 Corrector algorithm (VE SDE).</p><p>Algorithm 5 Corrector algorithm (VP SDE).</p><p>Require:</p><p>Progressive Arch. (input, output) none, none input_skip, none residual, none none, output_skip input_skip, output_skip residual, output_skip none, residual input_skip, residual residual, residual <ref type="figure">Figure 10</ref>: The effects of different architecture components for score-based models trained with VE perturbations.</p><p>Our architecture is mostly based on <ref type="bibr" target="#b18">Ho et al. (2020)</ref>. We additionally introduce the following components to maximize the potential improvement of score-based models.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Numerical continuation methods: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Allgower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Process. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1982-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to generate samples from noise through infusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06975</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning gradient fields for shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9916" to="9926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3608" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tweedie&apos;s formula and selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">496</biblScope>
			<biblScope unit="page" from="1602" to="1614" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational walkback: Learning a transition operator as a stochastic recurrent net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh Goyal Alias Parth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4392" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representations of knowledge in complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="581" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="450" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
		<title level="m">Rémi Tachet des Combes, and Ioannis Mitliagkas. Adversarial score matching and improved sampling for image generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Numerical solution of stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Interacting particle solutions of fokker-planck equations through gradient-log-density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Maoutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00702</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Permutation invariant graph generation via score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="26" to="28" />
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Øksendal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic differential equations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient learning of generative models via finite-difference score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03317</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Correlation functions and computer simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Physics B</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="384" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00810</idno>
		<title level="m">On linear identifiability of learned representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Applied stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The eigenvalues of mega-dimensional matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Skilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maximum Entropy and Bayesian Methods</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019</title>
		<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019<address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">204</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mintnet: Building invertible neural networks with masked convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11002" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">We follow the same implementation and hyper-parameters in StyleGAN-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2019. 1. Upsampling and downsampling images with anti-aliasing based on Finite Impulse Response (FIR) (Zhang, 2019)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Making convolutional networks shift-invariant again</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rescaling all skip connections by 1 { ? 2. This has been demonstrated effective in several bestin-class GAN models, including ProgressiveGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">StyleGAN (Karras et al., 2019) and StyleGAN</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Replacing the original residual blocks in DDPM with residual blocks from BigGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Increasing the number of residual blocks per resolution from 2 to 4</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
