<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching for A Robust Neural Architecture in Four GPU Hours</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dong@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Searching for A Robust Neural Architecture in Four GPU Hours</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional neural architecture search (NAS) approaches are based on reinforcement learning or evolutionary strategy, which take more than 3000 GPU hours to find a good model on CIFAR-10. We propose an efficient NAS approach learning to search by gradient descent. Our approach represents the search space as a directed acyclic graph (DAG). This DAG contains billions of sub-graphs, each of which indicates a kind of neural architecture. To avoid traversing all the possibilities of the sub-graphs, we develop a differentiable sampler over the DAG. This sampler is learnable and optimized by the validation loss after training the sampled architecture. In this way, our approach can be trained in an end-to-end fashion by gradient descent, named Gradient-based search using Differentiable Architecture Sampler (GDAS). In experiments, we can finish one searching procedure in four GPU hours on CIFAR-10, and the discovered model obtains a test error of 2.82% with only 2.5M parameters, which is on par with the state-of-the-art. Code is publicly available on GitHub: https://github.com/D-X-Y/NAS-Projects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing an efficient and effective neural architecture requires substantial human effort and takes a long time <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>. Since the birth of AlexNet <ref type="bibr" target="#b19">[20]</ref> in 2012, human experts have conducted a huge number of experiments, and consequently devised several useful structures, such as attention <ref type="bibr" target="#b6">[7]</ref> and residual connection <ref type="bibr" target="#b11">[12]</ref>. However, the infinite possible choices of network architecture make the manual search unfeasible <ref type="bibr" target="#b0">[1]</ref>. Recently, neural architecture search (NAS) has increasingly attracted the interest of researchers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46]</ref>. These approaches learn to automatically discover good architectures. They can thus reduce the labour of human experts and find * This paper was accepted to the IEEE CVPR 2019.</p><p>† Part of this work was done when Xuanyi Dong was a research intern with Baidu Research. ‡ Corresponding author: Yi Yang.</p><p>: input GDAS on a DAG : sampled : unsampled better neural architectures than the human-invented architectures. Therefore, NAS is an important research topic in machine learning.</p><p>Most NAS approaches apply evolutionary algorithms (EA) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref> or reinforcement learning (RL) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b2">3]</ref> to design neural architectures automatically. In both RLbased and EA-based approaches, their searching procedures require the validation accuracy of numerous architecture candidates, which is computationally expensive <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b31">32]</ref>. For example, the typical RL-based method utilizes the validation accuracy as a reward to optimize the architecture generator <ref type="bibr" target="#b45">[46]</ref>. An EA-based method leverages the validation accuracy to decide whether a model will be removed from the population or not <ref type="bibr" target="#b32">[33]</ref>. These approaches use a large amount of computational resources, which is inefficient and unaffordable. This motivates researchers to reduce the computational cost.</p><p>In this paper, we propose a Gradient-based searching approach using Differentiable Architecture Sampling (GDAS). It can search for a robust neural architecture in four hours with a single V100 GPU. GDAS significantly improves efficiency compared to the previous methods. We start by searching for a robust neural "cell" instead of a neural network <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. A neural cell contains multiple functions to transform features, and a neural network consists of many copies of the discovered neural cell <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>. <ref type="figure">Fig. 1</ref> illustrates our searching procedure in detail. We represent the search space of a cell by a DAG. Every grey square node indicates a feature tensor, numbered by the computation order. Different colored arrows indicate different kinds of operations, which transform one node into its intermediate features. Meanwhile, each node is the sum of the intermediate features transformed from the previous nodes. During training, the proposed GDAS samples a sub-graph from the whole DAG, indicated by solid connections in <ref type="figure">Fig. 1</ref>. In this sub-graph, each node only receives one intermediate feature from every previous node. Specifically, among the intermediate features between every two nodes, GDAS samples one feature in a differentiable way. In this way, GDAS can be trained by gradient descent to discover a robust neural cell in an end-to-end fashion.</p><p>The fast searching ability of GDAS is mainly due to the sampling behavior. A DAG contains hundreds of parametric operations with millions of parameters. Directly optimizing this DAG <ref type="bibr" target="#b23">[24]</ref> instead of sampling a sub-graph leads to two disadvantages. First, it costs a lot of time to update numerous parameters in one training iteration, increasing the overall training time to more than one day <ref type="bibr" target="#b23">[24]</ref>. Second, optimizing different operations together could make them compete with each other. For example, different operations could generate opposite values. The sum of these opposite values tends to vanish, breaking the information flow between the two connected nodes and destabilizing the optimization procedure. To solve these two problems, the proposed GDAS samples a sub-graph at one training iteration. As a result, we only need to optimize a part of the DAG at one iteration, which accelerates the training procedure. Moreover, the inappropriate competition is avoided, which makes the optimization effective.</p><p>In summary, GDAS has the following benefits: 1. Compared to previous RL-based and EA-based methods, GDAS makes the searching procedure differentiable, which allows us to end-to-end learn a robust searching rule by gradient descent. For RL-based and EA-based methods, feedback (reward) is obtained after a prolonged training trajectory, while feedback (loss) in our gradient-based method is instant and is given in every iteration. As a result, the optimization of GDAS is potentially more efficient.</p><p>2. Instead of using the whole DAG, GDAS samples one sub-graph at one training iteration, accelerating the searching procedure. Besides, the sampling in GDAS is learnable and contributes to finding a better cell. 3. GDAS delivers a strong empirical performances while using fewer GPU resources. On CIFAR-10, GDAS can finish one searching procedure in several GPU hours and discover a robust neural network with a test error of 2.82%. On PTB, GDAS discovers a RNN model with a test perplexity of 57.5. Moreover, the networks discovered on CIFAR and PTB can be successfully transferred to ImageNet and WT2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, researchers have made significant progress in automatically discovering good architectures <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. Most NAS approaches can be categorized in two modalities: macro search and micro search.</p><p>Macro search algorithms aim to directly discover the entire neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b20">21]</ref>. To search convolutional neural networks (CNNs) <ref type="bibr" target="#b19">[20]</ref>, typical approaches apply RL to optimize the searching policy to discover architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b30">31]</ref>. Baker et al. <ref type="bibr" target="#b0">[1]</ref> trained a learning agent by Q-learning to sequentially choose CNN layers. Zoph and Le <ref type="bibr" target="#b45">[46]</ref> utilized long short-term memory (LSTM) <ref type="bibr" target="#b12">[13]</ref> as a controller to configure each convolutional layer, such as the filter shape and the number of filters. In these macro search algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46]</ref>, the number of possible networks is exponential to the depth of a network, e.g., a depth of 12 can result in more than 10 29 possible networks <ref type="bibr" target="#b30">[31]</ref>. It is difficult and ineffective to search networks in such a large search space, and, therefore, these macro search methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5]</ref> usually limit the CNN models to be shallow, e.g., a depth is less than 12. Since macro-discovered networks are shallower than deep CNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, their accuracies are limited. In contrast, our GDAS allows the network to be much deeper by stacking tens of discovered cells <ref type="bibr" target="#b46">[47]</ref> and thus can achieve a better accuracy.</p><p>Micro search algorithms aim to discover neural cells and design a neural architecture by stacking many copies of the discovered cells <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>. A typical micro search approach is NASNet <ref type="bibr" target="#b46">[47]</ref>, which extends the approach of <ref type="bibr" target="#b45">[46]</ref> to search neural cells in the proposed "NASNet search space". Following NASNet <ref type="bibr" target="#b46">[47]</ref>, many researchers propose their methods based on the NASNet search space <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>. For example, Real et al. <ref type="bibr" target="#b31">[32]</ref> applied EA algorithm with a simple regularization technique to search neural cells. Liu et al. <ref type="bibr" target="#b21">[22]</ref> proposed a progressive approach to search cells from shallow to deep gradually. These micro search algorithms usually take more than 100 GPU days <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>. Even though some of them reduce the searching cost, they still take more than one GPU day <ref type="bibr" target="#b23">[24]</ref>. Our GDAS is a also micro search algorithm, focusing on search cost reduction. In experiments, we can find a robust network within fewer GPU hours, which is 1000× less than the standard NAS approach <ref type="bibr" target="#b46">[47]</ref>.</p><p>Improving Efficiency. Since NAS algorithms usually require expensive computational resources <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32]</ref>, an increasing number of researchers focus on improving the architecture search speed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24]</ref>. A variety of techniques have been proposed, such as progressivecomplexity search stages <ref type="bibr" target="#b21">[22]</ref>, accuracy prediction <ref type="bibr" target="#b1">[2]</ref>, Hy-perNet <ref type="bibr" target="#b3">[4]</ref>, Net2Net transformation <ref type="bibr" target="#b4">[5]</ref>, and parameter sharing <ref type="bibr" target="#b30">[31]</ref>. For instance, Cai et al. <ref type="bibr" target="#b4">[5]</ref> reused weights of previously discovered networks to amortize the training cost. Pham et al. <ref type="bibr" target="#b30">[31]</ref> shared parameters between different child networks to improve the efficiency of the searching procedure. Brock et al. <ref type="bibr" target="#b3">[4]</ref> utilized a network to generate model parameters given a discovered network, avoiding fully training from scratch. Liu et al. <ref type="bibr" target="#b23">[24]</ref> relaxed the search space to be continuous, so that they can use gradient descent to effectively search cells. Though these approaches successfully accelerate the architecture search procedure, several GPU days are still required <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref>. Our GDAS samples individual architecture in a differentiable way to effectively discover architecture. As a result, GDAS can finish the search procedure in several GPU hours on CIFAR-10, which is much faster than these efficient methods.</p><p>Contemporary to this work, Xie et al. <ref type="bibr" target="#b38">[39]</ref> applied a similar technique to relax the discrete candidate sampling as ours. They focus on fixing the inconsistency between the loss of attention-based NAS <ref type="bibr" target="#b23">[24]</ref> and their objective. In contrast, we focus on making the sampling procedure differentiable and accelerating the searching procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space as a DAG</head><p>We search for the neural cell in the search space and stack this cell in series to compose the whole neural network. For CNN, a cell is a fully convolutional network that takes output tensors of previous cells as inputs and generates another feature tensor. For recurrent neural network (RNN), a cell takes the feature vector of the current step and the hidden state of the previous step as inputs, and generates the current hidden state. For simplification, we take CNN as an example for the following description.</p><p>We represent the cell in CNN as a DAG G consisting of an ordered sequence of B computational nodes. Each computational node represents one feature tensor, which is transformed from two previous feature tensors. This procedure can be formulated as shown in Eq. (1) following <ref type="bibr" target="#b46">[47]</ref>.</p><formula xml:id="formula_0">I i = f i,j (I j ) + f i,k (I k ) s.t. j &lt; i &amp; k &lt; i, (1)</formula><p>where I i , I j , and I k indicate the i-th, j-th, and k-th nodes, respectively. f i,j and f i,k indicate two functions from the candidate function set F. We denote the computational  <ref type="figure">Figure 2</ref>. The strategy to design CIFAR architecture (top) and ImageNet architecture (bottom) based on the discovered cell. We use the same block structure (middle) in two cases. Both normal and reduction cells receive the outputs of two previous cells as inputs, as illustrated in the middle.</p><p>nodes of a cell as B. Taking B = 4 as an example, a cell contains 7 nodes in total, i.e., {I i |1 ≤ i ≤ 7}. I 1 and I 2 nodes are the cell outputs in the previous two layers. I 3 , I 4 , I 5 , and I 6 nodes are the computational nodes calculated by Eq. (1). I 7 indicates the output tensor of this cell, which is the concatenation of the four computational nodes, i.e., I 7 = I 3 I 4 I 5 I 6 . In GDAS, the candidate function set F contains the following 8 functions: (1) identity, (2) zeroize, (3) 3x3 depth-wise separate conv, (4) 3x3 dilated depth-wise separate conv, (5) 5x5 depth-wise separate conv, (6) 5x5 dilated depth-wise separate conv, (7) 3x3 average pooling, (8) 3x3 max pooling. We use the same candidate function set F as <ref type="bibr" target="#b23">[24]</ref>, which is similar to <ref type="bibr" target="#b46">[47]</ref> but removes some unused functions and adds some useful functions.</p><p>From cell to network. We search for two kinds of cells, i.e., a normal cell and a reduction cell. For the normal cell, each function in F has the stride of 1. For the reduction cell, each function in F has the stride of 2. Once we discover one normal cell and one reduction cell, we stack many copies of these discovered cells to make up a neural network. As shown in <ref type="figure">Fig. 2</ref>, for the CIFAR architecture, we stack N normal cells as one block. Given an image, it first forwards through the network head part, i.e., one 3 by 3 convolutional layer. It then forwards through three blocks with two reduction cells in between. The ImageNet architecture is similar to the CIFAR architecture, but the network head part consists of three 3 by 3 convolutional layers. We follow <ref type="bibr" target="#b23">[24]</ref> to setup these two overall structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Searching by Differentiable Model Sampling</head><p>Formally, we denote a neural architecture as α and the weights of this neural architecture as ω α . The goal of NAS is to find an architecture α, which can achieve the minimum validation loss after being trained by minimizeing the train-ing loss, as shown in Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_1">min α E (x ,y )∼D V − log Pr(y |x ; α, ω * α ), s.t. ω * α = arg min ω E (x,y)∼D T − log Pr(y|x; α, ω α ), (2)</formula><p>where ω * α is the best weight of α and achieves the minimum training loss. We use the negative log likelihood as the training objective, i.e., − log Pr. D T and D V indicate the training set and the validation set, respectively. (x, y) and (x , y ) are the data associated with its label, which are sampled from D T and D V , respectively.</p><p>An architecture α consists of many copies of the neural cell. This cell is sampled from the search space represented by G. Specifically, between node i and node j , we sample one transformation function from F from a discrete probability distribution T i,j . During the search, we calculate each node in a cell as:</p><formula xml:id="formula_2">I i = i−1 j=1 f i,j (I j ; W fi,j ) s.t. f i,j ∼ T i,j ,<label>(3)</label></formula><p>where f i,j is sampled from T i,j and W fi,j is its associated weight. The discrete probability distribution T i,j is characterized by a learnable probability mass function as in Eq. <ref type="formula" target="#formula_3">(4)</ref>:</p><formula xml:id="formula_3">Pr(f i,j = F k ) = exp(A k i,j ) K k =1 exp(A k i,j ) ,<label>(4)</label></formula><p>where A k i,j is the k-th element of a K-dimensional learnable vector A i,j ∈ R K , and F k indicates the k-th function in F. K is the cardinality of F, i.e., K = |F|. Actually, A i,j encodes the sampling distribution of the function between node i and node j . As a result, the sampling distribution of a neural cell is encoded by all A i,j , i.e., A = {A i,j }.</p><p>Given Eq. (3) and Eq. (4), we can obtain α and ω, and thus can calculate Pr(y|x; α, ω) in Eq. (2). However, since Eq. (3) needs to sample from a discrete probability distribution, we cannot back-propagate gradients through A i,j in Eq. (4) to optimize A i,j . To allow back-propagation, we first use the Gumbel-Max trick <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref> to re-formulate Eq. (3) as Eq. <ref type="formula" target="#formula_4">(5)</ref>, which provides an efficient way to draw samples from a discrete probability distribution.</p><formula xml:id="formula_4">I i = i−1 j=1 K k=1 h k i,j F k (I j ; W k i,j ),<label>(5)</label></formula><formula xml:id="formula_5">s.t. h i,j = one hot(arg max k (A k i,j + o k )),<label>(6)</label></formula><p>where o k are i.i.d samples drawn from Gumbel (0,1) 1 , and h k i,j is the k-th element of h i,j . W k i,j is the weight of F k for the transformation function between node i and node j . Then, we use the softmax function to relax the arg max function so as to make Eq. (5) being differentiable <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>. Formally, we use Eq. <ref type="formula" target="#formula_7">(7)</ref> to approximate Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_6">1 o i = − log(− log(u)) with u ∼ Unif [0,</formula><formula xml:id="formula_7">h k i,j = exp((log(Pr(f i,j = F k )) + o k )/τ ) K k =1 exp((log(Pr(f i,j = F k )) + o k )/τ ) ,<label>(7)</label></formula><p>where τ is the softmax temperature. When τ → 0,h k i,j = h k i,j . When τ → ∞, each element inh will be the same and the approximated distribution will be smooth. To be noticed, we use the arg max function in Eq. (5) during the forward pass but the soft max function in Eq. <ref type="formula" target="#formula_7">(7)</ref> during the backward pass to allow gradient back-propagation.</p><p>Training. Reviewing the objective of NAS in Eq. (2), the main challenge is learning to find architecture α. By utilizing Eq. <ref type="formula" target="#formula_7">(7)</ref>, we can make the sampling procedure differentiable and learn a distribution of neural cells (representing architectures). However, it is still intractable to directly solve Eq. (2), because the nested formulation in Eq. (2) needs to calculate high order derivatives. In practice, to avoid calculating high order derivatives, we apply the alternative optimization strategy to update the sampling distribution T A and the weights of all functions W in an iterative way. Given one data sample x and its associated label y, we calculate the loss as:</p><formula xml:id="formula_8">(x, y) = − log Pr(y|x; α, ω α ),<label>(8)</label></formula><formula xml:id="formula_9">s.t. α ∼ T A &amp; ω α ⊂ W,<label>(9)</label></formula><p>where T A is the distribution encoded by A and W = {W k i,j } represents the weights of all functions in all cells of the network. Note that, for one data sample, it first samples α from T A and then calculates the network output only on its associated weight ω α , which is a part of W. As shown in Alg. 1, we apply the alternative optimization strategy (AOS) to update A based on the validation losses from D V and update W based on the training losses from D T . It is essential to train W on D T and A on D V , because (1) this strategy is theoretically sound with the objective Eq. (2); and (2) this strategy can improve the generalization ability of the searched structure. Architecture. After training, we need to derive the final architecture from the learned A. Each node i connects with T previous nodes. Following the previous works, we use T = 2 for CNN <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b23">24]</ref> and T = 1 for RNN <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Suppose Ω is the candidate index set, we derive the final architecture by the following procedure: (1) define the importance of the connection between node i and node j as: max k∈Ω Pr(f i,j = F k ). (2) for each node i , retain T connections with the maximum importance from the previous nodes. (3) for the retained connection between node i and node j , we use the function F arg max k∈Ω Pr(fi,j =F k ) . Ω is {1, ..., K} by default.</p><p>Acceleration. In Eq. (5), h i,j is a one-hot vector. As a result, in the forward procedure, we only need to calculate the function F arg max(hi,j ) . During the backward procedure, we only back-propagate the gradient generated at the arg max(h i,j ). In this way, we can save most computation time and also reduce the GPU memory cost by about |F| times. Within one training batch, a different cell will produces a different h i,j , which was shared for each training examples.</p><p>One benefit of this acceleration trick is that it allows us to directly search on the large-scale dataset (e.g., Ima-geNet) due to the saved GPU memory. We did some experiments to directly search on ImageNet using the same hyper-parameters as on the small datasets, however, failed to obtain a good performance. Searching on a large-scale dataset might require different hyper-parameters and needs careful tuning. We will explore this in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion on the Reduction Cell</head><p>Revisiting state-of-the-art architectures designed by human experts, AlexNet <ref type="bibr" target="#b19">[20]</ref> and VGGNet <ref type="bibr" target="#b35">[36]</ref> use the max pooling to reduce the spatial dimension; ResNet <ref type="bibr" target="#b11">[12]</ref> uses a convolutional layer with stride of 2; and DenseNet <ref type="bibr" target="#b13">[14]</ref> uses a 1 by 1 convolutional layer followed by average pooling to reduce dimension. These human-designed reduction cells are simple and effective. The automatically discovered reduction cells are also usually similar and simple <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24]</ref>. For example, the reduction cell discovered by <ref type="bibr" target="#b23">[24]</ref> only has max pooling and identity operations.</p><p>Most human-designed and automatically discovered reduction cells are simple and can achieve a high accuracy.</p><p>Moreover, compared to searching one normal cell, jointly searching a normal cell and a reduction cell will greatly increase the search space and make the optimization difficult. We hope to find a better network by fixing the reduction cell. Inspired by <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref>, we design a fixed reduction cell as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. In the experiments, with this humandesigned reduction cell, GDAS finds a better architecture, yielding fewer parameters and higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b18">[19]</ref> consist of 50K training images and 10K test images. CIFAR-10 categorizes images into 10 classes, while CIFAR-100 has 100 classes.</p><p>ImageNet <ref type="bibr" target="#b33">[34]</ref> is a large-scale and well-known benchmark for image classification. It contains 1K classes, 1.28 million images for training, and 50K images for validation.</p><p>Penn Treebank (PTB) <ref type="bibr" target="#b27">[28]</ref> is a corpus consisting of over 4.5 million words of American English words. We preprocess PTB following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>WikiText-2 (WT2) <ref type="bibr" target="#b29">[30]</ref> is a collection of 2 million tokens from the set of verified Good and Featured articles on Wikipedia. The training set contains 600 articles with 2,088,628 tokens. The validation set contains 60 articles with 217,646 tokens. The test set contains 60 articles with 245,569 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Search for CNN</head><p>CNN Searching Setup. The neural cells for CNN are searched on CIFAR-10 following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. We randomly split the official training images into two groups, with each group containing 25K images. One group is used as the training set D T in Alg. 1, and the other is used as the validation set D V in Alg. 1. The candidate function set F has 8 different functions as introduced in Sec. 3.1. The default hyper-parameters for each function in F are the same in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b21">22]</ref>. By default, we set the number of initial channels in the first convolution layer C as 16; set the number of computational nodes in a cell B as 4; and the number of layers in one block N as 2. We train the model by 240 epochs in total. For ω, we use the SGD optimization. We start with a learning rate of 0.025 and anneal it down to 1e-3 following a cosine schedule. We use the momentum of 0.9 and the weight decay of 3e-4. For α, we use the Adam optimization <ref type="bibr" target="#b17">[18]</ref> with the learning rate of 3e-4 and the weight decay of 1e-3. The τ is initialized as 10 and is linearly reduced to 0.1. To search the normal cell and the reduction cell on CIFAR-10, our GDAS takes about five hours to finish the search procedure on a single NVIDIA Tesla V100 GPU. As discussed in Sec. 3.3, we also run experi-  <ref type="table">Table 1</ref>. Classification errors of GDAS and baselines on CIFAR. † indicates the results trained using our setup. "FRC" indicates that we fix the reduction cell and only search the normal cell. Note that researchers might run their algorithms on different kinds of machines. The searching costs are derived from the original papers, and we did not normalize them across different GPUs. Our experiments are based on the V100 GPU; and if we run on Titan 1080Ti, the searching cost will increase to about seven GPU hours. ments to only search the normal cell and fix the reduction cell as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, denoted as GDAS (FRC). When we use GDAS (FRC) to search on CIFAR-10, it takes less than four hours to finish one search procedure. Following <ref type="bibr" target="#b23">[24]</ref>, we run GDAS 4 times with different random seeds and pick the best cell based on its validation performance. This procedure can reduce the high variance of the searched results, especially when searching the RNN structure. Clarifications on the searching cost (GPU days) of different methods. The searching costs listed in Tab. 1 and Tab. 2 are not normalized across different GPU devices. Different algorithms might run on different machines, and we simply refer the searching costs reported in their papers. <ref type="bibr" target="#b1">2</ref> If we use other GPU devices, the searching cost of "GDAS (FRC)" could be a different number. For example, if we use Titan 1080Ti, the search cost will increase to about seven GPU hours.</p><p>Discussion on the acceleration step. If we do not apply the acceleration step introduced in Sec. 3.2, each iteration will cost |F|=8× more time and GPU memory than GDAS. In the same time, without this acceleration step, it requires 2 It is difficult for us to run all algorithms on the same GPU. less training epochs to converge but still costs more time than applying the acceleration step.</p><p>Results on CIFAR. After the searching procedure, we use C=36, B=4, and N=6 to form a CNN. Following the previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref>, we train the network by 600 epochs in total. We start the learning rate of 0.025 and reduce it to 0 with the cosine learning rate scheduler. We set the probability of path dropout as 0.2 and the auxiliary tower with the weight of 0.4 <ref type="bibr" target="#b45">[46]</ref>. We use the standard preprocessing and data augmentation, i.e., randomly cropping, horizontally flipping, normalization, and CutOut <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>We compare the models discovered by our approach with other state-of-the-art models in Tab. 1. The models discovered by the macro search algorithms obtain a higher error than the models discovered by the micro search algorithms. Using GDAS, we discover a model with 3.3M parameters, which achieves 2.93% error on CIFAR-10. Using GDAS (FRC), we discover a model with only 2.5M parameters, which achieves 2.82% error on CIFAR-10. NASNet-A achieves a lower error rate than ours, but it contains more than 80% of the parameters than the model discovered by GDAS (FRC). Notably, our GDAS discovers a comparable  <ref type="table">Table 3</ref>. Comparison w.r.t. the perplexity of different language models on PTB (lower perplexity is better). V-RHN indicates Variational RHN <ref type="bibr" target="#b44">[45]</ref>. LSTM + SC indicates LSTM with skip connection <ref type="bibr" target="#b28">[29]</ref>. LSTM + SE indicates LSTM with 15 softmax experts <ref type="bibr" target="#b39">[40]</ref>. The first four models are designed by human experts, and the last four models are automatically searched by machine.</p><p>model with the state-of-the-art, whereas the searching cost of our approach is much less than the others. For example, GDAS (FRC) takes less than 4 hours on a single V100 GPU, which is about 0.17 GPU days. It is faster than NASNet by almost 10 4 times. ENAS is a recent work that focuses on accelerating the searching procedure. ENAS is very efficient, whereas our GDAS (FRC) is three times faster than ENAS.</p><p>Results on ImageNet. Following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>, we use the ImageNet-mobile setting, in which the input size is 224×224 and the number of multiply-add operations is restricted to be less than 600M. We train models by SGD with 250 epochs and use the batch size of 128. We initialize the learning rate of 0.1 and reduce it by 0.97 after each epoch.</p><p>We  <ref type="table">Table 4</ref>. Comparison with different language models on WT2 (lower perplexity is better). LSTM + AL indicates LSTM with augmented loss <ref type="bibr" target="#b14">[15]</ref>. Other notation is the same as in Tab. 3. methods in Tab. 2. Most algorithms in Tab. 2 take more than 1000 GPU days to discover a good CNN cell. DARTS <ref type="bibr" target="#b23">[24]</ref> uses minimum resources among the compared algorithms, whereas ours is even faster than DARTS <ref type="bibr" target="#b23">[24]</ref> by more than 10 times. For GDAS (FRC), we use C=52 and N=4 to construct the model following the setting in <ref type="bibr" target="#b23">[24]</ref>. For GDAS, if we use C=52 and N=4, the number of multiply-add operations will be larger than 600 MB, and thus we use C=50 to restrict it to be less than 600MB. Our model, GDAS (FRC) [C=52,N=4], costs about 20% less multiply-add operations than <ref type="bibr" target="#b23">[24]</ref> but obtains the same top-5 error. AmoebaNet-A and Progressive NAS achieve a slightly lower test error than ours. However, their methods cost a prohibitive amount of GPU resources. The results in Tab. 2 show the discovered cell on CIFAR-10 can be successfully transferred to Ima-geNet and achieve competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Params Error on CIFAR-10 GDAS-N + GDAS-R 3.3 2.93 GDAS-N + FIX-R 3.0 2.87 FRC-N + GDAS-R 2.9 2.84 FRC-N + FIX-R 2.5 2.82 <ref type="table">Table 5</ref>. Different normal and reduction cell combinations. GDAS-N and GDAS-R indicate the normal and reduction cells discovered by GDAS, respectively. FRC-N and FIX-R mean the normal cell from GDAS (FRC) and our designed reduction cell, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Search for RNN</head><p>RNN Searching Setup. The neural cells for RNN are searched on PTB with the splits following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref> The candidate function set F contain 5 functions, i.e., zeroize, Tanh, ReLU, sigmoid, and identity. We use B=9 to search the RNN cell. The RNN model consists of one word embedding layer with a hidden size of 300, one RNN cell with a hidden size of 300, and one decoder layer. We train the model by 200 epochs with a batch size of 128 and a BPTT length of 35. We optimize ω by Adam with a learning rate of 20 and a weight decay of 5e-7. We optimize α by Adam with a learning rate of 3e-3 and a weight decay of 1e-3. Other setups are the same in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Results on PTB. We evaluate the RNN model formed by the discovered recurrent cell on PTB. We use a batch size of 64 and a hidden size of 850. We train the model using the A-SGD by 2000 epochs. The learning rate is fixed as 20 and the weight decay is 8e-7. DARTS <ref type="bibr" target="#b23">[24]</ref> and ENAS <ref type="bibr" target="#b30">[31]</ref> greatly reduce the search cost compared to previous methods. Our GDAS incurs a lower search cost than all the previous methods. Note that our code is not heavily optimized and the theoretical search cost should be less than the one reported in Tab. 3.</p><p>We compare different RNN models in Tab. 3. The model discovered by GDAS achieves a validation perplexity of 59.8 and a test perplexity of 57.5. The performance of our discovered RNN is on par with the state-of-the-art models in Tab. 3. LSTM + SE <ref type="bibr" target="#b39">[40]</ref> obtains better results than ours, but it is an ensemble method using mixture of softmax. By applying the SE technique <ref type="bibr" target="#b39">[40]</ref>, GDAS can achieve the lower perplexity without doubt. LSTM <ref type="bibr" target="#b28">[29]</ref> is an extensively tuned model, whereas our automatically discovered model is superior to it. Compared to other efficient approaches, the search cost of GDAS is the lowest.</p><p>Results on WT2. To train the model on WT2, we use the same experiment settings as PTB, but we use a hidden size of 700 and a weight decay of 5e-7. We train the model in 3000 epochs in total. Tab. 4 compares different RNN models on WT2. Our approach achieves competitive results among all automatically searching approaches. GDAS is worse than "LSTM + SC" <ref type="bibr" target="#b28">[29]</ref>. Since our model is searched on a small dataset PTB, and the transferable ability of the discovered model might be a little bit weak. If we directly search the RNN model on WT2, we could obtain a better model and improve the transferable ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>We visualize the discovered cells in <ref type="figure" target="#fig_2">Fig. 4</ref>. These automatically discovered cells are complex and hard to be designed manually. Moreover, networks with these discovered cells can achieve more superior performance than handcrafted networks. This demonstrates that automated neural architecture search is the future of architecture design.</p><p>Revisiting Sec. 3.3, we propose a new reduction cell as a replacement for automated reduction cell. With this reduction cell, we can more effectively search neural cells. For further analysis, we use the normal cell found by GDAS and the proposed reduction cell to construct a new CNN, denoted as "GDAS-N + FIX-R" in Tab. 5. The accuracy of this network on CIFAR-10 is similar to "GDAS-N + GDAS-R" and "FRC-N + FIX-R" in Tab. 5. This result implies that the reduction cell might have a negligible effect on the performance of networks and the hand-crafted reduction cell could be on par with the automatically discovered one.</p><p>Most recent NAS approaches search neural networks on the small-scale datasets, such as CIFAR, and then transfer the discovered networks to the large-scale datasets, such as ImageNet. The obstacle of directly searching on ImageNet is the huge computational cost. GDAS is an efficient NAS algorithm and gives us an opportunity to search on Ima-geNet. We will explore this research direction in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a Gradient-based neural architecture search approach using Differentiable Architecture Sampler (GDAS). Our approach is efficient and reduces the search cost of the standard NAS approach <ref type="bibr" target="#b46">[47]</ref> by about 10 4 times. Moreover, both CNN and RNN models discovered by our GDAS can achieve competitive performance compared to state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Figure 1 .</head><label>31</label><figDesc>We utilize a DAG to represent the search space of a neural cell. Different operations (colored arrows) transform one node (square) to its intermediate features (little circles). Meanwhile, each node is the sum of the intermediate features transformed from the previous nodes. As indicated by the solid connections, the neural cell in the proposed GDAS is a sampled sub-graph of this DAG. Specifically, among the intermediate features between every two nodes, GDAS samples one feature in a differentiable way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The designed reduction cell. "1x3 conv 1x2 stride" indicates a convolutional layer with 1 by 3 kernel and 1 by 2 stride.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The top block and the middle-left block show the normal cell and the reduction cell discovered by GDAS, respectively.The middle-right block shows the discovered normal cell when we fix the reduction cell. The bottom block shows the discovered recurrent cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 The GDAS algorithm Input: split the training set into two disjoint sets: DT and DV ; randomly initialized A and W, and the batch size n while not converge do searching an architecture Sample batch of data Dt = {(xi, yi)} n i=1 from DT Calculate LT = n i=1 (xi, yi) based on Eq. (8) Update W by gradient descent: W = W − W LT Sample batch of data Dv = {(xi, yi)} n i=1 from DV Calculate LV = n i=1 (xi, yi) based on Eq. (8) Update A by gradient descent: A = A − ALV end while Derive the final architecture from A Optimize the architecture on the training set</figDesc><table><row><cell>1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Top-1 and top-5 errors of GDAS and baselines on ImageNet. +× indicates the number of multiply-add operations. We refer results reported in<ref type="bibr" target="#b23">[24]</ref> for Progressive NAS, NASNet, and AmoebaNet.</figDesc><table><row><cell>Type</cell><cell></cell><cell>Method</cell><cell></cell><cell cols="2">Venue GPUs</cell><cell cols="4">Times Test Error (%) Params (days) Top-1 Top-5 (million) (million) +×</cell></row><row><cell></cell><cell></cell><cell cols="2">Inception-v1 [37]</cell><cell>CVPR15</cell><cell>−</cell><cell>−</cell><cell>30.2</cell><cell>10.1</cell><cell>6.6</cell><cell>1448</cell></row><row><cell>Human expert</cell><cell></cell><cell cols="2">MobileNet-V2 [35]</cell><cell>CVPR18</cell><cell>−</cell><cell>−</cell><cell>28.0</cell><cell>−</cell><cell>3.4</cell><cell>300</cell></row><row><cell></cell><cell></cell><cell cols="2">ShuffleNet [42]</cell><cell>CVPR18</cell><cell>−</cell><cell>−</cell><cell>26.3</cell><cell>−</cell><cell>∼5</cell><cell>524</cell></row><row><cell></cell><cell></cell><cell cols="2">Progressive NAS [22]</cell><cell cols="2">ECCV18 100</cell><cell>1.5</cell><cell>25.8</cell><cell>8.1</cell><cell>5.1</cell><cell>588</cell></row><row><cell></cell><cell></cell><cell cols="2">NASNet-A [47]</cell><cell cols="2">CVPR18 450</cell><cell>3-4</cell><cell>26.0</cell><cell>8.4</cell><cell>5.3</cell><cell>564</cell></row><row><cell></cell><cell></cell><cell cols="2">NASNet-B [47]</cell><cell cols="2">CVPR18 450</cell><cell>3-4</cell><cell>27.2</cell><cell>8.7</cell><cell>5.3</cell><cell>488</cell></row><row><cell></cell><cell></cell><cell cols="2">NASNet-C [47]</cell><cell cols="2">CVPR18 450</cell><cell>3-4</cell><cell>27.5</cell><cell>9.0</cell><cell>4.9</cell><cell>558</cell></row><row><cell></cell><cell></cell><cell cols="2">DARTS (2nd) [24]</cell><cell>ICLR19</cell><cell>1</cell><cell>1</cell><cell>26.9</cell><cell>9.0</cell><cell>4.9</cell><cell>595</cell></row><row><cell cols="2">Micro search space</cell><cell cols="2">GHN [41] AmoebaNet-A [32]</cell><cell>ICLR19 AAAI19</cell><cell>1 450</cell><cell>0.84 7</cell><cell>27.0 25.5</cell><cell>8.7 8.0</cell><cell>6.1 5.1</cell><cell>569 555</cell></row><row><cell></cell><cell></cell><cell cols="2">AmoebaNet-B [32]</cell><cell>AAAI19</cell><cell>450</cell><cell>7</cell><cell>26.0</cell><cell>8.5</cell><cell>5.3</cell><cell>555</cell></row><row><cell></cell><cell></cell><cell cols="2">AmoebaNet-C [32]</cell><cell>AAAI19</cell><cell>450</cell><cell>7</cell><cell>24.3</cell><cell>7.6</cell><cell>6.4</cell><cell>570</cell></row><row><cell></cell><cell></cell><cell cols="2">GDAS [C=50,N=4]</cell><cell>CVPR19</cell><cell>1</cell><cell>0.21</cell><cell>26.0</cell><cell>8.5</cell><cell>5.3</cell><cell>581</cell></row><row><cell></cell><cell></cell><cell cols="3">GDAS (FRC) [C=52,N=4] CVPR19</cell><cell>1</cell><cell>0.17</cell><cell>27.5</cell><cell>9.1</cell><cell>4.4</cell><cell>497</cell></row><row><cell>Architecture</cell><cell></cell><cell cols="3">Perplexity Params Search Cost val test (M) (GPU days)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>V-RHN [45]</cell><cell cols="2">67.9 65.4</cell><cell>23</cell><cell>−</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM [29]</cell><cell cols="2">60.7 58.8</cell><cell>24</cell><cell>−</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">LSTM + SC [29] 60.9 58.3</cell><cell>24</cell><cell>−</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">LSTM + SE [40] 58.1 56.0</cell><cell>22</cell><cell>−</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAS [46]</cell><cell></cell><cell>− 64.0</cell><cell>25</cell><cell>10 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ENAS [31]</cell><cell cols="2">60.8 58.6</cell><cell>24</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DARTS (1st) [24] 60.2 57.6</cell><cell>23</cell><cell>0.13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DARTS (2nd) [24] 58.1 55.7</cell><cell>23</cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GDAS</cell><cell cols="2">59.8 57.5</cell><cell>23</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural optimizer search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2787" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8713" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">More is less: A more complicated network with less inference complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervision-by-Registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><forename type="middle">Julius</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NBS Applied Mathematics Series</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="1954" />
			<publisher>AMS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Partial order pruning: for best speed/accuracy trade-off in neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9145" to="9153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3086" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning time/memoryefficient deep architectures with budgeted super networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Vniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3492" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">SNAS: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A highrank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2653" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnık</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4189" to="4198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
