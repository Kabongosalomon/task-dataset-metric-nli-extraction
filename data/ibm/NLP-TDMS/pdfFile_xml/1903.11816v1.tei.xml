<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
							<email>huikai.wu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
							<email>jgzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
							<email>kaiqi.huang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Yizhou</roleName><forename type="first">Kongming</forename><surname>Liang</surname></persName>
							<email>liangkongming@deepwise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deepwise</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b3">4]</ref> is one of the fundamental tasks in computer vision, with the goal of assigning a semantic label to each pixel of an image. Modern approaches usually employ a Fully Convolution Network (FCN) <ref type="bibr" target="#b21">[22]</ref> to address this task, achieving tremendous success among several segmentation benchmarks.</p><p>The original FCN is proposed by Long et al. <ref type="bibr" target="#b21">[22]</ref>, which is transformed from a Convolutional Neural Network (CNN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> designed for image classification. Inheriting from the design for image classification, the original FCN downsamples the input image progressively by stride convolutions and/or spatial pooling layers, resulting in a final feature map in low resolution. Although the final feature map encodes rich semantic information, the fine image structure information is lost, leading to inaccurate predictions around the object boundaries. As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, the original FCN typically downsamples the input image 5 times, reducing the spatial resolution of the final feature map by a factor of 32.</p><p>To obtain a high-resolution final feature map, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref> employ the original FCN as the encoder to capture high-level semantic information, and a decoder is designed to gradually recover the spatial information by combining multi-level feature maps from the encoder. As shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, we term such methods EncoderDecoder, of which the final prediction generated by the decoder is in high resolution. Alternatively, DeepLab <ref type="bibr" target="#b4">[5]</ref> removes the last two downsampling operations from the original FCN and introduces dilated (atrous) convolutions to maintain the receptive field of view unchanged. <ref type="bibr" target="#b0">1</ref> Following DeepLab, <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">36</ref>] employ a multi-scale context module on top of the final feature map, outperforming most EncoderDecoder methods significantly on several segmentation benchmarks. As shown in <ref type="figure" target="#fig_0">Figure 1c</ref>, the spatial resolution of the last feature map in DilatedFCN is 4 times larger than that in the original FCN, thus maintaining more structure and location information.</p><p>The dilated convolutions play an important role in maintaining the spatial resolution of the final feature map, leading to superior performance compared to most methods in EncoderDecoder. However, the introduced dilated convolutions bring heavy computation complexity and memory footprint, which limit the usage in many real-time applications. Taking ResNet-101 <ref type="bibr" target="#b12">[13]</ref> as an example, compared to the original FCN, 23 residual blocks (69 convolution layers) in DilatedFCN require to take 4 times more computation resources and memory usages, and 3 residual blocks (9 convolution layers) need to take 16 times more resources.</p><p>We aim at tackling the aforementioned issue caused by dilated convolutions in this paper. To achieve this, we propose a novel joint upsampling module to replace the time and memory consuming dilated convolutions, namely Joint Pyramid Upsampling (JPU). As a result, our method employs the original FCN as the backbone while applying JPU to upsample the low-resolution final feature map with output stride (OS) 32, resulting in a high-resolution feature map (OS=8). Accordingly, the computation time and memory footprint of the whole segmentation framework is dramatically reduced. Meanwhile, there's no performance loss when replacing the dilated convolutions with the proposed JPU. We attribute this to the ability of JPU to exploit multiscale context across multi-level feature maps.</p><p>To validate the effectiveness of our method, we first conduct a systematical experiment, showing that the proposed JPU can replace dilated convolutions in several popular approaches without performance loss. We then test the proposed method on several segmentation benchmarks. Results show that our method achieves the state-of-the-art performance while running more than 3 times faster. Concretely, we outperform all the baselines on Pascal Context dataset <ref type="bibr" target="#b22">[23]</ref> by a large margin, which achieves the state-ofthe-art performance with mIoU of 53.13%. On ADE20K dataset <ref type="bibr" target="#b40">[40]</ref>, we obtain the mIoU of 42.75% with ResNet-50 as the backbone, which sets a new record on the val set. Moreover, our method with ResNet-101 achieves the stateof-the-art performance in the test set of ADE20K dataset.</p><p>In summary, our contributions are three folds, which are: (1) We propose a computationally efficient joint upsampling module named JPU to replace the time and memory consuming dilated convolutions in the backbone. (2) Based on the proposed JPU, the computation time and memory footprint of the whole segmentation framework can be reduced by a factor of more than 3 and meanwhile achieves better performance. (3) Our method achieves the new state-ofthe-art performance in both Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (mIoU of 42.75% with ResNet-50 as the backbone on the val set and final score of 0.5584 with ResNet-101 on the test set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first give an overview on methods for semantic segmentation, which can be categorized into two directions. We then introduce some related works on upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>FCNs <ref type="bibr" target="#b21">[22]</ref> have achieved huge success in semantic segmentation. Following FCN, there're two prominent directions, namely DilatedFCN and EncoderDecoder. Dilated-FCNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b4">5]</ref> utilize dilated convolutions to keep the receptive field of view and employ a multi-scale context module to process high-level feature maps. Alternatively, EncoderDecoders <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37]</ref> propose to utilize an encoder to extract multi-level feature maps, which are then combined into the final prediction by a decoder.</p><p>DilatedFCN In order to capture multi-scale context information on the high-resolution final feature map, PSP-Net <ref type="bibr" target="#b38">[38]</ref> performs pooling operations at multiple grid scales while DeepLabV3 <ref type="bibr" target="#b5">[6]</ref> employs parallel atrous convolutions with different rates named ASPP. Alternatively, EncNet <ref type="bibr" target="#b36">[36]</ref> utilizes the Context Encoding Module to capture global contextual information. Differently, our method proposes a joint upsampling module named JPU to replace the dilated convolutions in the backbone of DilatedFCNs, which can JPU Encoding/PSP/ASPP Head  EncoderDecoder To gradually recover the spatial information, <ref type="bibr" target="#b27">[28]</ref> introduces skip connections to construct U-Net, which combines the encoder features and the corresponding decoder activations. <ref type="bibr" target="#b17">[18]</ref> proposes a multipath refinement network, which explicitly exploits all the information available along the down-sampling process. DeepLabV3+ <ref type="bibr" target="#b7">[8]</ref> combines the advantages of DilatedFCN and EncoderDecoder, which employs DeepLabV3 as the encoder. Our method is complementary to DeepLabV3+, which can reduce the computation overload of DeepLabV3 without performance loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Upsampling</head><p>In our method, we propose a module to upsample a lowresolution feature map given high-resolution feature maps as guidance, which is closely related to joint upsampling as well as data-dependent upsampling.</p><p>Joint Upsampling In the literature of image processing, joint upsampling aims at leveraging the guidance image as a prior and transferring the structural details from the guidance image to the target image. <ref type="bibr" target="#b16">[17]</ref> constructs a joint filter based on CNNs, which learns to recover the structure details in the guidance image. <ref type="bibr" target="#b31">[31]</ref> proposes an end-to-end trainable guided filtering module, which upsamples a lowresolution image conditionally. Our method is related to the aforementioned approaches. However, the proposed JPU is designed for processing feature maps with a large number of channels while <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">31]</ref> are specially designed for pro-cessing 3-channel images, which fail to capture the complex relations in high dimensional feature maps. Besides, the motivation and target of our method is completely different.</p><p>Data-Dependent Upsampling DUpsampling <ref type="bibr" target="#b28">[29]</ref> is also related to our method, which takes advantages of the redundancy in the segmentation label space and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. Compared to our method, DUpsampling has a strong dependency on the label space, which generalizes poorly to a larger or more complex label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first introduce the most popular methods for semantic segmentation, named DilatedFCNs. We then reform the architecture of DilatedFCNs with a novel joint upsampling module, Joint Pyramid Upsampling (JPU). Finally, we discuss the proposed JPU in details, before which joint upsampling, dilated convolution and stride convolution are briefly introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DilatedFCN</head><p>To exploit Deep CNNs in semantic segmentation, Long et al. <ref type="bibr" target="#b21">[22]</ref> transform the CNN designed for image classification into FCN. Taking ResNet-101 as an example, the original CNN contains 5 convolution stages, a global average pooling layer and a linear layer. To construct an FCN, the global average pooling layer and the linear layer are replaced by a convolution layer, which is used to generate the final label map, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. Between each two consecutive convolution stages, stride convolutions and/or spatial pooling layers are employed, resulting in 5 feature maps with gradually reduced spatial resolutions.</p><p>The spatial resolution of the last feature map in FCN is reduced by a factor of 32, leading to inaccurate predictions about the locations and details. To obtain a final feature map with high resolution, DeepLab <ref type="bibr" target="#b4">[5]</ref> removes the downsampling operations before the last two feature maps, as shown in <ref type="figure" target="#fig_0">Figure 1c</ref>. Besides, the convolution layers inside the last two convolution stages are replaced by dilated convolutions to maintain the receptive field of view, thus named Dilated-FCN. As a result, the resolution of the last feature map is reduced by a factor of 8, which reserves more location and detail information. Following DeepLab, <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b5">6]</ref> propose a multi-scale context module to capture context information from the last feature map, achieving tremendous success in several segmentation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Framework of Our Method</head><p>To obtain a high-resolution final feature map, methods in DilatedFCN remove the last two downsampling operations from the original FCN, which bring in heavy computation complexity and memory footprint due to the enlarged feature maps. In this paper, we aim at seeking an alternative way to approximate the final feature map of DilatedFCN without computation and memory overload. Meanwhile, we expect the performance of our method to be as good as that of the original DilatedFCNs.</p><p>To achieve this, we first put back all the stride convolutions removed by DilatedFCN, while replacing all the dilated convolutions with regular convolution layers. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the backbone of our method is the same as that of the original FCN, where the spatial resolutions of the five feature maps (Conv1−Conv5) are gradually reduced by a factor of 2. To obtain a feature map similar to the final feature map of DilatedFCN, we propose a novel module named Joint Pyramid Upsampling (JPU), which takes the last three feature maps (Conv3−Conv5) as inputs. Then a multi-scale context module (PSP <ref type="bibr" target="#b38">[38]</ref>/ASPP <ref type="bibr" target="#b5">[6]</ref>) or a global context module (Encoding <ref type="bibr" target="#b36">[36]</ref>) is employed to produce the final predictions.</p><p>Compared to DilatedFCN, our method takes 4 times fewer computation and memory resources in 23 residual blocks (69 layers) and 16 times fewer in 3 blocks (9 layers) when the backbone is ResNet-101. Thus, our method runs much faster than DilatedFCN while consuming less memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Pyramid Upsampling</head><p>The proposed JPU is designed for generating a feature map that approximates the activations of the final feature map from the backbone of DilatedFCN. Such a problem can be reformulated into joint upsampling, which is then resolved by a CNN designed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Background</head><p>Joint Upsampling Given a low-resolution target image and a high-resolution guidance image, joint upsampling aims at generating a high-resolution target image by transferring details and structures from the guidance image. Generally, the low-resolution target image y l is generated by employing a transformation f (·) on the low-resolution guidance image x l , i.e. y l = f (x l ). Given x l and y l , we are required to obtain a transformationf (·) to approximate f (·), where the computation complexity off (·) is much lower than f (·). For example, if f (·) is a multi-layer perceptron (MLP), thenf (·) can be simplified as a linear transformation. The high-resolution target image y h is then obtained by applyingf (·) on the high-resolution guidance image x h , i.e. y h =f (x h ). Formally, given x l , y l and x h , joint upsampling is defined as follows:</p><formula xml:id="formula_0">y h =f (x h ), wheref (·) = argmin h(·)∈H ||y l − h(x l )||, (1)</formula><p>where H is a set of all possible transformation functions, and || · || is a pre-defined distance metric.</p><p>Dilated Convolution Dilated convolution is introduced in DeepLab [5] for obtaining high-resolution feature maps while maintaining the receptive field of view. <ref type="figure" target="#fig_3">Figure 3a</ref> gives an illustration of the dilated convolution in 1D (dilation rate = 2), which can be divided into the following three steps: (1) split the input feature f in into two groups f 0 in and f 1 in according to the parity of the index, (2) process each feature with the same convolution layer, resulting in f 0 out and f 1 out , and (3) merge the two generated features interlaced to obtain the output feature f out .</p><p>Stride Convolution Stride convolution is proposed to transform the input feature into an output feature with reduced spatial resolution, which is equivalent to the following two steps as shown in <ref type="figure" target="#fig_3">Figure 3b</ref>: (1) process the input feature f in with a regular convolution to obtain the intermediate feature f m , and (2) remove the elements with an odd index, resulting in f out .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Reformulating into Joint Upsampling</head><p>The differences between the backbone of our method and DilatedFCN lie on the last two convolution stages. Taking the 4th convolution stage (Conv4) as an example, in Dilat-edFCN, the input feature map is first processed by a regular convolution layer, followed by a series of dilated convolutions (d=2). Differently, our method first processes the input feature map with a stride convolution (s=2), and then employs several regular convolutions to generate the output. Formally, given the input feature map x, the output feature map y d in DilatedFCN is obtained as follows: <ref type="figure" target="#fig_3">Fig 3a)</ref>,</p><formula xml:id="formula_1">y d = x → C r → C d → ...... → C d n = x → C r → SC r M → ...... → SC r M n (Fig 3a) = x → C r → S → C r → ...... → C r n → M = y m → S → C n r → M = {y 0 m , y 1 m } → C n r → M (</formula><p>while in our method, the output feature map y s is generated as follows:</p><formula xml:id="formula_3">y s = x → C s → C r → ...... → C r n = x → C r → R → C r → ...... → C r n (Fig 3b)</formula><p>= y m → R → C n r = y 0 m → C n r <ref type="figure" target="#fig_3">(Fig 3b)</ref>.</p><p>(3)</p><p>C r , C d , and C s represent a regular/dilated/stride convolution respectively, and C n r is n layers of regular convolutions. S, M and R are split, merge, and reduce operations in <ref type="figure" target="#fig_3">Figure 3</ref>, where adjacent S and M operations can be canceled out. Notably, the convolutions in Equations 2 and 3 are in 1D, which is for simplicity. Similar results can be obtained for 2D convolutions.</p><p>The aforementioned equations show that y s and y d can be obtained with the same function C n r with different inputs: y 0 m and y m , where the former is downsampled from the latter. Thus, given x and y s , the feature map y that ap-proximates y d can be obtained as follows:</p><formula xml:id="formula_4">y ={y 0 m , y 1 m } →ĥ → M whereĥ = argmin h∈H ||y s − h(y 0 m )||, y m = x → C r ,<label>(4)</label></formula><p>which is the same as the joint upsampling problem defined in Equation 1. Similar conclusions can be easily obtained for the 5th convolution stage (Conv5). <ref type="table" target="#tab_5">Equation 4</ref> is an optimization problem, which takes lots of time to converge through the iterative gradient descent. Alternatively, we propose to approximate the optimization process with a CNN module. To achieve this, we first require to generate y m given x, as shown in Equation 4. Then, features from y 0 m and y s need to be gathered for learning the mappingĥ. Finally, a convolution block is required to transform the gathered features into the final prediction y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Solving with CNNs</head><p>Following the aforementioned analysis, we design the JPU module as in <ref type="figure">Figure 4</ref>. Concretely, each input feature map is firstly processed by a regular convolution block <ref type="figure">(Fig. 4a)</ref>, which is designed for (1) generating y m given x, and (2) transforming f m into an embedding space with reduced dimensions. As a result, all the input features are mapped into the same space, which enables a better fusion and reduces the computation complexity.</p><p>Then, the generated feature maps are upsampled and concatenated, resulting in y c <ref type="figure">(Fig. 4b)</ref>. Four separable convolutions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref>  the convolution with dilation rate 1 is employed to capture the relation between y 0 m and the rest part of y m , as shown by the blue box in <ref type="figure" target="#fig_1">Figure 5</ref>. Alternatively, the convolutions with dilation rate 2, 4 and 8 are designed for learning the mappingĥ to transform y 0 m into y s , as shown by the green boxes in <ref type="figure" target="#fig_1">Figure 5</ref>. Thus, JPU can extract multi-scale context information from multi-level feature maps, which leads to a better performance. This is significantly different from ASPP <ref type="bibr" target="#b5">[6]</ref>, which only exploit the information in the last feature map.</p><p>The extracted features encode the mapping between y 0 m and y s as well as the relation between y 0 m and the rest part of y m . Thus, another regular convolution block is employed, which transforms the features into the final predictions <ref type="figure">(Fig. 4c)</ref>.</p><p>Notably, the proposed JPU module solves two closely related joint upsampling problems jointly, which are (1) upsampling Conv4 based on Conv3 (the 4th convolution stage), and (2) upscaling Conv5 with the guidance of the enlarged Conv4 (the 5th convolution stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we first introduce the datasets used in our experiments as well as the implementation details. We then conduct a systematic ablation study to show the effectiveness of the proposed JPU from the view of both performance and efficiency. Finally, to compare with the stateof-the-art methods, we report the performance on two segmentation datasets, Pascal Context <ref type="bibr" target="#b22">[23]</ref> and ADE20K <ref type="bibr" target="#b40">[40]</ref>, which are widely used as the segmentation benchmarks. Moreover, we also show some visual results to demonstrate the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset Pascal Context dataset <ref type="bibr" target="#b22">[23]</ref> is based on the PAS-CAL VOC 2010 detection challenge, which provides additional pixel-wise semantic annotations. There're 4,998 images for training (train) and 5,105 images for testing (val). Following the prior works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">36]</ref>, we use the most frequent 59 object categories plus background (60 classes in total) as the semantic labels.</p><p>Implementation Details Our method is implemented in PyTorch <ref type="bibr" target="#b24">[25]</ref>. For training on Pascal Context, we follow the protocol presented in <ref type="bibr" target="#b36">[36]</ref>. Concretely, we set the learning rate to 0.001 initially, which gradually decreases to 0 by following the "poly" strategy (power = 0.9). For data augmentation, we randomly scale (from 0.5 to 2.0) and leftright flip the input images. The images are then cropped to 480 × 480 and grouped with batch size 16. The network is trained for 80 epochs with SGD, of which the momentum is set to 0.9 and weight decay is set to 1e-4. All the experiments are conducted in a workstation with 4 Titan-Xp GPUs (12G per GPU). We employ pixel-wise cross-entropy as the loss function. ResNet-50 and ResNet-101 are used as the backbone, which are widely used in most existing segmentation methods as the standard backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>To show the effectiveness of the proposed method, we conduct a systematical ablation study on Pascal Context dataset with ResNet-50 as the backbone, as shown in Table 1. We report the standard evaluation metrics of pixel accuracy (pixAcc) and mean Intersection of Union (mIoU). Notably, no multi-scale testing and left-right flipping are applied to the val images.</p><p>Dilated Convolutions For methods in DilatedFCN, the downsampling operations in the last two convolution stages are removed, resulting in the output stride (OS) to be 8. Encoding-8-None in <ref type="table">Table 1</ref> represents the original Enc-Net <ref type="bibr" target="#b36">[36]</ref>. To show the effect of dilated convolutions, we replace the backbone of EncNet with that of the original FCN (the same as our method), resulting in the OS to be 32. We then upsample the last feature map by 4 times with bilinear interpolation before feeding it into the Encoding Head, noted as Encoding-32-Bilinear. As shown in <ref type="table">Table 1</ref>, Encoding-32-Bilinear performs significantly worse than Encoding-8-None, which shows that it's not trivial to replace the dilated convolutions in the backbone of Dilated-FCNs.</p><p>Upsampling Module To show the effectiveness of the proposed JPU, we compare it with other classic upsampling methods, bilinear upsampling and feature pyramid network (FPN) <ref type="bibr" target="#b19">[20]</ref>. As shown in <ref type="table">Table 1</ref>, FPN outperforms bilinear interpolation by a large margin. Even compared with Enc-Net, FPN achieves comparable performance in both pixAcc and mIoU. By replacing FPN with our JPU, our method outperforms both FPN and EncNet by more than 1% in mIoU, which achieves the state-of-the-art performance.</p><p>The visual results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Encoding-32-Bilinear <ref type="figure" target="#fig_5">(Fig. 6c</ref>) captures the global semantic information successfully, which gives a rough segmentation of the bird and sky. However, the boundary of the bird is inaccurate, and most parts of the branch are failed to be labeled out. When replacing bilinear interpolation with FPN ( <ref type="figure" target="#fig_5">Fig. 6d)</ref>, the bird and branch are labeled out successfully with accurate boundaries, which shows the effect of combing lowlevel and high-level feature maps. A slightly better result can be obtained with dilated convolutions <ref type="figure" target="#fig_5">(Fig. 6e</ref>). As for our method <ref type="figure" target="#fig_5">(Fig. 6f</ref>), it labels out both the main branch and the side shoot accurately, which shows the effectiveness of the proposed joint upsampling module. Particularly, the side shoot demonstrates the ability of JPU to extract multi-scale context from multi-level feature maps. Thus, our method can achieve a better performance.</p><p>Generalization to Other Methods To show the generalization ability of the proposed JPU, we replace EncNet with two popular methods in DilatedFCN, namely DeepLabV3 (ASPP Head) <ref type="bibr" target="#b5">[6]</ref> and PSPNet <ref type="bibr" target="#b38">[38]</ref>. As shown in <ref type="table">Table 1</ref>, our methods transformed from DeepLabV3 and PSP outperforms the corresponding original methods consistently.   FPS To compare the computation complexity, we employ frame per second (FPS) as the evaluation metric, which is measured on a Titan-Xp GPU with a 512 × 512 image as input. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the reported FPS is averaged among 100 runs. For ResNet-50, our method (Encoding-JPU) runs about two times faster than EncNet (Encoding-None). When changing the backbone to ResNet-101, our method runs more than three times faster than Enc-Net. The speed of our method is also comparable to FPN, but our method achieves much better performance. As for DeepLabV3 (ASPP) and PSP, our method can accelerate them to a certain degree while having a better performance.</p><p>Method Backbone mIoU% FCN-8s <ref type="bibr" target="#b21">[22]</ref> 37.8 CRF-RNN <ref type="bibr" target="#b39">[39]</ref> 39.3 ParseNet <ref type="bibr" target="#b20">[21]</ref> 40.4 BoxSup <ref type="bibr" target="#b9">[10]</ref> 40.5 HO CRF <ref type="bibr" target="#b1">[2]</ref> 41.3 Piecewise <ref type="bibr" target="#b18">[19]</ref> 43.3 VeryDeep <ref type="bibr" target="#b32">[32]</ref> 44.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Other Methods</head><p>Pascal Context In <ref type="table">Table 1</ref>, our method employs ResNet-50 as the backbone without multi-scale evaluation, and the 2 Following <ref type="bibr" target="#b36">[36]</ref>, the mIoU reported in <ref type="table">Table 1</ref> is on 59 classes w/o background. In this table, the mIoU is measured on 60 classes w/ background for a fair comparison with other methods. Besides, we average the network prediction in multiple scales for evaluation in this <ref type="table">table.</ref> metrics are calculated on 59 classes excluding background by following <ref type="bibr" target="#b36">[36]</ref>. To compare fairly with the state-of-theart methods, we average the prediction in multiple scales and calculate the metrics among 60 classes including background, which are then reported in <ref type="table" target="#tab_4">Table 3</ref>. With ResNet-50 as the backbone, our method outperforms DeepLabV2 (with COCO pretraining) and RefineNet by a large margin, which employ ResNet-101 and ResNet-152 as the backbone, respectively. Moreover, our method (ResNet-50) achieves competitive performance compared to EncNet with ResNet-101 as the backbone. By replacing ResNet-50 with a deeper network ResNet-101, our method gets an additional 1.9% improvement in mIoU, which outperforms EncNet (ResNet-101) and DUpsampling (Xception-71) significantly and achieves the state-of-the-art performance. Notably, Xception-71 is a much stronger backbone than ResNet-101. For completeness, we also report the mIoU on 59 classes (w/o background), which is 52.10%(ResNet-50) and 54.03% (ResNet-101).</p><p>ADE20K ADE20K dataset <ref type="bibr" target="#b40">[40]</ref> is a scene parsing benchmark, which contains 150 stuff/object categories. The dataset includes 20K/2K/3K images for training (train), validation (val), and testing (test).</p><p>We train our network on the train set for 120 epochs with learning rate 0.01. We then evaluate the model on the val set and report pixAcc and mIoU in <ref type="table" target="#tab_5">Table 4</ref>. When employing ResNet-50 as the backbone, our method outperforms EncNet (ResNet-50) by 1.64% in mIoU, while achieving a much better performance compared to RefineNet (ResNet-152). By replacing ResNet-50 with ResNet-101, our method obtains competitive performance compared to Enc-Net (ResNet-101) and PSPNet (ResNet-269). Our method (ResNet-101) performs a little worse than EncNet, and we attribute this to the spatial resolution of the training images. Concretely, in our method, the training images are cropped to 480 × 480 for processing 4 images in a GPU with 12G memory. However, EncNet is trained with 576×576 images on GPUs with memory larger than 12G.</p><p>We then fine-tune our network on the train set and val set for another 20 epochs with learning rate 0.001. The predictions on the test set are submitted to the evaluation server. As shown in <ref type="table" target="#tab_6">Table 5</ref>, our method outperforms two winning entries from the COCO-Place challenge 2017. Moreover, our method also achieves better performance compared to PSPNet and EncNet, although it performs worse on the val set. Notably, Final Score is the metric used in the evaluation server, which is the average of pixAcc and mIoU.</p><p>The visual results from both the Pascal Context dataset and the ADE20K dataset are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. More results are shown in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have analyzed the differences and connections between dilated convolution and stride convolution. Based on the analysis, we formulated the task of extracting high-resolution feature maps into a joint upsampling problem and proposed a novel CNN module JPU to solve the problem. By replacing the time and memory consuming dilated convolutions with our JPU, the computation complexity is reduced by more than three times without performance loss. The ablation study shows that the proposed JPU is superior to other upsampling modules. By plugging JPU, several modern approaches for semantic segmentation achieve a better performance while runs much faster than before. Results on two segmentation datasets show that our method achieves the state-of-the-art performance while reducing the computation complexity dramatically.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different types of networks for semantic segmentation. (a) is the original FCN, (b) follows the encoder-decoder style, and (c) employs dilated convolutions to obtain high-resolution final feature maps. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>C o n v 5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Framework Overview of Our Method. Our method employs the same backbone as the original FCN. After the backbone, a novel upsampling module named Joint Pyramid Upsampling (JPU) is proposed, which takes the last three feature maps as the inputs and generates a high-resolution feature map. A multi-scale/global context module is then employed to produce the final label map. Best viewed in color.reduce computation complexity dramatically without performance loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Dilated Convolution (dilation rate=2) and Stride Convolution (stride=2) in 1D. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>with different dilation rates(1, 2, 4, and 8)   are employed in parallel to extract features from y c , where different dilation rates take different functions. Concretely, The Proposed Joint Pyramid Upsampling (JPU). Best viewed in color. The convolution with dilation rate 1 focuses on y 0 m and the rest part of y m , and the convolution with dilation rate 2 aims at y 0 m and y s . Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparison of different upsampling modules with Encoding Head and ResNet-50 as the backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visual results of our method (ResNet-101). The first row is from Pascal Context val set, while the second row is from ADE20K val set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visual results of our method (ResNet-101) on the Pascal Context val set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Visual results of our method (ResNet-101) on the Pascal Context val set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Visual results of our method (ResNet-101) on the Pascal Context val set. Best viewed in color.(a) Input (b) GT (c) EncNet (d) Ours Visual results of our method (ResNet-101) on the ADE20K val set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Visual results of our method (ResNet-101) on the ADE20K val set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Computation Complexity. The FPS is measured on a Titan-Xp GPU with a 512×512 image as input, which is averaged among 100 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The state-of-the-art methods on the val set of the Pascal Context dataset.</figDesc><table><row><cell cols="2">Method Backbone</cell><cell cols="2">pixAcc% mIoU%</cell></row><row><cell>FCN [22]</cell><cell></cell><cell>71.32</cell><cell>29.39</cell></row><row><cell>SegNet [3]</cell><cell></cell><cell>71.00</cell><cell>21.64</cell></row><row><cell>DilatedNet [35]</cell><cell></cell><cell>73.55</cell><cell>32.31</cell></row><row><cell>CascadeNet [40]</cell><cell></cell><cell>74.52</cell><cell>34.90</cell></row><row><cell cols="2">RefineNet [18] ResNet-152</cell><cell>-</cell><cell>40.7</cell></row><row><cell>PSPNet [38]</cell><cell>ResNet-101 ResNet-269</cell><cell>81.39 81.69</cell><cell>43.29 44.94</cell></row><row><cell>EncNet [36]</cell><cell>ResNet-50 ResNet-101</cell><cell>79.73 81.69</cell><cell>41.11 44.65</cell></row><row><cell>Ours</cell><cell>ResNet-50 ResNet-101</cell><cell>80.39 80.99</cell><cell>42.75 44.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on the val set of ADE20K dataset.</figDesc><table><row><cell cols="2">Rank Team</cell><cell cols="2">Single Model Final Score</cell></row><row><cell>1</cell><cell>CASIA IVA JD</cell><cell></cell><cell>0.5547</cell></row><row><cell>2</cell><cell>WinterIsComing</cell><cell></cell><cell>0.5544</cell></row><row><cell>-</cell><cell>PSPNet [38]</cell><cell>ResNet-269</cell><cell>0.5538</cell></row><row><cell>-</cell><cell>EncNet [36]</cell><cell>ResNet-101</cell><cell>0.5567</cell></row><row><cell>-</cell><cell>Ours</cell><cell>ResNet-101</cell><cell>0.5584</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results on ADE20K test set. The first two entries ranked 1st and 2nd place in COCO-Place challenge 2017.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In most cases, dilated convolutions in this paper refer to (1) removing downsampling operations and (2) replacing regular convolutions with dilated convolutions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uijlings</surname></persName>
		</author>
		<title level="m">The devil is in the decoder. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bridging categorylevel and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<title level="m">Multi-scale context aggregation by dilated convolutions. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">More Visual Results See Figure 8−12 in the following pages for more visual results</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
