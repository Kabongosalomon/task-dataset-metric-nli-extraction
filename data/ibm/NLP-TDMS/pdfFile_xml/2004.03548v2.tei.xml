<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Pyramid Network for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
							<email>bzhou@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Pyramid Network for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual tempo characterizes the dynamics and the temporal scale of an action. Modeling such visual tempos of different actions facilitates their recognition. Previous works often capture the visual tempo through sampling raw videos at multiple rates and constructing an input-level frame pyramid, which usually requires a costly multi-branch network to handle. In this work we propose a generic Temporal Pyramid Network (TPN) at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-andplay manner. Two essential components of TPN, the source of features and the fusion of features, form a feature hierarchy for the backbone so that it can capture action instances at various tempos. TPN also shows consistent improvements over other challenging baselines on several action recognition datasets. Specifically, when equipped with TPN, the 3D ResNet-50 with dense sampling obtains a 2% gain on the validation set of Kinetics-400. A further analysis also reveals that TPN gains most of its improvements on action classes that have large variances in their visual tempos, validating the effectiveness of TPN. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While great progress has been made by deep neural networks to improve the accuracy of video action recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30]</ref>, an important aspect of characterizing dfferent actions is often missed in the design of these recognition networks -the visual tempos of action instances. Visual tempo actually describes how fast an action goes, which tends to determine the effective duration at the temporal scale for recognition. As shown at the bottom of <ref type="figure" target="#fig_0">Figure 1</ref>, action classes naturally have different visual tempos (e.g. hand clapping and walking). In some cases the key to distinguish different action classes is their visual tempos, as they might share high similarities in visual appearance, such as walking, jogging and running. Moreover, as shown at the top of <ref type="figure" target="#fig_0">Figure 1</ref>, when performing the same action, each performer <ref type="bibr" target="#b0">1</ref> Code and models are available at this link. â€  indicates equal contribution. may act at his/her own visual tempo, due to various factors such as age, mood, and energy level. e.g. ,an elder man tends to move slower than a younger man, so as a man with a heavier weight. Precise modeling of such intra-and inter-class variances in visual tempos of action instances can potentially bring a significant improvement to action recognition. Previous attempts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref> for extracting the dynamic visual tempos of action instances mainly rely on constructing a frame pyramid, where each pyramid level samples the input frames at a different temporal rate. For example, we can sample from the total 64 frames of an video instance at intervals 16 and 2 respectively, to construct a two-level frame pyramid consisting of 4 and 32 frames. Subsequently, frames at each level are fed into different backbone subnetworks, and their output features are further combined together to make the final prediction. By sampling frames at different rates as input, backbone networks in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref> are able to extract features of different receptive fields and represent the input action instance at different visual tempos. These backbone subnetworks thus jointly aggregate temporal information of both fast-tempo and slow-tempo, handling action instances at different temporal scales.</p><p>Previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref> have obtained noticeable improvements for action recognition, however it remains computationally expensive to deal with the dynamic visual tempos of action instances at the input frame level. It is not scalable to pre-define the tempos in the input frame pyramid and then feed the frames into multiple network branches, especially when we use a large number of sampling rates. On the other hand, many commonly-used models in video recognition, such as C3D and I3D <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref>, often stack a series of temporal convolutions. In these networks, as the depth of a layer increases, its temporal receptive field increases as well. As a result, the features at different depths in a single model already capture information of both fast-tempo and slow-tempo. Therefore, we propose to build a temporal pyramid network (TPN) to aggregate the information of various visual tempos at feature level. By leveraging the feature hierarchy formed inside the network, the proposed TPN is able to work with input frames fed at a single rate. As an auxiliary module, TPN could be applied in a plug-andplay manner to various existing action recognition models to bring consistent improvements.</p><p>In this work we first provide a general formulation of the proposed TPN, where several components are introduced to better capture the information at multiple visual tempos.We then evaluate TPNs on three benchmarks: Kinetics-400 <ref type="bibr" target="#b0">[1]</ref>, Something-Something V1 &amp; V2 <ref type="bibr" target="#b9">[10]</ref> and Epic-Kitchen <ref type="bibr" target="#b1">[2]</ref> with comprehensive ablation studies. Without any bells and whistles, TPNs bring consistent improvements when combined with both 2D and 3D networks. Besides, the ablation study shows that TPN obtains most of its improvements from the action classes that have significant variances in visual tempos. This result verifies our assumption that aggregating features in a single model is sufficient to capture the visual tempos of action instances for video recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Action Recognition. Attempts for video action recognition could be divided into two categories. Methods in the first category often adopt a 2D + 1D paradigm, where 2D CNNs are applied over per-frame inputs, followed by a 1D module that aggregates per-frame features. Specifcally, two-stream networks in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> utilize two separate CNNs on per-frame visual appearances and optical flows respectively, and an average pooling operation for temporal aggregation. Among its variants, TSN <ref type="bibr" target="#b30">[31]</ref> proposes to represent video clips by sampling from evenly divided segments. And TRN <ref type="bibr" target="#b37">[38]</ref> and TSM <ref type="bibr" target="#b17">[18]</ref> respectively replace the average pooling operation with an interpretable relational module and utilize a shift module, in order to better capture information along the temporal dimension. However, due to the deployment of 2D CNNs in these methods, semantics of the input frames could not interact with each other in the early stage, which limits their ability to capture the dynamics of visual tempos. Methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref> in the second category alternatively apply 3D CNNs that stack 3D convolutions to jointly model temporal and spatial semantics. Along this line of research, Non-local Network <ref type="bibr" target="#b31">[32]</ref> introduces a special non-local operation to better exploit the long-range temporal dependencies between video frames. Besides Non-local Network, different modifications for the 3D CNNs, including the inflating 2D convolution kernels <ref type="bibr" target="#b0">[1]</ref> and the decomposing 3D convolution kernels <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, can also boost the performances of 3D CNNs. Other effects <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> are taken on irregular convolution/pool for better feature alignment or study action instances in a fine-grained way. Although the aforementioned methods could better handle temporal information, the large variation of visual tempos remains neglected.</p><p>Visual Tempo Modeling in Action Recognition. The complex temporal structure of action instances, particularly in terms of the various visual tempos, raises a challenge for action recognition. In recent years, researchers have started exploring this direction. SlowFast <ref type="bibr" target="#b4">[5]</ref> hard-codes the variance of visual tempos using an input-level frame pyramid that has level-wise frames sampled at different rates. Each level of the pyramid is also separately processed by a network, where mid-level features of these networks are interactively combined. With the assist of both the frame pyramid and the level-specific networks, SlowFast could robustly handle the variance of visual tempos. The complex temporal structure inside videos, particularly tempo variation, raises a challenge for action recognition. DTPN <ref type="bibr" target="#b34">[35]</ref> also samples frames with different frame per seconds (FPS) to construct a natural pyramidal representation for arbitrary-length input videos. However, such a hard-coding scheme tends to require multiple frames , especially when the pyramid scales up. Different from previous feature-level pyramid networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref> which deal with the large variance of spatial scales in object detection, we instead leverage the feature hierarchy to handle the variance of temporal information i.e. visual tempos. In this way we could hide the concern about visual tempos inside a single network, and we only need frames sampled at a single rate at the input-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Pyramid Network</head><p>The visual tempo of an action instance is one of the key factors for recognizing it, especially when other factors are ambiguous. For example, we cannot tell if an action instance belongs to walking, jogging or running based on its visual appearance. However, it is difficult to capture the visual tempos due to their inter-and intra-class variance across different videos. Previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>  issue at the input-level. They utilize a frame pyramid that contains frames sampled at pre-defined rates to represent the input video instance at various visual tempos. Since each level of the frame pyramid requires a separate backbone network to handle, such an approach may be computationally expensive, especially when the level of pyramid scales up. Inspired by the observation that features at multiple depths in a single network already cover various visual tempos, we propose a feature-level temporal pyramid network (TPN) for modeling the visual tempo. TPN could operate on only a single network no matter how many levels are included in it. Moreover, TPN could be applied to different architectures in a plug-and-play manner. To fully implement TPN, two essential components of TPN must be designed properly, namely 1) the feature source and 2) the feature aggregation. We propose the spatial semantic modulation and temporal tempo modulation to control the relative differences of the feature source in Sec.3.1, and construct multiple types of information flows for feature aggregation in Sec.3.2. Finally we show how to adopt TPN for action recognition in Sec.3.3, taking <ref type="bibr" target="#b4">[5]</ref> as an exemplar backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>address this</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Source of TPN</head><p>Collection of Hierarchical Features. While TPN is built upon a set of M hierarchical features that have increasing temporal receptive fields from bottom to top, there are two alternative ways to collect these features from a backbone network. 1) Single-depth pyramid: a simple way is to choose a feature F base of size C Ã—T Ã—W Ã—H at some depth, and to sample along the temporal dimension with M different rates {r 1 , ..., r M ; r 1 &lt; r 2 &lt; ... &lt; r M }. We refer to such a TPN as a single-depth pyramid consisting of {F</p><formula xml:id="formula_0">(1) base , ..., F (M ) base } of sizes {C Ã— T r1 Ã— W Ã— H, ..., C Ã— T r M Ã— W Ã— H}.</formula><p>Features collected in this way could lighten the workload of fusion as they have identical shapes besides the temporal dimension. However, they may limit in effectiveness as they represent video semantics only at a single spatial granularity. 2) Multi-depth pyramid: a better way is to collect a set of M features with increasing depths, resulting in a TPN made of</p><formula xml:id="formula_1">{F 1 , F 2 , ..., F M } of sizes {C 1 Ã— T 1 Ã— W 1 Ã— H 1 , ..., C M Ã— T M Ã— W M Ã— H M }, where generally the dimensions satisfy {C i1 â‰¥ C i2 , W i1 â‰¥ W i2 , H i1 â‰¥ H i2 ; i 1 &lt; i 2 }.</formula><p>Such a multi-depth pyramid contains richer semantics in the spatial dimensions, yet raises the need of careful treatment in feature fusion, in order to ensure correct information flows between features.</p><p>Spatial Semantic Modulation. To align spatial semantics of features in the multi-depth pyramid, a spatial semantic modulation is utilized for TPN. The spatial semantic modulation works in two complementary ways. For each but the top-level feature, a stack of convolutions with level-specific stride are applied to it, matching its spatial shape and receptive field with the top one. Moreover, an auxiliary classification head is also appended to it to receive stronger supervision, leading to enhanced semantics. The overall objective for a backbone network with our proposed TPN thus becomes:</p><formula xml:id="formula_2">L total = L CE,o + M âˆ’1 i=1 Î» i L CE,i ,<label>(1)</label></formula><p>where L CE,o is the original Cross-Entropy loss, and L CE,i is the loss for i-th auxiliary head. {Î» i } are balancing coef-ficients. After spatial semantic modulation, features have aligned shapes and consistent semantics in the spatial dimensions. However, it remains uncalibrated in the temporal dimension, where we introduce the proposed temporal rate modulation.</p><p>Temporal Rate Modulation. Recall in the input-level frame pyramid used in <ref type="bibr" target="#b4">[5]</ref>, the sampling rates of frames could be adjusted dynamically to increase its applicability. On the contrary, TPN is limited in the flexibility, as it operates on features of a backbone network, so that the visual tempos of these features are only controlled by their depths in the original network. To equip TPN with a similar flexibility as in the input-level frame pyramid, a set of</p><formula xml:id="formula_3">hyper-parameters {Î± i } M i=1</formula><p>are further introduced to TPN for temporal tempo modulation. Specifically, Î± i denotes that after spatial semantic modulation, the updated feature at i-level will be temporally downsampled by a factor of Î± i , using a parametric sub-net. The inclusion of such hyper-parameters enables us to better control the relative differences of features in terms of temporal scales, so that feature aggregation could be conducted more effectively. With some abuse of notations, we refer to F i of size C i Ã— T i Ã— W i Ã— H i as the i-th feature after both the spatial semantic modulation and the temporal rate modulation in the following content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Information Flow of TPN</head><p>After collecting and pre-processing the hierarchical features as in Sec.3.1, so that they are dynamic in visual tempos and consistent in spatial semantics, we are ready to step in the second step of TPN construction -how to aggregate these features. Let F i be the aggregated feature at i-th level, generally there are three basic options:</p><formula xml:id="formula_4">F i = ï£± ï£´ ï£² ï£´ ï£³ F i Isolation Flow F i g(F iâˆ’1 , T i /T iâˆ’1 ) Bottom-up Flow F i g(F i+1 , T i /T i+1 ) Top-down Flow ,<label>(2)</label></formula><p>where denotes element-wise addition. And to ensure the compatibility of the addition between consecutive features, during aggregation a down/up-sampling operation, g(F, Î´) with F as the feature and Î´ is the factor, is applied along the temporal dimension. Note that the top/bottom features for top-down/bottom-up flow would not be aggregated by other features. Besides the above basic flows to aggregate features in TPN, we could also combine them to achieve two additional options, namely Cascade Flow and Parallel Flow. While applying a bottom-up flow after a top-down flow will lead to the cascade flow, applying them simultaneously will result in the parallel flow. See <ref type="figure" target="#fig_2">Fig. 3</ref> for an illustration of all the possible flows. It's worth noting that more complicated flow (e.g. path aggregation in <ref type="bibr" target="#b19">[20]</ref>) could be built on top of these flows. However, our attempts in this line of research  have not shown further improvement. Finally, following <ref type="figure" target="#fig_1">Fig.2</ref>, all aggregated features in TPN will be rescaled and concatenated for succeeding predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>Here we introduce the implementation of TPN for action recognition. Following <ref type="bibr" target="#b4">[5]</ref>, we use inflated ResNet <ref type="bibr" target="#b4">[5]</ref> as the 3D backbone network, for its promising performance on various datasets <ref type="bibr" target="#b0">[1]</ref>. Meanwhile, original ResNet <ref type="bibr" target="#b11">[12]</ref> serves as our 2D backbone. We use the output features of res2, res3, res4, res5 to build TPN, where they are spatially downsampled by respectively 4, 8, 16 and 32 times, compared to the input frames. We provide the structure of 3D ResNet-50 in Tab.1 for the reference. In the spatial semantic modulation, a stack of convolutions with M âˆ’ i stride to process the feature at i-th level in a M -level TPN and the feature dimension would be decreased or increased to 1024. Besides, the temporal rate modulation for each feature is achieved by a convolutional layer and a max-pooling layer. Finally, after feature aggregation through one of the five flows mentioned in Sec. 3.2, features of TPN will be separately rescaled by max-pooling operations, and their concatenation will be fed into a fully-connected layer to make the final predictions. TPN can be also jointly trained with the backbone network in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed TPN on various action recognition datasets, including Kinetics-400 <ref type="bibr" target="#b0">[1]</ref>, Something-Something V1 &amp; V2 <ref type="bibr" target="#b9">[10]</ref>, and Epic-Kitchen <ref type="bibr" target="#b1">[2]</ref>. The consistent improvements show the effectiveness and generality of TPN. Ablation studies on the components of TPN are also included. Moreover, we present several empirical analysis to verify our motivation of TPN, i.e. a feature-level temporal pyramid on a single backbone is beneficial for capturing the variance of visual tempos. All experiments are conducted with the single modality (i.e. RGB frames) on MMAction <ref type="bibr" target="#b36">[37]</ref> and evaluated on the validation set unless specified.</p><p>Dataset. Kinetics-400 <ref type="bibr" target="#b0">[1]</ref> contains around 240k training videos and 19k validation videos that last for 10 seconds. It includes 400 action categories in total. Something-Something V1 <ref type="bibr" target="#b9">[10]</ref> consists of 86k training videos and 11k validation videos belonging to 174 action categories, whose durations vary from 2 to 6 seconds. The second release (V2) of Something-Something increase the number of videos to 220k. Epic-Kitchen <ref type="bibr" target="#b1">[2]</ref> includes around 125 verb and 352 noun categories. Following <ref type="bibr" target="#b7">[8]</ref>, we randomly select 232 videos (23439 segments) for training and 40 videos (4979 segments) for validation.</p><p>Training. Unless specified otherwise, our models are defaultly initialized by pre-trained models on ImageNet <ref type="bibr" target="#b2">[3]</ref>.</p><p>Following the setting in <ref type="bibr" target="#b4">[5]</ref>, the input frames are sampled from a set of consecutive 64 frames at a specific interval Ï„ . Each frame is randomly cropped so that its short side ranges in [256, 320] pixels, as in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>. The augmentation of horizontal flip and a dropout <ref type="bibr" target="#b12">[13]</ref> of 0.5 are adopted to reduce overfitting. And BatchNorm (BN) <ref type="bibr" target="#b13">[14]</ref> is not frozen. We use a momentum of 0.9, a weight decay of 0.0001 and a synchronized SGD training over 8 GPUs <ref type="bibr" target="#b8">[9]</ref>. Each GPU has a batch-size of 8, resulting in a mini-batch of 64 in total. For Kinetics-400, the learning rate is 0.01 and will be reduced by a factor of 10 at 100, 125 epochs (150 epochs in total) respectively. For Something-Something V1 &amp; V2 <ref type="bibr" target="#b9">[10]</ref> and Epic-Kitchen <ref type="bibr" target="#b1">[2]</ref>, our model is trained for 150 and 55 epochs separately.</p><p>Inference. There exist two ways for inference: three-crop and ten-crop testing. a) Three-crop testing refers to three random crops of size 256 Ã— 256 from the original frames, which are resized firstly to have 256 pixels in their shorter sides. Three-crop testing is used as the approximation of spatially fully-convolutional testing as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5]</ref>. b) Tencrop testing basically follows the procedure of <ref type="bibr" target="#b30">[31]</ref>, which extracts 5 crops of size 224 Ã— 224 and flips these crops. Specially, we conduct three-crop testing on Kinetics-400. We also uniformly sample 10 clips of the whole video and average the softmax probabilities of all clips as the final prediction. For the other two datasets, ten-crop testing and TSN-like methods with 8 segments are adopted.</p><p>Backbone. We evaluate TPN on both 2D and 3D backbone networks. Specifically, the slow-only branch of SlowFast <ref type="bibr" target="#b4">[5]</ref> is applied as our backbone network (denoted as I3D) due to its promising performance on various datasets. The architecture of I3D is shown in <ref type="table">Table 1</ref>, which turns the 2D ResNet <ref type="bibr" target="#b11">[12]</ref> into a 3D version via inflating kernels <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref>. Specifically, a 2D kernel of size k Ã— k will be inflated to have the size t Ã— k Ã— k, with its original weights copied for t times and rescaled by 1/t. Note that there are no temporal downsampling operations in the slow-only backbone. ResNet-50 <ref type="bibr" target="#b11">[12]</ref> is used as 2D backbone to show that TPN could combine with various backbones. The final prediction follows the standard protocol of TSN <ref type="bibr" target="#b30">[31]</ref> unless specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Results on Kinetics-400. We compare our TPN with other state-of-the-art methods on Kinetics-400. The multi-depth pyramid and the parallel flow are used as the default setting for TPN. In detail, the multi-depth pyramid is built on the outputs of res4 and res5. And the hyper-parameters</p><formula xml:id="formula_5">{Î± i } M i=1</formula><p>are set to be {16, 32}. As discussed in the spatial semantic modulation, an additional auxiliary head is applied on the output of res4 with a balancing coefficient of 0.5. Sampling intervals of input frames Ï„ = 8, 4, 2 are compared.  <ref type="table">Table 3</ref>. Improvement of 2D backbone on the validation set of Kinetics-400. Only 8 segments are used for both training and validation for apple-to-apple comparison.</p><p>The performance of I3D-R50 + TPN (i.e. TPN-R50) is included in <ref type="table" target="#tab_2">Table 2</ref>. It is worth noting that in <ref type="table" target="#tab_2">Table 2</ref> backbones of methods with the same depth are slightly different, which also affect their final accuracies. TPN-R50 could achieve 77.7% top-1 accuracy, better than others with the same depth. TPN-R101 are also evaluated with the input setting of 32 Ã— 2, which obtains an accuracy of 78.9%, surpassing other methods with the same numbers of input frames.</p><p>Being a general module, TPN could be combined with 2D networks. To show this, we add TPN to the ResNet-50 <ref type="bibr" target="#b11">[12]</ref> in TSN (TSN-50 + TPN), and train such a combination with 8 segments (uniform sampling) in an end-to-end manner. Different from the original TSN <ref type="bibr" target="#b30">[31]</ref> which takes 25 segments for validation, we utilize only 8 segments and the ten-crop testing, comparing apples to apples. As shown in <ref type="table">Table 3</ref>, adding TPN to TSN-50 could boost the top-1 accuracy by 3.6%.</p><p>Results on Something-Something. Results of different baselines with and without TPN on the Something-Something are also included in <ref type="table">Table 4</ref>. For a fair comparison, we use the center crop of size 224 Ã— 224 in all 8 segments, following the protocol used in TSM <ref type="bibr" target="#b17">[18]</ref>. Both TSN and TSM receive a significant performance boost after combined with the proposed TPN. While TSM has a relatively larger capacity compared to TSN, such consistent  improvements on both backbones clearly demonstrate the generality of TPN. Besides, on the leaderboard (dated on 04/10/2020), TPN with backbone of TSM-101 16f achieves 67.7% Top-1 accuracy, following the standard protocol i.e. full resolution of 2 clips.</p><p>Results on Epic-Kitchen. As shown in <ref type="table" target="#tab_4">Table 5</ref>, we compare TSN+TPN to two baselines on Epic-Kitchen, following the settings in <ref type="bibr" target="#b1">[2]</ref>. Consequently, a similar improvement is observed as in other datasets, especially on verb classification, which has an increase of 12.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Ablation studies for the components of TPN are conducted on Kinetics-400. Specifically, the I3D-50 backbone and the sparse sampling strategy (i.e. 8 Ã— 8) are adopted unless specified otherwise.</p><p>Which feature source contributes the most to the classfication? As is mentioned in Sec.3.1, there exist two alternative ways to collect features from the backbone network, namely single-depth and multi-depth. For the single-depth pyramid, the output of res5 is sampled along the temporal dimension at {1, 2, 4, 8} intervals respectively to construct a four-level feature pyramid. For the multi-depth pyramid, we choose three possible combinations as shown in <ref type="table" target="#tab_6">Table 6a</ref>. The parallel flow is adopted as the default option for feature aggregation. Hyper-parameters {Î± i } M i=1 for the multi-depth pyramid are chosen to match its shape with the single-depth pyramid. For example, if res4 and res5 are selected as feature sources, the hyper-parameters will be {4, 8}.</p><p>The results of using different feature sources are included in <ref type="table" target="#tab_6">Table 6a</ref>, which suggests that the performance of TPN will  drop when we take features from relatively shallow sources e.g. res2 or res3. Intuitively there are two related factors: 1) different from object detection where the low-level features contribute to the position regression, action recognition mainly relies on high-level semantics. 2) Another factor might be that the I3D backbone <ref type="bibr" target="#b4">[5]</ref> only inflates the convolutions in the blocks of res4 and res5, so that both res2 and res3 is unable to capture useful temporal information. Unfortunately, inflating all 2D convolutions in the backbone will increase the computational complexity significantly and damage the performance as reported in <ref type="bibr" target="#b4">[5]</ref>. Compared to the multi-depth pyramid, the single-depth pyramid extracts various tempo representations by directly sampling from a single source. Although improvement is also observed, representing video semantics only at a single spatial granularity may be insufficient.</p><p>How important are the information flows? In Sec.3.2, several information flows are introduced for feature aggregation. How important are spatial semantic modulation and temporal rate modulation? The spatial semantic modulation and the temporal rate modulation are respectively introduced to overcome the semantic inconsistency in spatial dimensions and to adjust the relative rates of different levels in the temporal dimension. The effect of these two modulations are studied in <ref type="table" target="#tab_6">Table 6b</ref>, from which we observe: 1) TPN with all the components lead to the best result. 2) if the spatial semantic modulation contains no spatial convolutions, we have to up/down-sample the features of TPN simultaneously at spatial and temporal dimensions, which is ineffective for temporal feature aggregation.</p><p>How important is the number of input frames? While we use 8 frames sampled at the stride of 8 as the default input in our study experiments, we have also investigated different sample schemes. We denote T Ã— Ï„ as T frames sampled with the stride of Ï„ . And in <ref type="table" target="#tab_6">Table 6d</ref>, we include results of both I3D-50 and I3D-101 with inputs obtained by different sample schemes. Consequently, compared to the sparser sampling scheme (8Ã—8), the denser sampling scheme (32Ã—2) tends to bring it both rich and redundant information, leading to a slight over-fitting of I3D-50. I3D-50 + TPN, however, does not encounter such an over-fitting, obtaining an increase of 2%. Moreover, consistent improvements are observed for the stronger backbone I3D-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Empirical Analysis</head><p>To verify whether TPN has captured the variance of visual tempos, several empirical analyses are conducted on TPN.</p><p>Per-class performance gain vs. per-class variance of visual tempos. At first, we have to measure the variance of visual tempos for a set of action instances. Unlike the concept of scale in object detection, it is non-trivial to precisely compute the visual tempo of an action instance. Therefore, we propose a model-based measurement that utilizes the Full Width at Half Maximum (FWHM) of the frame-wise classification probability curve. FWHM is defined by the difference between the two points of a variable where its value is equal to half of its maximum value. We use a trained 2D TSN to collect per-frame classification probabilities for action instances in the validation set, and compute the FWHM for each instance as a measurement of its visual tempo, since when the sampling fps is fixed, a large FWHM intuitively means the action is going with a slow tempo, vice versa. We thus could compute the variance of visual tempos for each action category. The bottom in <ref type="figure" target="#fig_0">Figure 1</ref> shows the variances of visual tempos of all action categories, which reveals that not only the variance of visual tempos is large for some categories, different categories also have significantly different variances of visual tempos.</p><p>Subsequently, we also estimate the correlations between per-class performance gains when adopting a TPN module and per-class variances of visual tempos. We at first smooth the bar chart in <ref type="figure" target="#fig_0">Figure 1</ref> by dividing them into bins with an interval of 10. We then calculate the mean of performance gains in each bin. Finally, the statistics of all bins is shown in <ref type="figure">Figure 4</ref>, where performance gain is positively correlated with variance of visual tempos. This study strongly supports our motivation that TPN could bring a significant improvement for such actions with large variances of visual tempo.</p><p>Robustness of TPN to visual tempo variation. Human recognizes actions easily in spite of the large variance of the visual tempos. Does the proposed TPN module also possess such robustness? To study this, we at first train a I3D-50 + TPN on Kinetics-400 <ref type="bibr" target="#b0">[1]</ref> with 8 Ã— 8 (T Ã— Ï„ ) frames as the input. We then re-scale the original 8 Ã— 8 input by re-sample the frames with stride Ï„ equals to {2, 4, 6, 10, 12, 14, 16} respectively, so that we are adjusting the visual tempo of a given action instance. For instance, when feeding frames sampled as 8 Ã— 16 or 8 Ã— 2 into the trained I3D-50 + TPN, we are essentially speeding up / slowing down the original action instance since the temporal scope increases/decreases relatively. <ref type="figure">Figure 5</ref> includes the accuracy curves of varying  visual tempos for I3D-50 and I3D-50 + TPN, from which we can see TPN help improve the robustness of I3D-50, resulting in a curve with moderator fluctuations. Moreover, the robustness to the visual tempo variation becomes clearer as we vary the visual tempo harder, as TPN could adapt itself dynamically according to the need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a generic module called Temporal Pyramid Network is proposed to capture the visual tempos of action instances. Our TPN, as a feature-level pyramid, can be applied to existing 2D/3D architectures in the plug-andplay manner, bringing consistent improvements. Empirical analyses reveal the effectiveness of TPN, supporting our motivation and design. We will extend TPN for other video understanding tasks in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visual tempo variation of intra-and inter-class. The action examples above show that people tend to act at different tempos even for the same action. The plot below shows different action categories sorted by their variances of visual tempos. Specifically Somersaulting has the largest variance in the visual tempo of its instances while Shearing sheep has the smallest variance. Details of variation measurements can be found in the experiment section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Framework of TPN: Backbone Network to extract multiple level features. Spatial Semantic Modulation spatially downsamples features to align semantics. Temporal Rate Modulation temporally downsamples features to adjust relative tempo among levels. Information Flow aggregates features in various directions to enhance and enrich level-wise representations. Final Prediction rescales and concatenates all levels of pyramid along channel dimension. Note that the channel dimensions in Final Prediction and corresponding operations are omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Information Flow: Black arrows illustrate the aggregation directions while the orange arrows denote the IO stream from Temporal Modulation to Final Prediction of Figure 2. The channel dimensions and up/downsample operations are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Performance Gain vs. Variance of Visual Tempos. Each red point denotes the mean accuracy gain within a bin of variance. The blue line is the result of least squares approximation. Robustness to Variance of Visual Tempos. Both baseline and TPN models are trained on 8 Ã— 8 frames. The red line pictures the performance drop of baseline with TPN While the blue dash line denotes that of baseline only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other state-of-the-art methods on the validation set of Kinetics-400. Note that R50 and R101 denote the backbone networks and their depth respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">Frames Flow Top-1 Top-5</cell></row><row><cell>R(2+1)D [28]</cell><cell>16</cell><cell>73.9</cell><cell>90.9</cell></row><row><cell>I3D [1]</cell><cell>16</cell><cell>71.6</cell><cell>90.0</cell></row><row><cell>Two-Stream I3D [1]</cell><cell>64</cell><cell>75.7</cell><cell>92.0</cell></row><row><cell>S3D-G [34]</cell><cell>64</cell><cell>77.2</cell><cell>93.0</cell></row><row><cell>STC-X101 [4]</cell><cell>32</cell><cell>68.7</cell><cell>88.5</cell></row><row><cell>Nonlocal-R50 [32]</cell><cell>32</cell><cell>76.5</cell><cell>92.6</cell></row><row><cell>Nonlocal-R101 [32]</cell><cell>32</cell><cell>77.7</cell><cell>93.3</cell></row><row><cell>SlowFast-R50 [5]</cell><cell>32</cell><cell>77.0</cell><cell>92.6</cell></row><row><cell>SlowFast-R101 [5]</cell><cell>32</cell><cell>77.9</cell><cell>93.2</cell></row><row><cell>CSN-101 [27]</cell><cell>32</cell><cell>76.7</cell><cell>92.3</cell></row><row><cell>CSN-152 [27]</cell><cell>32</cell><cell>77.8</cell><cell>92.8</cell></row><row><cell>TPN-R50</cell><cell>32 Ã— 2</cell><cell>77.7</cell><cell>93.3</cell></row><row><cell>TPN-R101</cell><cell>32 Ã— 2</cell><cell>78.9</cell><cell>93.9</cell></row><row><cell>Backbone</cell><cell>Segments</cell><cell>Testing</cell><cell>Top-1</cell></row><row><cell>TSN-50 from [18]</cell><cell>8</cell><cell>ten-crop</cell><cell>69.9</cell></row><row><cell>TSM-50 from [18]</cell><cell>8</cell><cell>ten-crop</cell><cell>72.8</cell></row><row><cell>TSN-50 + TPN</cell><cell>8</cell><cell>ten-crop</cell><cell>73.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results on the validation set of Epic-Kitchen. TSN is equipped with TPN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Possible Sources: None denotes the I3D baseline with the depth of 50. res{i} means features are collected from the i-th stage in ResNet<ref type="bibr" target="#b11">[12]</ref>. Specially, res{5} takes the single-level pyramid as 3.1.</figDesc><table><row><cell cols="8">Possible Sources None res{2, 3, 4, 5} res{3, 4, 5} res{4, 5} res{5} (a) Head Top-1 Top-5 74.9 91.9 74.6 91.8 74.9 92.1 76.1 92.5 75.7 92.3 (b) Ablation Study on TPN components: We gradually add auxiliary Spatial Temporal Flow Top-1 74.9 74.6 75.2 75.4 76.1 75.9 75.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">head (Head), spatial convolutions in semantic modulation (Spatial),</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">temporal rate modulation (Temporal) and information flow (Flow) onto</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">the baseline model.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Information Flow</cell><cell>Top-1</cell><cell>Top-5</cell><cell cols="5">Backbone T Ã— Ï„ w/o TPN w/ TPN âˆ† Acc</cell></row><row><cell>Isolation</cell><cell>75.4</cell><cell>92.4</cell><cell></cell><cell>8Ã—8</cell><cell>74.9</cell><cell>76.1</cell><cell>+1.2</cell></row><row><cell>Bottom-up</cell><cell>75.8</cell><cell>92.3</cell><cell>I3D-50</cell><cell>16Ã—4</cell><cell>76.1</cell><cell>77.3</cell><cell>+1.2</cell></row><row><cell>Top-down</cell><cell>75.7</cell><cell>92.3</cell><cell></cell><cell>32Ã—2</cell><cell>75.7</cell><cell>77.7</cell><cell>+2.0</cell></row><row><cell>Cascade</cell><cell>75.9</cell><cell>92.3</cell><cell></cell><cell>8Ã—8</cell><cell>76.0</cell><cell>77.2</cell><cell>+1.2</cell></row><row><cell>Parallel</cell><cell>76.1</cell><cell>92.5</cell><cell>I3D-101</cell><cell>16Ã—4</cell><cell>77.0</cell><cell>78.1</cell><cell>+1.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>32Ã—2</cell><cell>77.4</cell><cell>78.9</cell><cell>+1.5</cell></row><row><cell cols="3">(c) Information Flow: Accuracy of several TPN variants men-tioned in Sec 3.2 are shown. The hyper-parameters {Î± i } M i=1 is set</cell><cell cols="5">(d) Input Frames: Different number of frames are adopted to evaluate whether TPN has the consistent improvement.</cell></row><row><cell>as {4, 8}.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies on Kinetics-400. Backbone is I3D-50 and takes 8 Ã— 8 frames as input unless specified.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6c</head><label>6c</label><figDesc>lists the performances of different information flows, keeping other components of TPN unmodified. Surprisingly, TPN with the Isolation Flow also boosts the performance by 0.58%, indicating that under proper modulations, the features with different temporal receptive fields indeed could help action recognition, even they come from a single backbone network. TPN with the Parallel Flow obtains the best result, leading to a performance of 76.1%. The success of parallel flow suggests that lower-level features could be enhanced by higher-level features via the top-down flow for they have larger temporal receptive fields. The semantics of higher-level features could also be enriched by lower-level features via the bottom-up flow. More importantly, such two opposing information flows are not contradictive but complementary to each other.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported in part by CUHK Direct Grant and SenseTime Group Limited. We also thank Yue Zhao for the wonderful codebase and insightful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rollingunrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature intertwiners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Intra-and inter-action understanding via temporal action parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic temporal pyramid network: A closer look at multi-scale modeling for activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="712" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>2019. 5</idno>
		<ptr target="https://github.com/open-mmlab/mmaction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
