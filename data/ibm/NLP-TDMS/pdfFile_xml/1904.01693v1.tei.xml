<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multigrid Predictive Filter Flow for Unsupervised Learning on Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
							<email>skong2@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
							<email>fowlkes@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multigrid Predictive Filter Flow for Unsupervised Learning on Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>[Project Page]</term>
					<term>[Github]</term>
					<term>[Demo]</term>
					<term>[Slides]</term>
					<term>[Poster]</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce multigrid Predictive Filter Flow (mgPFF), a framework for unsupervised learning on videos. The mgPFF takes as input a pair of frames and outputs per-pixel filters to warp one frame to the other. Compared to optical flow used for warping frames, mgPFF is more powerful in modeling sub-pixel movement and dealing with corruption (e.g., motion blur). We develop a multigrid coarse-to-fine modeling strategy that avoids the requirement of learning large filters to capture large displacement. This allows us to train an extremely compact model (4.6MB) which operates in a progressive way over multiple resolutions with shared weights. We train mgPFF on unsupervised, freeform videos and show that mgPFF is able to not only estimate long-range flow for frame reconstruction and detect video shot transitions, but also readily amendable for video object segmentation and pose tracking, where it outperforms the state-of-the-art by a notable margin without bells and whistles. Moreover, owing to mgPFF's nature of per-pixel filter prediction, we have the unique opportunity to visualize how each pixel is evolving during solving these tasks, thus gaining better interpretability 1 .</p><p>2 By "free-form", we emphasize the videos are long (versus short synthetic ones <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>), raw and unlabeled, and do not contain either structural pattern (e.g., ego-motion videos [20, 10, 101]) or those with restricted background <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b74">75]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Videos contain rich information for humans to understand the scene and interpret the world. However, providing detailed per-frame ground-truth labels is challenging for large-scale video datasets, prompting work on leveraging weak supervision such as video-level labels to learn visual features for various tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>. Video constrained to contain primarily ego-motion has also been leveraged for unsupervised learning of stereo, depth, odometry, and optical flow <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b102">103]</ref>.</p><p>Cognitively, a newborn baby can easily track an ob-ject without understanding any high-level semantics by watching the ambient environment for only one month <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b88">89]</ref>. However, until recently very few work has demonstrated effective unsupervised learning on free-form videos <ref type="bibr" target="#b1">2</ref> . For example, Wei et al. exploit the physicsinspired observation called arrow of time <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b21">22]</ref> to learn features by predicting whether frames come with the correct temporal order <ref type="bibr" target="#b95">[96]</ref>, and show the features are useful in action classification and video forensic analysis. Vondrick et al. use video colorization as a proxy task and show that the learned features capture objects and parts which are useful for tracking objects <ref type="bibr" target="#b90">[91]</ref>.</p><p>In this paper we explore how to train on unsupervised, free-form videos for video object segmentation and tracking using a new framework we call multigrid Predictive Filter Flow (mgPFF), illustrated by the conceptual flowchart in <ref type="figure" target="#fig_0">Fig. 1</ref>. mgPFF makes direct, fine-grained predictions of how to reconstruct a video frame from pixels in the previous frame and is trained using simple photometric reconstruction error. We find these pixel-level flows are accurate enough to carry out high-level tasks such as video object segmentation and human pose.</p><p>A straightforward approach to learning a flow between frames is to employ a differentiable spatial transform (ST) layer (a.k.a grid sampling) <ref type="bibr" target="#b30">[31]</ref>, to output per-pixel coordinate offset for sampling pixels with bilinear interpolation and apply the transform to the frame to estimate photometric reconstruction error. This has been widely used in unsupervised optical flow learning <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b94">95]</ref>. However, we and others observe that unsupervised learning on freeform videos with a simple ST-layer is challenging. Detlefsen et al. give an excellent explanation on why it is hard to train with ST-layer in the supervised learning setup <ref type="bibr" target="#b82">[83]</ref>. Briefly, training with ST-layer requires the invertibility of the spatial transform which is not guaranteed during train-ing. Additionally, we note that fixed grids for sampling (usually 2x2 for bilinear interpolation) typically only provide meaningful gradients once the predicted flow is nearly correct (i.e., within 1 pixel of the correct flow). This necessitates training at a coarse scale first to provide a good initialization and avoid getting caught in bad local-minima.</p><p>Inspired by the conceptual framework Filter Flow <ref type="bibr" target="#b78">[79]</ref>, we propose to learn in the mgPFF framework per-pixel filters instead of the per-pixel offset as in the ST-layer. For each output pixel, we predict the weights of a filter kernel that when applied to the input frame reconstruct the output. Conceptually, we reverse the order of operations from the ST-layer. Rather than predicting an offset and then constructing filter weights (via bilinear interpolation), we directly predict filter weights which can vote for the offset vector. We observe that training this model is substantially easier since we get useful gradient information for all possible flow vectors rather than just those near the current prediction.</p><p>Since the filter-flow approach outputs per-pixel kernels during training, capturing large displacements is computationally expensive. We address this using a multigrid strategy <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b23">24]</ref> to approximate the kernel. Concretely, we run the model over multi-resolution inputs with a fixed filter size (11x11 used in this paper) and compose the filters generated at multiple scales to produce the final flow fields (detailed in Section 3.2 and illustrated by <ref type="figure" target="#fig_1">Fig. 2)</ref>. The model thus only outputs 11*11=121 per-pixel filter weights at each resolution scale (smaller than the channel dimension in modern CNN architectures). We further assume self-similarity across scales and learn only a single set of shared learned model weights. This makes our model quite efficient w.r.t running time and model size. As a result, our final (un-optimized) model is only 4.6MB in size and takes 0.1 seconds to process a pair of 256x256-pixel resolution images.</p><p>To summarize our contributions: (1) conceptually, we introduce a simple multigrid Predictive Filter Flow (mgPFF) framework allowing for unsupervised learning on free-form videos; (2) technically, we show the filter flow overcomes the limitation of spatial-transform layer and the multigrid strategy significantly reduces model size; (3) practically, we show through experiments that mgPFF substantially outperforms other state-of-the-art applications of unsupervised flow learning on challenging tasks including video object segmentation, human pose tracking and long-range flow prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Learning for Vision: Our work builds upon a flurry of recent work that trains visual models without human supervision. A common approach is to leverage the natural context in images and video for learning the visual features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64]</ref>, which can be transferred to down-stream tasks, such as object detection. Other approaches include interaction with an environment to learn visual features <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b96">97]</ref>, which is useful for applications in robotics. A related but different line of work explores how to learn geometric properties or cycle consistencies with self-supervision, for example for motion capture or correspondence <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b93">94]</ref>. Ours also develop an unsupervised model, but with the signal from temporal consistency between consecutive frames in free-form videos, without the requirement of synthetic data <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b28">29]</ref>. Unsupervised Learning on Free-Form Videos: Though there are a lot methods for unsupervised optical flow learning <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b94">95]</ref> on videos (either synthetic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> or structured <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>), there is very few work about unsupervised learning on free-form videos: <ref type="bibr" target="#b91">[92]</ref> uses an offline tracker to provide signal to guide feature learning; <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b15">16]</ref> learn to verify whether frames come with the correct order, and transfer the feature to action classification; <ref type="bibr" target="#b63">[64]</ref> learns for region segmentation on image by considering the moving pattern of rigid objects; <ref type="bibr" target="#b90">[91]</ref> learns for video colorization and shows that the learned features capture object or parts which are useful for object tracking; <ref type="bibr" target="#b93">[94]</ref> learns correspondence at patch level on videos with reconstruction between frames. Filter Flow <ref type="bibr" target="#b78">[79]</ref> is a powerful framework which models a wide range of low-level vision problems as estimating a spatially varying linear filter. This includes tasks such as optical flow <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b101">102]</ref>, deconvolution <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b26">27]</ref>, nonrigid morphing <ref type="bibr" target="#b56">[57]</ref>, stereo <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b48">49]</ref> defocus <ref type="bibr" target="#b44">[45]</ref>, affine alignment <ref type="bibr" target="#b41">[42]</ref>, blur removal <ref type="bibr" target="#b25">[26]</ref>, etc. However, as it requires an optimization-based solver, it is very computationally expensive, requiring several hours to compute filters for a pair of medium-size images <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b71">72]</ref>. Kong and Fowlkes propose Predictive Filter Flow, which learns to predict perpixel filters with a CNN conditioned on a single input image to solve various low-level image reconstruction tasks <ref type="bibr">[39]</ref>. There are other methods embracing the idea of predicting per-pixel filters, e.g., <ref type="bibr" target="#b51">[52]</ref> and <ref type="bibr" target="#b58">[59]</ref> do so for solving burst denoising and video frame interpolation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multigrid Predictive Filter Flow</head><p>Our multigrid Predictive Filter Flow (mgPFF) is rooted in the Filter Flow framework <ref type="bibr" target="#b78">[79]</ref>, which models the image transformations I B → I A as a linear mapping where each pixel in I A only depends on the local neighborhood centered at same place in I B . Finding such a flow of per-pixel filter can be framed as solving a constrained linear system</p><formula xml:id="formula_0">I A = T B→A · I B , T B→A ∈ Γ.<label>(1)</label></formula><p>where T B→A is a matrix whose rows act separately on a vectorized version of the source image I B . T B→A ∈ Γ serves as a placeholder for the entire set of additional constraints on the operator which enables a unique solution that satisfies our expectations for particular problems of interest. For example, standard convolution corresponds to T B→A being a circulant matrix whose rows are cyclic permutations of a single set of filter weights which are typically constrained to have compact localized non-zero support. For a theoretical perspective, Filter Flow model 1 is simple and elegant, but directly solving Eq. 1 is intractable for image sizes we typically encounter in practice, particularly when the filters are allowed to vary spatially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Predictive Filter Flow (PFF) on Video</head><p>Instead of optimizing over T, Kong and Fowlkes propose the Predictive Filter Flow (PFF) framework that learns function f w (·) parameterized by w that predicts the transformation T specific to image I B taken as input [39]:</p><formula xml:id="formula_1">I A ≈ T B→A · I B , T B→A ≡ f w (I B ),<label>(2)</label></formula><p>The function f w (·) is learned with a CNN model under the assumption that (I A , I B ) are drawn from some fixed joint distribution. Therefore, given sampled image pairs,</p><formula xml:id="formula_2">{(I i A , I i B )}, where i = 1, .</formula><p>. . , N , we can learn parameters w that minimize the difference between a recovered imagê I A and the real one I A measured by some loss .</p><p>In this work, to tailor the PFF idea to unsupervised learning on videos, under the same assumption that (I A , I B ) are drawn from some fixed joint distribution, we can have the predictable transform T B→A ≡ f w (I B , I A ), parametrized by w. To learn the function f w (·), we use the Charbonnier function <ref type="bibr" target="#b5">[6]</ref> to measure the pixel-level reconstruction error, defined as φ(s) = √ s 2 + 0.001 2 , and learn parameters w by minimizing the following objective function: Note that the above loss can take image pairs in different order simply by concatenating the pixel embedding features from the two frames one over another, as demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. After concatenation, we train a few more layers to produce the per-pixel filters. Also note that, when exploiting the locality constraints (similar to convolution), we implement the operation T B→A · I B with the "im2col" function which vectorizes the local neighborhood patch centered at each pixel and computes the inner product of this vector with the corresponding predicted filter. Note that "im2col" and the follow-up inner product are highly optimized for available hardware architectures in most deep learning libraries, exactly the same used in modern convolution operation; thus our model is quite efficient in computation.</p><formula xml:id="formula_3">rec (I B , I A ) = φ(I A − T B→A · I B ),<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multigrid PFF</head><p>While the PFF described above is elegant and simple for unsupervised learning over videos, it faces the substantial challenge that, to capture large displacement, one must predict per-pixel filters with very large spatial support. To address this problem, we are inspired by the multigrid strategy which seeks to solve high-dimensional systems of equations using hierarchical, multiscale discretizations of linear operators <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b23">24]</ref>, to produce a coarse-to-fine series of smaller, more easily solved problems.</p><p>To explain this, mathematically, suppose we have filter flow T in original resolution that maps from X to Y, i.e. Y = T · X. Then if we downsample X and Y by half, we</p><formula xml:id="formula_4">have D 1 2 Y =D 1 2 T · X ≈ (D 1 2 T) · (U 2× D 1 2 X),<label>(4)</label></formula><p>where the upsampling U 2× and downsampling D 1 2 operators are approximately inverse to each other. Then we write a reduced system:</p><formula xml:id="formula_5">Y 1 2 ≈ (D 1 2 TU 2× ) · (D 1 2 X) = T 1 2 X 1 2 (5)</formula><p>The above derivation implies we can solve a smaller system for T 1 2 on the input X 1 2 , e.g., an image with half the resolution and then upsample T 1 2 to get an approximate solution to the original problem.</p><p>In practice, to avoid assembling the full resolution T, we always represent it as a composition of residual transformations at each scale.</p><formula xml:id="formula_6">T = T 1 ·U 2× ·T 1 2 . . . U 2× ·T 1 2 L , where T 1</formula><p>2 l is estimated filter flow over frames at resolution scale 1/2 l−1 . In our work, we set L=5. Each individual transformation has a fixed filter support (sparse). By construction, the effective filter "sizes" grow spatially larger as it goes up in the pyramid but the same filter weight is simply applied to larger area (we use nearest-neighbor upsampling). Then the total number of filter coefficients to be predicted for the pyramid would be just 4/3 more than just the finest level (ref. geometric series 4/3 = 1 + 1</p><formula xml:id="formula_7">2 2 + 1 4 2 + 1 8 2 + . . . )</formula><p>. Concretely, suppose we need the kernel size as 80x80 to capture large displacement, we can work on coarse scale of 8x smaller input region with kernel size 11x11, this will reflect on the original image of receptive field as large as 88x88. But merely working on such coarse scale introduces checkerboard effect if we resize the filters 8x larger. Therefore, we let the model progressively generate a series of 11x11 filters at smaller scales of [8x,4x,2x,1x], as demonstrated by <ref type="figure" target="#fig_1">Fig. 2</ref>. Finally, we can accumulate all the generated filter flows towards the single map, which can be a long-range flow (studied in Section 4.4). We train our system with the same model at all these scales. We have also trained scale-specific models, but we do not observe any obvious improvements in our experiments. We conjecture that in diverse, free-form videos there is substantial selfsimilarity in the (residual) flow across scales.</p><p>We note that coarse-to-fine estimation of residual motion is a classic approach to estimating optical flow (see, e.g. <ref type="bibr" target="#b16">[17]</ref>). It has also been used to handle problems of temporal aliasing <ref type="bibr" target="#b80">[81]</ref> and as a technique for imposing a prior smoothness constraint <ref type="bibr" target="#b84">[85]</ref>. Framing flow as a linear operator draws a close connection to multigrid methods in numerical analysis <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b23">24]</ref>. However, in literature there is primarily focused on solving for X where the residuals are additive, rather than T where the residuals are naturally multiplicative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Imposing Constraints and Training Loss</head><p>We note that training with above reconstruction loss alone gives very good reconstruction performance, but we need other constraints to regularize training to make it work on video segmentation and tracking. Now we describe useful constraints used in this work. Non-negativity and Sum-to-One With the PFF framework, it is straightforward to impose the non-negativity and sum-to-one constraints by using the softmax layer to output the per-pixel filters, as softmax operation on the kernels naturally provides a transformation on the weights into the range of [0,1], and sum-to-one constraint mimics the brightness constancy assumption of optical flow. Warping with Flow Vector In order to encourage the estimated filter kernels to behave like optical flow (i.e., a translated delta function) we define a projection of the filter weights on to the best approximate flow vector by treating the (positive) weights as a distribution and computing an expectation. Given a filter flow T we define the nearest optical flow as</p><formula xml:id="formula_8">F(T) ≡ v x (i, j) v y (i, j) = x,y T ij,xy x − i y − j<label>(6)</label></formula><p>As discussed in Section 1, directly learning to predict F is difficult but when keep T as an intermediate representation, learning becomes much easier. To encourage predicted T towards a unimodal offset, we add a loss term based on the optical flow F with grid sampling layer just as done in literature of unsupervised optical flow learning <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b31">32]</ref>. We denote the loss terms as f low (I B , I A ) meaning the reconstruction loss computed by warping with optical flow F(T B→A ) from I B to I A . Forward-Backward Flow Consistency As we know, there are many solutions to the reconstruction problem. To constrain this for more robust learning, we adopt a forwardbackward consistency constraint as below:</p><formula xml:id="formula_9">f b (f , b) ≡ 1 |I| i∈I φ(p i − b(f (p i )))<label>(7)</label></formula><p>where forward and backward flow are f = F(T B→A ) and b = F(T A→B ), and p i ≡ [x i , y i ] T is the spatial coordinate. We note that such constraint is useful for addressing the chicken-and-egg problem related to optical flow and occlusion/disocclusion <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50]</ref>. But here we do not threshold the consistency error to find occlusion regions or ignore the errors in the region. We note that it is crucial to train the mgPFF model with this constraint when applying the model later for video segmentation and tracking; otherwise pixels in the object would diffuse to the background easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smoothness and Sparsity</head><p>Smoothness constraints can be done easily using traditional penalties on the norm of the flow field gradient, i.e. sm ≡ F(T) 1 . The smoothness penalty helps avoid big transitions on flow field, especially at coarse scales where very few big flows are expected. The sparsity constraint is imposed on the flow field as well with L1 norm, i.e. sp ≡ F(T) <ref type="bibr" target="#b0">1</ref> . This forces the model not to output too many abrupt flows especially at finer scales. Our overall loss for training mgPFF model minimizes the following combination of the terms across multiple scales l = 1 . . . , L:</p><formula xml:id="formula_10">min w L l=1 rec(I l B , I l A ) + λ f l · f l (I l B , I l A ) +λ f b · f b (f l , b l ) + λsm · sm(f l ) + λsp · sp(f l ) s.t. T l B→A = fw(I l B , I l A ), T l A→B = fw(I l A , I l B ), f l = F(T l B→A ), b l = F(T l A→B ).<label>(8)</label></formula><p>For simplicity, we only write the losses involving flow from B to A; in practice, we also include those A to B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation and Training</head><p>Our basic framework is largely agnostic to the choice of architectures. In this paper, we modify the ResNet18 <ref type="bibr" target="#b24">[25]</ref> by removing res 4 and res 5 (the top 9 residual blocks, see appendix) and reducing the unique channel size from <ref type="bibr" target="#b63">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256</ref>, 512] to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">196]</ref>. We also add in bilateral connection and upsampling layers to make it a U-shape <ref type="bibr" target="#b76">[77]</ref>, whose output is at the original resolution. Furthermore, we build another shallow stream but in fullresolution manner with batch normalization <ref type="bibr" target="#b29">[30]</ref> between a convolution layer and ReLU layer <ref type="bibr" target="#b55">[56]</ref> that learns to take care of aliasing effect caused by pooling layers in the first steam. We note that our mgPFF is very compact that the overall model size is only 4.6MB; it also performs fast that the wall-clock time for processing a pair of 256x256 frames is 0.1 seconds. Two-stream architecture is popular in multiple domain learning <ref type="bibr" target="#b81">[82]</ref>, but we note that such design on a single domain was first used in <ref type="bibr" target="#b67">[68]</ref> which is more computationally expensive that the two streams talk to each other along the whole network flow; whereas ours is cheaper that they only talk at the top layer. We note that our architecture is different from FlowNetS and FlowNetC <ref type="bibr" target="#b13">[14]</ref> in that, 1) unlike FlowNetS, ours produces pixel embedding features for each of frame, which can be potentially transferred to other tasks (though we did not explore this under the scope of this paper); 2) unlike FlowNetC, ours does not exploit the computationally expensive correlation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments to show the mgPFF can be trained in an unsupervised learning fashion on diverse, freeform videos, and applicable to addressing challenging tasks including video object segmentation, pose tracking and long-range flow learning in terms of frame reconstruction. By training our mgPFF on the Sintel movie, we can detect the transition shot purely based on the reconstruction error. This helps develop a stage-wise training that we train mgPFF first on the whole movie, and then simply threshold the reconstruction errors for shot detection and get discrete groups for finer training.</p><p>We also visualize how each pixel evolves during solving these problems to gain better interpretability of the model.</p><p>We evaluate our mgPFF model on the challenging video propagation tasks: DAVIS2017 <ref type="bibr" target="#b68">[69]</ref> for video object segmentation and long-range flow learning in terms of frame reconstruction, and JHMDB dataset <ref type="bibr" target="#b34">[35]</ref> for human pose tracking. Compared methods include the simplistic identity mapping (always copying the first frame labels), SIFT flow <ref type="bibr" target="#b45">[46]</ref> which is an off-the-shelf toolbox for dense correspondence alignment, learning-based optical flow (FlowNet2) <ref type="bibr" target="#b28">[29]</ref> which is trained on large-scale synthetic data, DeepCluster <ref type="bibr" target="#b8">[9]</ref> which is unsupervised trained for clustering on Ima-geNet <ref type="bibr" target="#b10">[11]</ref>, ColorPointer <ref type="bibr" target="#b90">[91]</ref> which learns video colorization and shows effective in object tracking, and Cycle-Time <ref type="bibr" target="#b93">[94]</ref> which exploits the cycle consistence along time and is trained for patch reconstruction with mid-level feature activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Training. We train our mgPFF model from scratch over a combined datasets consisting of the whole Sintel Movie <ref type="bibr" target="#b43">[44]</ref>, training set of DAVIS2017 <ref type="bibr" target="#b68">[69]</ref>, and training set of JHMDB (split1) <ref type="bibr" target="#b34">[35]</ref>. It is worth noting that our whole training set contains only ∼6×10 4 frames, whereas our compared methods train over orders magnitude larger dataset. For example, ColorPointer <ref type="bibr" target="#b90">[91]</ref> is trained over 300K videos (∼9×10 7 frames) from Kinetics dataset <ref type="bibr" target="#b35">[36]</ref>, and CycleTime <ref type="bibr" target="#b93">[94]</ref> is trained over 114K videos (344-hour recording, ∼3.7×10 7 frames) from VLOG dataset <ref type="bibr" target="#b17">[18]</ref>. Moreover, most interestingly, in training our mgPFF on the Sintel movie, we find mgPFF automatically learns to detect the video shot/transition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> purely based on the reconstruction errors between input frames (see <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><p>We use ADAM optimization method during training <ref type="bibr" target="#b37">[38]</ref>, with initial learning 0.0005 and coefficients 0.9 and 0.999 for computing running averages of gradient and its square. We randomly initialize the weights and train from scratch over free form videos. We train our model using PyTorch <ref type="bibr" target="#b62">[63]</ref> on a single NVIDIA TITAN X GPU, and terminate after 500K iteration updates. <ref type="bibr" target="#b2">3</ref> During training, we randomly sample frame pairs (resized to 256×256pixel resolution) within N =5 consecutive frames. We also augment the training set by randomly flipping and rotating the frame pairs. After training the model on the combined dataset, we train specifically over the training set (without annotation) of DAVIS2017 and JHMDB respectively for video object segmentation and human pose tracking.</p><p>Inference. We essentially propagate the given mask/pose at the first frame along the time. We also set the temporal window size K, meaning we warp towards the target frame using previous K frames. We test different temporal window size for video segmentation and tracking and find K=3 works the best. Specifically, for video object segmentation on DAVIS2017, we threshold with 0.8 the propagated mask at each tracking update, since pixels on the foreground (within the mask) may diffuse to background, and filter flow gives probabilities around the mask boundary. For human pose tracking, we dilate the joints for propagation, and vote for the tracked joint after propagation as the track. This gives stable tracking though sometimes the track may stay at the background especially when the background is similar to the foreground (3rd video in <ref type="figure">Fig. 5</ref>). We note that there are other methods using low-level cues for higher-level tasks, e.g., using boundary for semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Unsupervised Learning for Video Segmentation</head><p>We analyze our model on video segmentation over the DAVIS 2017 validation set <ref type="bibr" target="#b68">[69]</ref>, where the initial segmentation mask is given and the task is to predict the segmentation in the rest of the video. This is a very challenging task as the videos contain multiple objects that undergo significant occlusion, deformation and scale change with clutter background, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We use the provided code and report two metrics that score segment overlap and boundary accuracy. The Jacaard index J is defined as the intersection-over-union of the estimated segmentation and the ground-truth mask, measuring how well the pixels of two masks match <ref type="bibr" target="#b14">[15]</ref>. The J recall measures the fraction of sequences with IoU&gt;0.5. The F-measure denoted by F considers both contour-based precision and recall that measure the accuracy of the segment contours <ref type="bibr" target="#b47">[48]</ref>.</p><p>We compare our mgPFF with other unsupervised methods as well as some supervised ones <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b7">8]</ref> in <ref type="table" target="#tab_0">Table 1</ref>. The first two supervised methods are trained explicitly using the annotated masks along with training video frames. As in literature there are methods always using the given mask at the first frame to aid tracking, we also follow this practice with mgPFF to report the performance. But before doing so, we ablate how much gain we can get from using only the given mask for the tracking. To this end, we setup the mgPFF by always propagating the given mask for tracking, as noted by mgPFF (1st only) in <ref type="table" target="#tab_0">Table 1</ref>. Surprisingly, this simple setup works very well, even better than flow based methods, such as SIFTflow 1st and FlowNet2 1st , both of which not only use the first frame but also the previous N =4 frames for tracking. This suggests the mgPFF is able to capture longrange flow even though we did not train our model with frames across large intervals. We explicitly study this longrange flow in Section 4.4 quantitatively.</p><p>When we perform tracking with the only one previous propagated mask (K=1), our mgPFF outperforms all the other unsupervised methods, except CycleTime (on J measure only), which is explicitly trained at patch level thus captures better object segment. When additionally using the mask given at the first frame for tracking in subsequent frames, mgPFF 1st (K=1) outperforms all other unsupervised methods by a notable margin, and our mgPFF 1st (K=3) achieves the best performance. In particuar, in terms of the boundary measure, we can see mgPFF performs significantly better than the other unsupervised methods. This demonstrates the benefit of propagating masks with fine-grained pixel-level flows instead of flows learned at patch level through mid-level feature activations <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b93">94]</ref>.</p><p>Overall, we note that our mgPFF even outperforms sev- eral supervised methods, but only worse than the first two supervised models in <ref type="table" target="#tab_0">Table 1</ref> which are explicitly trained with DAVIS pixel-level annotations at all training frames. Moreover, it is worth noting that our mgPFF model is trained over two orders magnitude less data than other unsupervised methods, e.g., DeepCluster, ColorPointer and CycleTime. This demonstrates the benefit brought by the low-vision nature of mgPFF that it does not demand very large-scale training data. In <ref type="figure" target="#fig_3">Fig. 4</ref>, we visualize the tracking results (N =3) and the predicted filter flow (from previous one frame only). Specifically, we transform the filter flow into the flow vector (Eq. 6) and treat this as optical flow for visualization. As mgPFF performs at pixel level, we are able to visualize the tracking through more fine-grained details. We paint on the mask with the color chart from optical flow, and visualize to see how the pixels evolve over time. Interestingly, from this visualization, we can see how tracking is accomplished in front of heavy occlusion, big deformation and similar background situation (see descriptions under <ref type="figure" target="#fig_3">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised Learning for Pose Tracking</head><p>We validate our mgPFF for human pose tracking on the JHMDB dataset <ref type="bibr" target="#b34">[35]</ref>. During testing, we are given an initial frame labeled with 15 human joints and the task is to predict the joints in the subsequent frames. To this end, we stack the 15 maps for the 15 joints as a 3D array, and propagate the array using the predicted filter flow. To report performance, we use the Probability of Correct Keypoint (PCK@τ ) from <ref type="bibr" target="#b99">[100]</ref>, which measures the portion of predicted points that are within a radius to their groundtruth, where the radius is τ times the size of the human pose bounding box.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we list the performance of different unsupervised learning methods, and report two setups of the mgPFF on the validation set (split1): 1) with the model trained on the combined dataset, and 2) with the model further finetuned on JHMDB in an unsupervised way <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b26">27]</ref>. Similar to video segmentation, without using the provided joints at the first frame for all subsequent tracking, our mgPFF outperforms all other methods except CycleTime which always uses the first frame (with the provided keypoints) for pose tracking. By fine-tuning our model on the videos of this dataset (without using the joint annotations), we obtain further improvement; but the improvement is less than additionally using the first frame for tracking. We conjecture the reason is that by using the provided mask at the first frame, mgPFF is able to warp all the available joints toward current frame; otherwise it may lose track once the joints move outside the image (see 2nd video in <ref type="figure">Fig. 5</ref>). It is worth noting that mgPFF as well as the learning based optical flow method performs fast in propagating the joints for tracking, whereas DeepCluster, ColorPointer and CycleTime require computing affinity matrix over all pixels from previous K frames <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b93">94]</ref>. Moreover, although it seems unfair to compare our mgPFF with unsupervised fine-tuning on the same JHMDB dataset, we note that ColorPointer and CycleTime are trained on much larger dataset consisting mainly of human actions/activities. In <ref type="figure">Fig. 5</ref>, we visualize the pose tracking results as well as the filter flow and how each pixel along the skeleton evolves over time. We plot in last row the frames on which our mgPFF starts to fail in tracking. The failure cases are largely due to challenging situations, like heavy occlusion (1st video), joint moving outside the image, similar background (3rd video) and big motion blur (4th video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Long-Range Flow for Frame Reconstruction</head><p>We highlight our mgPFF is particularly good at learning long-range flow for reconstructing frames. To validate this, specifically, given two frames I t and I t+m distant in time in a video, we predict the filter flow between them, and then <ref type="table">Table 3</ref>: Long-range flow for frame Reconstruction: We compute the long-range flow on two frames and warp the 1st one with the flow. We compare the warped frame with the 2nd frame measured by pixel-level L1 distance. The gaps are 5 and 10, respectively. We perform this experiment on DAVIS2017 validation set, and report the performance in <ref type="table">Table 3</ref>, in which we set the time gap as m=5 or m=10, meaning the two frames are m frames apart from each other. In both frame gaps, our mgPFF significantly outperforms the compared methods, demonstrating the powerfulness of mgPFF in modeling pixel level movement, even though our model is trained over frame pairs within 5-frame interval without seeing any frames far away from 5 frames. In <ref type="figure">Fig. 6</ref>, we clearly see that mgPFF performs quite well visually on long-range flow learning for frame reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a simple, compact framework for unsupervised learning on free-form videos, named multigrid Predictive Filter Flow (mgPFF). Through experiments, we show mgPFF outperforms other state-of-the-art methods notably in video object segmentation and human pose tracking with the unsupervised learning setup; it also exhibits great power in long-range flow learning in terms of frame re-construction. In this sense, it is reminiscent of a variety of other flow-based tasks, such as video compression <ref type="bibr" target="#b75">[76]</ref>, frame interpolation <ref type="bibr" target="#b59">[60]</ref>, unsupervised optical flow learning <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b31">32]</ref>, etc., which are all candidates for future extensions. Moreover, based on the filter flow output which is fast in computation, it is also interesting to use it for action classification where the flow stream consistently improves performance <ref type="bibr" target="#b81">[82]</ref> but optical flow estimation is slow. Finally the pixel embedding features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b95">96]</ref> could also be used as video frame representation for action classification <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b15">16]</ref>. We expect further improvement on these tasks by taking as mgPFF as initial proposal generation with followup mechanisms for fine video segmentation <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b63">64</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Intermediate Reconstruction by mgPFF</head><p>As our mgPFF performs progressively from coarse to fine, it produces the predicted filter flows and reconstruction frames at each resolution scale. We visualize all the intermediate results in <ref type="figure" target="#fig_5">Fig. 7</ref>. We also accumulate the filter flow maps at all scales and convert it into the coordinate flow, which can be thought as optical flow. We use this coordinate flow to warp masks for propagating the track results in our experiments. Please pay attention to how mgPFF achieves excellent reconstruction results from coarse to fine, like resolving the aliasing and block effects, refining reconstruction at finer scales, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Graph Visualization of mgPFF architecture</head><p>In <ref type="figure">Fig. 8</ref>, we plot the architecture of our model using the HiddenLayer toolbox <ref type="bibr" target="#b0">[1]</ref>. As the visualization is too "long" to display, we chop it into four parts. We modify the ResNet18 <ref type="bibr" target="#b24">[25]</ref> by removing res 4 and res 5 (the top 9 residual blocks, and reducing the unique channel size from <ref type="bibr" target="#b63">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref> to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">196]</ref>. The two macro towers take the two frames, respectively; in each tower, there are two streams, one is of U-shape <ref type="bibr" target="#b76">[77]</ref> with pooling and upsampling layers to increase the receptive fields, the other is full-resolution yet shallow in channel depth. The two-stream architecture is popular in multiple domain learning <ref type="bibr" target="#b81">[82]</ref>, but we note that such design on a single domain was first used in <ref type="bibr" target="#b67">[68]</ref> which is more computationally expensive that the two streams talk to each other along the whole network flow; whereas ours is cheaper that they only talk at the top layer. Our mgPFF is very compact that the overall model size is only 4.6MB; it also performs fast that the wall-clock time for processing a pair of 256x256 frames is 0.1 seconds.</p><p>As we did not search over architecture design in our work, tt is worth exploring other sophisticated modules to make it more compact for deploying in mobile devices, e.g., using meta-learning for architecture search <ref type="bibr" target="#b107">[108]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pixel Embedding in mgPFF</head><p>As our model produces per-image pixel embeddings <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b39">40]</ref> (the output before "concatenation layer" as shown in the architecture <ref type="figure">Fig. 8</ref>), we are interested in visualizing the pixel embeddings to see what the model learns. To visualize the pixel embeddings, we use PCA to project the embedding feature map H×W×D at each resolution/grid into an H×W×3 array, and visualize the projection as an RGB image. We also concatenate the embedding maps at all the resolutions/grids for visualization (with necessary nearest neighbor upsampling). <ref type="figure">Fig. 9</ref> lists these visualizations, from which we can see the embedding colors largely come from the original RGB intensities. We conjecture this is due to two reasons. First, we use a simplistic photometric loss on the RGB values, this explains why the visualization colors group the pixels which have similar RGB values in local neighborhood. Second, our mgPFF by nature is based on low-level vision, i.e., flow field, and in such a way it does not necessarily depend on mid/high-level understanding of the frames. Therefore, part/instance grouping does not appear in the embedding visualization, which is shown in midlevel methods <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b93">94]</ref>. This suggests further exploration of using other losses and combining other mid/high-level cues to force the model to learn more abstract features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Video Demos</head><p>The attached videos demonstrate how mgPFF performs with different setup 4 . Note how it improves performance with different setup in terms of dealing with occlusion and large displacement.</p><p>Among the videos, it is worth noting how far the model can go with tracking correctly. As we adopt the multigrid computing strategy, the filter of size 11x11 on the coarsest grid (16x downsample) implies the largest displacement we can represent is D=88. If we simply warp from the first frame to the t th frame, it only works well when the total displacement is less that D. This can be seen from video soccerball, K=1, frame- <ref type="bibr" target="#b0">[1]</ref> as an example. When the soccerball moves further than D from its initial location at the first frame, the model suddenly fails in tracking that the mask is no longer correctly warped. We show the relevant frames in <ref type="figure" target="#fig_0">Fig. 10</ref>. It is clear that not only the tracking is missing, but also the filter flow changes abruptly and the reconstruction becomes very different. It turns out that in the reconstruction, the soccerball's color is from the grass and tree trunk.</p><p>Here is the list of videos with brief description:</p><p>1. soccerball, K=3, frame-[1, t − 2, t − 1]: this video shows the results on soccerball from DAVIS dataset when we feed the first frame-1 and two previous <ref type="bibr" target="#b3">4</ref> Here is a Youtube list Bottom: we accumulate all the filter flows (with necessary upsampling using nearest neighbor interpolation), and transform into a coordinate flow which can be thought as optical flow. Then we use the overall flow to warp from one frame to the other. This introduces some artifacts due to information loss, but the reconstruction appears good generally, e.g., capturing the bird wings' movement. In our experiment of tracking, we use the coordinate flow in the same way to warp the given masks (or the predicted mask at previous frames) to propagate the track results.</p><p>frame (t − 2 and t − 1) to predict the filter flow, warp frame and track the object at current frame-t. (video url https://youtu.be/M49nLtT1UmY).</p><p>2. soccerball, K=3, frame-[t − 3, t − 2, t − 1]: this video shows the results on soccerball from DAVIS dataset when we feed the previous three frames (t − 3, t − 2 and t − 1) to predict the filter flow, warp frame and track the object at current frame-t (video url https://youtu.be/q_FNk-3lh3g).</p><p>3. soccerball, K=2, frame-[1, t − 1], this video shows the results on soccerball from DAVIS dataset when we feed the first frame-1 and one previous frame-(t − 1) to predict the filter flow, warp frame and track the object. (video url https://youtu.be/ u6IdVS2L7-M).</p><p>4. soccerball, K=1, frame- <ref type="bibr" target="#b0">[1]</ref>, this video shows the results on soccerball from DAVIS dataset when we feed the first frame only at which the mask is given to predict the filter flow, warp frame and track the object.</p><p>(video url https://youtu.be/vsXZgdR4XEY) <ref type="figure">Figure 8</ref>: Graph visualization of mgPFF architecture using HiddenLayer toolbox <ref type="bibr" target="#b0">[1]</ref>. Zoom in to see clearly. <ref type="bibr" target="#b4">5</ref>. soccerball, K=1, frame-[t − 1], this video shows the results on soccerball from DAVIS dataset when we feed the the previous frame-(t − 1) to predict the filter flow, warp frame and track the object at cur-rent frame-t. (video url https://youtu.be/ 8AZ9wPF15QE) 6. dog, K=3, frame-[1, t − 2, t − 1]: this video shows <ref type="figure">Figure 9</ref>: Visualization of learned pixel embedding: We use PCA to project the pixel embedding (3D array of size H×W×D) into H×W×3, and visualize it as an RGB image. Individual embedding map has D = 16 in channel dimension. We also concatenate the pixel embeddings of all resolutions and apply PCA, in which case D = 16 * 5 = 80. From the visualization, we can see that the visualization colors largely come from the RGB intensities. This is largely due to two reasons: 1) the photometric loss we are using during training is based on RGB intensities, 2) our mgPFF by nature is based on low-level vision that it does not need understanding of mid/high-level perspective of the frames.</p><p>the results on dog from DAVIS dataset when we feed the first frame-1 and two previous frame (t − 2 and t − 1) to predict the filter flow, warp frame and track the object at current frame-t. (video url https: //youtu.be/seg5tFSMFX8).</p><p>7. dog, K=3, frame-[t − 3, t − 2, t − 1]: this video shows the results on dog from DAVIS dataset when we feed the previous three frames (t − 3, t − 2 and t − 1) to predict the filter flow, warp frame and track the object at current frame-t (video url https://youtu.be/BqM4-OctYwA).</p><p>8. dog, K=2, frame-[1, t − 1], this video shows the results on dog from DAVIS dataset when we feed the first frame-1 and one previous frame-(t − 1) to predict the filter flow, warp frame and track the object.</p><p>(video url https://youtu.be/dOao8qQMsv0).</p><p>9. dog, K=1, frame- <ref type="bibr" target="#b0">[1]</ref>, this video shows the results on dog from DAVIS dataset when we feed the first frame only at which the mask is given to predict the filter flow, warp frame and track the object. (video url https://youtu.be/xNMuMlcvfJY) <ref type="bibr" target="#b9">10</ref>. dog, K=1, frame-[t − 1], this video shows the results on dog from DAVIS dataset when we feed the the previous frame-(t − 1) to predict the filter flow, warp frame and track the object at current frame-t. (video url https://youtu.be/Yu5amZf1KEc) <ref type="figure" target="#fig_0">Figure 10</ref>: How far the model can track the object correctly? As we adopt the multigrid computing strategy, the filter of size 11x11 on the coarsest grid (16x downsample) implies the largest displacement we can represent is D=88. If the object moves further than D from its last location, the model fails in tracking it. This happens at frame-10, in which we can see that not only the tracking is missing, but also the filter flow changes abruptly and the reconstruction becomes very different. It turns out that in the reconstruction, the soccerball's color is from the grass and tree trunk.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The flowchart of multigrid Predictive Filter Flow framework (mgPFF). Conceptually we draw a single scale for demonstrating how we train our model in an unsupervised way with the photometric reconstruction loss along with constraints imposed on the filter flow maps. The multigrid strategy is illustrated inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of how multigrid Predictive Filter Flow (mgPFF) performs progressively by warping images from one to the other at multiple resolution scales from coarse to fine. After the finest scale, one can accumulate all the intermediate filter flow maps for the final one, which can be either transformed into optical flow or used for video segmentation and tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Shot Detection arising from training on free-form videos:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of unsupervised learning for video segmentation on video from DAVIS2017: soccerball, dog and bear. We show the tracking results with temporal window size K=3 for soccerball (otherwise it loses track due to heavy occlusion) and K=1 for others. Note that in soccerball, there are heavy occlusions but our mgPFF model can still track the ball. In dog, we can see how each pixel moves along with the dog: when the dog turns from right side to left side, the colors from the neck are propagated for tracking. This demonstrates how mgPFF tracks each pixel in the physical manifold flavor. In bear, the disocclusion shadow arises from the bottom border of the image, connecting with the bear, then mgPFF propagates the bear leg to the shadow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Human Pose Tracking on JHMDB dataset. We show results by propagating only the previous mask (K=1), and overlay the tracked joints on the RGB frames. Besides, with the predicted filter flow, we also propagate the colorful skeleton to visualize how pixels on the skeleton evolve over time. In last row, we pick the results around the end of each video to show how mgPFF fails in tracking, mainly due to heavy occlusion (knees in the 1st video), joints moving outside the image (ankle in the 2nd video), similar background (hair color in the 3rd video), and motion blur (elbow in the 4th video). (Best viewed in color and zoom-in.) Long-range flow for frame reconstruction (rightmost column) by warping It (1st column) with the coordinate flow (2nd column) which is transformed from the predicted multigrid filter flow. The target frames It+10 are shown in the 3rd column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of intermediate results at each resolution scale (grid). Top: we show the predicted filter flows and the reconstruction results from warping A to B, or B to A. Note how mgPFF resolves the aliasing effect reflected by the blocks in the reconstruction images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Tracking Segmentation on the DAVIS2017 validation set. Methods marked with 1st additionally use the first frame and its mask (provided) for tracking in the rest of the video. The number in bracket is the estimated number of frames used for training the corresponding method.</figDesc><table><row><cell>Method</cell><cell>Supervision</cell><cell cols="4">J (segments) F (boundaries) mean↑ recall↑ mean↑ recall↑</cell></row><row><cell>OSVOS [8]</cell><cell cols="3">ImageNet, DAVIS 55.1 60.2</cell><cell>62.1</cell><cell>71.3</cell></row><row><cell>MaskTrack [37]</cell><cell cols="3">ImageNet, DAVIS 51.2 59.7</cell><cell>57.3</cell><cell>65.5</cell></row><row><cell>OSVOS-B [8]</cell><cell>ImageNet</cell><cell cols="2">18.5 15.9</cell><cell>30.0</cell><cell>20.0</cell></row><row><cell cols="2">MaskTrack-B [37] ImageNet</cell><cell cols="2">35.3 37.8</cell><cell>36.4</cell><cell>36.0</cell></row><row><cell>OSVOS-M [99]</cell><cell>ImageNet</cell><cell cols="2">36.4 34.8</cell><cell>39.5</cell><cell>35.3</cell></row><row><cell>Identity</cell><cell>None</cell><cell cols="2">22.1 15.9</cell><cell>23.6</cell><cell>11.7</cell></row><row><cell>SIFTflow [46]</cell><cell>None</cell><cell>13.0</cell><cell>7.9</cell><cell>15.1</cell><cell>5.5</cell></row><row><cell cols="2">SIFTflow 1st [46] None</cell><cell>33.0</cell><cell>-</cell><cell>35.0</cell><cell>-</cell></row><row><cell>FlowNet2 [29]</cell><cell>Synthetic</cell><cell>16.7</cell><cell>9.5</cell><cell>19.7</cell><cell>7.6</cell></row><row><cell cols="2">FlowNet2 1st [29] Synthetic</cell><cell>26.7</cell><cell>-</cell><cell>25.2</cell><cell>-</cell></row><row><cell cols="3">DeepCluster 1st [9] Self (1.3×10 6 ) 37.5</cell><cell>-</cell><cell>33.2</cell><cell>-</cell></row><row><cell cols="4">ColorPointer [91] Self (9.0×10 7 ) 34.6 34.1</cell><cell>32.7</cell><cell>26.8</cell></row><row><cell cols="3">CycleTime 1st [94] Self (3.7×10 7 ) 40.1</cell><cell>-</cell><cell>38.3</cell><cell>-</cell></row><row><cell>mgPFF (1st only)</cell><cell></cell><cell cols="2">31.6 29.5</cell><cell>36.2</cell><cell>30.8</cell></row><row><cell>mgPFF (K=1) mgPFF 1st (K=1)</cell><cell>Self (6.0×10 4 )</cell><cell cols="2">38.9 38.5 41.9 41.4</cell><cell>41.1 45.2</cell><cell>38.6 43.9</cell></row><row><cell>mgPFF 1st (K=3)</cell><cell></cell><cell cols="2">42.2 41.8</cell><cell>46.9</cell><cell>44.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Human Pose Tracking on JHMDB dataset. Methods marked with 1st additionally use the first frame with its mask for propagating on the rest frames. "mgPFF+ft" means that we fine-tune mgPFF model particularly on the videos from this dataset in an unsupervised way (no annotations used).</figDesc><table><row><cell>Method / PCK↑</cell><cell cols="5">@0.1 @0.2 @0.3 @0.4 @0.5</cell></row><row><cell cols="2">fully-supervised [84] 68.7</cell><cell>92.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Identity</cell><cell>43.1</cell><cell>64.5</cell><cell>76.0</cell><cell>83.5</cell><cell>88.5</cell></row><row><cell>SIFTflow 1st [46]</cell><cell>49.0</cell><cell>68.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FlowNet2 [29]</cell><cell>45.2</cell><cell>62.9</cell><cell>73.5</cell><cell>80.6</cell><cell>85.5</cell></row><row><cell>DeepCluster 1st [9]</cell><cell>43.2</cell><cell>66.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ColorPointer [91]</cell><cell>45.2</cell><cell>69.6</cell><cell>80.8</cell><cell>87.5</cell><cell>91.4</cell></row><row><cell>CycleTime 1st [94]</cell><cell>57.3</cell><cell>78.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>mgPFF</cell><cell>49.3</cell><cell>72.8</cell><cell>82.4</cell><cell>88.6</cell><cell>91.9</cell></row><row><cell>mgPFF 1st</cell><cell>55.6</cell><cell>77.1</cell><cell>85.2</cell><cell>89.6</cell><cell>92.1</cell></row><row><cell>mgPFF+ft</cell><cell>52.7</cell><cell>75.1</cell><cell>84.0</cell><cell>89.5</cell><cell>92.3</cell></row><row><cell>mgPFF+ft 1st</cell><cell>58.4</cell><cell>78.1</cell><cell>85.9</cell><cell>89.8</cell><cell>92.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>to indicate where to copy pixels from I t . With the coordinate flow, we warp frame I t to generate a new framê I t+m . We compare the pixel-level L1 distance between I t andÎ t+m in original uint8 RGB space ([0,255] scale).</figDesc><table><row><cell>method/error↓</cell><cell cols="2">5-Frame 10-Frame</cell></row><row><cell>Identity</cell><cell>82.0</cell><cell>97.7</cell></row><row><cell>Optical Flow (FlowNet2) [29]</cell><cell>62.4</cell><cell>90.3</cell></row><row><cell>CycleTime [94]</cell><cell>60.4</cell><cell>76.4</cell></row><row><cell>mgPFF</cell><cell>7.32</cell><cell>8.83</cell></row><row><cell cols="3">transform the filter flow into coordinate flow according to</cell></row><row><cell>Eq. 6</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Due to that arxiv limits the size of files, we put high-resolution figures in the project page.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The code and models can be found in https://github.com/ aimerykong/predictive-filter-flow</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This project is supported by NSF grants IIS-1813785, IIS-1618806, IIS-1253538 and a hardware donation from NVIDIA. Shu Kong personally thanks Teng Liu and Etthew Kong who initiated this research, and the academic uncle Alexei A. Efros for the encouragement and discussion.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the appendix, we first show all intermediate results of multigrid Predictive Filter Flow (mgPFF) from multiresolution inputs, to have an idea how these outputs look like in terms of frame reconstruction. Then, we plot the graph visualization of our model architecture with detailed design of the two stream architecture. Furthermore, we visualize pixel embedding generated by our architecture to understand what the model learns. Finally, along this document, we provide some demo videos of the object segmentation/tracking results with different setup.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural network graphs and training metrics for pytorch and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferriere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to poke by poking: Experiential learning of intuitive physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5074" to="5082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparison of video shot boundary detection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Boreczky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards ultimate motion estimation: Combining highest accuracy with real-time performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference on Computer Vision (ICCV 2005)</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10-20" />
			<biblScope unit="page" from="749" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of mathematical models in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="237" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From lifestyle vlogs to everyday interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance characterization of video-shot-change detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gargi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual following and pattern discrimination of face-like stimuli by newborn infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Goren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="544" to="549" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<title level="m">Multi-grid methods and applications</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Springer Science &amp; Business Media</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient filter flow for space-variant multiframe blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mirrorflow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno>abs/1612.02646</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11482</idno>
		<title level="m">Image reconstruction with predictive filter flow</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-local affine parts for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Association (BMVA)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
	<note>British Machine Vision Conference (BMVC&apos;04)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding and evaluating blind deconvolution algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1964" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roosendaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sintel</surname></persName>
		</author>
		<title level="m">ACM SIGGRAPH ASIA 2010 Computer Animation Festival</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="82" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian depth-from-defocus with shading constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="819" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segment-tree based cost aggregation for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2502" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual tracking in young infants: Evidence for object identity or object permanence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Borton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Darby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="198" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual tracking as an index of the object concept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infant Behavior and Development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A clearer picture of total variation blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1041" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The curious robot: Learning visual representations via physical interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Popper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">4507</biblScope>
			<biblScope unit="page">538</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Filter flow made practical: Massively parallel and lock-free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3549" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Learned video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06981</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Filter flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Bayesian multi-scale differential optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep diffeomorphic transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Skafte Detlefsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4403" to="4412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Thinslicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4220" to="4229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Bayesian modeling of uncertainty in low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Trottenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multigrid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Predictive tracking over occlusions by 4-month-old infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Von Hofsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kochukhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="625" to="640" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07593</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning physical object properties from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02772</idno>
		<title level="m">Object discovery in videos as foreground motion clustering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Katsaggelos. Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Every pixel counts: Unsupervised geometry learning with holistic 3d motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="691" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Direct, dense, and deformable: Template-based non-rigid 3d reconstruction from rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="918" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
