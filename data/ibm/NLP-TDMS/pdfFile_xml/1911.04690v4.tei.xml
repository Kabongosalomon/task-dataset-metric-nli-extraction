<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLEN: Leveraging Field for Scalable CTR Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-24">2020. August 24, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Chen</surname></persName>
							<email>wenqiang.cwq@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhang</forename><surname>Zhan</surname></persName>
							<email>lizhangzhan@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Ci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
							<email>chenlin@xmu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dugang</forename><surname>Liu</surname></persName>
							<email>dugang.ldg@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhang</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Ci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dugang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Data Intelligence</orgName>
								<orgName type="institution">Meitu Inc Xiamen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Advertisement Recommendation Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Meitu Inc Xiamen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Meitu Inc Xiamen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FLEN: Leveraging Field for Scalable CTR Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2nd International Workshop on Deep Learning Practice for High-Dimensional Sparse Data (DLP &apos;20)</title>
						<meeting>the 2nd International Workshop on Deep Learning Practice for High-Dimensional Sparse Data (DLP &apos;20) <address><addrLine>San Diego</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2020-08-24">2020. August 24, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3306307.3328180</idno>
					<note>ACM ISBN 978-1-4503-6317-4/19/07. . . $15.00 Click-through rate, Inter-field, Intra-field, Dropout ACM Reference Format: California USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3306307.3328180</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS • Information systems → Computational advertising; * Co-corresponding authors</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-Through Rate (CTR) prediction systems are usually based on multi-field categorical features, i.e., every feature is categorical and belongs to one and only one field. Modeling feature conjunctions is crucial for CTR prediction accuracy. However, it usually requires a massive number of parameters to explicitly model all feature conjunctions, which is not scalable for real-world production systems.</p><p>In this paper, we describe a novel Field-Leveraged Embedding Network (FLEN) which has been deployed in the commercial recommender systems in Meitu and serves the main traffic. FLEN devises a field-wise bi-interaction pooling technique. By suitably exploiting field information, the field-wise bi-interaction pooling layer captures both inter-field and intra-field feature conjunctions with a small number of model parameters and an acceptable time complexity for industrial applications. We show that some classic shallow CTR models can be regarded as special cases of this technique, i.e., MF, FM and FwFM. We identify a unique challenge in this technique, i.e., the FM module in our model may suffer from the coupled gradient issue, which will damage the performance of the model. To solve this challenge, we develop Dicefactor: a novel dropout method to prevent independent latent features from co-adapting.</p><p>Extensive experiments, including offline evaluations and online A/B testing on real production systems, demonstrate the effectiveness and efficiency of FLEN against the state-of-the-art models. In particular, compared to the previous version deployed on the system (i.e. NFM), FLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and computation time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-Through Rate (CTR) prediction is the task of predicting the probabilities of users clicking items or advertisements (ads). It is a critical problem in recommender systems and online advertising, which provide substantial revenue for Internet companies. As such, CTR prediction has attracted much attention from both academia and industry communities in the past few years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The data in CTR prediction task is multi-field categorical data, i.e., every feature is categorical and belongs to one and only one field. For example, feature "gender=Female" belongs to field "gender", feature "age=24" belongs to field "age" and feature "item cat-egory=cosmetics" belongs to field "item category". The value of feature "gender" is either "male" or"female. Feature "age" is discretized to several age groups: "0-18", "18-25", "25-30", and so on. It is well regarded that, feature conjunctions are essential for accurate CTR prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11</ref>]. An example of informative feature conjunctions is: age group "18-25" combined with gender "female" for item category "cosmetics". It indicates that young girls are more likely to click on cosmetic products.</p><p>Modeling sparse feature conjunctions has been continually improved and refined by a number of prior work. Most models follow the Factorization Machines (FM) <ref type="bibr" target="#b18">[19]</ref> and its inspired extensions because of its effectiveness and flexibility. For example, FFM <ref type="bibr" target="#b8">[9]</ref> and FwFM <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref> explicitly model field-aware feature interactions, NFFM <ref type="bibr" target="#b24">[25]</ref> combines FFM and MLP to capture operation-aware feature conjunctions with additional parameters, and FPENN <ref type="bibr" target="#b11">[12]</ref> estimates the probability distribution of the field-aware embedding rather than using the single point estimation (the maximum a posteriori estimation). Although these models have achieved promising results, the challenge in real-world online advertising or recommender systems is the strict latency limit at serving time and the scalability for high-dimensionality of features. We need to predict hundreds of items for each user in less than 10 milliseconds. The model complexity of FFM and FwFM is O(N 2 ), where N is the number of features. The drawback of directly applying FFM and FwFM in real-world applications is the dramatically increased use of computational resources, because any uniform increase in the number of features will cause a quadratic increase of computation. FFM is also restricted by space complexity, which further weakens its practicality. FFM-based deep models use additional parameters to capture non-linear high-order feature conjunctions. However, increasing model complexity sometimes only marginally improve performance while leading to severe over-fitting problems <ref type="bibr" target="#b7">[8]</ref>. In addition, these works usually consume huge memory, resulting in restricted scalability.</p><p>In this paper we describe a novel Field-Leveraged Embedding Network (FLEN) which has been successfully deployed in the online recommender systems in Meitu, serving the main traffic. FLEN devises a new operation in neural network modeling âĂŤ Fieldwise Bilinear Interaction (FwBI) pooling, to address the restriction of time and space complexity when applying field-aware feature conjunctions in real industrial system. The field-wise bi-interaction pooling technique is based on the observation that features from various fields interact with each other differently. We show that some classic shallow CTR models can be regarded as special cases of this technique, including FM <ref type="bibr" target="#b18">[19]</ref>, MF <ref type="bibr" target="#b9">[10]</ref>, and FwFM <ref type="bibr" target="#b15">[16]</ref>. The combination of this technique and the traditional MLP layer constitutes our final model. The idea behind our model is to use multiple modules to extract feature interactions at different levels, and finally merge them to get a better representation of the interaction. The parameters in each part are only responsible for a certain level of feature interaction, which helps reduce the parameters of the model and speed up the training of the model.</p><p>As noted in previous work <ref type="bibr" target="#b16">[17]</ref>, FM may cause the coupled gradient issue due to using the same latent vectors in different types of inter-field interactions, i.e. two supposedly independent features are updated in the same direction during the gradient update process. In FLEN, this issue is partly tackled by leveraging field information. We also propose a novel dropout method: Dicefactor to decouple independent features. Dicefactor randomly drops bi-linear paths (i.e. cross-feature edge in FM module of the field-wise bi-interaction pooling layer) to prevent a feature from adapting to other features.</p><p>To demonstrate the effectiveness and efficiency of FLEN, we conduct extensive offline evaluations and online A/B testing. In offline evaluations, FLEN outperforms state-of-the-art methods on both a well-known benchmark and an industrial dataset consisting of historical click records collected in our system. Online A/B testing shows that FLEN enhances the CTR prediction accuracy (i.e. increases CTR by 5.19%) with a fraction of computation resources (i.e. 1/6 memory usage and computation time), compared with the last version of our ranking system (i.e. NFM <ref type="bibr" target="#b7">[8]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>CTR prediction has been extensively studied in the literature, as online advertising systems have become the financial backbone of most Internet companies. Related literature can be roughly categorized into shallow and deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shallow Models</head><p>Successful shallow models represent features as latent vectors. For example, matrix factorization (MF) <ref type="bibr" target="#b9">[10]</ref> is successfully applied in recommender systems. It factorizes a rating matrix into a product of lower-dimensional sub-matrix, where each sub-matrix is the latent feature space for users and items. Factorization machines (FM) <ref type="bibr" target="#b18">[19]</ref> is a well-known model to learn feature interactions. In FM, the effect of feature conjunction is explicitly modeled by inner product of two latent feature vectors (a.k.a. embedding vectors). Many variants have been proposed based on FM. For example, Rendle et al. <ref type="bibr" target="#b17">[18]</ref> proposed a context-aware CTR prediction method which factorized a three-way &lt; user, ad, context &gt; tensor. Oentaryo et al. <ref type="bibr" target="#b14">[15]</ref> developed hierarchical importance-aware factorization machine to model dynamic impacts of ads.</p><p>Field information has been acknowledged as crucial in CTR prediction. A number of recent work has exploited field information. For example, Field-aware Factorization Machines (FFM) <ref type="bibr" target="#b8">[9]</ref> represents a feature based on separate latent vectors, depending on the multiplying feature field. GBFM <ref type="bibr" target="#b1">[2]</ref> and AFM <ref type="bibr" target="#b23">[24]</ref> consider the importance of different field feature interactions. Field-weighted Factorization Machines (FwFM) <ref type="bibr" target="#b15">[16]</ref> assigns interaction weights on each field pair.</p><p>The field-wise bi-interaction technique of FLEN is inspired by FwFM. It can be viewed as a special case of factorized FwFM in a computationally efficient manner. However, the field-wise biinteraction technique of FLEN generalizes and ensembles MF, FM and FwFM. Furthermore, shallow models are limited as they focus on modeling linear, low-order feature interactions. FLEN is capable of capturing not only low-order but also high-order, nonlinear interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Models</head><p>An increased interest in designing deep models for CTR prediction has emerged in recent years. The majority of them utilize feature bi-interactions. To name a few, NFM <ref type="bibr" target="#b7">[8]</ref> generalizes FM by stacking neural network layers on top of a bi-interaction pooling layer. The architecture of DeepFM <ref type="bibr" target="#b6">[7]</ref> resembles with Wide&amp;Deep <ref type="bibr" target="#b2">[3]</ref>, which also has a shared raw feature input to both its "wide" (i.e. for bi-interaction) and "deep" (i.e. for high-order interaction) components. DCN <ref type="bibr" target="#b21">[22]</ref> learns certain bounded-degree feature interactions. xDeepFM <ref type="bibr" target="#b10">[11]</ref> improves over DeepFM and DCN by generating feature interactions in an explicit fashion and at the field-wise level. NFFM <ref type="bibr" target="#b24">[25]</ref> learns different feature representations for convolutional operations and product operations. However, the space complexity of NFFM and the time complexity of xDeepFM restrict them from applying in industrial systems. FGCNN <ref type="bibr" target="#b12">[13]</ref> leverages the strength of CNN to generate local patterns and recombines them to generate new features. Then deep classifier is built upon the augmented feature space. PIN <ref type="bibr" target="#b16">[17]</ref> generalizes the kernel product of feature bi-interactions in a net-in-net architecture.</p><p>The rest of literature learns the high-order feature interactions in an implicit way, e.g. PNN <ref type="bibr" target="#b16">[17]</ref>, FNN <ref type="bibr" target="#b25">[26]</ref>, DeepCrossing <ref type="bibr" target="#b19">[20]</ref>, and so on. Some tree-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref> combine the power of embedding-based models and tree-based models to boost explainability. One drawback of these approaches is having to break training procedure into multiple stages. A recent work FPENN <ref type="bibr" target="#b11">[12]</ref> also groups feature embedding vectors based on field information in deep neural network structure. It estimates the probability distribution of the field-aware embedding rather than using the single point estimation (the maximum a posteriori estimation) to prevent overfitting. However, as FPENN assigns several latent vectors to each field (i.e. one for a field which is not equivalent as the multiplying field), it requires much more model parameters than FLEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>An overview of the model architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><formula xml:id="formula_0">Let X = {x 1 , x 2 , · · · , x s , · · · } denote the set of samples, x s = [x s 1 , · · · , x s N ]</formula><p>denotes the s-th sample, and x s n ∈ R K n denotes the n-th categorical feature in the sample x s (details described in Section 3.1). Suppose FLEN takes x s as input. First, all features pass through an embedding layer, which outputs a concatenation of field-wise feature embedding vectors e s = [e s 1 , · · · , e s M ], where e s m ∈ R K e denotes the m-th hierarchical fields (details described in Section 3.2). Then the embedding vectors flow a Field-wise Bi-Interaction pooling layer (FwBI) and an MLP component. The fieldwise bi-interaction pooling layer (Sec 3.3) consists of three submodules which capture all single and field-wise feature interactions (degree one or two), and outputs h F w BI ∈ R K e +1 . The MLP component captures non-linear, high-order feature interactions (details described in Section 3.5). The output of field-wise bi-interaction pooling layer and output of MLP component are concatenated to feed the last prediction layer (Section 3.6).</p><p>We will show that previous CTR prediction models such as MF <ref type="bibr" target="#b9">[10]</ref>, FM <ref type="bibr" target="#b18">[19]</ref> and FwFM <ref type="bibr" target="#b15">[16]</ref> can be expressed and generalized under the proposed framework. Alternatively, the FwBI layer can be regarded as a combination of single feature, MF-based inter-field feature interactions and FM-based intra-field feature interactions. The above three parts together with the MLP layer that captures non-linear, high-order feature interactions form our model. The idea behind our model is to use multiple modules to extract feature interactions at different levels, and finally merge them to get a better representation of the interaction. The parameters in each part are only responsible for a certain level of feature interaction, which helps reduce the parameters of the model and speed up the training of the model. Furthermore,the parallel structures of FwBI allows the computational budget be distributed in a distributed environment.</p><p>Hereafter, unless stated otherwise, we use lower-case letters for indices, upper-case letters for universal constants, lower-case bold-face letters for vectors and upper-case bold-face letters for matrices, calligraphic letters for sets. We use square brackets to denote elements in a vector or a matrix, e.g. x[j] denotes the j−th elements of x. We will omit superscripts whenever no ambiguity results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Representation</head><p>It is natural to represent categorical features as one-hot or multihot vectors. Suppose there are N unique features and each feature x n has K n unique values, we represent each instance as</p><formula xml:id="formula_1">x = [x 1 , · · · , x N ], where x n ∈ R K n , x n [j] ∈ {0, 1}, j = 1, · · · , K n . K n j=1 x n [j] = k</formula><p>. Vector x n with k = 1 refers to one-hot encoding and k &gt; 1 refers to multi-hot encoding. Suppose there are M fields, F (n) denotes the field of feature x n , we organize the feature representations in a field-wise manner for complexity reduction. Specifically, x = concat(x 1 , · · · , x M ), where x m is the concatenation of feature vectors in field m, i.e. x m = concat(x n |F (n) = m). The organized instance is illustrated in the example below. Note that there is a hierarchical structure of fields. For example, "item tags field" and "item id field" belong to the more general "item field". In practice, inspired by YouTube's work <ref type="bibr" target="#b3">[4]</ref>, we also classify features according to whether they describe properties of the item or properties of the user/context (namely user field, item field and context field as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>) and achieve maximal performance enhancement and complexity reduction. A practically useful aspect of this hierarchical design is that it aligns with the intuition that the output of each hierarchical field is highly inter-correlated.</p><formula xml:id="formula_2">[0, 1, 0, ..., 0 age field ] ... [ 1, 0 gender field ] user field [0, 1, 0, ..., 0 item id field ] [0, 1, 0, 1, ..., 0 item tags field ] item field</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding Layer</head><p>Since the feature representations of the categorical features are very sparse and high-dimensional, we employ an embedding procedure to transform them into low dimensional, dense real-value vectors. Firstly, we transform each feature x n to e n .</p><formula xml:id="formula_3">e n = V n x n ,<label>(1)</label></formula><p>where V n ∈ R K e ×K n is an embedding matrix for the corresponding feature that will be optimized together with other parameters in the network. Note that feature size can be various.</p><p>Next we apply sum-pooling to e n to obtain the field-wise embedding vectors.</p><formula xml:id="formula_4">e m = n |F (n)=m e n<label>(2)</label></formula><p>Finally, we concatenate all field-wise embedding vectors to build e, as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Field-wise Bi-Interaction Pooling Layer</head><p>The field-wise bi-interaction pooling layer learns a mapping</p><formula xml:id="formula_5">Φ F w BI (W S , R, W F w BI ) : (R N , R M K e ) → R K e +1 .</formula><p>There are three submodules in this layer. The first sub-module (denoted as S) is a linear regression part similar to that of FWFM, which models global bias of data and weight of categorical features. It operates on the categorical vectors,</p><formula xml:id="formula_6">i.e. h S = w 0 + N i=1 K i j=1 w i [j]x i [j]</formula><p>where w 0 is bias term. Note that for ease of description, we let W S denote the set of all w 0 ∪ w i .</p><p>The second sub-module is called the MF module, which focuses on learning inter-field feature interactions between each pair of the hierarchical fields. It first operates element-wise product on all pairs of field-wise embedding vectors, i.e.</p><formula xml:id="formula_7">h M F = M i=1 M j=i+1 e i ⊙ e j r [i][j], where r [i]</formula><p>[j] ∈ R is a weight to model the interaction strength between field i and j. We use ⊙ to denote the element-wise product of two vectors, that is,</p><formula xml:id="formula_8">(e i ⊙ e j )[k] = e i [k]e j [k].</formula><p>In real industrial system, the quantity of feature fields is usually 10 or more, for example there are 33 feature fields in our industrial dataset, but the hierarchical field number, M, is usually less than 4 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> for reducing computation and avoiding overfitting. This hierarchical field manner is inspired by YouTube, according to whether they describe properties of the item or properties of the user/context <ref type="bibr" target="#b3">[4]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there are 3 MF models to learn field-wise feature interactions for each pair of the hierarchical user, item and context fields, respectively.</p><p>The third sub-module is called the FM module, which focuses on learning intra-field feature interactions in each FM module. It first computes self element-wise product on each field embedding vectors, i.e. hf m = e m ⊙ e m . Similar operations are also conducted on each feature embedding vectors, i.e. ht m = n, F (n)=m e n ⊙ e n . Note that for ease of description, we let R denote the set of all</p><formula xml:id="formula_9">r [i][j] ∪ r [m][m].</formula><p>Clearly, the output of MF module and FM module is a K e -dimension vector that encodes the inter-field and intra-field feature interactions in the embedding space, respectively.</p><p>We concatenate the output of S and the sum pooling of MF and FM module, i.e.</p><formula xml:id="formula_10">h in = [h S , h M F + h F M ].</formula><p>Then h in is fed to a hidden layer, i.e. h F w BI = σ (W T F w BI h in ). In practice, we use the ReLU as the active function σ .</p><p>By leveraging multiple MF and FM modules to learn both the inter-field and intra-field feature interactions, we end up with more disentangled parameters and therefore with faster training. Furthermore, the parallel structures of FwBI allows the computational budget be distributed in a distributed environment.</p><p>It is worth pointing out that the FwBI pooling layer is much more memory-efficient than FFM and furthermore, it can be efficiently computed in O(MK e N + K e M 2 ). In real industrial system, the quantity of feature fields is usually 10 or more, for example there are 33 feature fields in our industrial dataset, but the hierarchical field number, M, is usually less than 4 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>. There are 3 hierarchical field, i.e. user, item and context, in our industrial dataset. When M ≪ N , FwBI can be efficiently trained and serverd online in linear time, which is very attractive in industrial systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relation to Previous CTR Prediction Systems</head><p>A variety of shallow models can be expressed and generalized under the field-wise bi-interaction technique. To start with, we set the active function σ as an identify function, the weight matrix W T F w BI = I, where I is a unit matrix. Since the embedding vectors e m , e n are transformed from the original feature representations, e m = n |F (n)=m e n , e n = V n x n , we can re-write the computation in field-wise bi-interaction pooling layer as:</p><formula xml:id="formula_11">Φ F w BI = W T F w BI w 0 + N i=1 K i j=1 w i [j]x i [j] S , M i=1 M j=i+1 ( n, F (n)=i V i x n )( n, F (n)=j V j x n )r [i][j] MF + M m [( n, F (n)=m V n x n ) 2 − ( n, F (n)=m (V n x n ) 2 )]r [m][m] FM<label>(3)</label></formula><p>where we use the symbol (Vx) 2 to denote Vx ⊙ Vx. The first term in Equation 3 corresponds to the first sub-module which is based on single feature values. The second term corresponds to the sum pooling over the second sub-module, which resembles the matrix factorization form when written in feature representations, and the third sub-module, which resembles the factorization machine form.</p><p>Clearly, if there are only one field, i.e., M = 1 and r m,m = 1 2 , then we can exactly recover the FM model.</p><formula xml:id="formula_12">Φ F w BI = Φ F M = w 0 + N i=1 K i j=1 w i [j]x i [j], 1 2 [( N i=1 V i x i ) 2 − N i=1 (V i x i ) 2 ]<label>(4)</label></formula><p>If there are N fields, i.e., M = N , there is not any intra-field feature interactions, then we can recover the FwFM model.</p><formula xml:id="formula_13">Φ F w BI = Φ F w F M = w 0 + M i=1 K i j=1 w i [j]x i [j], M i=1 M j &gt;i (V i x i )(V j x j )r [i][j]<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">MLP component</head><p>We employ an MLP component to capture non-linear, high-order feature interactions. The input is simply a concatenation of all fieldwise embedding vectors, i.e. h 0 = concat(e 1 , · · · , e M ). A stack of fully connected layers is constructed on the input h 0 . Formally, the definition of fully connected layers are as follows:</p><formula xml:id="formula_14">h 1 = σ 1 (W 1 h 0 + b 1 ), h 2 = σ 2 (W 2 h 1 + b 2 ), ...... h L = σ L (W L h L−1 + b L ),<label>(6)</label></formula><p>where L denotes the number of hidden layers, W l , b l and σ l denote the weight matrix, bias vector and activation function for the l-th layer, respectively. We use ReLU as the active function for each layer. Note that for ease of description, we let W M LP denote the set of all W l , and b M LP denote the set of all b l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Prediction Layer</head><p>The output vector of the last hidden MLP layer h L is concatenated with the output vector of field-wise bi-interaction pooling layer Φ F w BI to form h F = concat(h F w BI , h L ). The concatenation h F goes through one last hidden layer and is transformed to</p><formula xml:id="formula_15">z = σ (w T F h F ),<label>(7)</label></formula><p>where vector w F denotes the neuron weights of the final hidden layer. Finally, we apply a sigmoid layer to make predictions.</p><formula xml:id="formula_16">σ (z) = 1 1 + e −z<label>(8)</label></formula><p>Our loss function is negative log-likelihood, which is defined as follows:</p><formula xml:id="formula_17">L = − 1 |X| | X | s=1 (y s loд(Φ(x s )) + (1 − y s )loд(1 − Φ(x s ))),<label>(9)</label></formula><p>where y s ∈ {0, 1} as the label, Φ(x s ) is the output of the network, representing the estimated probability of the instance x s being clicked. The parameters to learn in our model are represented as V, W = {W S , W F w BI , W M LP , w F }, b M LP , R, which are updated via minimizing the total negative log-likelihood using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DICEFACTOR: DROPOUT METHOD</head><p>As noted in previous work <ref type="bibr" target="#b16">[17]</ref>, FM may cause the coupled gradient issue because it uses the same latent vectors in different types of inter-field interactions, i.e. two supposedly independent features are updated in the same direction during the gradient update process. It will damage the performance of the model. To solve this challenge, we propose a novel dropout method to decouple independent features, i.e., Dicefactor. Dicefactor is inspired by Dropout <ref type="bibr" target="#b20">[21]</ref>. It randomly drops bi-linear paths (i.e. cross-feature edge in the FM module of field-wise bi-interaction pooling layer) to prevent a feature from adapting to other features. We first reformulate Eq. (4) as:</p><formula xml:id="formula_18">Φ F M = w 0 + N i=1 K i j=1 w i [j]x i [j], M i=1 M j=1&amp;i j e i ⊙ e j ,<label>(10)</label></formula><p>where e i represents the i-th field's embedding vector. Based on Eq. (10), <ref type="figure" target="#fig_4">Fig. 3</ref> shows the expanding structure of the bi-linear interaction in two field embedding vectors. There are K e bi-linear paths, each bi-linear path connects corresponding elements in two embedding vectors. The key idea of DiceFactor is to randomly drop the bi-linear paths during the training. This partly prevents e i from co-adapting to e j , i.e., e i is updated through the direction of e j . In our implementation, each factor is retained with a predefined probability β during training. With the DiceFactor, the formulation of bi-linear interaction of FM part in the training becomes:</p><formula xml:id="formula_19">Φ F M = w 0 + N i=1 K i j=1 w i [j]x i [j], M i=1 M j=1&amp;i j pe i ⊙ e j ,<label>(11)</label></formula><p>where p ∈ R K e and p[i] ∼ Bernoulli(β). With the DiceFactor, the network can be seen as a set of 2 K e thinned networks with shared weights. In each iteration, one thinned network is sampled randomly and trained by back-propagation as shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>. For inference, instead of explicitly averaging the outputs from all 2 K e thinned networks, we use the approximate "Mean Network" scheme in <ref type="bibr" target="#b20">[21]</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 3b</ref>, each factor term e i ⊙ e j is multiplied by β at inference phase:</p><formula xml:id="formula_20">+</formula><formula xml:id="formula_21">Φ F M = w 0 + N i=1 K i j=1 w i [j]x i [j], M i=1 M j=1&amp;i j βe i ⊙ e j<label>(12)</label></formula><p>In this way, the output of each neuron at inference phase is the same as the expectation of output of 2 K e different networks at train phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OFFLINE MODEL EVALUATION</head><p>To assess the validity of our CTR prediction models, we run a traditional offline evaluation based on a well-known public benchmark and an industrial dataset.</p><p>Avazu 1 The first dataset we adopt is originally used in the Kaggle CTR prediction competition. It contains users' mobile behaviors, i.e. whether a displayed mobile ad is clicked by a user. It has 23 feature fields spanning from user/device features to ad attributes. The data set is collected during a time span of 10 days. We use 9 days of clicks for training and the last 1 day of data for test.</p><p>Note that in this paper we do not use another well-known benchmark, i.e., Criteo dataset, because the semantic of its features is undisclosed. We do not known which hierarchical field each feature belongs to.</p><p>Meitu The second dataset used to run the experiments is a uniformly generated sample of our internal historical data. As a training set we extract a sample of 2 million items shown during a seven-day time period from 2019-07-26 to 2019-08-01 to users of a photo and video-sharing social networking app, namely Meitu. We collect over 1.5 billion users' records. The test set contains a sample of 1 million items shown the next day, i.e. 2019-08-02. There are around 5 million features (e.g., user age, clicked feed ids, and etc) organized in hierarchical field manner, including "user", "item" and "context". Features used in our system are described in <ref type="table" target="#tab_0">Table 1</ref>. The statistics of the data sets are summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Competitors</head><p>We compare FLEN with 7 state-of-the-art models.</p><p>(1) FFM [9]: a shallow model in which the prediction is aggregated over inner products of feature vectors. It represents a feature by several separate vectors, depending on the multiplying feature field.</p><p>(2) FwFM [16]: a shallow model which also explicitly aggregates over feature products. Interaction weights are assigned for each field pair.  All methods are implemented in TensorFlow 2 . We use an embedding dimension of 32 and batch size of 512 for all compared methods. Hidden units d ′ are set to 64, 32. We use AdaGrad <ref type="bibr" target="#b4">[5]</ref> to optimize all deep neural network-based models. DCN has two interaction layers, following by two feed-forward layers. We use one hidden layer of size 200 on top of Bi-Interaction layer for NFM as recommended by their paper. We use 2 layers with size (64, 32) in the MLP component of FLEN. All experiments are executed on one NVIDIA TITAN Xp Card with 128G memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We use two commonly adopted evaluation metrics.</p><p>AUC Area Under the ROC Curve (AUC) measures the probability that a CTR predictor will assign a higher score to a randomly chosen positive item than a randomly chosen negative item. A higher AUC indicates a better performance.</p><p>Logloss Since all models attempt to minimize the Logloss defined by Equation 9, we use it as a straightforward metric.</p><p>It is now generally accepted that increase in terms of AUC and Logloss at 0.001-level is significant <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparative Performance</head><p>We report the AUC and Logloss performance of different models in <ref type="table" target="#tab_2">Table 3</ref>. We distinguish the original FLEN (denoted as FLEN) and the model with Dicefactor implementation (denoted as FLEN+D). We can see that on both datasets, FLEN has achieved the best performance in terms of AUC and Logloss. We point out that FLEN has impressively boosted the AUC performance of the best competitor (i.e. NFFM) by 0.002 on the industrial dataset, which validates the superiority of FLEN on large-scale CTR systems. We also observe that Dicefactor further significantly enhances AUC performance of FLEN on both datasets. Furthermore, we can find an interesting observation: leveraging field information makes modeling feature interactions more precisely. This observation is derived from the fact that by exploiting field information, FLEN, NFFM and xDeepFM perform better than NFM, DeepFM and DCN do on the Avazu and Meitu datasets. This phenomenon can be found in more literature, including FFM <ref type="bibr" target="#b8">[9]</ref> and FwFM <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Memory Consumption and Running Time</head><p>To illustrate the scalability of FLEN, we first compare the model complexity and actual parameter size on Avazu dataset of each  <ref type="table" target="#tab_3">Table 4</ref> . For a fair comparison, in computing parameter size, we assume the number of deep layers denoted as H = 3, the number of hidden layers denoted as L = 3 and embedding size K e = 32. We do not take into account parameters for the feedforward neural network. The number of features is N =1,544,448 and the number of fields is M = 23 on Avazu dataset. We can see that FLEN is one of the models that make use of the smallest number of parameters. For a detailed study, we report the number of instances being processed by different models per second on the two datasets. As shown in <ref type="figure" target="#fig_6">Figure 4</ref>, FLEN operates on the most instances on Meitu dataset. On Avazu dataset, FLEN is comparable with other state-ofthe-art methods. But the high efficiency makes FLEN applicable in real industrial systems to handle large scale and high-dimensional data.</p><p>We keep track of AUC and Logloss during the training process after each training "epoch" (i.e., 5,000 iterations through all the training data on Avazu and 20,000 iterations on Meitu ). As shown in <ref type="figure">Figure 5</ref>, FLEN obtains the best AUC (i.e. highest) and Logloss (i.e. lowest) on both datasets in each iteration. Furthermore, we point out that although NFFM (which is the best competitor) is close with FLEN, FLEN achieves faster convergence towards optimization. For example, FLEN has a sharper increase of AUC and more steep decrease of Logloss on Meitu dataset. Thus FLEN requires less training time than NFFM, which is desirable in real-world production systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Impact of Parameters</head><p>We analyze the impact of an important parameter of Dicefactor, the probability β to keep bi-linear paths. We empirically find that a small value β ∈ (0, 0.5) leads to poor performance. Henceforth, we set β = 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 respectively and report the AUC and Logloss results. As shown in <ref type="figure">Figure 6</ref>, performance of FLEN is affected by β, the keep probability. The best parameter settings for AUC and Logloss are consistent. For example, best AUC and Logloss are both obtained at β = 0.7 on Avazu dataset. On Meitu dataset, the best β = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ONLINE EVALUATION</head><p>To measure the impact that FLEN has on users we conduct an online evaluation through a 7-day A/B testing on Meitu app. We split 10% of the incoming traffic as experiment group, the rest as control group. The items of the control group in the A/B testing period are provided by the previous version of online ranking system, which is based on NFM. The items offered to the experiment group are items that are predicted by FLEN , i.e. we deliver top 12 items with highest prediction score by FLEN. We report the CTR different during the A/B testing period in <ref type="figure" target="#fig_8">Figure 7</ref>. We observe stable and significant increase of CTR during the A/B testing period. The minimal CTR increase is above 4.9%. The mean CTR improvement over seven days is 5.195% with variance 0.282%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we describe the FLEN model that is deployed in the online recommender systems in Meitu, serving the main traffic. FLEN has obtained significant performance increase by exploiting field information with an acceptable memory usage and computing latency in the real-time serving system. As future work, we plan to explore the usage of the attention mechanism to attend to important field embedding. Furthermore, we are interested in extending FLEN to multi-task learning, i.e. predict conversions-rate and click-through-rate simultaneously. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>feed ID ... feed Tag s .Architecture overview of FLEN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of embedding layer with K e = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Finally, the two vectors are merged over all field-wise subtraction of the two intermediate vectors, i.e. h F M = m (hf m − ht m )r [m][m], where r [m][m] ∈ R is a weight for each field m, discriminating the importance of each field that contribute to the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>+ + + present with p (a) train phase + + + + multiply with β (b) inference phase Dicefactor keeps a bi-linear path with probability β at train phase. At inference phase, every bi-linear path is kept and the output is multiplied by β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 3 )</head><label>3</label><figDesc>DCN [22]: a deep model that takes the outer product of feature vectors at bit-wise level to a feed-forward neural network. (4) DeepFM [7]: a deep model that consists of a wide component that models factorization machine and a deep component. (5) NFM [8]: a deep model which stacks MLP on top of a bi-interaction 1 https://www.kaggle.com/c/avazu-ctr-prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Number of instances per second processed by different models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>AUC and Logloss change in training time. AUC and logloss with different keep probability of dicefactor on two datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>CTR increase during our online A/B testing period.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example features in Meitu dataset.</figDesc><table><row><cell>Field</cell><cell>Feature</cell><cell>Dimemsionality</cell><cell>Type</cell><cell>AverageNonzero Ids per Instance</cell></row><row><cell></cell><cell>gender</cell><cell>2</cell><cell>one-hot</cell><cell>1</cell></row><row><cell></cell><cell>age</cell><cell>∼ 10</cell><cell>one-hot</cell><cell>1</cell></row><row><cell>User Field</cell><cell>clicked_feed_ids</cell><cell>∼ 10 7</cell><cell>multi-hot</cell><cell>∼ 10 2</cell></row><row><cell></cell><cell>liked_feed_ids</cell><cell>∼ 10 5</cell><cell>multi-hot</cell><cell>∼ 10 2</cell></row><row><cell></cell><cell>feed_id</cell><cell>∼ 10 7</cell><cell>one-hot</cell><cell>1</cell></row><row><cell></cell><cell>tags</cell><cell>∼ 10 3</cell><cell>multi-hot</cell><cell>∼ 10 1</cell></row><row><cell>Item Field</cell><cell>clicked_rate</cell><cell>10</cell><cell>one-hot</cell><cell>10</cell></row><row><cell></cell><cell>liked_rate</cell><cell>10</cell><cell>one-hot</cell><cell>10</cell></row><row><cell></cell><cell>author_id</cell><cell>∼ 10 7</cell><cell>one-hot</cell><cell>1</cell></row><row><cell></cell><cell>author_gender</cell><cell>2</cell><cell>one-hot</cell><cell>1</cell></row><row><cell></cell><cell>author_age</cell><cell>∼ 10</cell><cell>one-hot</cell><cell>1</cell></row><row><cell></cell><cell>author_clicked_rate</cell><cell>10</cell><cell>one-hot</cell><cell>10</cell></row><row><cell></cell><cell>author_liked_rate</cell><cell>10</cell><cell>one-hot</cell><cell>10</cell></row><row><cell></cell><cell>network_type</cell><cell>∼ 4</cell><cell>one-hot</cell><cell>1</cell></row><row><cell>Context Field</cell><cell>time brand</cell><cell>∼ 10 ∼ 15</cell><cell>one-hot one-hot</cell><cell>1 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of evaluation data sets.</figDesc><table><row><cell>Data</cell><cell>#Samples</cell><cell cols="2">#Fields #Features (Sparse)</cell></row><row><cell>Avazu</cell><cell>40,428,967</cell><cell>23</cell><cell>1,544,488</cell></row><row><cell cols="2">Meitu 1,508,149,301</cell><cell>33</cell><cell>5,476,029</cell></row><row><cell>pooling layer.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(6) xDeepFM [11]: a deep model that explicitly generates features</cell></row><row><cell cols="3">with a compressed interaction network.</cell><cell></cell></row><row><cell cols="4">(7) NFFM[25]: a deep model which learns different feature repre-</cell></row><row><cell cols="4">sentations for convolutional operations and product operations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>AUC and Logloss performance of different models</figDesc><table><row><cell>Model</cell><cell cols="2">Avazu AUC Logloss</cell><cell cols="2">Meitu AUC Logloss</cell></row><row><cell>FFM</cell><cell>0.7400</cell><cell>0.3994</cell><cell>0.6300</cell><cell>0.5625</cell></row><row><cell>FwFM</cell><cell>0.7406</cell><cell>0.3988</cell><cell>0.6306</cell><cell>0.5621</cell></row><row><cell>DCN</cell><cell>0.7421</cell><cell>0.3981</cell><cell>0.6337</cell><cell>0.5606</cell></row><row><cell cols="2">DeepFM 0.7438</cell><cell>0.3982</cell><cell>0.6329</cell><cell>0.5612</cell></row><row><cell>NFM</cell><cell>0.7449</cell><cell>0.3973</cell><cell>0.6359</cell><cell>0.5596</cell></row><row><cell cols="2">xDeepFM 0.7509</cell><cell>0.3947</cell><cell>0.6440</cell><cell>0.5576</cell></row><row><cell>NFFM</cell><cell>0.7513</cell><cell>0.3945</cell><cell>0.6443</cell><cell>0.5565</cell></row><row><cell>FLEN</cell><cell cols="4">0.7519 0.3944 0.6463 0.5558</cell></row><row><cell cols="5">FLEN+D 0.7528 0.3944 0.6475 0.5554</cell></row><row><cell>method in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Model complexity and parameter size in Avazu dataset for different models</figDesc><table><row><cell></cell><cell>.</cell><cell></cell></row><row><cell>Model</cell><cell>Model Complexity</cell><cell>Parameter Size</cell></row><row><cell cols="2">FFM FwFM DCN DeepFM NFM xDeepFM O(N K e + MH 2 L + MK e H ) O(N MK e ) O(N K e + M 2 ) O(N K e + MK e L + MK e H ) O(N K e + MK e H ) O(N K e + K e H ) NFFM O(N MK e )</cell><cell>1.14 × 10 9 4.94 × 10 7 4.94 × 10 7 4.94 × 10 7 4.94 × 10 7 4.94 × 10 7 1.14 × 10 9</cell></row><row><cell>FLEN</cell><cell>O(N K</cell><cell></cell></row></table><note>e + M 2 + MK e H ) 4.94 × 10 7</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Codes are available at https://github.com/aimetrics/jarvis</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank the Runquan Xie for the valuable comments, which are beneficial to the authorsâĂŹ thoughts on recommender systems and the revision of the paper. The authors also thank Haoxuan Huang for his discussions and help in the extensive experiments at Meitu. Chen Lin is supported by the National Natural Science Foundation of China Nos. 61972328. Dugang Liu is supported by the National Natural Science Foundation of China Nos. 61872249, 61836005 and 61672358.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simple and scalable response prediction for display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient boosting factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender systems</title>
		<meeting>the 8th ACM Conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Omnipress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines in a real-world online advertising system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lefortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05170</idno>
		<title level="m">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Field-aware probabilistic embedding neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems, RecSys &apos;18</title>
		<meeting>the 12th ACM Conference on Recommender Systems, RecSys &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="412" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature generation by convolutional neural network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting response in mobile advertising with hierarchical importance-aware factorization machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finegold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fieldweighted factorization machines for click-through rate prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD</title>
		<meeting>the ADKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tem: Tree-enhanced embedding model for explainable recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1543" to="1552" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Operation-aware neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep embedding forest: Forest-based serving with deep embedding features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1703" to="1711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pbodl: Parallel bayesian online deep learning for click-through rate prediction in tencent advertising system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00802</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A sparse deep factorization machine for efficient ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06987</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
