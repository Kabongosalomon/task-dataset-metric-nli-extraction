<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Adaptive Self-Training for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
							<email>czhu@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
							<email>jqzou@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Adaptive Self-Training for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>domain adaptation</term>
					<term>semantic segmentation</term>
					<term>self-training</term>
					<term>regularization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such a problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing scalability and performance. In this paper, we propose an instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. Besides, we propose the region-guided regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. Our method is so concise and efficient that it is easy to be generalized to other unsupervised domain adaptation methods. Experiments on 'GTA5 to Cityscapes' and 'SYN-THIA to Cityscapes' demonstrate the superior performance of our approach compared with the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain shifts refer to the divergence between the training data (source domain) and the testing data (target domain), induced by factors such as the variance in illumination, object viewpoints, and image background <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref>. Such domain shifts often lead to the phenomenon that the trained model suffers from a significant performance drop in the unlabeled target domain. The unsupervised domain adaptation (UDA) methods aim to improve the model generalization performance by transferring knowledge from labeled source domain to unlabeled target domain.</p><p>More recently, an alternative research line to reduce domain shift focuses on building schemes based on the self-training (ST) framework <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>. These works iteratively train the model by using both the labeled source domain data and the generated pseudo-labels for the target domain and thus achieve the alignment between source and target domains. Besides, several works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> have explored to combine AT and ST methods, which shows great potential on semantic segmentation UDA. Through carefully designed network structure, these methods achieve state-of-the-art performance on the benchmark. <ref type="table">Table 1</ref>. Performance comparison of AT and ST. AT : adversarial training based methods; ST : self-training based methods; AT + ST : the mixed methods Method BLF <ref type="bibr" target="#b18">[19]</ref> AdaptMR <ref type="bibr" target="#b33">[34]</ref> AdaptSeg <ref type="bibr" target="#b26">[27]</ref> AdvEnt <ref type="bibr" target="#b28">[29]</ref> PyCDA <ref type="bibr" target="#b19">[20]</ref> CRST <ref type="bibr" target="#b34">[35]</ref>  Despite the success of these AT and ST methods, a natural question comes up: what is the most effective one among these methods? AT or ST? <ref type="table">Table 1</ref> lists some of the above representative methods performance on the GTA5 to Cityscapes benchmark. All these methods use the same segmentation network for a fair comparison. In terms of performance, an explicit conclusion is: AT + ST (49.0) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref> &gt; ST (47.8) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref> &gt; AT (43.7) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. The mixed methods, such as BLF <ref type="bibr" target="#b18">[19]</ref> and AdaptMR <ref type="bibr" target="#b33">[34]</ref>, both have achieved great performance gains (+ 4.2, + 5.6) after using ST. However, in order to achieve better performance, these mixed methods generally have serious coupling between sub-modules (such as network structure dependency), thus losing scalability and flexibility. This paper aims to propose a self-training framework for semantic segmentation UDA, which has good scalability that can be easily applied to other nonself-training methods and achieves state-of-the-art performance. To achieve this, we locate the main obstacle of existing self-training methods is how to generate high-quality pseudo-labels. This paper designs a new pseudo-label generation strategy and model regularization to solve this obstacle. The pseudo-label generation suffers from information redundancy and noise. The generator tends to keep pixels with high confidence as pseudo-labels and ignore pixels with low confidence. Because of this conservative threshold selection, they are inefficient when more similar samples with high confidence are applied to training. The existing class-balanced self-training (CBST) <ref type="bibr" target="#b35">[36]</ref> utilized rank-based reference confidence for each class among all related images. This results in the ignorance of key information from the hard images with most of the pixels having low prediction scores. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, the pseudolabels generated by CBST are concentrated on the road, while pedestrians and trucks are ignored, which loses much learnable information. Therefore, we try to design a pseudo-label generation that can be adjusted adaptively according to the instance strategy to reduce data redundancy and increase the diversity of pseudo-labels.</p><formula xml:id="formula_0">M G AT ( ) R G ( ) AT  (a) (b)  S  Fig. 2. IAST framework.</formula><p>(a) Warm-up phase, an initial model G is trained using any existing non-self-training method (eg. AT). (b) Self-training phase, the selector S filters the pseudo-labels generated by G, and R is the regularization</p><p>In this work, we propose an instance adaptive self-training framework (IAST) for semantic segmentation UDA, as shown in <ref type="figure">Fig. 2</ref>. We employ an instance adaptive selector in considering pseudo-label diversity during the training process. Besides, we design region-guided regularization in our framework, which has different roles in the pseudo-label region and the non-pseudo-label region. The main contributions of our work are summarized as follows:</p><p>-We propose a new self-training framework. Our methods significantly outperform the current state-of-the-art methods on the public semantic segmentation UDA benchmark. -We design an instance adaptive selector to involve more useful information for training. It effectively improves the quality of pseudo-labels. Besides, region-based regularization is designed to smooth the prediction of the pseudo-label region and sharpen the prediction of the non-pseudo-label region. -We propose a general approach that makes it easy to apply other non-selftraining methods to our framework. Moreover, our framework can also be extended to semi-supervised semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Adversarial training for UDA: A large number of UDA schemes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> are proposed to reduce the domain gap by building shared embedding space to both the source and target domain. Following the same idea, many adversarial training based UDA methods are proposed by adding a domain discriminator in recent years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32]</ref>. With adversarial training, the domain adversarial loss can be minimized to directly align features between two domains. Motivated by the recent image-to-image translation works, some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> regard the mapping from the source domain to the target domain as the image synthesis problem that reduce the domain discrepancy before training.</p><p>Self-training: Self-training schemes are commonly used in semi-supervised learning (SSL) areas <ref type="bibr" target="#b17">[18]</ref>.These works iteratively train the model by using both the labeled source domain data and the generated pseudo-labels in the target domain and thus achieve the alignment between the source and target domain <ref type="bibr" target="#b25">[26]</ref>. However, these methods directly choose pseudo-labels with high prediction confidence, which will result in the model bias towards easy classes and thus ruin the transforming performance for the hard classes. To solve this problem, the authors in <ref type="bibr" target="#b35">[36]</ref> proposed a class-balanced self-training (CBST) scheme for semantic segmentation, which shows comparable domain adaptation performance to the best adversarial training based methods. <ref type="bibr" target="#b19">[20]</ref> proposed a self-motivated pyramid curriculum domain adaptation method using self-training. More recently, CRST <ref type="bibr" target="#b34">[35]</ref> further integrated a variety of confidence regularizers to CBST, producing better domain adaption results.</p><p>Regularization: Regularization refers to schemes that are intended to reduce the testing error and thus make the trained model generalize well to unseen data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. For deep neural network learning, different kinds of regularization schemes such as weight decay <ref type="bibr" target="#b13">[14]</ref> and label smoothing <ref type="bibr" target="#b24">[25]</ref> are proposed. The recent work <ref type="bibr" target="#b34">[35]</ref> designed labels and model regularization under self-training architecture for UDA. However, the proposed regularization scheme is just applied to the pseudolabel region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UDA for Semantic Segmentation</head><p>It is assumed that there are two domains: source domain S and target domain T . The source domain includes image X S = {x s }, semantic mask Y S = {y s }, and the target domain only has image X T = {x t }. In UDA, the semantic segmentation model is trained only from the ground truth Y S as the supervisory signal. UDA semantic segmentation model can be defined as follows:</p><formula xml:id="formula_1">{X S , Y S , X T } ⇒ M U DA</formula><p>M U DA uses some special losses and domain adaptation methods to align the distribution of two domains to learn domain-invariant feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-training for UDA</head><p>Because the ground truth labels of target domain are not available, we can treat the target domain as an extra unlabeled dataset. In this case, the UDA task can be transformed into a semi-supervised learning (SSL) task. Self-training is an effective method for SSL. The problem can be described as the following forms:</p><formula xml:id="formula_2">min w L CE = − 1 |X S | xs∈X S C c=1 y (c) s log p(c|x s , w) − 1 |X T | xt∈X T C c=1ŷ (c) t log p(c|x t , w) (1)</formula><p>where C is the number of classes, y In particular,Ŷ T = {ŷ t } are the "pseudo-labels" generated according to the existing model, which is limited to a one-hot vector (only single 1 and all the others 0) or an all-zero vector. The pseudo-labels can be used as approximate target ground truth labels.</p><p>Initially, pseudo-labels are generated before the training process. After this, Eq.(1) can be used to directly minimize the cross-entropy loss of the source and target. Pseudo-labels are updated periodically during the self-training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial training for UDA</head><p>Adversarial training uses an additional discriminator to align feature distributions. The discriminator D attempts to distinguish the feature distribution in the output space of the source and target. The segmentation model M attempts to fool the discriminator to confuse the feature distributions of the source and target, thereby aligning the feature distributions. The optimization process is as follows:</p><formula xml:id="formula_3">min w max D LX AT = − 1 |X S | xs∈X S C c=1 y (c) s log p(c|x s , w) + λ adv |X T | xt∈X T [D(M(x t , w)) − 1] 2<label>(2)</label></formula><p>The first term is the cross-entropy loss of source, and the second term uses a mean squared error as the adversarial loss, where λ adv is the weight of the adversarial loss. Eq. (2) is used to optimize M and D alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>An overview of our framework is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We propose an instance adaptive self-training framework (IAST) with instance adaptive selector (IAS) and region-guided regularization. IAS selects an adaptive pseudo-label threshold for each semantic category in units of images and dynamically reduces the proportion of "hard" classes, to eliminate noise in the pseudo-labels. Besides, regionguided regularization is designed to smooth the prediction of the confident region and sharpen the prediction of the ignored region. Our overall objective function is as follows:</p><formula xml:id="formula_4">min w L CE (w,Ŷ T ) + L R (w) = L CE (w,Ŷ T ) + (λ i R i (w) + λ c R c (w)) (3)</formula><p>where L CE is the cross-entropy loss, which is different from Eq.(1) and only calculates the cross-entropy loss of the target domain images.Ŷ T is the set of pseudo-labels, and the detailed generation process is described in Section 4.1. R i and R c are regularization of the ignored and confidence regions, which is described in Section 4.2. And λ i , λ c are regularization weights. The IAST training process consists of three phases.</p><p>-(a) In the warm-up phase, a non-self-training method uses both the source data and the target data to train an initial segmentation model M 0 as the initial pseudo-label generator G 0 . -(b) In the pseudo-label generation phase, G is used to obtain the prediction result of the target data, and a pseudo-label is generated by an instance adaptive selector.</p><p>-(c) In the self-training phase, according to Eq.(3), the segmentation model M is trained using the target data.</p><p>Why warm-up? Before self-training, we expect to have a stable pre-trained model so that IAST can be trained in the right direction and avoid disturbances caused by constant fitting the noise of pseudo-labels. We use the adversarial training method described in Section 3.3 to obtain a stable model by roughly aligning the output of the source and target. In addition, in the warm-up phase, we can optionally apply any other semantic segmentation UDA method as the basic method, and it can be retained even in the (c) phase. In fact, we can use IAST as a decorator to decorate other basic methods.</p><p>Multi-round self-training. Performing (b) phase and (c) phase once counts as one round. As with other self-training tasks, in this experiment we performed a total of three rounds. At the end of each round, the parameters of model M will be copied into model G to generate better target domain prediction results in the next round.</p><formula xml:id="formula_5">! = 0.9</formula><p>road person road person road person road person</p><formula xml:id="formula_6">x &amp;'( x &amp; x &amp;'( x &amp; x &amp;'( x &amp; (#$%&amp;) (()#*$+)</formula><p>,-. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pseudo-Label Generation Strategy with an Instance Adaptive Selector</head><p>Pseudo-labelsŶ T have a decisive effect on the quality of self-training. The generic pseudo-label generation strategy can be simplified to the following form when segmentation model parameter w is fixed:</p><formula xml:id="formula_7">min Y T − 1 |X T | xt∈X T C c=1ŷ (c) t log p(c|x t , w) θ (c) t s.t.ŷ t ∈ {[onehot] C } ∪ 0 , ∀ŷ t ∈Ŷ T (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 pseudo-labels generation</head><p>Input: Model M, target instance {xt} T , Parameter: proportion α, momentum β, weight decay γ, Output: target pseudo-labels 1: init θ0 = 0.9 2: for t = 1 to T do 3: P index = arg max(M(xt)) 4:</p><p>P value = max(M(xt)) 5:</p><p>for c = 1 to C do 6: P (c)  </p><formula xml:id="formula_8">x t = sort(P value [P index = c], descending) 7: θ (c) x t = Ψ (xt, θ</formula><p>When class c output probability p(c|x t , w) &gt; θ (c) , these pixels are regarded as confidence region (pseudo-label region), and the rest are ignored regions (nonpseudo-label region). Therefore, θ (c) become the key to the pseudo-labels generation process. As shown in <ref type="figure" target="#fig_3">Fig.4: (a)</ref> the traditional pseudo-labels generation strategy based on a constant confidence threshold; (b) the generation strategy which uses the same class-balanced θ for all target images; (c) we propose a data diversity-driven pseudo-labels generation strategy with an instant adaptive selector (IAS).</p><p>IAS maintains two thresholds {θ t , θ xt }, where θ t indicates the historical threshold and θ xt indicates the threshold of current instance x t . During the generation process, IAS dynamically updates θ t based on θ xt of the current instance x t , so each instance gets an adaptive threshold, combining global and local information. Specifically, for each instance x t , we sort the confidence probability of each class in descending order, and then take the α × 100% confidence probability as the local threshold θ (c) xt for each class in instance x t . Finally, we use the exponentially weighted moving average to update the threshold θ t containing historical information as the global threshold. The details are summarized in Algorithm1.</p><p>Exponential moving average (EMA) threshold. When generating pseudolabels one by one, we use an exponential moving average method, denoted as Eq. <ref type="formula">(6)</ref>, which can smooth the threshold of each instance, introduce past historical information, and avoid noise interference. Eq.(7) Ψ (x t , θ (c) t−1 ) represents the threshold for acquiring the current instance x t . β is a momentum factor used to preserve past threshold information. As β increases, the threshold θ</p><formula xml:id="formula_10">(c) t becomes smoother. θ (c) t = βθ (c) t−1 + (1 − β)Ψ (x t , θ (c) t−1 ) (6) Ψ (x t , θ (c) t−1 ) = P (c) xt αθ (c) t−1 γ |P (c) xt |<label>(7)</label></formula><p>"Hard" classes weight decay (HWD). For "hard" classes, pseudo-labels tend to bring more noise labels. In Eq. <ref type="formula" target="#formula_10">(7)</ref>, we design θ (c) t−1 γ to modify the proportion of pseudo-labels α. γ is a weight decay parameter, which is used to control the decay degree. The thresholds θ (c) t−1 of the "hard" classes are usually smaller, so HWD reduces more pseudo-labels of "hard" classes. On the contrary the thresholds θ (c) t−1 of easy classes is usually larger, so HWD has a weaker impact. It is easy to prove that when Ψ (x t , θ (c)</p><formula xml:id="formula_11">t−1 ) = θ (c)</formula><p>t−1 , θ will converge to a larger value, thereby reduce the amount of the "hard" classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Region-Guided Regularization</head><p>Confident region KLD minimization. During training, the model is prone to overfit pseudo-labels, which will damage the model. For the confidence region</p><formula xml:id="formula_12">I xt = {1 |ŷ (h,w) t &gt; 0}</formula><p>, there are pseudo labels as supervising signals to supervise the model for learning. However, as shown in <ref type="table">Table 4</ref>, although a series of techniques for generating high-confidence pseudo labels have been used, the quality of the pseudo labels is still not as good as the ground truth labels, which means that there are some noise labels in the pseudo-labels. How to reduce the impact of noise labels is a key issue. Zou et al. <ref type="bibr" target="#b34">[35]</ref> has proposed various regularization for this. We use the KLD which works best in <ref type="bibr" target="#b34">[35]</ref> to smooth the prediction results of the confidence region, so that the prediction results do not overfit the pseudo-labels.</p><formula xml:id="formula_13">R c = − 1 |X T | xt∈X T I xt C c=1 1 C log p(c|x t , w)<label>(8)</label></formula><p>As shown in Eq.(8), when the prediction result log p(c|x t , w) is approximately close to the uniform distribution (the probability of each class is 1 C ), R c gets smaller. KLD minimization promotes smoothing of confidence regionsand avoid the model blindly trusting false labels.</p><p>Ignored region entropy minimization. On the other hand, for the ignored region I xt = {1 |ŷ (h,w) t = 0}, there is no supervision signal during the training process. Because the prediction result of the region I xt is smooth and has low confidence, we use the minimized entropy of the ignored region to prompt the model to predict the low entropy result, which makes the prediction result look more "sharper".</p><formula xml:id="formula_14">R i = − 1 |X T | xt∈X T I xt C c=1 p(c|x t , w) log p(c|x t , w)<label>(9)</label></formula><p>As shown in Eq.(9), sharpening the prediction result of the ignored region by minimizing R i can promote the model to learn more useful features from the ignored region without any supervised signal, which has also been proved to be effective for UDA in the work <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Network architecture and datasets. We adapt Deeplab-v2 <ref type="bibr" target="#b2">[3]</ref>, which is widely used in the semantic segmentation UDA problem, as our basic network architecture. ResNet-101 <ref type="bibr" target="#b7">[8]</ref> is selected as the backbone network of the model. All experiments in this work are carried out under this network architecture. We evaluate our UDA methods for semantic segmentation on the popular syntheticto-real adaptation scenarios: (a) GTA5 <ref type="bibr" target="#b22">[23]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref>, (b) SYNTHIA <ref type="bibr" target="#b23">[24]</ref> to Cityscapes. The GTA5 dataset has 24966 images that are rendered from the GTA5 game and 19 classes with Cityscapes. SYNTHIA dataset includes 9400 images and 16 common classes with Cityscapes. Cityscapes is split into training set, validation set, and testing set. Following the standard protocols in <ref type="bibr" target="#b26">[27]</ref>, we use the training set which has 2975 images as the target dataset and use the validation dataset to evaluate our models with mIoU. Implementation details. In our experiments, we implement IAST using Py-Torch on an NVIDIA Tesla V100. The training images are randomly cropped and resized to 1024 × 512, the aspect ratio of the crop window is 2.0, and the window height is randomly selected from [341 ∼ 950] for GTA5 and [341 ∼ 640] for SYNTHIA. All weights of batch normalization layers were frozen. Deeplab-v2 is pre-trained on ImageNet. In IAST, we adopt Adam with learning rate 2.5 × 10 −5 , batch size 6 for 4 epochs. The pseud-label parameters α, β, γ are set to 0.2, 0.9 and 8.0. The regularization weights λ i and λ c are set to 3.0 and 0.1. Our code code and pre-trained molels are available at: https: //github.com/Raykoooo/IAST</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion and Ablation Study</head><p>Why IAS works? <ref type="table" target="#tab_1">Table 2</ref> shows a sensitivity analysis on the parameter α and β. When we set α = 0.2 and β = 0, it means IAS takes 20% of each image as the <ref type="figure">Fig. 5</ref>. Visualization of pseudo-labels. Columns correspond to original images with ground truth labels, our method, and class-balanced method <ref type="bibr" target="#b35">[36]</ref> confidence region. As a comparison, the class-balanced method <ref type="bibr" target="#b35">[36]</ref> takes 20% of pixels in the whole target set as the confidence region. As shown in <ref type="figure">Fig. 5</ref>, pseudo-labels of class-balanced method miss some pixels for persons, cars and bikes. In contrast, the pseudo-labels of our method are more diverse, especially for some "hard" classes. When we set α = 0.2 and β = 0.9, IAS combines global and local information to get more diverse content so that the model achieve the best performance.  <ref type="figure" target="#fig_3">(Fig. 4 a)</ref> 38.6 45.1 Class-balanced( <ref type="figure" target="#fig_3">Fig. 4 b)</ref> 20.0 47.9  <ref type="figure" target="#fig_6">Fig.6</ref> shows that as the γ increases, the proportion of some easy classes (sky, car) that have a high prediction score does not decrease significantly, while the proportion of some "hard" classes (motor, wall, fence and pole) that have a low prediction score decreases sharply. This proves that Eq.(7) can effectively reduce the pseudo-labels of "hard" classes and suppress noise interference in the pseudo-labels. <ref type="table">Table 4</ref> shows a sensitivity analysis on the parameter γ. We find that as the γ increases, pseudo-labels have smaller proportions but have better quality. Therefore, we let γ = 8 as the trade-off between the proportion and the quality of pseudo-labels. On the contrary, moderate regularization helps the model to improve the prediction accuracy and avoid overfitting the noise labels. <ref type="table" target="#tab_2">Table 3</ref> shows a sensitivity analysis of the parameter λ i and λ c . We performed multiple sets of experiments with fixed λ i and λ c , respectively. When λ c = 0.1 is fixed and λ i is gradually increased, the overall model performance tends to improve until λ i = 4. It can be expected that when the low entropy prediction is excessively performed in the non-pseudo-label region, the influence of noise will be amplified and the model will be damaged.  Ablation studies. The results of the ablation studies are reported in <ref type="table" target="#tab_3">Table 5</ref>.</p><p>We attempt the methods proposed in Section 4.1 and Section 4.2 one by one to study their performance in the test set. From the data in <ref type="table" target="#tab_3">Table 5</ref>, after using selftraining ( <ref type="figure" target="#fig_3">Fig. 4 a)</ref> without using any other techniques, the model performance has a gain of 1.3%. After adding IAST modules (IAS, R i , R c ), the performance of the model is gradually and steadily improved, and finally, 51.5% mIoU is achieved. In addition, we also try multi-scale testing and the combined result achieved the best 52.2% mIoU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>Comparison with the state-of-the-art methods:The results of IAST and some other state-of-the-art methods on GTA5 to Cityscapes are present in Ta-ble6. From the overall results, IAST has the best mIoU 52.2% and has obvious advantages over other methods. Compared with some adversarial training methods AdaptSegNet <ref type="bibr" target="#b26">[27]</ref> and SIBAN <ref type="bibr" target="#b21">[22]</ref>, IAST improves by 9.6% mIoU and have significant gains in almost all classes. Compared with the same self-training methods such as MRKLD <ref type="bibr" target="#b34">[35]</ref>, IAST improves by 4.8% mIoU. In addition, BLF <ref type="bibr" target="#b18">[19]</ref> is a method that combines adversarial training and self-training, which has the second-best 48.5% mIoU. Compared to BLF, IAST still has a significant improvement. <ref type="table">Table 7</ref> is the results of the SYNTHIA to Cityscapes dataset. For a comprehensive comparison, as in the previous work, we also report two mIoU metrics: 13 classes of mIoU* and 16 classes of mIoU. The domain gap between SYN-THIA and Cityscapes is much larger than the domain gap between GTA5 and Cityscapes. Many of the methods that performed well on GTA5 to Cityscapes have experienced a significant performance degradation on this dataset. Correspondingly, the performance gap between different methods is becoming more apparent. IAST also achieves the best results, which are 49.8% mIoU and 57.0% mIoU* and significantly higher than all recent state-of-the-art methods. Apply to other UDA methods. Because IAST has no special structure or model dependencies, it can be directly used to decorate other UDA methods. We chose two typical adversarial training methods, AdaptSeg <ref type="bibr" target="#b26">[27]</ref> and AdvEnt <ref type="bibr" target="#b28">[29]</ref> for experiments. As shown in <ref type="table">Table 9</ref>, these two methods have significantly improved performance under the IAST framework.</p><p>Extension: other tasks. The self-training method can also be applied to semisupervised semantic segmentation task. We use the same configuration as <ref type="bibr" target="#b10">[11]</ref> in Cityscapes for semi-supervised training with different proportions of data as labeled data. As shown in <ref type="table">Table 8</ref>, we have significantly better performance than <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose an instance adaptive self-training framework for semantic segmentation UDA. Compared with other popular UDA methods, IAST still has a significant improvement in performance. Moreover, IAST is a method with no model or special structure dependency, which means that it can be easily applied to other UDA methods with almost no additional cost to improve performance. In addition, IAST can also be applied to semi-supervised semantic segmentation tasks, which also achieves state-of-the-art performance. We hope this work will prompt people to rethink the potential of self-training on UDA or semi-supervised learning tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Pseudo-label results. Columns correspond to original images with ground truth labels, class-balanced method, and our method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) s indicates the label of class c in source domain, andŷ (c) t indicates the pseudo-label of class c in target domain. x s and x t are input images, w indicates weights of M, p(c|x, w) is the probability of class c in softmax output, and |X| indicates the number of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed IAST framework overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(Fig. 4 .</head><label>4</label><figDesc>Illustration of three different thresholding methods. xt−1 and xt represent two consecutive instances, the bars approximately represent the probabilities of each class. (a) A constant threshold is used for all instances. (b) class-balanced thresholds are used for all instances. (c) Our method adaptively adjusts the threshold of each class based on the instance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>θt = βθt−1 + (1 − β)θx t Eq.(6) 10:ŷt = onehot(P index [P value &gt; θt]) 11: end for 12: return {ŷt} T where θ (c) indicates the confidence threshold for class c, andŷ t = [ŷ (1) t , ...,ŷ (C) t ] is required to be a one-hot vector or a all-zero vector. Therefore,ŷ (c) t can be solved by Eq.(5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 ,</head><label>1</label><figDesc>if c = arg max c p(c|x t , w) and p(c|x t , w) &gt; θ (c) 0, otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Relationship between the pseudolabels proportion and γ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 4 .</head><label>4</label><figDesc>γ sensitivity analysis (α = 0.2, β = 1.0). P-mIoU means mIoU of pseudo-labels (%) 65.6 66.3 67.4 68.2 69.0 mIoU (%) 50.5 50.8 51.2 51.5 50.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>α and β sensitivity analysis (GTA5 to Cityscapes)</figDesc><table><row><cell>α</cell><cell>β</cell><cell cols="2">Proportion(%) mIoU(%)</cell></row><row><cell>.20</cell><cell>.0</cell><cell>20.0</cell><cell>49.8</cell></row><row><cell>.20</cell><cell>.50</cell><cell>31.2</cell><cell>50.3</cell></row><row><cell>.20</cell><cell>.90</cell><cell>36.5</cell><cell>50.5</cell></row><row><cell>.20</cell><cell>.99</cell><cell>40.1</cell><cell>50.0</cell></row><row><cell>.30</cell><cell>.90</cell><cell>42.5</cell><cell>49.7</cell></row><row><cell>.50</cell><cell>.90</cell><cell>48.6</cell><cell>48.2</cell></row><row><cell>Constant</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>λi and λc sensitivity analysis (GTA5 to Cityscapes)</figDesc><table><row><cell>λi</cell><cell>λc</cell><cell>mIoU(%)</cell></row><row><cell>.5</cell><cell>.10</cell><cell>50.6</cell></row><row><cell>1.0</cell><cell>.10</cell><cell>51.1</cell></row><row><cell>2.0</cell><cell>.10</cell><cell>50.9</cell></row><row><cell>3.0</cell><cell>.10</cell><cell>51.5</cell></row><row><cell>4.0</cell><cell>.10</cell><cell>51.2</cell></row><row><cell>5.0</cell><cell>.10</cell><cell>51.3</cell></row><row><cell>3.0</cell><cell>.05</cell><cell>50.6</cell></row><row><cell>3.0</cell><cell>.15</cell><cell>51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results of ablation study (GTA5 to Cityscapes)</figDesc><table><row><cell>Method</cell><cell cols="5">ST IAS Rc Ri mIoU</cell><cell>∆</cell></row><row><cell>Source</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35.6</cell><cell>0</cell></row><row><cell>Warm-up</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.8</cell><cell>+8.2</cell></row><row><cell>+ Constant ST(Fig. 4 a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.1</cell><cell>+1.3</cell></row><row><cell>+ Instance adaptive selector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.8</cell><cell>+4.7</cell></row><row><cell>+ Confidence region R.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.7</cell><cell>+0.9</cell></row><row><cell>+ Ignored region R.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.5</cell><cell>+0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Results of our proposed method IAST and other state-of-the-art methods (GTA5 to Cityscapes). A&amp;S means a mixed method of AT and ST 25.1 43.1 34.2 84.8 34.6 88.7 62.7 30.3 87.6 42.3 50.3 24.7 35.2 40.2 52.2 Results of our proposed method IAST and other state-of-the-art methods (SYNTHIA to Cityscapes) 83.3 17.7 4.6 32.3 30.9 28.8 83.4 85.0 65.5 30.8 86.5 38.2 33.1 52.7 49.8 57.0</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>Road</cell><cell>SW</cell><cell>Build</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>TL</cell><cell>TS</cell><cell>Veg.</cell><cell>Terrain</cell><cell>Sky</cell><cell>PR</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>Motor</cell><cell>Bike</cell><cell>mIoU</cell></row><row><cell>Source [27]</cell><cell></cell><cell cols="20">75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6</cell></row><row><cell>AdaptSegNet [27]</cell><cell></cell><cell cols="20">86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4</cell></row><row><cell>SIBAN [22] SSF-DAN [6]</cell><cell>AT</cell><cell cols="20">88.5 35.4 79.5 26.3 24.3 28.5 32.5 18.3 81.2 40.0 76.5 58.1 25.8 82.6 30.3 34.3 3.4 21.6 21.5 42.6 90.3 38.9 81.7 24.8 22.9 30.5 37.0 21.2 84.8 38.8 76.9 58.8 30.7 85.7 30.6 38.1 5.9 28.3 36.9 45.4</cell></row><row><cell>AdvEnt [29]</cell><cell></cell><cell cols="20">89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.4</cell></row><row><cell>APODA [30]</cell><cell></cell><cell cols="20">85.6 32.8 79.0 29.5 25.5 26.8 34.6 19.9 83.7 40.6 77.9 59.2 28.3 84.6 34.6 49.2 8.0 32.6 39.6 45.9</cell></row><row><cell>Source [36]</cell><cell></cell><cell cols="20">71.3 19.2 69.1 18.4 10.0 35.7 27.3 6.8 79.6 24.8 72.1 57.6 19.5 55.5 15.5 15.1 11.7 21.1 12.0 33.8</cell></row><row><cell>CBST [36] PyCDA[20]</cell><cell>ST</cell><cell cols="20">91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 90.5 36.3 84.4 32.4 28.7 34.6 36.4 31.5 86.8 37.9 78.5 62.3 21.5 85.6 27.9 34.8 18.0 22.9 49.3 47.4</cell></row><row><cell>MRKLD [35]</cell><cell></cell><cell cols="20">91.0 55.4 80.0 33.7 21.4 37.3 32.9 24.5 85.0 34.1 80.8 57.7 24.6 84.1 27.8 30.1 26.9 26.0 42.3 47.1</cell></row><row><cell>BLF [19]</cell><cell></cell><cell cols="20">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5</cell></row><row><cell>AdaptMR [34]</cell><cell>A&amp;S</cell><cell cols="20">90.5 35.0 84.6 34.3 24.0 36.8 44.1 42.7 84.5 33.6 82.5 63.1 34.4 85.8 32.9 38.2 2.0 27.1 41.8 48.3</cell></row><row><cell>PatchAlign [28]</cell><cell></cell><cell cols="20">92.3 51.9 82.1 29.2 25.1 24.5 33.8 33.0 82.4 32.8 82.2 58.6 27.2 84.3 33.4 26.3 2.2 29.5 32.3 46.5</cell></row><row><cell>Source(ours)</cell><cell></cell><cell cols="20">64.8 21.7 74.3 15.4 21.2 18.2 30.7 13.0 80.9 33.7 76.3 55.6 20.0 43.9 27.0 35.5 4.4 24.9 14.3 35.6</cell></row><row><cell>IAST(ours)</cell><cell>A&amp;S</cell><cell cols="20">93.8 57.8 85.1 39.5 26.7 26.2 43.1 34.7 84.9 32.9 88.0 62.6 29.0 87.3 39.2 49.6 23.2 34.7 39.6 51.5</cell></row><row><cell cols="7">IAST-MST(ours) 94.1 58.8 85.4 39.7 29.2 Method Arch. Road SW Build Wall* Fence*</cell><cell>Pole*</cell><cell>TL</cell><cell>TS</cell><cell>Veg.</cell><cell></cell><cell>Sky</cell><cell>PR</cell><cell>Rider</cell><cell>Car</cell><cell>Bus</cell><cell>Motor</cell><cell>Bike</cell><cell cols="3">mIoU mIoU*</cell></row><row><cell>Source [27]</cell><cell></cell><cell cols="3">55.6 23.8 74.6</cell><cell></cell><cell>--</cell><cell cols="12">-6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0</cell><cell>-</cell><cell></cell><cell>38.6</cell></row><row><cell>AdaptSegNet [27]</cell><cell></cell><cell cols="3">84.3 42.7 77.5</cell><cell></cell><cell>--</cell><cell cols="12">-4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3</cell><cell>-</cell><cell></cell><cell>46.7</cell></row><row><cell>SIBAN [22] SSF-DAN [6]</cell><cell>AT</cell><cell cols="3">82.5 24.0 79.4 84.6 41.7 80.8</cell><cell></cell><cell>----</cell><cell cols="12">-16.5 12.7 79.2 82.8 58.3 18.0 79.3 25.3 17.6 25.9 -11.5 14.7 80.8 85.3 57.5 21.6 82.0 36.0 19.3 34.5</cell><cell>--</cell><cell></cell><cell>46.3 50.0</cell></row><row><cell>AdvEnt [29]</cell><cell></cell><cell cols="18">85.6 42.2 79.7 8.7 0.4 25.9 5.4 8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2</cell><cell></cell><cell>48.0</cell></row><row><cell>APODA [30]</cell><cell></cell><cell cols="3">86.4 41.3 79.3</cell><cell></cell><cell>--</cell><cell cols="12">-22.6 17.3 80.3 81.6 56.9 21.0 84.1 49.1 24.6 45.7</cell><cell>-</cell><cell></cell><cell>53.1</cell></row><row><cell>Source [36]</cell><cell></cell><cell cols="18">64.3 21.3 73.1 2.4 1.1 31.4 7.0 27.7 63.1 67.6 42.2 19.9 73.1 15.3 10.5 38.9 34.9</cell><cell></cell><cell>40.3</cell></row><row><cell>CBST [36] PyCDA [20]</cell><cell>ST</cell><cell cols="18">68.0 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5 18.8 39.8 42.6 75.5 30.9 83.3 20.8 0.7 32.7 27.3 33.5 84.7 85.0 64.1 25.4 85.0 45.2 21.2 32.0 46.7</cell><cell></cell><cell>48.9 53.3</cell></row><row><cell>MRKLD [35]</cell><cell></cell><cell cols="18">67.7 32.2 73.9 10.7 1.6 37.4 22.2 31.2 80.8 80.5 60.8 29.1 82.8 25.0 19.4 45.3 43.8</cell><cell></cell><cell>50.1</cell></row><row><cell>BLF [19]</cell><cell></cell><cell cols="3">86.0 46.7 80.3</cell><cell></cell><cell>--</cell><cell cols="12">-14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell></cell><cell>51.4</cell></row><row><cell>AdaptMR [34]</cell><cell>A&amp;S</cell><cell cols="18">83.1 38.2 81.7 9.3 1.0 35.1 30.3 19.9 82.0 80.1 62.8 21.1 84.4 37.8 24.5 53.3 46.5</cell><cell></cell><cell>53.8</cell></row><row><cell>PatchAlign [28]</cell><cell></cell><cell cols="18">82.4 38.0 78.6 8.7 0.6 26.0 3.9 11.1 75.5 84.6 53.5 21.6 71.4 32.6 19.3 31.7 40.0</cell><cell></cell><cell>46.5</cell></row><row><cell>Source(ours) IAST(ours)</cell><cell>A&amp;S</cell><cell cols="18">63.4 24.1 66.7 7.1 0.1 28.4 11.6 16.8 77.0 74.6 60.4 20.5 75.6 22.0 14.4 21.2 36.5 81.9 41.5</cell><cell></cell><cell>42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Semi-supervised learning results on the Cityscapes val set. Extension analysis, applying IAST to non-self-learning UDA methods<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> (test on Cityscapes), and Source means training IAST without warmup</figDesc><table><row><cell cols="2">1/8, 1/4 and 1/2 mean the propor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tion of labeled images</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Data Amount 1/8 1/4 1/2 Full</cell><cell>Method</cell><cell>GTA5 Base +IAST ∆</cell><cell>SYNTHIA Base +IAST ∆</cell></row><row><cell cols="2">Baseline Univ-full[12] 55.9 -57.3 59.0 61.2 70.2 --AdvSemi[11] 58.8 62.3 65.7 67.7</cell><cell cols="3">AdaptSeg[27] 42.4 50.2 +7.8 46.7 54.7 +8.0 AdvEnt[29] 45.4 49.8 +4.4 48.0 55.1 +7.1 Source 35.6 48.8 +13.2 42.2 54.2 +12.0</cell></row><row><cell cols="2">IAST(ours) 64.6 66.7 69.8 70.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the Natural Science Foundation of Beijing Municipality under Grant 4182044.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ssfdan: Separated semantic feature based domain adaptation network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>1, 2, 6, 7</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bi-shifting auto-encoder for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3846" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kukačka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10686</idno>
		<title level="m">Regularization for deep learning: A taxonomy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10285" to="10295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-human parsing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Setred: Self-training with editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="611" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2019) 1, 1, 2, 5.3, 6</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<meeting><address><addrLine>1, 1, 2, 6, 7</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6758" to="6767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<title level="m">Conditional adversarial domain adaptation. In: NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2019) 5.3, 6</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KAIS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="284" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019) 1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2019) 1, 1, 2, 4.2, 6, 7, 9</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards pose invariant face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2207" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Selfsupervised neural aggregation networks for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11164</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>1, 1, 6, 7</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. pp</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>2019) 1, 1, 2, 4.2, 5.3, 6</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
