<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><forename type="middle">Majumder</forename><surname>†≡</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>≡ means authors contributed equally</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, and so on. Currently systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the stateof-the-art by a significant margin on two different datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion detection in conversations has been gaining increasing attention from the research community due to its applications in many important tasks such as opinion mining over chat history and social media threads in YouTube, Facebook, Twitter, and so on. In this paper, we present a method based on recurrent neural networks (RNN) that can cater to these needs by processing the huge amount of available conversational data.</p><p>Current systems, including the state of the art <ref type="bibr" target="#b10">(Hazarika et al. 2018)</ref>, do not distinguish different parties in a conversation in a meaningful way. They are not aware of the speaker of a given utterance. In contrast, we model individual parties with party states, as the conversation flows, by relying on the utterance, the context, and current party state. Our model is based on the assumption that there are three major aspects relevant to the emotion in a conversation: the speaker, the context from the preceding utterances, and the emotion of the preceding utterances. These three aspects are not necessarily independent, but their separate modeling significantly outperforms the state-of-the-art <ref type="table" target="#tab_7">(Table 2</ref>). In dyadic conversations, the parties have distinct roles. Hence, to extract the context, it is crucial to consider the preceding turns of both the speaker and the listener at a given moment <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Our proposed DialogueRNN system employs three gated recurrent units (GRU) <ref type="bibr" target="#b5">(Chung et al. 2014)</ref> to model these aspects. The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. state, respectively. The global GRU encodes corresponding party information while encoding an utterance.</p><p>Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation. The speaker state depends on this context through attention and the speaker's previous state. This ensures that at time t, the speaker state directly gets information from the speaker's previous state and global GRU which has information on the preceding parties. Finally, the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance, which is used for emotion classification. At time t, the emotion GRU cell gets the emotion representation of t − 1 and the speaker state of t.</p><p>The emotion GRU, along with the global GRU, plays a pivotal role in inter-party relation modeling. On the other hand, party GRU models relation between two sequential states of the same party. In DialogueRNN, all these three different types of GRUs are connected in a recurrent manner. We believe that DialogueRNN outperforms state-of-the-art contextual emotion classifiers such as <ref type="bibr" target="#b10">(Hazarika et al. 2018;</ref><ref type="bibr" target="#b17">Poria et al. 2017</ref>) because of better context representation.</p><p>The rest of the paper is organized as follows: Section 2 discusses related work; Section 3 provides detailed description of our model; Sections 4 and 5 present the experimental results; finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Emotion recognition has attracted attention in various fields such as natural language processing, psychology, cognitive science, and so on <ref type="bibr" target="#b16">(Picard 2010)</ref>. <ref type="bibr" target="#b7">Ekman (1993)</ref> found correlation between emotion and facial cues. <ref type="bibr" target="#b6">Datcu and Rothkrantz (2008)</ref> fused acoustic information with visual cues for emotion recognition. <ref type="bibr" target="#b0">Alm, Roth, and Sproat (2005)</ref> introduced text-based emotion recognition, developed in the work of <ref type="bibr" target="#b21">Strapparava and Mihalcea (2010)</ref>.  used contextual information for emotion recognition in multimodal setting. Recently, <ref type="bibr" target="#b17">Poria et al. (2017)</ref> successfully used RNN-based deep networks for multimodal emotion recognition, which was followed by other works <ref type="bibr" target="#b25">Zadeh et al. 2018a;</ref><ref type="bibr" target="#b26">2018b)</ref>.</p><p>Reproducing human interaction requires deep understanding of conversation. <ref type="bibr" target="#b19">Ruusuvuori (2013)</ref>   gued that emotional dynamics in a conversation is an interpersonal phenomenon <ref type="bibr" target="#b18">(Richards, Butler, and Gross 2003)</ref>. Hence, our model incorporates inter-personal interactions in an effective way. Further, since conversations have a natural temporal nature, we adopt the temporal nature through recurrent network . Memory networks <ref type="bibr" target="#b22">(Sukhbaatar et al. 2015)</ref> has been successful in several NLP areas, including question answering <ref type="bibr" target="#b22">(Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b14">Kumar et al. 2016)</ref>, machine translation <ref type="bibr" target="#b2">(Bahdanau, Cho, and Bengio 2014)</ref>, speech recognition <ref type="bibr" target="#b9">(Graves, Wayne, and Danihelka 2014)</ref>, and so on. Thus, <ref type="bibr" target="#b10">Hazarika et al. (2018)</ref> used memory networks for emotion recognition in dyadic conversations, where two distinct memory networks enabled inter-speaker interaction, yielding state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let there be M parties/participants p 1 , p 2 , . . . , p M (M = 2 for the datasets we used) in a conversation. The task is to predict the emotion labels (happy, sad, neutral, angry, excited, and frustrated) of the constituent utterances u 1 , u 2 , . . . , u N , where utterance u t is uttered by party p s(ut) , while s being the mapping between utterance and index of its corresponding party. Also, u t ∈ R Dm is the utterance representation, obtained using feature extractors described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unimodal Feature Extraction</head><p>For a fair comparison with the state-of-the-art method, conversational memory networks (CMN) <ref type="bibr" target="#b10">(Hazarika et al. 2018)</ref>, we follow identical feature extraction procedures.</p><p>Textual Feature Extraction We employ convolutional neural networks (CNN) for textual feature extraction. Following <ref type="bibr" target="#b12">Kim (2014)</ref>, we obtain n-gram features from each utterance using three distinct convolution filters of sizes 3, 4, and 5 respectively, each having 50 feature-maps. Outputs are then subjected to max-pooling followed by rectified linear unit (ReLU) activation. These activations are concatenated and fed to a 100 dimensional dense layer, which is regarded as the textual utterance representation. This network is trained at utterance level with the emotion labels.</p><p>Audio and Visual Feature Extraction Identical to <ref type="bibr" target="#b10">Hazarika et al. (2018)</ref>, we use 3D-CNN and openSMILE (Eyben, Wöllmer, and Schuller 2010) for visual and acoustic feature extraction, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Our Model</head><p>We assume that the emotion of an utterance in a conversation depends on three major factors:</p><p>1. the speaker.</p><p>2. the context given by the preceding utterances.</p><p>3. the emotion behind the preceding utterances.</p><p>Our model DialogueRNN, 1 shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>, models these three factors as follows: each party is modeled using a party state which changes as and when that party utters an utterance. This enables the model to track the parties' emotion dynamics through the conversations, which is related to the emotion behind the utterances. Furthermore, the context of an utterance is modeled using a global state (called global, because of being shared among the parties), where the preceding utterances and the party states are jointly encoded for context representation, necessary for accurate party state representation. Finally, the model infers emotion representation from the party state of the speaker along with the preceding speakers' states as context. This emotion representation is used for the final emotion classification.</p><p>We use GRU cells <ref type="bibr" target="#b5">(Chung et al. 2014)</ref> to update the states and representations. Each GRU cell computes a hidden state defined as h t = GRU * (h t−1 , x t ), where x t is the current input and h t−1 is the previous GRU state. h t also serves as the current GRU output. We provide the GRU computation details in the supplementary. GRUs are efficient networks with trainable parameters: W {r,z,c} * ,{h,x} and b {r,z,c} * . We model the emotion representation of the current utterance as a function of the emotion representation of the previous utterance and the state of the current speaker. Finally, this emotion representation is sent to a softmax layer for emotion classification.</p><p>Global State (Global GRU) Global state aims to capture the context of a given utterance by jointly encoding utterance and speaker state. Each state also serves as speaker-specific utterance representation. Attending on these states facilitates the inter-speaker and inter-utterance dependencies to produce improved context representation. The current utterance u t changes the speaker's state from q s(ut),t−1 to q s(ut),t . We capture this change with GRU cell GRU G with output size D G , using u t and q s(ut),t−1 :</p><formula xml:id="formula_0">g t = GRU G (g t−1 , (u t ⊕ q s(ut),t−1 )),<label>(1)</label></formula><p>where D G is the size of global state vector, D P is the size of party state vector, W </p><formula xml:id="formula_1">{r,z,c} G,h ∈ R D G ×D G , W {r,z,c} G,x ∈ R D G ×(Dm+D P ) , b {r,z,c} G ∈ R D G , q s(ut),t−1 ∈ R D P , g t , g t−1 ∈ R D G , D P is</formula><formula xml:id="formula_2">y t GRU P GRU P GRU G GRU G q A, t−1 q A, t q B, t q B, t+1 T A V u t Attention c t c t+1 T A V u t+1 Input Attention Input g t g t+1 time t time t+1 Person B GRU E e t GRU E e t+1 y tŷ t+1 q A, t−1 q B, t Note: Concatenation Dot Product Matmul Speaker-state Global-state Emotion-rep g t−1 g 1 g 1 g t Attention block for time t c t u t</formula><p>Context-vector:  Party State (Party GRU) DialogueRNN keeps track of the state of individual speakers using fixed size vectors q 1 , q 2 , . . . , q M through out the conversation. These states are representative of the speakers' state in the conversation, relevant to emotion classification. We update these states based on the current (at time t) role of a participant in the conversation, which is either speaker or listener, and the incoming utterance u t . These state vectors are initialized with null vectors for all the participants. The main purpose of this module is to ensure that the model is aware of the speaker of each utterance and handle it accordingly.</p><formula xml:id="formula_3">GRUG GRUG g 0 g 1 g t−2 g t−1 (a) GRU P q i, t−1 GRU G u t g t−1 c t GRU L q j, t−1 g t q i, t q j,</formula><p>Speaker Update (Speaker GRU): Speaker usually frames the response based on the context, which is the preceding utterances in the conversation. Hence, we capture context c t relevant to the utterance u t as follows:</p><formula xml:id="formula_4">α = softmax(u T t W α [g 1 , g 2 , . . . , g t−1 ]), (2) softmax(x) = [e x1 Σ i e xi , e x2 Σ i e xi , . . . ],<label>(3)</label></formula><formula xml:id="formula_5">c t = α[g 1 , g 2 , . . . , g t−1 ] T ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">g 1 , g 2 , . . . , g t−1 are preceding t − 1 global states (g i ∈ R D G ), W α ∈ R Dm×D G , α T ∈ R (t−1) , and c t ∈ R D G . In Eq.</formula><p>(2), we calculate attention scores α over the previous global states representative of the previous utterances. This assigns higher attention scores to the utterances emotionally relevant to u t . Finally, in Eq. (4) the context vector c t is calculated by pooling the previous global states with α. Now, we employ a GRU cell GRU P to update the current speaker state q s(ut),t−1 to the new state q s(ut),t based on incoming utterance u t and context c t using GRU cell GRU P of output size D P</p><formula xml:id="formula_7">q s(ut),t = GRU P (q s(ut),t−1 , (u t ⊕ c t )),<label>(5)</label></formula><formula xml:id="formula_8">where W {r,z,c} P,h ∈ R D P ×D P , W {r,z,c} P,x ∈ R D P ×(Dm+D G ) , b {r,z,c} P ∈ R D P , and q s(ut),t , q s(ut),t−1 ∈ R D P .</formula><p>This encodes the information on the current utterance along with its context from the global GRU into the speaker's state q s(ut) , which helps in emotion classification down the line.</p><p>Listener Update: Listener state models the listeners' change of state due to the speaker's utterance. We tried two listener state update mechanisms: • Simply keep the state of the listener unchanged, that is</p><formula xml:id="formula_9">∀i ≠ s(u t ), q i,t = q i,t−1 .<label>(6)</label></formula><p>• Employ another GRU cell GRU L to update the listener state based on listener visual cues (facial expression) v i,t and its context c t , as</p><formula xml:id="formula_10">∀i ≠ s(u t ), q i,t = GRU L (q i,t−1 , (v i,t ⊕ c t )),<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">v i,t ∈ R D V , W {r,z,c} L,h ∈ R D P ×D P , W {r,z,c} L,x ∈ R D P ×(D V +D G ) , and b {r,z,c} L ∈ R D P .</formula><p>Listener visual features of party i at time t v i,t are extracted using the model introduced by Arriaga, Valdenegro-Toro, and Plöger <ref type="formula" target="#formula_0">(2017)</ref>, pretrained on FER2013 dataset, where feature size D V = 7. The simpler first approach turns out to be sufficient, since the second approach yields very similar result while increasing number of parameters. This is due to the fact that a listener becomes relevant to the conversation only when he/she speaks. In other words, a silent party has no influence in a conversation. Now, when a party speaks, we update his/her state q i with context c t which contains relevant information on all the preceding utterances, rendering explicit listener state update unnecessary. This is shown in <ref type="table" target="#tab_7">Table 2</ref>.</p><p>Emotion Representation (Emotion GRU) We infer the emotionally relevant representation e t of utterance u t from the speaker's state q s(ut),t and the emotion representation of the previous utterance e t−1 . Since context is important to the emotion of the incoming utterance u t , e t−1 feeds fine-tuned emotionally relevant contextual information from other the party states q s(u&lt;t),&lt;t into the emotion representation e t . This establishes a connection between the speaker state and the other party states. Hence, we model e t with a GRU cell</p><formula xml:id="formula_12">(GRU E ) with output size D E as e t = GRU E (e t−1 , q s(ut),t ), (8) where D E is the size of emotion representation vector, e {t,t−1} ∈ R D E , W {r,z,c} E,h ∈ R D E ×D E , W {r,z,c} E,x ∈ R D E ×D P , and b {r,z,c} E ∈ R D E .</formula><p>Since speaker state gets information from global states, which serve as speaker-specific utterance representation, one may claim that this way the model already has access to the information on other parties. However, as shown in the ablation study (Section 5.6) emotion GRU helps to improve the performance by directly linking states of preceding parties. Further, we believe that speaker and global GRUs (GRU P , GRU G ) jointly act similar to an encoder, whereas emotion GRU serves as a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion Classification</head><p>We use a two-layer perceptron with a final softmax layer to calculate c = 6 emotion-class probabilities from emotion representation e t of utterance u t and then we pick the most likely emotion class:</p><formula xml:id="formula_13">l t = ReLU(W l e t + b l ),<label>(9)</label></formula><formula xml:id="formula_14">P t = softmax(W smax l t + b smax ),<label>(10)</label></formula><formula xml:id="formula_15">y t = argmax i (P t [i]),<label>(11)</label></formula><p>where</p><formula xml:id="formula_16">W l ∈ R D l ×D E , b l ∈ R D l , W smax ∈ R c×D l , b smax ∈ R c , P t ∈ R c , andŷ t is the predicted label for utterance u t .</formula><p>Training We use categorical cross-entropy along with L2regularization as the measure of loss (L) during training:</p><formula xml:id="formula_17">L = − 1 ∑ N s=1 c(s) N i=1 c(i) j=1 log P i,j [y i,j ] + λ θ 2 ,<label>(12)</label></formula><p>where N is the number of samples/dialogues, c(i) is the number of utterances in sample i, P i,j is the probability distribution of emotion labels for utterance j of dialogue i, y i,j is the expected class label of utterance j of dialogue i, λ is the L2-regularizer weight, and θ is the set of trainable parameters where θ = {W α , W </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DialogueRNN Variants</head><p>We use DialogueRNN (Section 3.3) as the basis for the following models:</p><p>DialogueRNN + Listener State Update (DialogueRNN l ): This variant updates the listener state based on the the resulting speaker state q s(ut),t , as described in Eq. <ref type="formula" target="#formula_10">(7)</ref>.</p><p>Bidirectional DialogueRNN (BiDialogueRNN): Bidirectional DialogueRNN is analogous to bidirectional RNNs, where two different RNNs are used for forward and backward passes of the input sequence. Outputs from the RNNs are concatenated in sequence level. Similarly, in BiDia-logueRNN, the final emotion representation contains information from both past and future utterances in the dialogue through forward and backward DialogueRNNs respectively, which provides better context for emotion classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DialogueRNN + attention (DialogueRNN+Att):</head><p>For each emotion representation e t , attention is applied over all surrounding emotion representations in the dialogue by matching them with e t (Eqs. <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref>). This provides context from the relevant (based on attention score) future and preceding utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional DialogueRNN + Emotional attention (BiDi-alogueRNN+Att):</head><p>For each emotion representation e t of BiDialogueRNN, attention is applied over all the emotion representations in the dialogue to capture context from the other utterances in dialogue:</p><formula xml:id="formula_18">β t = softmax(e T t W β [e 1 , e 2 , . . . , e N ]),<label>(13)</label></formula><formula xml:id="formula_19">e t = β t [e 1 , e 2 , . . . , e N ] T ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_20">e t ∈ R 2D E , W β ∈ R 2D E ×2D E ,ẽ t ∈ R 2D E , and β T t ∈ R N .</formula><p>Further,ẽ t are fed to a two-layer perceptron for emotion classification, as in Eqs. (9) to (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets Used</head><p>We use two emotion detection datasets IEMOCAP <ref type="bibr" target="#b3">(Busso et al. 2008</ref>) and AVEC <ref type="bibr" target="#b20">(Schuller et al. 2012)</ref> to evaluate Dia-logueRNN. We partition both datasets into train and test sets with roughly 80 20 ratio such that the partitions do not share any speaker.   <ref type="figure">0, ∞)</ref>). The annotations are available every 0.2 seconds in the original database. However, in order to adapt the annotations to our need of utterance-level annotation, we averaged the attributes over the span of an utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and State of the Art</head><p>For a comprehensive evaluation of DialogueRNN, we compare our model with the following baseline methods: c-LSTM+Att : In this variant attention is applied applied to the c-LSTM output at each timestamp by following Eqs. <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref>. This provides better context to the final utterance representation.</p><p>TFN : This is specific to multimodal scenario. Tensor outer product is used to capture intermodality and intra-modality interactions. This model does not capture context from surrounding utterances.</p><p>MFN <ref type="bibr" target="#b25">(Zadeh et al. 2018a</ref>): Specific to multimodal scenario, this model utilizes multi-view learning by modeling view-specific and cross-view interactions. Similar to TFN, this model does not use contextual information.</p><p>CNN <ref type="bibr" target="#b12">(Kim 2014)</ref>: This is identical to our textual feature extractor network (Section 3.2) and it does not use contextual information from the surrounding utterances.</p><p>Memnet <ref type="bibr" target="#b22">(Sukhbaatar et al. 2015)</ref>: As described in <ref type="bibr" target="#b10">Hazarika et al. (2018)</ref>, the current utterance is fed to a memory network, where the memories correspond to preceding utterances. The output from the memory network is used as the final utterance representation for emotion classification.</p><p>CMN <ref type="bibr" target="#b10">(Hazarika et al. 2018)</ref>: This state-of-the-art method models utterance context from dialogue history using two distinct GRUs for two speakers. Finally, utterance representation is obtained by feeding the current utterance as query to two distinct memory networks for both speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modalities</head><p>We evaluated our model primarily on textual modality. However, to substantiate efficacy of our model in multimodal scenario, we also experimented with multimodal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We compare DialogueRNN and its variants with the baselines for textual data in <ref type="table" target="#tab_7">Table 2</ref>. As expected, on average Di-alogueRNN outperforms all the baseline methods, including the state-of-the-art CMN, on both of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with the State of the Art</head><p>We compare the performance of DialogueRNN against the performance of the state-of-the-art CMN on IEMOCAP and AVEC datasets for textual modality.</p><p>IEMOCAP As evidenced by <ref type="table" target="#tab_7">Table 2</ref>, for IEMOCAP dataset, our model surpasses the state-of-the-art method CMN by 2.77% accuracy and 3.76% f1-score on average. We think that this enhancement is caused by the fundamental differences between CMN and DialogueRNN, which are 1. party state modeling with GRU P in Eq. (5), 2. speaker specific utterance treatment in Eqs. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_7">(5)</ref>, 3. and global state capturing with GRU G in Eq. (1). Since we deal with six unbalanced emotion labels, we also explored the model performance for individual labels. Dia-logueRNN outperforms the state-of-the-art method CMN in five out of six emotion classes by significant margin. For frustrated class, DialogueRNN lags behind CMN by 1.23% f1-score. We think that DialogueRNN may surpass CMN using a standalone classifier for frustrated class. However, it can be observed in <ref type="table" target="#tab_7">Table 2</ref> that some of the other variants of DialogueRNN, like BiDialogueRNN has already outperformed CMN for frustrated class.</p><p>AVEC DialogueRNN outperforms CMN for valence, arousal, expectancy, and power attributes; see <ref type="table" target="#tab_7">Table 2</ref>. It yields significantly lower mean absolute error (M AE) and higher Pearson correlation coefficient (r) for all four attributes. We believe this to be due to the incorporation of party state and emotion GRU, which are missing from CMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DialogueRNN vs. DialogueRNN Variants</head><p>We discuss the performance of different DialogueRNN variants on IEMOCAP and AVEC datasets for textual modality.</p><p>DialogueRNN l : Following <ref type="table" target="#tab_7">Table 2</ref>, using explicit listener state update yields slightly worse performance than regular DialogueRNN. This is true for both IEMOCAP and AVEC datasets in general. However, the only exception to this trend is for happy emotion label for IEMOCAP, where DialogueRNN l outperforms DialogueRNN by 1.71% f1score. We surmise that, this is due to the fact that a listener becomes relevant to the conversation only when he/she speaks. Now, in DialogueRNN, when a party speaks, we update his/her state q i with context c t which contains relevant information on all the preceding utterances, rendering explicit listener state update of DialogueRNN l unnecessary.</p><p>BiDialogueRNN: Since BiDialogueRNN captures context from the future utterances, we expect improved performance from it over DialogueRNN. This is confirmed in <ref type="table" target="#tab_7">Table 2</ref>, where BiDialogueRNN outperforms DialogueRNN on average on both datasets.</p><p>DialogueRNN+Attn: DialogueRNN+Attn also uses information from the future utterances. However, here we take information from both past and future utterances by matching them with the current utterance and calculating attention score over them. This provides relevance to emotionally important context utterances, yielding better performance than  BiDialogueRNN. The improvement over BiDialogueRNN is 1.23% f1-score for IEMOCAP and consistently lower M AE and higher r in AVEC.</p><p>BiDialogueRNN+Attn: Since this setting generates the final emotion representation by attending over the emotion representation from BiDialogueRNN, we expect better performance than both BiDialogueRNN and Dia-logueRNN+Attn. This is confirmed in <ref type="table" target="#tab_7">Table 2</ref>, where this setting performs the best in general than any other methods discussed, on both datasets. This setting yields 6.62% higher f1-score on average than the state-of-the-art CMN and 2.86% higher f1-score than vanilla DialogueRNN for IEMOCAP dataset. For AVEC dataset also, this setting gives the best performance across all the four attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multimodal Setting</head><p>As both IEMOCAP and AVEC dataset contain multimodal information, we have evaluated DialogueRNN on multimodal features as used and provided by <ref type="bibr" target="#b10">Hazarika et al. (2018)</ref>. We use concatenation of the unimodal features as a fusion method by following <ref type="bibr" target="#b10">Hazarika et al. (2018)</ref>, since fusion mechanism is not a focus of this paper. Now, as we can see in <ref type="table" target="#tab_8">Table 3</ref>, DialogueRNN significantly outperforms the strong baselines and state-of-the-art method CMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Studies</head><p>Dependency on preceding utterances (DialogueRNN) One of the crucial components of DialogueRNN is its attention module over the outputs of global GRU (GRU G ). <ref type="figure">Figure 3b</ref> shows the α attention vector (Eq. (2)) over the history of a given test utterance compared with the attention vector from the CMN model. The attention of our model is more focused compared with CMN: the latter gives diluted attention scores leading to misclassifications. We observe this trend of focused-attention across cases and posit that it can be interpreted as a confidence indicator. Further in this example, the test utterance by P A (turn 44) comprises of a change in emotion from neutral to frustrated. DialogueRNN anticipates this correctly by attending to turn 41 and 42 that are spoken by P A and P B , respectively. These two utterances provide self and inter-party influences that trigger the emotional shift. CMN, however, fails to capture such dependencies and wrongly predicts neutral emotion.</p><p>Dependency on future utterances (BiDialogueRNN+Att) <ref type="figure">Fig. 3a</ref> visualizes the β (Eq. (13)) attention over the emotion representations e t for a segment of a conversation between a couple. In the discussion, the woman (P A ) is initially at a neutral state, whereas the man (P B ) is angry throughout. The figure reveals that the emotional attention of the woman is localized to the duration of her neutral state (turns 1-16 approximately). For example, in the dialogue, turns 5, 7, and 8 strongly attend to turn 8. Interestingly, turn 5 attends to both past (turn 3) and future (turn 8) utterances. Similar trend across other utterances establish inter-dependence between emotional states of future and past utterances. The beneficial consideration of future utterances through GRU E is also apparent through turns 6, 9. These utterances focus on the distant future (turn 49, 50) where the man is at an enraged state, thus capturing emotional correlations across time. Although, turn 6 is misclassified by our model, it still manages to infer a related emotional state (anger) against the correct state (frustrated). We analyze more of this trend in section 5.5.</p><p>Dependency on distant context For all correct predictions in the IEMOCAP test set in <ref type="figure">Fig. 3d</ref> we summarize the distribution over the relative distance between test utterance and (2 nd ) highest attended utterance -either in the history or future -in the conversation. This reveals a decreasing trend with the highest dependence being within the local context. However, a significant portion of the test utterances (∼ 18%), attend to utterances that are 20 to 40 turns away from themselves, which highlights the important role of long-term emotional dependencies. Such cases primarily occur in conversations that maintain a specific affective tone and do not incur frequent emotional shifts. <ref type="figure">Fig. 3c</ref>   sion, when seen with the global context, it reveals the excitement present in the speaker. To disambiguate such cases, our model attends to distant utterances in the past (turn 11, 14) which serve as prototypes of the emotional tonality of the overall conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error Analysis</head><p>A noticeable trend in the predictions is the high level of cross-predictions amongst related emotions. Most of the misclassifications by the model for happy emotion are for excited class. Also, anger and frustrated share misclassifications amongst each other. We suspect this is due to subtle difference between those emotion pairs, resulting in harder disambiguation. Another class with high rate of false-positives is the neutral class. Primary reason for this could be its majority in the class distribution over the considered emotions.</p><p>At the dialogue level, we observe that a significant amount of errors occur at turns having a change of emotion from the previous turn of the same party. Across all the occurrences of these emotional-shifts in the testing set, our model correctly predicts 47.5% instances. This stands less as compared to the 69.2% success that it achieves at regions of no emotional-shift. Changes in emotions in a dialogue is a complex phenomenon governed by latent dynamics. Further improvement of these cases remain as an open area of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>The main novelty of our method is the introduction of party state and emotion GRU (GRU E ). To comprehensively study the impact of these two components, we remove them one at a time and evaluate their impact on IEMOCAP.  As expected, following <ref type="table" target="#tab_11">Table 4</ref>, party state stands very important, as without its presence the performance falls by 4.33%. We suspect that party state helps in extracting useful contextual information relevant to parties' emotion.</p><p>Emotion GRU is also impactful, but less than party state, as its absence causes performance to fall by only 2.51%. We believe the reason to be the lack of context flow from the other parties' states through the emotion representation of the preceding utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented an RNN-based neural architecture for emotion detection in a conversation. In contrast to the stateof-the-art method, CMN, our method treats each incoming utterance taking into account the characteristics of the speaker, which gives finer context to the utterance. Our model outperforms the current state-of-the-art on two distinct datasets in both textual and multimodal settings. Our method is designed to be scalable to multi-party setting with more than two speakers, which we plan to explore in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>In this dialogue, P A 's emotion changes are influenced by the behavior of P B .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) DialogueRNN architecture. (b) Update schemes for global, speaker, listener, and emotion states for t th utterance in a dialogue. Here, Person i is the speaker and Persons j ∈ [1, M ] and j ≠ i are the listeners.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Illustration of the β attention over emotion representations e t ; (b) Comparison of attention scores over utterance history of CMN and DialogueRNN (α attention). (c) An example of long-term dependency among utterances. (d) Histogram of ∆t = distance between the target utterance and its context utterance based on β attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>states that emotion plays a pivotal role in conversations. It has been ar-arXiv:1811.00405v4 [cs.CL] 25 May 2019 She's been in New York three and a half years. Why all of the sudden? [ neutral ] Why does that bother you? [ neutral ] What's going on here Joe? [frustrated] PA Maybe he just wanted to see her again? [ neutral ] He lived next door to the girl all his life, why wouldn't he want to see her again? [ neutral ]</figDesc><table><row><cell>How do you know he is even thinking about</cell></row><row><cell>it? [frustrated]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, W l , b l , W smax , b smax }. We used stochastic gradient descent based Adam (Kingma and Ba 2014) optimizer to train our network. Hyperparameters are optimized using grid search (values are added to the supplementary material).</figDesc><table><row><cell>{r,z,c} P,{h,x} , b</cell><cell>{r,z,c} P</cell><cell>, W</cell><cell>{r,z,c} G,{h,x} , b</cell><cell>{r,z,c} G</cell><cell>, W</cell><cell>{r,z,c} E,{h,x} ,</cell></row><row><cell>{r,z,c} b E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>shows the distribution of train and test samples for both dataset.</figDesc><table><row><cell>Dataset</cell><cell>Partition</cell><cell cols="2">Utterance Dialogue Count Count</cell></row><row><cell>IEMOCAP</cell><cell>train + val test</cell><cell>5810 1623</cell><cell>120 31</cell></row><row><cell>AVEC</cell><cell>train + val test</cell><cell>4368 1430</cell><cell>63 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Dataset split ((train + val) / test ≈ 80% 20%). AVEC (Schuller et al. 2012) dataset is a modification of SEMAINE database (McKeown et al. 2012) containing interactions between humans and artificially intelligent agents. Each utterance of a dialogue is annotated with four real valued affective attributes: valence ([−1, 1]), arousal ([−1, 1]), expectancy ([−1, 1]), and power ([</figDesc><table><row><cell>IEMOCAP: IEMOCAP (Busso et al. 2008) dataset con-</cell></row><row><cell>tains videos of two-way conversations of ten unique speak-</cell></row><row><cell>ers, where only the first eight speakers from session one</cell></row></table><note>to four belong to the train-set. Each video contains a sin- gle dyadic dialogue, segmented into utterances. The utter- ances are annotated with one of six emotion labels, which are happy, sad, neutral, angry, excited, and frustrated.AVEC:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the baseline methods for textual modality; Acc. = Accuracy, M AE = Mean Absolute Error, r = Pearson correlation coefficient; bold font denotes the best performances. Average(w) = Weighted average.</figDesc><table><row><cell>Methods</cell><cell>IEMOCAP F1</cell><cell>Valence (r)</cell><cell cols="2">AVEC Arousal (r) Expectancy (r)</cell><cell>Power (r)</cell></row><row><cell>TFN</cell><cell>56.8</cell><cell>0.01</cell><cell>0.10</cell><cell>0.12</cell><cell>0.12</cell></row><row><cell>MFN</cell><cell>53.5</cell><cell>0.14</cell><cell>25</cell><cell>0.26</cell><cell>0.15</cell></row><row><cell>c-LSTM</cell><cell>58.3</cell><cell>0.14</cell><cell>0.23</cell><cell>0.25</cell><cell>-0.04</cell></row><row><cell>CMN</cell><cell>58.5</cell><cell>0.23</cell><cell>0.30</cell><cell>0.26</cell><cell>-0.02</cell></row><row><cell>BiDialogueRNN+atttext</cell><cell>62.7</cell><cell>0.35</cell><cell>0.59</cell><cell>0.37</cell><cell>0.37</cell></row><row><cell>BiDialogueRNN+attMM</cell><cell>62.9</cell><cell>0.37</cell><cell>0.60</cell><cell>0.37</cell><cell>0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Comparison with the baselines for trimodal</cell></row><row><cell>(T+V+A) scenario. BiDialogueRNN+att M M = BiDia-</cell></row><row><cell>logueRNN+att in multimodal setting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>demonstrates a case of long-term context dependency. The presented conversation maintains a happy mood throughout the dialogue. Although the 34 th turn comprising the sentence Horrible thing. I hated it. seems to be a negative expres-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Emotion</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>neu</cell><cell>fru</cell><cell>neu</cell><cell>neu</cell></row><row><cell>Turn</cell><cell></cell><cell>PA</cell><cell></cell><cell cols="2">Utterance</cell><cell>PB</cell><cell></cell><cell cols="2">Emotion Label</cell><cell></cell><cell cols="2">Prediction</cell><cell></cell><cell cols="2">1 2 3</cell><cell>4 5</cell><cell cols="6">Turns : 6 7 8 9 …. 49 50 51 …</cell><cell></cell><cell>CMN</cell><cell>0.8</cell></row><row><cell>5</cell><cell cols="4">Oh, maybe a little. Nothing serious.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>neutral</cell><cell></cell><cell cols="2">neutral</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.30</cell><cell>Turns Speaker</cell><cell>31 PB</cell><cell>32 PB</cell><cell>33 PA</cell><cell>34 PB</cell><cell>35 PA</cell><cell>36 PB</cell><cell>37 PA</cell><cell>38 PB</cell><cell>39 PA</cell><cell>40 PB</cell><cell>41 PA</cell><cell>42 PB</cell><cell>43 PA</cell><cell>0.6 0.4</cell></row><row><cell cols="8">6 7 Well, what of it. He let him kiss you. You said you did</cell><cell cols="3">frustration neutral</cell><cell cols="2">anger neutral</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.15</cell><cell>DialogueRNN</cell><cell>0.2 0.0</cell></row><row><cell>8 9</cell><cell cols="7">It gave him a lot of pleasure and it didn't hurt me. of view I must say That's a nice point</cell><cell></cell><cell>neutral anger</cell><cell></cell><cell cols="2">neutral anger</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.00</cell><cell>Turn 41</cell><cell>Utterance Turn PA: Because I paid it on time.</cell><cell>Emotion Utterance fru</cell><cell>Turn 42</cell><cell>Utterance Emotion DialogueRNN CMN Emotion PB: Yeah, since it was a computer glitch … you should just call back neu</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>past</cell><cell cols="2">future</cell><cell></cell><cell></cell><cell></cell><cell>Test Utterance:</cell><cell>44</cell><cell>PA: But if I call back, I have to go through this whole rigmarole again.</cell><cell>fru</cell><cell>fru</cell><cell>neu</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Δt = 23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Distance between test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.20</cell><cell>utterance and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Highest attention</cell></row><row><cell>Turns:</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell><cell>20</cell><cell>22</cell><cell>24</cell><cell>26</cell><cell>28</cell><cell>30</cell><cell>32</cell><cell>34</cell><cell>36</cell><cell>38</cell><cell>40</cell><cell>42</cell><cell></cell><cell>2nd highest attention</cell></row><row><cell>Turn</cell><cell></cell><cell></cell><cell cols="2">Utterance</cell><cell></cell><cell cols="2">Emotion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.10</cell></row><row><cell>11</cell><cell></cell><cell cols="4">Uhh, It's amazing how once you feel ..</cell><cell cols="2">excited</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Turn</cell><cell></cell><cell></cell><cell cols="2">Utterance</cell><cell></cell><cell></cell><cell cols="3">Emotion</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>True</cell><cell cols="3">Predicted</cell><cell></cell></row><row><cell>Turn 14</cell><cell></cell><cell cols="4">Utterance Being that sacred and wonderful thing, love.</cell><cell cols="2">Emotion happy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell cols="8">Horrible thing. I hated it. excited Test Utterance</cell><cell cols="2">excited</cell><cell>-0.00</cell><cell>8 0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Ablated DialogueRNN for IEMOCAP dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Implementation available at https://github.com/ senticnet/conv-emotion</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotions from text: machine learning for text-based emotion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Alm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Realtime convolutional neural networks for emotion and gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valdenegro-Toro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plöger</surname></persName>
		</author>
		<idno>abs/1710.07557</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">IEMOCAP: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic audio-visual data fusion for automatic emotion recognition. Euromedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Affective computing: From laughter to ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-Dependent Sentiment Analysis in User-Generated Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emotion regulation in romantic relationships: The cognitive consequences of concealing feelings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Social and Personal Relationships</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="599" to="620" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Emotion, affect and conversation. The handbook of conversation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="330" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AVEC 2012: The Continuous Audio/Visual Emotion Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Multimodal Interaction, ICMI &apos;12</title>
		<meeting>the 14th ACM International Conference on Multimodal Interaction, ICMI &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Annotating and identifying emotions in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Information Access</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tensor Fusion Network for Multimodal Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memory Fusion Network for Multi-view Sequential Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5634" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5642" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
