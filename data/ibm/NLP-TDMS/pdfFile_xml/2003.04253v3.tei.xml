<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion-Attentive Transition for Zero-Shot Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artifical Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunzhou</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artifical Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Beijing Key Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artifical Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Motion-Attentive Transition for Zero-Shot Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/tfzhou/MATNet</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel Motion-Attentive Transition Network (MATNet) for zero-shot video object segmentation, which provides a new way of leveraging motion information to reinforce spatio-temporal object representation. An asymmetric attention block, called Motion-Attentive Transition (MAT), is designed within a two-stream encoder, which transforms appearance features into motion-attentive representations at each convolutional stage. In this way, the encoder becomes deeply interleaved, allowing for closely hierarchical interactions between object motion and appearance. This is superior to the typical two-stream architecture, which treats motion and appearance separately in each stream and often suffers from overfitting to appearance information. Additionally, a bridge network is proposed to obtain a compact, discriminative and scale-sensitive representation for multilevel encoder features, which is further fed into a decoder to achieve segmentation results. Extensive experiments on three challenging public benchmarks (i.e. DAVIS-16, FBMS and Youtube-Objects) show that our model achieves compelling performance against the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The task of automatically segmenting primary object(s) from videos has gained significant attention in recent years, and has a powerful impact in many areas of computer vision, including surveillance, robotics and autonomous driving. However, due to the lack of human intervention, in addition to the common challenges posed by video data (e.g. appearance variations, scale changes, background clutter), the task faces great difficulties in accurately discovering the most distinct objects throughout a video sequence. Early nonlearning methods typically address this using handcrafted features, e.g. motion boundary <ref type="bibr" target="#b19">(Papazoglou and Ferrari 2013)</ref>, saliency <ref type="bibr" target="#b35">(Wang, Shen, and Porikli 2015)</ref> and point trajectories <ref type="bibr" target="#b18">(Ochs, Malik, and Brox 2013)</ref>. More recently, research has turned towards the deep learning paradigm, with several studies attempting to fit this problem into a zero-shot solution <ref type="bibr" target="#b30">(Ventura et al. 2019;</ref><ref type="bibr" target="#b32">Wang et al. 2019a</ref>).</p><p>These methods generally learn a powerful object representation from large-scale training data and then adapt the models to test videos without any annotations.</p><p>Even before the era of deep learning, object motion has always been considered as an informative cue for automatic video object segmentation. This is largely inspired by the remarkable capability of motion perception in the human visual system (HVS) <ref type="bibr" target="#b29">(Treisman and Gelade 1980;</ref><ref type="bibr" target="#b16">Mital et al. 2013)</ref>, which can quickly orient attentions towards moving objects in dynamic scenarios. In fact, human beings are more sensitive to moving objects than static ones, even if the static objects are strongly contrasted against their surroundings. Nevertheless, motion does not work alone. Recent studies <ref type="bibr" target="#b2">(Cloutman 2013)</ref> have revealed that, in HVS, the dorsal pathway (for motion perception) has a faster activation response than the ventral pathway (for objectness/semantic perception), and tends to send signals to the ventral pathway along multi-level connections to prompt it to focus on processing the most salient objects. This multimodal system enables human beings to focus on the moving parts of objects first and then transfer the attention to appearance for the whole picture. Thus, it is desirable to take this biological mechanism into account and incorporate object motion into appearance learning, in a hierarchical way, for more effective spatio-temporal object representation.</p><p>By considering information flow from motion to appearance, we can alleviate ambiguity in object appearance (e.g. visually similar to the surroundings), thus easing the pressure in representation learning of objects. However, in the context of deep learning, most segmentation models do not leverage this potential. Many approaches <ref type="bibr" target="#b26">(Tokmakov, Alahari, and Schmid 2017a;</ref><ref type="bibr" target="#b21">Perazzi et al. 2017;</ref><ref type="bibr" target="#b10">Jain, Xiong, and Grauman 2017;</ref><ref type="bibr" target="#b1">Cheng et al. 2017</ref>) simply treat motion cues as equal to appearance cues and learn to directly map optical flow to the corresponding segmentation mask. A few methods <ref type="bibr" target="#b37">(Xiao et al. 2018;</ref><ref type="bibr" target="#b13">Li et al. 2018b</ref>) have attempted to enhance object representation with motion; however, they rely on complex heuristics and only operate at a single scale, ignoring the critical hierarchical structure.</p><p>Motivated by these observations, we propose a Motion-Attentive Transition Network (MATNet) for zero-shot video object segmentation (ZVOS) within an encoder-bridge- decoder framework, as shown in <ref type="figure">Fig. 1</ref>. The core of MATNet is a deeply interleaved two-stream encoder which not only inherits the superiorities of two-stream models for multimodal feature learning, but also progressively transfers intermediate motion-attentive features to facilitate appearance learning. The transition is carried out by multiple Motion-Attentive Transition (MAT) blocks. Each block takes as input the intermediate features of both the input image and optical flow map at a convolutional stage. Inside the block, we build an asymmetric attention mechanism that first infers regions of interest based on optical flow, and then transfers the inference to provide better selectivity for appearance features. Each block outputs the attentive appearance and motion features for the following convolutional stage.</p><formula xml:id="formula_0">C C C C Res2 Conv1 Res4 Res3 Res5 Res2 Conv1 Res3 Res4 Res5 SSA SSA SSA SSA BAR5 BAR4 BAR3 BAR2 GC (c) Boundary-Aware Decoder Network (b) Bridge Network ℒ "# Deep MAT Deep MAT ℒ $%&amp;' Boundary HEM G ) M ) Boundary GT (a) Interleaved Encoder Network M + $ M - $ Z - Z / Z 0 Z + F - F / F 0 F + V 3,0 V 5,</formula><p>In addition, our decoder accepts learnt features from the encoder as inputs and progressively refines coarse features scale-by-scale to obtain accurate segmentation. The decoder consists of multiple Boundary-Aware Refinement (BAR) blocks organized in a cascaded manner. Each BAR explicitly exploits multi-scale features and conducts segmentation inference with the assistance of object boundary prediction to obtain results with a finer structure. Moreover, instead of directly connecting the encoder and decoder via skip connections, we present the Scale-Sensitive Attention (SSA) to adaptively select and transform encoder features. Specifically, SSA, which is added to each pair of encoder and decoder layers, consists of a two-level attention scheme in which the local-level attention serves to select a focused region, while the global-level one helps to re-calibrate features for objects at different scales.</p><p>MATNet can be easily instantiated with various backbones, and optimized in an end-to-end manner. We evaluate it on three popular video object segmentation benchmarks, i.e. DAVIS-16 <ref type="bibr" target="#b20">(Perazzi et al. 2016)</ref>, FBMS <ref type="bibr" target="#b18">(Ochs, Malik, and Brox 2013)</ref>, and Youtube-Objects <ref type="bibr" target="#b22">(Prest et al. 2012)</ref>, and claim state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Automatic Video Object Segmentation. Automatic, or unsupervised, video object segmentation aims to segment conspicuous and eye-catching objects without any human intervention. Traditional methods require no training data and typically design heuristic assumptions (e.g. motion boundary <ref type="bibr" target="#b19">(Papazoglou and Ferrari 2013)</ref>, objectness (Faktor and Irani 2014; <ref type="bibr" target="#b42">Zhou et al. 2016;</ref><ref type="bibr" target="#b14">Li, Zhou, and Lu 2017)</ref>, saliency <ref type="bibr" target="#b35">(Wang, Shen, and Porikli 2015)</ref> and longterm point trajectories <ref type="bibr" target="#b18">(Ochs, Malik, and Brox 2013;</ref><ref type="bibr" target="#b11">Keuper, Andres, and Brox 2015;</ref><ref type="bibr" target="#b17">Ochs and Brox 2011)</ref>) for segmentation. In recent years, benefitting from the establishment of large datasets <ref type="bibr" target="#b20">(Perazzi et al. 2016;</ref><ref type="bibr" target="#b40">Xu et al. 2018</ref>), many approaches <ref type="bibr" target="#b10">(Jain, Xiong, and Grauman 2017;</ref><ref type="bibr" target="#b26">Tokmakov, Alahari, and Schmid 2017a;</ref><ref type="bibr" target="#b13">Li et al. 2018b;</ref><ref type="bibr" target="#b5">Fan et al. 2018;</ref><ref type="bibr" target="#b15">Lu et al. 2019;</ref><ref type="bibr" target="#b8">Hu, Huang, and Schwing 2018;</ref><ref type="bibr" target="#b33">Wang et al. 2019b;</ref><ref type="bibr" target="#b30">Ventura et al. 2019;</ref><ref type="bibr" target="#b28">Tokmakov, Schmid, and Alahari 2019;</ref><ref type="bibr" target="#b12">Li et al. 2018a;</ref><ref type="bibr" target="#b13">2018b;</ref><ref type="bibr" target="#b24">Song et al. 2018;</ref><ref type="bibr" target="#b3">Faisal et al. 2019;</ref><ref type="bibr" target="#b6">Fan et al. 2019</ref>) propose to solve this task with zero-shot solutions and improve the performance greatly.</p><p>Among them, a large number of approaches utilize motion because of its complementary role to object appearance. They typically adopt heuristic methods to fuse motion and appearance cues <ref type="bibr" target="#b26">(Tokmakov, Alahari, and Schmid 2017a;</ref><ref type="bibr" target="#b13">Li et al. 2018b)</ref> or use two-stream networks <ref type="bibr" target="#b10">(Jain, Xiong, and Grauman 2017;</ref><ref type="bibr" target="#b27">Tokmakov, Alahari, and Schmid 2017b)</ref> to learn spatio-temporal representations in an end-to-end fashion. However, a major drawback of these approaches is that they fail to consider the importance of deep interactions between appearance and motion in learning rich spatiotemporal features. To address this issue, we propose a deep interleaved two-stream encoder, in which a motion transition module is leveraged for more effective representation learning.</p><p>Neural Attention. Neural attention has been widely used in recent neural networks for various tasks, such as object recognition <ref type="bibr" target="#b9">(Hu, Shen, and Sun 2018;</ref><ref type="bibr" target="#b36">Woo et al. 2018</ref>;  <ref type="bibr" target="#b34">(Wang et al. 2019c</ref>) and medical imaging <ref type="bibr" target="#b43">(Zhou et al. 2019)</ref>. It allows the networks to focus on the most informative parts of the inputs. In this work, neural attention is used in two ways: first, in the encoder network, soft attention is applied independently to intermediate appearance or motion feature maps, and motion attention is further transferred to enhance the appearance attention. Second, in the bridge network, a scale-sensitive attention module is designed to obtain more compact features.</p><formula xml:id="formula_1">(C/d) × H × W ! ! " # " (C/d) × H × W $ ! " # # HW × HW % HW × HW % $ C × H × W # " C × H × W # # softmax ! C × H × W &amp; # C × H × W " # # Soft Attention C × H × W &amp; " C × H × W " # " Soft Attention Attention Transition (2C) × H × W # c 1×1 conv Softmax + FC 1×1 conv Softmax + FC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method Network Overview</head><p>As illustrated in <ref type="figure">Fig. 1</ref>, MATNet is an end-to-end deep neural network for ZVOS, consisting of three concatenated networks, i.e. an interleaved encoder, a bridge network and a decoder.</p><p>Interleaved Encoder Network. Our encoder relies on a two-stream structure to jointly encode object appearance and motion, which has been proven effective in many related video tasks. Unlike previous works that treat the two streams equally, our encoder includes a MAT block at each network layer, which offers a motion-to-appearance pathway for information propagation. To be specific, we take the first five convolutional blocks of ResNet-101 <ref type="bibr" target="#b7">(He et al. 2016)</ref> as the backbone for each stream. Given an RGB frame I a and its optical flow map I m , the encoder extracts intermediate fea-</p><formula xml:id="formula_2">tures V a,i ∈ R W ×H×C and V m,i ∈ R W ×H×C , respectively, at the i-th (i ∈ {2, 3, 4, 5}) residual stage.</formula><p>The MAT block F MAT enhances these features as follows:</p><formula xml:id="formula_3">U a,i , U m,i = F MAT (V a,i , V m,i ),<label>(1)</label></formula><p>where U ·,i ∈ R W ×H×C indicates the enhanced features. We then obtain the spatio-temporal object representation U i at the i-th stage as</p><formula xml:id="formula_4">U i = Concat(U a,i ,U m,i ) ∈ R W ×H×2C</formula><p>which is further fed into the down-stream decoder via a bridge network. Bridge Network. The bridge network is expected to selectively transfer encoder features to the decoder. It is formed by SSA modules, each of which takes advantage of the encoder feature U i at the i-th stage and predicts an attention-aware feature Z i . This is achieved by a two-level attention scheme, wherein the local-level attention adopts channel-wise and spatial-wise attention mechanisms to focus input features on the correct object regions as well as suppress possible noises existing in the redundant features, while the global-level attention aims to re-calibrate the features to account for objects of different sizes. Decoder Network. The decoder network takes a coarseto-fine scheme to carry out segmentation. It is formed by four BAR modules, i.e. BAR i , i ∈ {2, 3, 4, 5}, each corresponding to the i-th residual block. From BAR 5 to BAR 2 , the resolution of feature maps gradually increases by compensating for high-level coarse features with more low-level details. The BAR 2 produces the finest feature map, whose resolution is 1/4 of the input image size. It is processed by two additional layers, conv(3×3,1) → sigmoid, to obtain the final mask output M s ∈ R W ×H .</p><p>As follows, we will introduce the three proposed modules (i.e. MAT, SSA, BAR) in detail. For simplicity, we omit the subscript i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion-Attentive Transition Module</head><p>The MAT module is comprised of two units: a soft attention (SA) unit and an attention transition (AT) unit, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The former unit helps to focus on the important regions of the inputs, while the latter transfers the attentive motion features to facilitate appearance learning.</p><p>Soft Attention: This unit softly weights the input feature map V m (or V a ) at each spatial location. Taking V m as the input, the SA unit outputs a motion-attentive featureŨ m ∈ R W ×H×C as follows:</p><p>Softmax attention:</p><formula xml:id="formula_5">A m = softmax(w m * V m ), Attention-enhanced feature:Ũ c m = A m V c m ,<label>(2)</label></formula><p>where w m ∈ R 1×1×C is a 1×1 conv kernel that maps V m to a significance matrix, which is normalized using softmax to achieve a soft attention map A m ∈ R W ×H . * indicates the conv operation.Ũ m ∈ R W ×H×C is the attention-aware feature map.Ũ c m and V c m indicate the 2D feature slices ofŨ m and V m at the c-th channel, respectively. indicates the element-wise multiplication. Similarly, given V a , we can obtain the appearance-attentive featureŨ a by Eq. 2.</p><p>Attention Transition: To transfer motion-attentive fea-turesŨ m , we first seek the affinity betweenŨ a andŨ m in a non-local manner using the following multi-modal bilinear model:</p><formula xml:id="formula_6">S =Ũ m WŨ a ∈ R (W H)×(W H) ,<label>(3)</label></formula><p>where W ∈ R C×C is a trainable weight matrix. The affinity matrix S can effectively capture pairwise relationships between the two feature spaces. However, it also introduces a huge number of parameters, which increases the computational cost and creates the risk of over-fitting. To overcome this problem, W is approximately factorized into two lowrank matrices P ∈ R C× C d and Q ∈ R C× C d , where d (d &gt; 1) is a reduction ratio. Then, Eq. 3 can be rewritten as:</p><formula xml:id="formula_7">S =Ũ m PQ Ũ a = (P Ũ m ) (Q Ũ a ).<label>(4)</label></formula><p>This operation is equal to applying channel-wise feature transformations toŨ m andŨ a before computing the similarity. This not only significantly reduces the number of parameters by 2/d times, but also generates a compact channelwise feature representation for each modal. Then, we normalize S row-wise to derive an attention map S r conditioned on motion features and achieve enhanced appearance features U a ∈ R W ×H×C :</p><p>Motion conditioned attention: S r = softmax r (S),</p><formula xml:id="formula_8">Attention-enhanced feature: U a =Ũ a S r ,<label>(5)</label></formula><p>where softmax r indicates row-wise softmax.</p><p>Deep MAT: Deep network structures have achieved great success due to their powerful representational ability. Therefore, we extend the MAT module into a deep structure consisting of L MAT layers cascaded in depth (denoted by F MAT in a recursive manner:</p><formula xml:id="formula_9">U (l) a , U (l) m = F (l) MAT (U (l−1) a , U (l−1) m ),<label>(6)</label></formula><p>where U </p><formula xml:id="formula_10">m = V m .</formula><p>It is worth noting that stacking MAT modules directly leads to an obvious performance drop. Inspired by , we propose stacking multiple MAT modules in a residual form by modifying the outputs of Eq. 6 as follows:</p><formula xml:id="formula_11">U (l) a = U (l−1) a +Ũ (l−1) a S r = U (l−1) a + (A (l−1) a V (l−1) a )S r . U (l) m = U (l−1) m +Ũ (l−1) m = U (l−1) m + A (l−1) m V (l−1) m .<label>(7)</label></formula><p>Here, we combine Eq. 2 and Eq. 5 to provide a global view of our deep residual MAT modules. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we show the effects of our MAT block. We see that the features in V a <ref type="figure" target="#fig_1">(Fig. 3 (c)</ref>) are well refined by the MAT block to produce features in U a <ref type="figure" target="#fig_1">(Fig. 3 (d)</ref>). For each test image in (e), we see that the boundary results with HEM (h) are more accurate than those without HEM (g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale-Sensitive Attention Module</head><p>The SSA module is extended from a simplified CBAM F CBAM <ref type="bibr" target="#b36">(Woo et al. 2018</ref>) by adding a global-level attention F g . Given a feature map U ∈ R W ×H×2C , our SSA module refines it as follows:</p><formula xml:id="formula_12">Z = F g (F CBAM (U)) ∈ R W ×H×2C .<label>(8)</label></formula><p>The CBAM module F CBAM consists of two sequential sub-modules: channel and spatial attention, which can be formulated as:</p><p>Channel attention: s = F s (U), e = F e (s), Z c = e U, Spatial attention: p = F p (Z c ), Z CBAM = p Z c ,</p><p>where F s is a squeeze operator that gathers the global spatial information of U into a vector s ∈ R 2C , while F e is an excitation operator that captures channel-wise dependencies and outputs an attention vector e ∈ R 2C . Following <ref type="bibr" target="#b9">(Hu, Shen, and Sun 2018)</ref>, F s is implemented by applying avgpooling on each feature channel, and F e is formed by four consecutive operations: fc( 2C 16 ) → ReLU → fc(2C) → sigmoid. Z c ∈ R W ×H×2C denotes channel-wise attentive features, and indicates the channel-wise multiplication. In the spatial attention, F p exploits the inter-spatial relationship of Z c and produces a spatial-wise attention map p ∈ R W ×H by conv(7 × 7, 1) → sigmoid. Then, we achieve the attention glimpse Z CBAM ∈ R W ×H×2C as the local-level feature.</p><p>The global-level attention F g shares a similar spirit to the channel attention layer in Eq. 9, in that it shares the same squeeze layer but modifies the excitation layer as fc( 2C 16 ) → fc(1) → sigmoid to output a scale-selection factor g ∈ R 1 and then obtain scale-sensitive features Z as follows: Z = (g * Z CBAM ) + U.</p><p>(10) Note that we use identity mapping to avoid losing important information on the regions with attention values close to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary-Aware Refinement Module</head><p>In the decoder network, each BAR module, e.g. BAR i , receives two inputs, i.e. Z i from the corresponding SSA module and F i from the previous BAR. To obtain a sharp mask output, the BAR first performs object boundary estimation using an extra boundary detection module F bdry , which compels the network to emphasize finer object details. The predicted boundary map is then combined with the two inputs to produce finer features for the next BAR module. It can be formulated as:</p><formula xml:id="formula_14">M b i = F bdry (F i ), F i−1 = F BARi (Z i , F i , M b i ),<label>(11)</label></formula><p>where F bdry consists of a stack of convolutional layers and a sigmoid layer, M b i ∈ R W ×H indicates the boundary map and F i−1 is the output feature map of BAR i . The computational graph of BAR i is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>BAR benefits from two key factors: the first is that we apply Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b0">(Chen et al. 2017)</ref> on convolutional features to transform them into a multi-scale representation. This helps to enlarge the receptive field and obtain more spatial details for decoding.</p><p>The second benefit is that we introduce a heuristic method for automatically mining hard negative pixels to support the training of F bdry . Specifically, for each training frame, we use the popular off-the-shelf HED model <ref type="bibr" target="#b38">(Xie and Tu 2015)</ref> to predict a boundary map E ∈ [0, 1] W ×H , wherein each value E k represents the probability of pixel k being an edge pixel. Then, pixel k is regarded as a hard negative pixel if it has a high edge probability (e.g. E k &gt; 0.2) and falls outside the dilated ground-truth region. If pixel k is a hard pixel, then its weights w k = 1 + E k ; otherwise, w k = 1.</p><p>Then, w k is used to weight the following boundary loss so that it can be penalized heavily if the hard pixels are misclassified:</p><formula xml:id="formula_15">L bdry (M b , G b ) = − k w k ((1−G b k ) log(1−M b k ) + G b k log(M b k )),<label>(12)</label></formula><p>where M b and G b are the boundary prediction and groundtruth, respectively. <ref type="figure" target="#fig_4">Fig. 4</ref> offers an illustration of the above hard example mining (HEM) scheme. Clearly, by explicitly discovering hard negative pixels, the network can produce more accurate boundary prediction with well-suppressed background pixels (see <ref type="figure" target="#fig_4">Fig. 4 (g) and (h)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Training Loss. Given an input frame I a ∈ R 473×473×3 , our MATNet predicts a segmentation mask M s ∈ [0, 1] 473×473 and four boundary predictions {M b i ∈ [0, 1] 473×473 } 4 i=1 using BAR modules. Let G s ∈ {0, 1} 473×473 be the binary segmentation ground-truth, and G b ∈ {0, 1} 473×473 be the boundary ground-truth which can be easily computed from G s . The overall loss function is formulated as: where L CE indicates the classic cross entropy loss. Training Settings. We train the proposed neural network in an end-to-end manner. Our training data consist of two parts: i) all training data in DAVIS-16 <ref type="bibr" target="#b20">(Perazzi et al. 2016)</ref>, which includes 30 videos with about 2K frames; ii) a subset of 12K frames selected from the training set of Youtube-VOS <ref type="bibr" target="#b40">(Xu et al. 2018)</ref>, which is obtained by sampling images every ten frames in each video. In total, we have 14K training samples, basically matching AGS <ref type="bibr" target="#b33">(Wang et al. 2019b</ref>), which uses 13K training samples. For each training image of size 473×473× 3, we first estimate its optical flow using PWC-Net ) due to its high efficiency and accuracy. The entire network is trained using the SGD optimizer with an initial learning rate of 1e-4 for the encoder and the bridge network, and 1e-3 for the decoder. During training, the batch size, momentum and weight decay are set to 2, 0.9, and 1e-5, respectively. The data are augmented online with horizontal flip and rotations covering a range of degrees (−10, 10).</p><formula xml:id="formula_16">L = L CE (M s , G s ) + 1 N N =4 i=1 L bdry (M b i , G b ),<label>(13)</label></formula><p>The network is implemented with PyTorch, and all the experiments are conducted using a single Nvidia RTX 2080Ti GPU and an Intel(R) Xeon Gold 5120 CPU. Test Settings. Once the network is trained, we apply it to unseen videos. Given a test video, we resize all the frames to 473 × 473, and feed each frame, along with its optical flow, to the network for segmentation. We follow the common protocol used in previous works <ref type="bibr" target="#b33">(Wang et al. 2019b;</ref><ref type="bibr" target="#b21">Perazzi et al. 2017;</ref><ref type="bibr" target="#b37">Xiao et al. 2018)</ref> and employ CRF to obtain the final binary segmentation results.</p><p>Runtime. For each test image of size 473 × 473 × 3, the forward inference of our MATNet takes about 0.05s, while optical flow estimation and CRF-based post-processing take about 0.2s and 0.5s, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setup</head><p>We carry out comprehensive experiments on three popular datasets: DAVIS-16 <ref type="bibr" target="#b20">(Perazzi et al. 2016</ref>), Youtube-Objects <ref type="bibr" target="#b22">(Prest et al. 2012</ref>) and FBMS <ref type="bibr" target="#b18">(Ochs, Malik, and Brox 2013)</ref>. DAVIS-16 consists of 50 high-quality video sequences (30 for training and 20 for validation) in total. Each frame contains pixel-accurate annotations for foreground objects. For quantitative evaluation, we use three standard metrics suggested by <ref type="bibr" target="#b20">(Perazzi et al. 2016)</ref>, namely region similarity J , boundary accuracy F, and time stability T .</p><p>Youtube-Objects is a large dataset of 126 web videos with 10 semantic object categories and more than 20,000 frames. Following its protocol, we use the region similarity J metric to measure the performance.</p><p>FBMS is composed of 59 video sequences with groundtruth annotations provided in a subset of the frames. Following the standard protocol <ref type="bibr" target="#b27">(Tokmakov, Alahari, and Schmid 2017b)</ref>, we do not use any sequences for training and only evaluate on the validation set consisting of 30 sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Tab. 1 summarizes the ablation analysis of our MATNet on DAVIS-16.</p><p>MAT Block. We first study the effects of the MAT block by comparing our full model to one of the same architecture without MAT, denoted as MATNet w/o MAT. The encoder in this network is thus equivalent to a standard two-stream model, where convolutional features from the two streams are concatenated at each residual stage for object representation. As shown in Tab. 1, this model encounters a huge performance degradation (2.9% in Mean J and 3.4% in Mean F), which demonstrates the effectiveness of the MAT block.</p><p>Moreover, we also evaluate the performance of MATNet with a different number of MAT blocks in each deep residual MAT layer. As shown in Tab. 2, the performance of the model gradually improves as L increases, reaching saturation at L = 5. Based on this analysis, we use L = 5 as the default number of MAT blocks in MATNet.</p><p>SSA Block. To measure the effectiveness of the SSA block, we design another network variant, MATNet w/o SSA, by replacing the SSA block with a simple skip layer. As can be observed, its performance is 1.7% lower than our full model in terms of Mean J , and 1.0% lower in Mean F. The performance drop is mainly caused by the redundant spatio-temporal features from the encoder. Our SSA block aims to eliminate the redundancy by only focusing on the features that are beneficial to segmentation.</p><p>Effectiveness of HEM. We also study the influence of using HEM during training. HEM is expected to facilitate the learning of more accurate object boundaries, which should  further boost the segmentation procedure. The results in Tab. 1 (see MATNet w/o HEM) indicate the importance of HEM. By directly controlling the loss function in Eq. 12, HEM helps to improve the contour accuracy by 2.3%. Impact of Backbone. To verify that the high performance of our network is not mainly due to the powerful backbone, we replace ResNet-101 with ResNet-50 to construct another network, i.e. MATNet w/ Res50. We see that the performance slightly degrades, but it still outperforms AGS in terms of both Mean J and Mean F. This further confirms the effectiveness of MATNet.</p><p>Qualitative Comparison. <ref type="figure" target="#fig_7">Fig. 7</ref> shows visual results of the above ablation studies on two sequences. We see that all of the network variants produce worse results compared with MATNet. It should also be noted that the MAT block has the greatest impact on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-arts</head><p>Evaluation on DAVIS-16. We compare our MATNet with the top performing ZVOS methods in the public leaderboard of DAVIS-16. The detailed results are shown in Tab. 3. We see that MATNet outperforms all the reported methods across most metrics. Compared with the second-best, AGNN <ref type="bibr" target="#b32">(Wang et al. 2019a</ref>), MATNet obtains improvements    <ref type="bibr" target="#b3">(Faisal et al. 2019)</ref>, use motion cues to improve segmentation. Our MATNet outperforms all of these methods by a large margin. The reason lies in that these methods learn motion and appearance features independently, without considering the close interactions between them. In contrast, our MATNet can learn more effective multi-modal object representation with the interleaved encoder.</p><p>Evaluation on Youtube-Objects. Tab. 4 reports the detailed results on Youtube-Objects. Our model also shows favorable performance, second only to AGS. The performance gap is mainly caused by sequences in the Airplane and Boat categories, which contain objects that move very slowly and have visually similar appearances to their surroundings. Both factors result in inaccurate estimation of optical flow. In other categories, our model obtains a consistent performance improvement in comparison with AGS.</p><p>Evaluation on FBMS. For completeness, we also evaluate our method on FBMS. As shown in Tab. 5, MATNet produces the best results with 76.1% over Mean J , which outperforms the second-best result, i.e. PDB <ref type="bibr" target="#b24">(Song et al. 2018</ref>), by 2.1%. Qualitative Results. <ref type="figure" target="#fig_6">Fig. 6</ref> depicts sample results for representative sequences from the three datasets. The dancetwirl sequence from DAVIS-16 contains many challenging factors, such as object deformation, motion blur and background clutter. We can see that our method is robust to these challenges and delineates the target with accurate contours. The effectiveness is further proved in cat-0001 from Youtube-Objects, in which the cat has a similar appearance to the surroundings and encounters large deformation. In addition, our model also works well in dogs02, in which the target suffers from large scale variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Inspired by the inherent multi-modal perception mechanism in HVS, we present a novel model, MATNet, for ZVOS, which introduces a new way of learning rich spatio-temporal object features. This is achieved by MAT blocks within a two-stream interleaved encoder, which allow the transition of attentive motion features to enhance appearance learning at each convolution stage. The encoder features are further processed by a bridge network to produce a compact and scale-sensitive representation, which is fed into a decoder to obtain accurate segmentation in a top-down fashion. Extensive experimental results indicate that MATNet achieves favorable performance against current state-of-the-art methods. The proposed interleaved encoder is a novel two-stream framework for spatio-temporal representation learning in videos, and can be easily extended to other video analysis tasks, such as action recognition and video classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Computational graph of the MAT block. ⊗ and c indicate matrix multiplication and concatenation operations, respectively. Xie et al. 2019), re-identification (Zhou and Shao 2018), visual saliency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the effects of our MAT block. (a) Image. (b) Optical flow. Comparing the feature maps in V a (c) and in U a (d), we find that our MAT block can emphasize important regions and suppress background responses, providing more effective object representation for segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of hard example mining (HEM) for object boundary detection. During training, for each training image in (a), our method first estimates an edge map (c) using off-the-shelf HED<ref type="bibr" target="#b38">(Xie and Tu 2015)</ref>, and then determines hard pixels (d) to facilitate training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Computational graph of the BAR i block. c and ⊕ indicate concatenation and element-wise addition operations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on three sequences. From top to bottom: dance-twirl from DAVIS-16, dogs02 from FBMS, and cat-0001 from Youtube-Objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pipleline of MATNet. The frame I a and flow I m are first input into the interleaved encoder to extract multi-scale spatio-temporal features U i . At each residual stage, we break the original information flow in ResNet. Instead, a deep MAT block is proposed to create a new interleaved information flow by simultaneously considering motion V m,i and appearance V a,i . U i is further fed into the decoder via the bridge network to obtain boundary results M b 2 ∼ M b 5 and the segmentation M s .</figDesc><table><row><cell>473x473</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Soft Attention</cell></row><row><cell></cell><cell></cell><cell>U 3,0</cell><cell></cell><cell>Attention Transition</cell></row><row><cell>I 3</cell><cell></cell><cell>C</cell><cell></cell><cell>Concatenation</cell></row><row><cell>Deep</cell><cell>MAT</cell><cell>U 0</cell><cell>GC</cell><cell>Global Conv. Layer</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>U 5,0</cell><cell></cell><cell>Appearance Feat. Flow</cell></row><row><cell></cell><cell></cell><cell>Segmentation GT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Motion Feat. Flow</cell></row><row><cell>I 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spatio-Temporal Feat. Flow</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of the proposed network on DAVIS-16, measured by the Mean J and Mean F.</figDesc><table><row><cell>Network Variant</cell><cell cols="2">Mean J ↑ ∆J</cell><cell cols="2">Mean F ↑ ∆F</cell></row><row><cell>MATNet w/o MAT</cell><cell>79.5</cell><cell>-2.9</cell><cell>77.3</cell><cell>-3.4</cell></row><row><cell>MATNet w/o SSA</cell><cell>80.7</cell><cell>-1.7</cell><cell>79.7</cell><cell>-1.0</cell></row><row><cell>MATNet w/o HEM</cell><cell>81.4</cell><cell>-1.0</cell><cell>78.4</cell><cell>-2.3</cell></row><row><cell>MATNet w/ Res50</cell><cell>81.1</cell><cell>-1.3</cell><cell>79.3</cell><cell>-1.4</cell></row><row><cell>MATNet w/ Res101</cell><cell>82.4</cell><cell>-</cell><cell>80.7</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons with different numbers of MAT blocks cascaded in each MAT layer on DAVIS-16.</figDesc><table><row><cell>Metric</cell><cell cols="5">L = 0 L = 1 L = 3 L = 5 L = 7</cell></row><row><cell>Mean J ↑</cell><cell>79.5</cell><cell>80.6</cell><cell>81.6</cell><cell>82.4</cell><cell>82.2</cell></row><row><cell>Mean F ↑</cell><cell>77.3</cell><cell>80.3</cell><cell>80.7</cell><cell>80.7</cell><cell>80.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of ZVOS methods on the DAVIS-16 validation set. The best result for each metric is boldfaced. All the results are borrowed from the public leaderboard maintained by the DAVIS challenge.</figDesc><table><row><cell></cell><cell>Measure</cell><cell cols="12">SFL FSEG LVO ARP PDB LSMO MotAdapt EPO AGS COSNet AGNN MATNet</cell></row><row><cell></cell><cell>Mean↑</cell><cell>67.4</cell><cell>70.7</cell><cell>75.9</cell><cell>76.2</cell><cell>77.2</cell><cell>78.2</cell><cell>77.2</cell><cell>80.6</cell><cell>79.7</cell><cell>80.5</cell><cell>80.7</cell><cell>82.4</cell></row><row><cell>J</cell><cell>Recall↑</cell><cell>81.4</cell><cell>83.5</cell><cell>89.1</cell><cell>91.1</cell><cell>90.1</cell><cell>89.1</cell><cell>87.8</cell><cell>95.2</cell><cell>91.1</cell><cell>93.1</cell><cell>94.0</cell><cell>94.5</cell></row><row><cell></cell><cell>Decay↓</cell><cell>6.2</cell><cell>1.5</cell><cell>0.0</cell><cell>7.0</cell><cell>0.9</cell><cell>4.1</cell><cell>5.0</cell><cell>2.2</cell><cell>1.9</cell><cell>4.4</cell><cell>0.0</cell><cell>5.5</cell></row><row><cell></cell><cell>Mean↑</cell><cell>66.7</cell><cell>65.3</cell><cell>72.1</cell><cell>70.6</cell><cell>74.5</cell><cell>75.9</cell><cell>77.4</cell><cell>75.5</cell><cell>77.4</cell><cell>79.5</cell><cell>79.1</cell><cell>80.7</cell></row><row><cell>F</cell><cell>Recall↑</cell><cell>77.1</cell><cell>73.8</cell><cell>83.4</cell><cell>83.5</cell><cell>84.4</cell><cell>84.7</cell><cell>84.4</cell><cell>87.9</cell><cell>85.8</cell><cell>89.5</cell><cell>90.5</cell><cell>90.2</cell></row><row><cell></cell><cell>Decay↓</cell><cell>5.1</cell><cell>1.8</cell><cell>1.3</cell><cell>7.9</cell><cell>-0.2</cell><cell>3.5</cell><cell>3.3</cell><cell>2.4</cell><cell>1.6</cell><cell>5.0</cell><cell>0.0</cell><cell>4.5</cell></row><row><cell>T</cell><cell>Mean↓</cell><cell>28.2</cell><cell>32.8</cell><cell>26.5</cell><cell>39.3</cell><cell>29.1</cell><cell>21.2</cell><cell>27.9</cell><cell>19.3</cell><cell>26.7</cell><cell>18.4</cell><cell>33.7</cell><cell>21.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results for each category on Youtube-Objects over Mean J .</figDesc><table><row><cell cols="3">Category LVO SFL FSEG PDB AGS MATNet</cell></row><row><cell>Airplane</cell><cell>86.2 65.6 81.7 78.0 87.7</cell><cell>72.9</cell></row><row><cell>Bird</cell><cell>81.0 65.4 63.8 80.0 76.7</cell><cell>77.5</cell></row><row><cell>Boat</cell><cell>68.5 59.9 72.3 58.9 72.2</cell><cell>66.9</cell></row><row><cell>Car</cell><cell>69.3 64.0 74.9 76.5 78.6</cell><cell>79.0</cell></row><row><cell>Cat</cell><cell>58.8 58.9 68.4 63.0 69.2</cell><cell>73.7</cell></row><row><cell>Cow</cell><cell>68.5 51.2 68.0 64.1 64.6</cell><cell>67.4</cell></row><row><cell>Dog</cell><cell>61.7 54.1 69.4 70.1 73.3</cell><cell>75.9</cell></row><row><cell>Horse</cell><cell>53.9 64.8 60.4 67.6 64.4</cell><cell>63.2</cell></row><row><cell cols="2">Motorbike 60.8 52.6 62.7 58.4 62.1</cell><cell>62.6</cell></row><row><cell>Train</cell><cell>66.3 34.0 62.2 35.3 48.2</cell><cell>51.0</cell></row><row><cell cols="2">Mean J ↑ 67.5 57.1 68.4 65.5 69.7</cell><cell>69.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on FBMS over Mean J .</figDesc><table><row><cell cols="2">Measure ARP MSTP FSEG IET OBN PDB MATNet</cell></row><row><cell>Mean J ↑ 59.8 60.8 68.4 71.9 73.9 74.0</cell><cell>76.1</cell></row><row><cell cols="2">of 1.7% and 1.6% in terms of Mean J and Mean F, respec-</cell></row><row><cell cols="2">tively. In Tab. 3, some of the deep learning-based models,</cell></row><row><cell cols="2">e.g. FSEG (Jain, Xiong, and Grauman 2017), LVO (Tok-</cell></row><row><cell cols="2">makov, Alahari, and Schmid 2017b), MoTAdapt (Siam et</cell></row><row><cell>al. 2019) and EPO</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction between dorsal and ventral processing streams: Where, when and how?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Cloutman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="263" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploiting geometric constraints on dense trajectories for motion saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="8" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation using motion saliency-guided spatiotemporal propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6526" to="6535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation with motion-based bilateral networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="207" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to generate video object segment proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="787" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3623" to="3632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do lowlevel visual features have a causal influence on gaze during dynamic scene viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="144" to="144" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1583" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3282" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video object segmentation using teacherstudent adaptation in a human robot interaction (hri) setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3386" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4481" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="282" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giro-I</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5277" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9236" to="9245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">So</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attentive region embedding network for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9384" to="9393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-tosequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aware attentive multi-view inference for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6489" to="6498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video object segmentation aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative learning of semi-supervised segmentation and classification for medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2079" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
