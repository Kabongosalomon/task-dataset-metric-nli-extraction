<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Glyce: Glyph-vectors for Chinese Character Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
							<email>yuxianmeng@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<email>weiwu@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<email>feiwang@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
							<email>xiaoyali@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Nie</surname></persName>
							<email>pingnie@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
							<email>fanyin@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyu</forename><surname>Li</surname></persName>
							<email>muyuli@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
							<email>qinghonghan@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
							<email>xiaofeisun@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiweili@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Glyce: Glyph-vectors for Chinese Character Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and <ref type="formula">(3)</ref> We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new stateof-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8% on the Fudan corpus for text classification. 1 2 Recently, there have been some efforts applying CNN-based algorithms on the visual features of characters. Unfortunately, they do not show consistent performance boosts [Liu et al., 2017, Zhang    </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese is a logographic language. The logograms of Chinese characters encode rich information of their meanings. Therefore, it is intuitive that NLP tasks for Chinese should benefit from the use of the glyph information. Taking into account logographic information should help semantic modeling. Recent studies indirectly support this argument: Radical representations have proved to be useful in a wide range of language understanding tasks <ref type="bibr" target="#b0">[Shi et al., 2015</ref><ref type="bibr" target="#b1">, Li et al., 2015</ref><ref type="bibr" target="#b2">, Yin et al., 2016</ref><ref type="bibr" target="#b3">, Sun et al., 2014</ref><ref type="bibr" target="#b4">, Shao et al., 2017</ref>. Using the Wubi scheme -a Chinese character encoding method that mimics the order of typing the sequence of radicals for a character on the computer keyboard --is reported to improve performances on Chinese-English machine translation <ref type="bibr" target="#b5">[Tan et al., 2018]</ref>. <ref type="bibr" target="#b6">Cao et al. [2018]</ref> gets down to units of greater granularity, and proposed stroke n-grams for character modeling. and <ref type="bibr" target="#b8">LeCun, 2017]</ref>, and some even yield negative results <ref type="bibr" target="#b9">[Dai and Cai, 2017]</ref>. For instance, <ref type="bibr" target="#b9">Dai and Cai [2017]</ref> run CNNs on char logos to obtain Chinese character representations and used them in the downstream language modeling task. They reported that the incorporation of glyph representations actually worsens the performance and concluded that CNN-based representations do not provide extra useful information for language modeling. Using similar strategies, <ref type="bibr" target="#b7">Liu et al. [2017]</ref> and <ref type="bibr" target="#b8">Zhang and LeCun [2017]</ref> tested the idea on text classification tasks, and performance boosts were observed only in very limited number of settings. Positive results come from <ref type="bibr" target="#b10">Su and Lee [2017]</ref>, which found glyph embeddings help two tasks: word analogy and word similarity. Unfortunately, <ref type="bibr" target="#b10">Su and Lee [2017]</ref> only focus on word-level semantic tasks and do not extend improvements in the word-level tasks to higher level NLP tasks such as phrase, sentence or discourse level. Combined with radical representations, <ref type="bibr" target="#b4">Shao et al. [2017]</ref> run CNNs on character figures and use the output as auxiliary features in the POS tagging task.</p><p>We propose the following explanations for negative results reported in the earlier CNN-based models <ref type="bibr" target="#b9">[Dai and Cai, 2017]</ref>: (1) not using the correct version(s) of scripts: Chinese character system has a long history of evolution. The characters started from being easy-to-draw, and slowly transitioned to being easy-to-write. Also, they became less pictographic and less concrete over time. The most widely used script version to date, the Simplified Chinese, is the easiest script to write, but inevitably loses the most significant amount of pictographic information. For example, "人" (human) and "入" (enter), which are of irrelevant meanings, are highly similar in shape in simplified Chinese, but very different in historical languages such as bronzeware script.</p><p>(2) not using the proper CNN structures: unlike ImageNet images <ref type="bibr" target="#b11">[Deng et al., 2009]</ref>, the size of which is mostly at the scale of 800*600, character logos are significantly smaller (usually with the size of 12*12). It requires a different CNN architecture to capture the local graphic features of character images; (3) no regulatory functions were used in previous work: unlike the classification task on the imageNet dataset, which contains tens of millions of data points, there are only about 10,000 Chinese characters. Auxiliary training objectives are thus critical in preventing overfitting and promoting the model's ability to generalize.</p><p>In this paper, we propose GLYCE, the GLYph-vectors for Chinese character representations. We treat Chinese characters as images and use CNNs to obtain their representations. We resolve the aforementioned issues by using the following key techniques:</p><p>• We use the ensemble of the historical and the contemporary scripts (e.g., the bronzeware script, the clerical script, the seal script, the traditional Chinese etc), along with the scripts of different writing styles (e.g, the cursive script) to enrich pictographic information from the character images. • We utilize the Tianzige-CNN (田字格) structures tailored to logographic character modeling.</p><p>• We use multi-task learning methods by adding an image-classification loss function to increase the model's ability to generalize.</p><p>Glyce is found to improve a wide range of Chinese NLP tasks. We are able to obtain the SOTA performances on a wide range of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification (BQ, LCQMC, XNLI, NLPCC-DBQA), single sentence classification tasks (ChnSentiCorp, the Fudan corpus, iFeng), dependency parsing, and semantic role labeling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Glyce</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Using Historical Scripts</head><p>As discussed in Section 1, pictographic information is heavily lost in the simplified Chinese script. We thus propose using scripts from various time periods in history and also of different writing styles. We collect the following major historical script with details shown in <ref type="table" target="#tab_0">Table 1</ref>. Scripts from different historical periods, which are usually very different in shape, help the model to integrate pictographic evidence from various sources; Scripts of different writing styles help improve the model's ability to generalize. Both strategies are akin to widely-used data augmentation strategies in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Tianzige-CNN Structure for Glyce</head><p>Directly using deep CNNs , <ref type="bibr" target="#b13">Szegedy et al. [2016]</ref>, <ref type="bibr" target="#b14">Ma et al. [2018a]</ref> in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of Imagenet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the imageNet dataset utilizes tens of millions of different images. In contrast, there are only about 10,000 distinct Chinese characters. To tackle these issues, we propose the Tianzige-CNN structure, which is tailored to Chinese character modeling as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Tianzige (田字格) is a traditional form of Chinese Calligraphy. It is a four-squared format (similar to Chinese character 田) for beginner to learn writing Chinese characters. The input image x image is first passed through a convolution layer with kernel size 5 and output channels 1024 to capture lower level graphic features. Then a max-pooling of kernel size 4 is applied to the feature map which reduces the resolution from 8 × 8 to 2 × 2, . This 2 × 2 tianzige structure presents how radicals are arranged in Chinese characters and also the order by which Chinese characters are written. Finally, we apply group convolutions <ref type="bibr" target="#b15">[Krizhevsky et al., 2012</ref> rather than conventional convolutional operations to map tianzige grids to the final outputs . Group convolutional filters are much smaller than their normal counterparts, and thus are less prone to overfitting. It is fairly easy to adjust the model from single script to multiple scripts, which can be achieved by simply changing the input from 2D (i.e., d font × d font ) to 3D (i.e., d font × d font × N script ), where d font denotes the font size and N script the number of scripts we use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Classification as an Auxiliary Objective</head><p>To further prevent overfitting, we use the task of image classification as an auxiliary training objective. The glyph embedding h image from CNNs will be forwarded to an image classification objective to predict its corresponding charID. Suppose the label of image x is z. The training objective for the image classification task L(cls) is given as follows:</p><formula xml:id="formula_0">L(cls) = − log p(z|x) = − log softmax(W × h image )<label>(1)</label></formula><p>Let L(task) denote the task-specific objective for the task we need to tackle, e.g., language modeling, word segmentation, etc. We linearly combine L(task) and L(cl), making the final objective training function as follows:</p><formula xml:id="formula_1">L = (1 − λ(t)) L(task) + λ(t)L(cls)</formula><p>(2) where λ(t) controls the trade-off between the task-specific objective and the auxiliary imageclassification objective. λ is a function of the number of epochs t: denotes the starting value, λ 1 ∈ [0, 1] denotes the decaying value. This means that the influence from the image classification objective decreases as the training proceeds, with the intuitive explanation being that at the early stage of training, we need more regulations from the image classification task. Adding image classification as a training objective mimics the idea of multi-task learning.</p><formula xml:id="formula_2">λ(t) = λ 0 λ t 1 , where λ 0 ∈ [0, 1]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combing Glyph Information with BERT</head><p>The glyph embeddings can be directly output to downstream models such as RNNs, LSTMs, transformers.</p><p>Since large scale pretraining systems using language models, such as BERT <ref type="bibr" target="#b17">[Devlin et al., 2018]</ref>, <ref type="bibr">ELMO [Peters et al., 2018]</ref> and GPT <ref type="bibr" target="#b19">[Radford et al., 2018]</ref>, have proved to be effective in a wide range of NLP tasks, we explore the possibility of combining glyph embeddings with BERT embeddings. Such a strategy will potentially endow the model with the advantage of both glyph evidence and large-scale pretraining. The overview of the combination is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The model consists of four layers: the BERT layer, the glyph layer, the Glyce-BERT layer and the task-specific output layer.</p><p>• BERT Layer Each input sentence S is concatenated with a special CLS token denoting the start of the sentence, and a SEP token, denoting the end of the sentence. Given a pre-trained BERT model, the embedding for each token of S is computed using BERT. We use the output from the last layer of the BERT transformer to represent the current token. • Glyph Layer the output glyph embeddings of S from tianzege-CNNs.</p><p>• Glyce-BERT layer Position embeddings are first added to the glyph embeddings. The addition is then concatenated with BERT to obtain the full Glyce representations. • Task-specific output layer Glyce representations are used to represent the token at that position, similar as word embeddings or Elmo emebddings <ref type="bibr" target="#b18">[Peters et al., 2018]</ref>. Contextualaware information has already been encoded in the BERT representation but not glyph representations. We thus need additional context models to encode contextual-aware glyph representations. Here, we choose multi-layer transformers <ref type="bibr" target="#b20">[Vaswani et al., 2017]</ref>. The output representations from transformers are used as inputs to the prediction layer. It is worth noting that the representations the special CLS and SEP tokens are maintained at the final task-specific embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks</head><p>In this section, we describe how glypg embeddings can be used for different NLP tasks. In the vanilla version, glyph embeddings are simply treated as character embeddings, which are fed to models built on top of the word-embedding layers, such as RNNs, CNNs or more sophisticated ones. If combined with BERT, we need to specifically handle the integration between the glyph embeddings and the pretrained embeddings from BERT in different scenarios, as will be discussed in order below:</p><p>Sequence Labeling Tasks Many Chinese NLP tasks, such as name entity recognition (NER), Chinese word segmentation (CWS) and part speech tagging (POS), can be formalized as characterlevel sequence labeling tasks, in which we need to predict a label for each character. For glyce-BERT model, the embedding output from the task-specific layer (described in Section 2.4) is fed to the CRF model for label predictions.</p><p>Single Sentence Classification For text classification tasks, a single label is to be predicted for the entire sentence. In the BERT model, the representation for the CLS token in the final layer of BERT is output to the softmax layer for prediction. We adopt the similar strategy, in which the representation for the CLS token in the task-specific layer is fed to the softmax layer to predict labels.</p><p>Sentence Pair Classification For sentence pair classification task like SNIS <ref type="bibr" target="#b21">[Bowman et al., 2015]</ref>, a model needs to handle the interaction between the two sentences and outputs a label for a pair of sentences. In the BERT setting, a sentence pair (s 1 , s 2 ) is concatenated with one CLS and two SEP tokens, denoted by [CLS, s 1 , SEP, s 2 , SEP]. The concatenation is fed to the BERT model, and the obtained CLS representation is then fed to the softmax layer for label prediction. We adopt the similar strategy for Glyce-BERT, in which [CLS, s 1 , SEP, s 2 , SEP] is subsequently passed through the BERT layer, Glyph layer, Glyce-BERT layer and the task-specific output layer. The CLS representation from the task-specific output layer is fed to the softmax function for the final label prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To enable apples-to-apples comparison, we perform grid parameter search for both baselines and the proposed model on the dev set. Tasks that we work on are described in order below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tagging</head><p>NER For the task of Chinese NER, we used the widely-used OntoNotes, MSRA, Weibo and resume datasets. Since most datasets don't have gold-standard segmentation, the task is normally treated as a char-level tagging task: outputting an NER tag for each character. The currently most widely used non-BERT model is Lattice-LSTMs , achieving better performances than CRF+LSTM <ref type="bibr" target="#b24">[Ma and Hovy, 2016</ref>].</p><p>CWS : The task of Chinese word segmentation (CWS) is normally treated as a char-level tagging problem. We used the widely-used PKU, MSR, CITYU and AS benchmarks from SIGHAN 2005 bake-off for evaluation.</p><p>POS The task of Chinese part of speech tagging is normally formalized as a character-level sequence labeling task, assigning labels to each of the characters within the sequence. We use the CTB5, CTB9 and UD1 (Universal Dependencies) benchmarks to test our models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence Pair Classification</head><p>For sentence pair classification tasks, we need to output a label for each pair of sentence. We employ the following four different datasets: (1) BQ (binary classification task) <ref type="bibr" target="#b21">[Bowman et al., 2015]</ref>; (2) LCQMC (binary classification task) <ref type="bibr" target="#b28">[Liu et al., 2018]</ref>, (3) XNLI (three-class classification task) <ref type="bibr">[Williams and Bowman]</ref>, and (4) NLPCC-DBQA (binary classification task) 3 .  The current non-BERT SOTA model is based on the bilateral multi-perspective matching model (BiMPM) , which specifically tackles the subunit matching between sentences. Glyph embeddings are incorporated into BiMPMs, forming the Glyce+BiMPM baseline. Results regarding each model on different datasets are given in    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single Sentence Classification</head><p>For single sentence/document classification, we need to output a label for a text sequence. The label could be either a sentiment indicator or a news genre. Datasets that we use include: (1) ChnSentiCorp (binary classification); (2) the Fudan corpus (5-class classification) <ref type="bibr" target="#b31">[Li, 2011]</ref>; and (3) Ifeng (5-class classification).</p><p>Results for different models on different tasks are shown in <ref type="table" target="#tab_8">Table 6</ref>. We observe similar phenomenon as before: Glyce+BERT achieves SOTA results on all of the datasets. Specifically, the Glyce+BERT model achieves an almost perfect accuracy (99.8) on the Fudan corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dependency Parsing and Semantic Role Labeling</head><p>For dependency parsing <ref type="bibr">Manning, 2014, Dyer et al., 2015]</ref>, we used the widely-used Chinese Penn Treebank 5.1 dataset for evaluation. Our implementation uses the previous state-of-theart Deep Biaffine model <ref type="bibr" target="#b34">Dozat and Manning [2016]</ref> as a backbone. We replaced the word vectors from the biaffine model with Glyce-word embeddings, and exactly followed its model structure and training/dev/test split criteria. We report scores for unlabeled attachment score (UAS) and labeled attachment score (LAS). Results for previous models are copied from <ref type="bibr" target="#b34">[Dozat and Manning, 2016</ref><ref type="bibr" target="#b35">, Ballesteros et al., 2016</ref><ref type="bibr" target="#b36">, Cheng et al., 2016</ref>. Glyce-word pushes SOTA performances up by +0.9 and +0.8 in terms of UAS and LAS scores.</p><p>For the task of semantic role labeling (SRL) <ref type="bibr" target="#b38">[Roth and Lapata, 2016</ref><ref type="bibr" target="#b39">, Marcheggiani and Diego, 2017</ref><ref type="bibr" target="#b40">, He et al., 2018</ref>, we used the CoNLL-2009 shared-task. We used the current SOTA model, the k-order pruning algorithm <ref type="bibr" target="#b40">[He et al., 2018]</ref> as a backbone. <ref type="bibr">4</ref> We replaced word embeddings with Glyce embeddings. Glyce outperforms the previous SOTA performance by 0.9 with respect to the F1 score, achieving a new SOTA score of 83.7.</p><p>BERT does not perform competitively in these two tasks, and results are thus omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>In this section, we discuss the influence of different factors of the proposed model. We use the LCQMC dataset of the sentence-pair prediction task for illustration. Factors that we discuss include training strategy, model architecture, auxiliary image-classification objective, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Strategy</head><p>This section talks about a training tactic (denoted by BERT-glyce-joint), in which given task-specific supervisions, we first fine-tune the BERT model, then freeze BERT to fine-tune the glyph layer, and finally jointly tune both layers until convergence. We compare this strategy with other tactics, including (1) the Glyph-Joint strategy, in which BERT is not fine-tuned in the beginning: we first    freeze BERT to tune the glyph layer, and then jointly tune both layers until convergence; and (2) the joint strategy, in which we directly jointly training two models until converge.</p><p>Results are shown in <ref type="table" target="#tab_11">Table 8</ref>. As can be seen, the BERT-glyce-joint outperforms the rest two strategies. Our explanation for the inferior performance of the joint strategy is as follows: the BERT layer is pretrained but the glyph layer is randomly initialized. Given the relatively small amount of training signals, the BERT layer could be mislead by the randomly initialized glyph layer at the early stage of training, leading to inferior final performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Structures of the task-specific output layer</head><p>The concatenation of the glyph embedding and the BERT embedding is fed to the task-specific output layer. The task-specific output layer is made up with two layers of transformer layers. Here we change transformers to other structures such as BiLSTMs and CNNs to explore the influence. We also try the BiMPM structure  to see the results.</p><p>Performances are shown in <ref type="table" target="#tab_12">Table 9</ref>. As can be seen, transformers not only outperform BiLSTMs and CNNs, but also the BiMPM structure, which is specifically built for the sentence pair classification task. We conjecture that this is because of the consistency between transformers and the BERT structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The image-classification training objective</head><p>We also explore the influence of the image-classification training objective, which outputs the glyph representation to an image-classification objective. <ref type="table" target="#tab_0">Table 10</ref> represents its influence. As can be seen, this auxiliary training objective given a +0.8 performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CNN structures</head><p>Results for different CNN structures are shown in <ref type="table" target="#tab_0">Table 11</ref>. As can be seen, the adoption of tianzige-CNN structure introduces a performance boost of F1 about +1.0. Directly using deep CNNs in our task results in very poor performances because of (1) relatively smaller size of the character images: the size of ImageNet images is usually at the scale of 800*600, while the size of Chinese character images is significantly smaller, usually at the scale of 12*12; and (2) the lack of training examples: classifications on the ImageNet dataset utilizes tens of millions of different images. In contrast, there are only about 10,000 distinct Chinese characters. We utilize the Tianzige-CNN (田字 格) structures tailored to logographic character modeling for Chinese. This tianzige structure is of significant importance in extracting character meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose Glyce, Glyph-vectors for Chinese Character Representations. Glyce treats Chinese characters as images and uses Tianzige-CNN to extract character semantics. Glyce provides a general way to model character semantics of logographic languages. It is general and fundamental. Just like word embeddings, Glyce can be integrated to any existing deep learning system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the Tianzege-CNN used in Glyce.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Combing glyph information with BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Using Glyce-BERT model for different tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scripts and writing styles used in Glyce.</figDesc><table><row><cell>Chinese</cell><cell>English</cell><cell>Time Period</cell></row><row><cell>金文</cell><cell>Bronzeware script</cell><cell>Shang and Zhou dynasty (2000 BC -300 BC)</cell></row><row><cell>隶书</cell><cell>Clerical script</cell><cell>Han dynasty (200BC-200AD)</cell></row><row><cell>篆书</cell><cell>Seal script</cell><cell>Han dynasty and Wei-Jin period (100BC -420 AD)</cell></row><row><cell>魏碑</cell><cell>Tablet script</cell><cell>Northern and Southern dynasties 420AD -588AD</cell></row><row><cell>繁体中文</cell><cell>Traditional Chinese</cell><cell>600AD -1950AD (mainland China). still currently used in HongKong and Taiwan</cell></row><row><cell>简体中文(宋体)</cell><cell>Simplified Chinese -Song</cell><cell>1950-now</cell></row><row><cell cols="2">简体中文(仿宋体) Simplified Chinese -FangSong</cell><cell>1950-now</cell></row><row><cell>草书</cell><cell>Cursive script</cell><cell>Jin Dynasty to now</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for NER tasks. This is due to the discrepancy between the dataset which BERT is pretrained on (i.e., wikipedia) and weibo. The Glyce-BERT model outperforms BERT and sets new SOTA results across all datasets, manifesting the effectiveness of incorporating glyph information. We are able to achieve SOTA performances on all of the datasets using either Glyce model itself or BERT-Glyce model.</figDesc><table><row><cell></cell><cell>PKU</cell><cell></cell><cell></cell><cell></cell><cell>CITYU</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Yang et al. [2017]</cell><cell>-</cell><cell>-</cell><cell>96.3</cell><cell>Yang et al. [2017]</cell><cell>-</cell><cell>-</cell><cell>96.9</cell></row><row><cell>Ma et al. [2018b]</cell><cell>-</cell><cell>-</cell><cell>96.1</cell><cell>Ma et al. [2018b]</cell><cell>-</cell><cell>-</cell><cell>97.2</cell></row><row><cell cols="2">Huang et al. [2019] -</cell><cell>-</cell><cell>96.6</cell><cell cols="2">Huang et al. [2019] -</cell><cell>-</cell><cell>97.6</cell></row><row><cell>BERT</cell><cell cols="3">96.8 96.3 96.5</cell><cell>BERT</cell><cell cols="3">97.5 97.7 97.6</cell></row><row><cell>Glyce+BERT</cell><cell cols="3">97.1 96.4 96.7</cell><cell>Glyce+BERT</cell><cell cols="3">97.9 98.0 97.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+0.2)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.3)</cell></row><row><cell></cell><cell>MSR</cell><cell></cell><cell></cell><cell></cell><cell>AS</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Yang et al. [2017]</cell><cell>-</cell><cell>-</cell><cell>97.5</cell><cell>Yang et al. [2017]</cell><cell>-</cell><cell>-</cell><cell>95.7</cell></row><row><cell>Ma et al. [2018b]</cell><cell>-</cell><cell>-</cell><cell>98.1</cell><cell>Ma et al. [2018b]</cell><cell>-</cell><cell>-</cell><cell>96.2</cell></row><row><cell cols="2">Huang et al. [2019] -</cell><cell>-</cell><cell>97.9</cell><cell cols="2">Huang et al. [2019] -</cell><cell>-</cell><cell>96.6</cell></row><row><cell>BERT</cell><cell cols="3">98.1 98.2 98.1</cell><cell>BERT</cell><cell cols="3">96.7 96.4 96.5</cell></row><row><cell>Glyce+BERT</cell><cell cols="3">98.2 98.3 98.3</cell><cell>Glyce+BERT</cell><cell cols="3">96.6 96.8 96.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+0.2)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.2)</cell></row></table><note>Results for NER, CWS and POS are respectively shown in Tables 2, 3 and 4. When comparing with non-BERT models, Lattice-Glyce performs better than all non-BERT models across all datasets in all tasks. BERT outperforms non-BERT models in all datasets except Weibo.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results for CWS tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b4">Shao et al. [2017]</ref> (Sig) 93.68 94.47 94.07 Shao et al. [2017] (Ens) 93.95 94.81 94.38 Shao et al. [2017] (Sig) 91.81 94.47 91.89 Shao et al. [2017] (Ens) 92.28 92.40 92.34</figDesc><table><row><cell></cell><cell>CTB5</cell><cell></cell><cell></cell><cell></cell><cell>CTB9</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Lattice-LSTM</cell><cell cols="3">94.77 95.51 95.14</cell><cell>Lattice-LSTM</cell><cell cols="3">92.53 91.73 92.13</cell></row><row><cell>Glyce+Lattice-LSTM</cell><cell cols="3">95.49 95.72 95.61</cell><cell>Lattice-LSTM+Glyce</cell><cell cols="3">92.28 92.85 92.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+0.47)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.25)</cell></row><row><cell>BERT</cell><cell cols="3">95.86 96.26 96.06</cell><cell>BERT</cell><cell cols="3">92.43 92.15 92.29</cell></row><row><cell>Glyce+BERT</cell><cell cols="3">96.50 96.74 96.61</cell><cell>Glyce+BERT</cell><cell cols="3">93.49 92.84 93.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+0.55)</cell><cell></cell><cell></cell><cell></cell><cell>(+0.86)</cell></row><row><cell></cell><cell>CTB6</cell><cell></cell><cell></cell><cell></cell><cell>UD1</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell cols="2">Shao et al. [2017] (Sig) -</cell><cell>-</cell><cell>90.81</cell><cell cols="4">Shao et al. [2017] (Sig) 89.28 89.54 89.41</cell></row><row><cell>Lattice-LSTM</cell><cell cols="3">92.00 90.86 91.43</cell><cell cols="4">Shao et al. [2017] (Ens) 89.67 89.86 89.75</cell></row><row><cell>Glyce+Lattice-LSTM</cell><cell cols="3">92.72 91.14 91.92</cell><cell>Lattice-LSTM</cell><cell cols="3">90.47 89.70 90.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+0.49)</cell><cell>Lattice-LSTM+Glyce</cell><cell cols="3">91.57 90.19 90.87</cell></row><row><cell>BERT</cell><cell cols="3">94.91 94.63 94.77</cell><cell></cell><cell></cell><cell></cell><cell>(+0.78)</cell></row><row><cell>Glyce+BERT</cell><cell cols="3">95.56 95.26 95.41</cell><cell>BERT</cell><cell cols="3">95.42 94.17 94.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(+0.64)</cell><cell>Glyce+BERT</cell><cell cols="3">96.19 96.10 96.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+1.35)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>3 https://github.com/xxx0624/QA_Model</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results for POS tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>As can be seen, BiPMP+Glyce outperforms BiPMPs, achieving the best results among non-bert models. BERT outperforms all non-BERT models, and BERT+Glyce performs the best, setting new SOTA results on all of the four benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell>BQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LCQMC</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>A</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>A</cell></row><row><cell>BiMPM</cell><cell cols="3">82.3 81.2 81.7</cell><cell>81.9</cell><cell>BiMPM</cell><cell cols="3">77.6 93.9 85.0</cell><cell>83.4</cell></row><row><cell cols="4">Glyce+BiMPM 81.9 85.5 83.7</cell><cell>83.3</cell><cell cols="4">Glyce+BiMPM 80.4 93.4 86.4</cell><cell>85.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(+2.0) (+1.4)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(+1.4) (+1.9)</cell></row><row><cell>BERT</cell><cell cols="3">83.5 85.7 84.6</cell><cell>84.8</cell><cell>BERT</cell><cell cols="3">83.2 94.2 88.2</cell><cell>87.5</cell></row><row><cell>Glyce+BERT</cell><cell cols="3">84.2 86.9 85.5</cell><cell>85.8</cell><cell>Glyce+BERT</cell><cell cols="3">86.8 91.2 88.8</cell><cell>88.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(+0.9) (+1.0)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(+0.6) (+1.2)</cell></row><row><cell></cell><cell></cell><cell>XNLI</cell><cell></cell><cell></cell><cell></cell><cell cols="2">NLPCC-DBQA</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>A</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>A</cell></row><row><cell>BiMPM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.5</cell><cell>BiMPM</cell><cell cols="3">78.8 56.5 65.8</cell><cell>-</cell></row><row><cell cols="2">Glyce+BiMPM -</cell><cell>-</cell><cell>-</cell><cell>67.7</cell><cell cols="4">Glyce+BiMPM 76.3 59.9 67.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.2)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(+1.3) -</cell></row><row><cell>BERT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.4</cell><cell>BERT</cell><cell cols="3">79.6 86.0 82.7</cell><cell>-</cell></row><row><cell>Glyce+BERT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.2</cell><cell>Glyce+BERT</cell><cell cols="3">81.1 85.8 83.4</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+0.8)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(+0.7) -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results for sentence-pair classification tasks.</figDesc><table><row><cell>Model</cell><cell cols="3">ChnSentiCorp the Fudan corpus iFeng</cell></row><row><cell>LSTM</cell><cell>91.7</cell><cell>95.8</cell><cell>84.9</cell></row><row><cell>LSTM + Glyce</cell><cell>93.1</cell><cell>96.3</cell><cell>85.8</cell></row><row><cell></cell><cell>(+ 1.4)</cell><cell>(+0.5)</cell><cell>(+0.9)</cell></row><row><cell>BERT</cell><cell>95.4</cell><cell>99.5</cell><cell>87.1</cell></row><row><cell>Glyce+BERT</cell><cell>95.9</cell><cell>99.8</cell><cell>87.5</cell></row><row><cell></cell><cell>(+0.5)</cell><cell>(+0.3)</cell><cell>(+0.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Accuracies for Single Sentence Classification task.</figDesc><table><row><cell cols="2">Dependency Parsing</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>UAS</cell><cell>LAS</cell></row><row><cell>Ballesteros et al. [2016]</cell><cell></cell><cell>87.7</cell><cell>86.2</cell></row><row><cell cols="2">Kiperwasser and Eliyahu [2016]</cell><cell>87.6</cell><cell>86.1</cell></row><row><cell>Cheng et al. [2016]</cell><cell></cell><cell>88.1</cell><cell>85.7</cell></row><row><cell>Biaffine</cell><cell></cell><cell>89.3</cell><cell>88.2</cell></row><row><cell>Biaffine+Glyce</cell><cell cols="3">90.2 (+0.9) (+0.8) 89.0</cell></row><row><cell cols="3">Semantic Role Labeling</cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Roth and Lapata [2016]</cell><cell>76.9</cell><cell>73.8</cell><cell>75.3</cell></row><row><cell>Marcheggiani and Diego [2017]</cell><cell>84.6</cell><cell>80.4</cell><cell>82.5</cell></row><row><cell>He et al. [2018]</cell><cell>84.2</cell><cell>81.5</cell><cell>82.8</cell></row><row><cell>k-order pruning+Glyce</cell><cell cols="3">85.4 (+0.8) (+0.6) (+0.9) 82.1 83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results for dependency parsing and SRL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Impact of different training strategies.</figDesc><table><row><cell>Strategy</cell><cell cols="3">Precision Recall F1</cell><cell>Accuracy</cell></row><row><cell cols="2">Transformers 86.8</cell><cell>91.2</cell><cell cols="2">88.8 88.7</cell></row><row><cell>BiLSMTs</cell><cell>81.8</cell><cell>94.9</cell><cell cols="2">87.9 86.9</cell></row><row><cell>CNNs</cell><cell>81.5</cell><cell>94.8</cell><cell cols="2">87.6 86.6</cell></row><row><cell>BiMPM</cell><cell>81.1</cell><cell>94.6</cell><cell cols="2">87.3 86.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Impact of structures for the task-specific output layer.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Strategy</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>Acc</cell><cell>Vanilla-CNN</cell><cell cols="2">85.3 89.8 87.4</cell></row><row><cell>W image-cls</cell><cell cols="4">86.8 91.2 88.8 88.7</cell><cell cols="3">He et al. [2016] 84.5 90.8 87.5</cell></row><row><cell cols="5">WO image-cls 83.9 93.6 88.4 87.9</cell><cell>Tianzige-CNN</cell><cell cols="2">86.8 91.2 88.8</cell></row><row><cell cols="5">Table 10: Impact of the auxilliary image classifica-</cell><cell></cell><cell></cell></row><row><cell cols="2">tion training objective.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Impact of CNN structures.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code open sourced at https://github.com/bcmi220/srl_syn_pruning</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Radical embedding: Delving deeper to chinese radicals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="594" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Component-enhanced chinese character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06669</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-granularity chinese word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongchao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="981" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Radical-enhanced chinese character embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Character-based joint segmentation and pos tagging for chinese using bidirectional rnn-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01314</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuang</forename><surname>Mi Xue Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nikola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard Hr</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahnloser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03330</idno>
		<title level="m">Character-level chinese-english translation through ascii encoding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">cw2vec: Learning chinese word embeddings with stroke n-gram information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning character-level compositionality with visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04859</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Which encoding is the best for text classification in chinese, english, japanese and korean?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02657</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Glyph-aware embedding of chinese characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00028</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Ray</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04755</idno>
		<title level="m">Learning chinese word representations from glyphs of characters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11164</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Subword encoding in lattice LSTM for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1810.12594</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chinese NER using lattice LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural word segmentation with rich pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">State-of-the-art chinese word segmentation with bi-lstms. CoRR, abs/1808.06511</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1808.06511" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Toward fast and accurate neural chinese word segmentation with multi-criteria learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.04190" />
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lcqmc: A large-scale chinese question matching corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1952" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The multi-genre nli corpus 0.2: Repeval shared task preliminary version description paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fudan corpus for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronglu</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.08075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training with exploration improves a greedy stack-lstm parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03793</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bi-directional attention with agreement for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02076</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eliyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04351</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07515</idno>
		<title level="m">Neural semantic role labeling with dependency path embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04826</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Syntax for semantic role labeling, to be, or not to be</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiao</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2061" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
