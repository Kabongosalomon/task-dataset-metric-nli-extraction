<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jiao</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">are with the State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>MANUSCRIPT 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image restoration</term>
					<term>auto-encoder</term>
					<term>convolutional/de-convolutional networks</term>
					<term>skip connection</term>
					<term>image denoising</term>
					<term>super resolution</term>
					<term>image inpainting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results. The skip connections from convolutional layers to their mirrored corresponding deconvolutional layers exhibit two main advantages. First, they allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the clean image. Significantly, with the large capacity, we show it is possible to cope with different levels of corruptions using a single model. Using the same framework, we train models on tasks of image denoising, super resolution removing JPEG compression artifacts, non-blind image deblurring and image inpainting. Our experiment results on benchmark datasets show that our network can achieve best reported performance on all of the four tasks, and set new state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Image restoration <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> is a classical problem in low-level vision, which has been widely studies in the literature. Yet, it remains an active research topic and provides a test bed for many image modeling techniques. Generally speaking, image restoration is the operation of taking a corrupted image and estimating the original image, which is known to be an ill-posed inverse problem. A corrupted image Y can be represented as</p><formula xml:id="formula_0">y = H(x) + n<label>(1)</label></formula><p>where x is the clean version of y; H is the degradation function and n is the additive noise. By accommodating different types of degradation operators and noise distributions, the same mathematical model applies to most lowlevel imaging problems such as image denoising <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, super-resolution <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, inpainting <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and recovering raw images from compressed images <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In the past decades, extensive studies have been carried out to develop various of image restoration methods.</p><p>Recently, deep neural networks (DNNs) have shown their superior performance in image processing and computer vision tasks, ranging from high-level recognition, semantic segmentation to low-level denoising and superresolution. One of the early deep learning models which has been used for image denoising is the Stacked Denoising Auto-encoders (SdA) <ref type="bibr" target="#b23">[24]</ref>. It is an extension of the stacked auto-encoder <ref type="bibr" target="#b24">[25]</ref> and was originally designed for unsupervised feature learning. Denoising auto-encoders can be stacked to form a deep network by feeding output of the previous layer to the current layer as input. Jain and Seung <ref type="bibr" target="#b25">[26]</ref> proposed to use Convolutional Neural Networks (CNN) to denoise natural images. Their framework is the same as the recent Fully Convolutional Neural Networks (FCN) for semantic image segmentation <ref type="bibr" target="#b26">[27]</ref> and other tasks such as super-resolution <ref type="bibr" target="#b27">[28]</ref>, although their network is not as deep as today's models. Their network accepts an image as the input and produces an entire image as the output through four hidden layers of convolutional filters. The weights are learned by minimizing the difference between the output and the clean image.</p><p>By observing recent superior performance of CNN on image processing tasks, here we propose a very deep fully convolutional CNN-based framework for image restoration. The input of our framework is a corrupted image, and the output is its clean version. We observe that it is beneficial to train a very deep model for low-level tasks like denoising, super-resolution and JPEG deblocking. Our network is much deeper compared to that in <ref type="bibr" target="#b25">[26]</ref> and recent lowlevel image processing models such as <ref type="bibr" target="#b27">[28]</ref>. Instead of using image priors, the proposed framework learns fully convolutional and deconvolutional mappings from corrupted images to the clean ones in an end-to-end fashion. The network is composed of multiple layers of convolution and deconvolution operators. As deeper networks tend to be more difficult to train, we further propose to symmetrically link convolutional and deconvolutional layers with multiple skip-layer connections, with which the training converges much faster and better performance is achieved.</p><p>Our main contributions can be summarized as follows. <ref type="bibr">â€¢</ref> We propose a very deep network architecture for image restoration. The network consists of a chain of symmetric convolutional layers and deconvolutional layers. The convolutional layers act as the feature extractor which encode the primary components of image contents while eliminating the corruptions. The deconvolutional layers then decode the image abstraction to recover the image content details. To the best of our knowledge, the proposed framework is the first attempt to used both convolution and deconvolution for low-level image restoration.</p><p>â€¢ To better train the deep network, we propose to add skip connections between corresponding convolutional and deconvolutional layers. These shortcuts divide the network into several blocks. These skip connections help to back-propagate the gradients to bottom layers and pass image details to the top layers. These two characteristics make training of the end-to-end mapping from corrupted image to the clean one easier and more effective, and thus achieve performance improvement while the network going deeper.</p><p>â€¢ We apply the same network for tasks such as image denoising, image super-resolution, JPEG deblocking, non-blind image deblurring and image inpainting. Experiments on a few widely-used benchmark datasets demonstrate the advantages of our network over other recent state-of-the-art methods. Moreover, relying on the large capacity and fitting ability, our network can be trained to obtain good restoration performance on different levels of corruption even using a single model.</p><p>The remaining content is organized as follows. We provide a brief review of related work in Section 2. We present the architecture of the proposed network, as well as training, testing details in Section 3. In Section 4, we discuss some relevant issues. Experimental results and analysis are provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the past decades, extensive studies have been conducted to develop various image restoration methods. See detailed reviews in a survey <ref type="bibr" target="#b28">[29]</ref>. Traditional methods such as the BM3D algorithm <ref type="bibr" target="#b29">[30]</ref> and dictionary learning based methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref> have shown very promising performance on image restoration topics such as image denoising and super-resolution. Due to the ill-posed nature of image restoration, image prior knowledge formulated as regularization techniques are widely used. Classic regularization models, such as total variation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, are effective in removing noise artifacts, but also tend to over-smooth the images. As an alternative, sparse representation <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> based prior modeling is popular, too. Mathematically, the sparse representation model assumes that a data point x can be linearly reconstructed by an overcompleted dictionary, and most of the coefficients are zero or close to zero.</p><p>An active (and probably more promising) category for image restoration is the neural network based methods. The most significant difference between neural network methods and other methods is that they typically learn parameters for image restoration directly from training data (e.g., pairs of clean and corrupted images) rather than relying on pre-defined image priors.</p><p>Stacked denoising auto-encoder <ref type="bibr" target="#b23">[24]</ref> is one of the most well-known deep neural network models which can be used for image denoising. Unsupervised pre-training, which minimizes the reconstruction error with respect to inputs, is done for one layer at a time. Once all layers are pre-trained, the network goes through a fine-tuning stage. Xie et al. <ref type="bibr" target="#b17">[18]</ref> combined sparse coding and deep networks pre-trained with denoising auto-encoder for low-level vision tasks such as image denoising and inpainting. The main idea is that the sparsity-inducing term for regularization is proposed for improved performance. Deep network cascade (DNC) <ref type="bibr" target="#b39">[40]</ref> is a cascade of multiple stacked collaborative local autoencoders for image super-resolution. High frequency texture enhanced image patches are fed into the network to suppress the noises and collaborate the compatibility of the overlapping patches.</p><p>Other neural network based image restoration methods using networks such as multi-layer perceptron. Early works, such as a multi-layer perceptron with a multilevel sigmoidal function <ref type="bibr" target="#b40">[41]</ref>, have been proposed and proved to be effective in image restoration tasks. Burger et al. <ref type="bibr" target="#b41">[42]</ref> presented a patch-based algorithm learned on a large dataset with a plain multi-layer perceptron and is able to compete with the state-of-the-art traditional image denoising methods such as BM3D. They also concluded that with large networks, large training data, neural networks can achieve state-of-the-art image denoising performance, which is confirmed in the work here.</p><p>Compared to auto-encodes and multilayer perceptron, it seems that convolutional neural networks have achieved even more significant success in the field of image restoration. Jain and Seung <ref type="bibr" target="#b25">[26]</ref> proposed fully convolutional CNN for denoising. The network is trained by minimizing the loss between a clean image and its corrupted version by adding noises on it. They found that CNN works well on both blind and non-blind image denoising, providing comparable or even superior performance to wavelet and Markov Random Field (MRF) methods. Recently, Dong et al. <ref type="bibr" target="#b27">[28]</ref> proposed to directly learn an end-to-end mapping between the low/high-resolution images for image superresolution. They observed that convolutional neural networks are enseentially related to sparse coding based methods, i.e., the three layers in their network can be viewed as patch representation extractor, non-linear mapping and image reconstructor. They also proposed variant networks for other image restoration tasks such as JPEG debloking <ref type="bibr" target="#b20">[21]</ref>. Wang et al. <ref type="bibr" target="#b16">[17]</ref> argued that domain expertise represented by the conventional sparse coding is still valuable and can be combined to achieve further improved results in image super-resolution. Instead of training with different levels of scaling factors, they proposed to use a cascade structure to repeatedly enlarge the low-resolution image by a fixed scale until reaching a desired size. In general, DNN-based methods learn restoration parameters directly from data, which tends to been more effective in real-world image restoration applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VERY DEEP CONVOLUTIONAL AUTO-ENCODER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FOR IMAGE RESTORATION</head><p>The proposed framework mainly contains a chain of convolutional layers and symmetric deconvolutional layers, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Skip connections are connected symmetrically from convolutional layers to deconvolutional layers. We term our method "RED-Net"-very deep Residual Encoder-Decoder Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The framework is fully convolutional (and deconvolutional. Deconvolution is essentially unsampling convolution). Rectification layers are added after each convolution and deconvolution. For low-level image restoration problems, we use neither pooling nor unpooling in the network as usually pooling discards useful image details that are essential for these tasks. It is worth mentioning that since the convolutional and deconvolutional layers are symmetric, the network is essentially pixel-wise prediction, thus the size of input image can be arbitrary. The input and output of the network are images of the same size w Ã— h Ã— c, where w, h and c are width, height and number of channels.</p><p>Our main idea is that the convolutional layers act as a feature extractor, which preserve the primary components of objects in the image and meanwhile eliminating the corruptions. After forwarding through the convolutional layers, the corrupted input image is converted into a "clean" one. The subtle details of the image contents may be lost during this process. The deconvolutional layers are then combined to recover the details of image contents. The output of the deconvolutional layers is the recovered clean version of the input image. Moreover, we add skip connections from a convolutional layer to its corresponding mirrored deconvolutional layer. The passed convolutional feature maps are summed to the deconvolutional feature maps element-wise, and passed to the next layer after rectification. Deriving from the above architecture, we have used two networksvin our experiments, which are of 20 layers and 30 layers respectively, for image denoising, image super-resolution, JPEG deblocking and image inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deconvolution decoder</head><p>Architectures combining layers of convolution and deconvolution <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> have been proposed for semantic segmentation recently. In contrast to convolutional layers, in which multiple input activations within a filter window are fused to output a single activation, deconvolutional layers associate a single input activation with multiple outputs. Deconvolution is usually used as learnable up-sampling layers.</p><p>In our network, the convolutional layers successively down-sample the input image content into a small size abstraction. Deconvolutional layers then up-sample the abstraction back into its original resolution.</p><p>Besides the use of skip connections, a main difference between our model and <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> is that our network is fully convolutional and deconvolutional, i.e., without pooling and un-pooling. The reason is that for low-level image restoration, the aim is to eliminate low level corruption while preserving image details instead of learning image abstractions. Different from high-level applications such as segmentation or recognition, pooling typically eliminates the abundant image details and can deteriorate restoration performance.</p><p>One can simply replace deconvolution with convolution, which results in an architecture that is very similar to recently proposed very deep fully convolutional neural networks <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. However, there exist essential differences between a fully convolution model and our model. Take image denoising as an example. We compare the 5layer and 10-layer fully convolutional network with our network (combining convolution and deconvolution, but without skip connection). For fully convolutional networks, we use padding or up-sampling the input to make the input and output be of the same size. For our network, the first 5 layers are convolutional and the second 5 layers are deconvolutional. All the other parameters for training are identical, i.e., trained with SGD and learning rate of 10 âˆ’6 , noise level Ïƒ = 70. The Peak Signal-to-Noise Ratio (PSNR) on the validation set is reported, which shows that using deconvolution works better than the fully convolutional counterpart, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Furthermore, in <ref type="figure" target="#fig_4">Figure 3</ref>, we visualize some results that are outputs of layer 2, 5, 8 and 10 from the 10-layer fully convolutional network and ours. In the fully convolution case, the noise is eliminated step by step, i.e., the noise level is reduced after each layer. During this process, the details of the image content may be lost. Nevertheless, in our network, convolution preserves the primary image content. Then deconvolution is used to compensate the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Skip connections</head><p>An intuitive question is that, is a network with deconvolution able to recover image details from the image abstraction only? We find that in shallow networks with only a few layers of convolution layers, deconvolution is able to recover the details. However, when the network goes deeper or using operations such as max pooling, even with  deconvolution layers, it does not work that well, possibly because too much details are already lost in the convolution and pooling. The second question is that, when our network goes deeper, does it achieve performance gain? We observe that deeper networks in image restoration tasks tend to easily suffer from performance degradation. The reason may be two folds. First of all, with more layers of convolution, a significant amount of image details could be lost or corrupted. Given only the image abstraction, recovering its details is an under-determined problem. Secondly, in terms of optimization, deep networks often suffer from gradients vanishing and become much harder to train-a problem that is well addressed in the literature of neural networks.</p><p>To address the above two problems, inspired by highway networks <ref type="bibr" target="#b44">[45]</ref> and deep residual networks <ref type="bibr" target="#b0">[1]</ref>, we add skip connections between two corresponding convolutional and deconvolutional layers as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. A building block is shown in <ref type="figure" target="#fig_5">Figure 4</ref>. There are two reasons for using such connections. First, when the network goes deeper, as mentioned above, image details can be lost, making deconvolution weaker in recovering them. However, the feature maps passed by skip connections carry much image detail, which helps deconvolution to recover an improved clean version of the image. Second, the skip connections also achieve benefits on back-propagating the gradient to  bottom layers, which makes training deeper network much easier as observed in <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b0">[1]</ref>.</p><p>Note that our skip layer connections are very different from the ones proposed in <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b0">[1]</ref>, where the only concern is on the optimization side. In our case, we want to pass information of the convolutional feature maps to the corresponding deconvolutional layers. The very deep highway networks <ref type="bibr" target="#b44">[45]</ref> are essentially feedforward long short-term memory (LSTMs) with forget gates, and the CNN layers of deep residual network <ref type="bibr" target="#b0">[1]</ref> are feedforward LSTMs without gates. Note that our networks are in general not in the format of standard feedforward LSTMs.</p><p>Instead of directly learning the mappings from the input X to the output Y , we would like the network to fit the residual <ref type="bibr" target="#b0">[1]</ref> of the problem, which is denoted as F(X) = Y âˆ’ X. Such a learning strategy is applied to inner blocks of the encoding-decoding network to make training more effective. Skip connections are passed every two convolutional layers to their mirrored deconvolutional layers. Other configurations are possible and our experiments show that this configuration already works very well. Using such shortcuts makes the network easier to be trained and gains restoration performance by increasing the network depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>In general, there are three types of layers in our network: convolution, deconvolution and element-wise sum. Each layer is followed by a Rectified Linear Unit (ReLU) <ref type="bibr" target="#b45">[46]</ref>. Let X be the input, the convolutional and deconvolutional layers are expressed as:</p><formula xml:id="formula_1">F (X) = max(0, W k * X + B k ),<label>(2)</label></formula><p>where W k and B k represent the filters and biases, and * denotes either convolution or deconvolution operation for the convenience of formulation. For element-wise sum layer, the output is the element-wise sum of two inputs of the same size, followed by the ReLU activation:</p><formula xml:id="formula_2">F (X 1 , X 2 ) = max(0, X 1 + X 2 )<label>(3)</label></formula><p>Learning the end-to-end mapping from corrupted images to clean images needs to estimate the weights Î˜ represented by the convolutional and deconvolutional kernels. Specifically, given a collection of N training sample pairs {X i , Y i }, where X i is a noisy image and Y i is the clean version as the groundtruth. We minimize the following Mean Squared Error (MSE):</p><formula xml:id="formula_3">L(Î˜) = 1 N N i=1 F(X i ; Î˜) âˆ’ Y i 2 F .<label>(4)</label></formula><p>Traditionally, a network can learn the mapping from the corrupted image to the clean version directly. However, our network learns for the additive corruption from the input since there is a skip connection between the input and the output of the network. We found that optimizing for the corruption converges better than optimizing for the clean image. In the extreme case, if the input is a clean image, it would be easier to push the network to be zero mapping (learning the corruption) than to fit an identity mapping (learning the clean image) with a stack of nonlinear layers.</p><p>We implement and train our network using Caffe <ref type="bibr" target="#b46">[47]</ref>. Empirically, we find that using Adam <ref type="bibr" target="#b47">[48]</ref> with base learning rate of 10 âˆ’4 for training converges faster than traditional stochastic gradient descent (SGD). The base learning rate for all layers are the same, different from <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b25">[26]</ref>, in which a smaller learning rate is set for the last layer. This is not necessary in our network. Specifically, gradients with respect to the parameters of ith layer is firstly computed as:</p><formula xml:id="formula_4">g = âˆ‡ Î¸i L(Î¸ i ).<label>(5)</label></formula><p>Then, the two momentum vectors are computed as:</p><formula xml:id="formula_5">m = Î² 1 m + (1 âˆ’ Î² 1 )g, v = Î² 2 v + (1 âˆ’ Î² 2 )g 2 . (6)</formula><p>The update rule is:</p><formula xml:id="formula_6">Î± = Î± 1 âˆ’ Î² t 2 /(1 âˆ’ Î² t 1 ), Î¸ i = Î¸ i âˆ’ Î±m/( âˆš v + ). (7)</formula><p>Î² 1 , Î² 2 and are set as the recommended values in <ref type="bibr" target="#b47">[48]</ref>. 300 images from the Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b48">[49]</ref> are used to generate image patches as the training set for each image restoration task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Testing</head><p>Although trained on local patches, our network can perform restoration on images of arbitrary sizes. Given a testing image, one can simply go forward through the network, which is already able to outperform existing methods. To achieve even better results, we propose to process a corrupted image on multiple orientations. Different from segmentation, the filter kernels in our network only eliminate the corruptions, which is usually not sensitive to the orientation of image contents in low level restoration tasks. Therefore, we can rotate and mirror flip the kernels and perform forward multiple times, and then average the output to achieve an ensemble of multiple tests. We see that this can lead to slightly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis on the architecture</head><p>Assume that we have a network with L layers, and skip connections are passed every layer in the first half of the network. For the convenience of presentation, we denote F c and F d the convolution and deconvolution operation in each layer and do not use ReLU. According to the architecture described in the last section, we can obtain the output of the i-th layer as follows:</p><formula xml:id="formula_7">X i = X Lâˆ’i + F d (X iâˆ’1 ), i â‰¥ L/2; F c (X iâˆ’1 ). i &lt; L/2.<label>(8)</label></formula><p>It is easy to observe that our skip connections indicate identity mapping. The output of the network is:</p><formula xml:id="formula_8">X L = X 0 + F d (X Lâˆ’1 ).<label>(9)</label></formula><p>Recursively, we can compute X L more specifically as follows according to Equation <ref type="formula" target="#formula_7">(8)</ref>:</p><formula xml:id="formula_9">X L = X 0 + F d (X Lâˆ’1 ) = X 0 + F d (X 1 + F d (X Lâˆ’2 )) = X 0 + F d (X 1 ) + F 2 d (X 2 + F d (X Lâˆ’3 )) ...... = X 0 + F d (X 1 ) + F 2 d (X 2 ) + ... + F L/2âˆ’1 d (X L/2âˆ’1 ) + F L/2 d (X L/2 ). (10) Since F L/2 d (X L/2 ) can be expressed as F L/2 d (F L/2 c (X 0 ))</formula><p>, we convert Equation <ref type="formula" target="#formula_0">(10)</ref> as:</p><formula xml:id="formula_10">X L = F L/2 d (F L/2 c (X 0 )) + L/2âˆ’1 i=0 F i d (X i ).<label>(11)</label></formula><p>In Equation <ref type="formula" target="#formula_0">(11)</ref>, the term F</p><formula xml:id="formula_11">L/2 d (F L/2 c (X 0 ))</formula><p>is actually the output of the given network without skip connections. The difference here is that by adopting the skip connection, we decode each feature maps X i , 0 â‰¤ i &lt; L/2 in the first half network and integrate them to the output. The most significant benefit is that they carry important image details, which helps to reconstruct clean image. Moreover, the term</p><formula xml:id="formula_12">L/2âˆ’1 i=0 F i d (X i )</formula><p>indicates that these details are represented at different levels. It is intuitive to see the following fact. It may not be easy to tell what information is needed for reconstructing clean images using only one feature maps encoding the image abstraction; but much easier if there are multiple feature maps encoding different levels of image abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gradient back-propagation</head><p>For back-propagation, a layer receives gradients from the layers that it is connected to. As an example shown in <ref type="figure" target="#fig_5">Figure  4</ref>, X is the input of the first layer, after two convolutional layers c1 and c2, the output is X 1 . To update the parameters represented as Î¸ 2 of c2, we compute the derivative of L with respect to Î¸ 2 as follows:</p><formula xml:id="formula_13">âˆ‡ Î¸2 L(Î¸ 2 ) = âˆ‚L âˆ‚X 1 âˆ‚X 1 âˆ‚Î¸ 2 + âˆ‚L âˆ‚X 2 âˆ‚X 2 âˆ‚Î¸ 2<label>(12)</label></formula><p>where using X 1 and X 2 is only for the clarity of presentation, they are essentially the same. We can further formulate <ref type="bibr" target="#b11">(12)</ref> as:</p><formula xml:id="formula_14">âˆ‡ Î¸2 L(Î¸ 2 ) = âˆ‚L âˆ‚X 4 âˆ‚X 4 âˆ‚X 3 âˆ‚X 3 âˆ‚X 1 âˆ‚X 1 âˆ‚Î¸ 2 + âˆ‚L âˆ‚X 4 âˆ‚X 4 âˆ‚X 2 âˆ‚X 2 âˆ‚Î¸ 2 .<label>(13)</label></formula><p>Only âˆ‚L âˆ‚X4 âˆ‚X4 âˆ‚X3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âˆ‚X3 âˆ‚X1</head><p>âˆ‚X1 âˆ‚Î¸2 is computed if we do not use skip connections, and its magnitide may become very small after back-propagating through many layers from the top in very deep networks. However, âˆ‚L âˆ‚X4 âˆ‚X4 âˆ‚X2 âˆ‚X2 âˆ‚Î¸2 carries larger gradients since it does not have to go through layers of d2, d1, c4 and c3 in this example. Thus with the first term only, it is more unlikely to approach zero grdients. As we can see, the skip connection helps to update the filters in bottoms layers, and thus makes training easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training with symmetric skip connections</head><p>The aim of restoration is to eliminate corruption while preserving the image details as mush as possible. Previous works typically use shallow networks for low-level image restoration tasks. The reason may be that deeper networks can destroy the image details, which is undesired for pixel-wise dense regression. Even worse, using very deep networks may easily suffer from training issues such as gradient vanishing. Using skip connections in a very deep network can address both of the above two problems.</p><p>Firstly, we design experiments to show that using skip connections is beneficial for image detail presering. Specifically, two networks are trained for image denoising with a noise level of Ïƒ = 70.</p><p>(a) In the first network, we use 5 layers of 3 Ã— 3 convolution with stride 3. The input size of training data is 243Ã—243, which results in a vector after 5 layers of convolution, encoding the very high level abstraction of the image. Then deconvolution is used to recover the input from the feature vector. The results are shown in <ref type="figure" target="#fig_6">Figure 5</ref>. We can observe that it is challenging for deconvolution to recover details from only a vector encoding the abstraction of the input. This phenomenon implies that simply using deep networks for image restoration may not lead to satisfactory results.</p><p>(b) The second network uses the same settings as the first one, but adding skip connections. The results are show in <ref type="figure" target="#fig_6">Figure 5</ref>. Compared to the first network, the one with skip connections can recover the input and achieves much better PSNR values. This is easy to understand since the feature maps with abundant details at bottom layers are directly passed to the top layers. Secondly, we train and compare five different networks to show that using skip connections help to back-propagate gradient in training to better fit the end-to-end mapping, as shown in <ref type="figure">Figure 6</ref>. The five networks are: 10, 20 and 30 layer networks without skip connections; and 20, 30 layer networks with skip connections. As can be seen, the training loss increases when the network going deeper without shortcuts (similar phenomenon is also observed in <ref type="bibr" target="#b0">[1]</ref>). On the validation set, deeper networks without shortcuts achieve lower PSNR and we even observe overfitting for the 30-layer network. These results may be due to the gradient vanishing problem. However, we obtain smaller training errors on the training set and higher PSNR and better generalization capability on the testing set when using skip connections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the deep residual network [1]</head><p>One may use different types of skip connections in our network. A straightforward alternate is that in <ref type="bibr" target="#b0">[1]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, skip connections are added to divide the network into sequential blocks. A benefit of our model is that our skip connections have element-wise correspondence, which can be very important in pixel-wise prediction problems such image denoising. We carry out experiments to compare these two types of skip connections. Here the block size indicates the span of the connections. The results are shown in <ref type="figure" target="#fig_8">Figure 7</ref>. We can observe that our connections often converge to a better optimum, demonstrating that elementwise correspondence can be important. Meanwhile, our long range skip connections pass the image detail directly from bottom layers to top layers. If we use the skip connection type in <ref type="bibr" target="#b0">[1]</ref>, the network may still lose some image details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Testing efficiency</head><p>To apply deep learning models on devices with limited computing power such as mobile phones, one has to speedup the testing phase. For our network, we propose to use down-sampling in convolutional layers to reduce the size of the feature maps. In order to obtain an output of the same size as the input, deconvolution is used to up-sample the feature maps in the symmetric deconvolutional layers. Thus, the testing efficiency can be well improved with almost negligible performance degradation. In specific, we use stride = 2 in convolutional layers to down-sample the feature maps. Down-sampling at different convolutional layers are tested on image denoising, as shown in <ref type="figure">Figure 8</ref>. We test an image of size 160Ã—240 on an i7-2600 CPU, the testing time for "no down-sample", "down-sample at conv1", "down-sample at conv5", "downsample at conv9", "down-sample at conv5,9" are 3.17s, 0.84s, 1.43s, 2.00s and 1.17s respectively.</p><p>The main observation is that the testing PSNRs may slightly degrade according to the scale reduction of the feature map in the entire network. The down-sampling in the first convolutional layer reduces the size of the feature maps to 1/4, which leads to alomst 4x faster in testing, but the PSNR only degrades less than 0.1 compared to the network  <ref type="figure">Fig. 8</ref>. The PSNRs on the validation set with different down-sampling strategies. "down-sample at conv-i" denotes that down-sampling is used in the ith convolutional layer, and up-sampling is used in its symmetric deconvolutional layer.</p><p>without down-sampling. The down-sampling in "conv9" reduces 1/3 of the testing time, but the performance is almost as well as that without down-sampling. As a result, an "earlier" down-sampling may lead to slightly worse performance, but it achieves much faster testing efficiency. It should be a trade-off in different application situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first provide some experimental results and analysis on different parameters, including filter number, filter size, training patch size and skip connection step size, of the network. Then, evaluation of image restoration tasks including image denoising, image super-resolution, JPEG image deblocking, non-blind image debluring and image inpainting are conducted and compared against a few existing stateof-the-art methods in each topic. Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) index are calculated for evaluation. For our method, which is denoted as RED-Net, we implement three versions: RED10 contains 5 convolutional and deconvolutional layers without shortcuts, RED20 contains 10 convolutional and deconvolutional layers with shortcuts of step size 2, and RED30 contains 15 convolutional and deconvolutional layers with shortcuts of step size 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network parameters</head><p>Although we have observed that deeper networks tend to achieve better image restoration performance, there exist more problems related to different parameters to be investigated. We carried out image denoising experiments on three folds: (a) filter number, (b) filter size, (c) training patch size and (d) step size of skip connections, to show the effects of different parameters.</p><p>For different filter numbers, we fix the filter size as 3 Ã— 3, training patch size as 50Ã—50 and skip connection step size as 2. Different filter numbers of 32, 64 and 128 are tested, and the PSNR values recorded on the validation set during training are shown in <ref type="figure" target="#fig_9">Figure 9</ref>. To converge, the training iterations for different number of filters are similar, but better optimum can be obtained with more filters. However, a smaller number of filters is preferred if a fast testing speed is desired.  For the experiments on filter size, we set the filter number to be 64, training patch size as 50Ã—50, skip connection step size as 2.</p><p>Filter size of 3Ã—3, 5Ã—5, 7Ã—7, 9Ã—9 are tested. <ref type="figure" target="#fig_0">Figure 10</ref> show the PSNR values on the validation set while training. It is clear that larger filter size leads to better performance. Different from high-level tasks <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> which favor smaller filter sizes, larger filter size tends to obtain better performance in low-level image restoration applications.</p><p>However, there may exist a bottle neck as the performance of 9Ã—9 is almost as the same as 7Ã—7 in our experiments. The reason may be that for high-level tasks, the networks have to learn image abstraction for classification, which is usually very different from the input pixels. Larger filter size may result in larger respective fields, but also made the networks more difficult to train and converge to a poor optimum. Using smaller filter size is mainly beneficial for convergence in such complex mappings.</p><p>In contrast, for low-level image restoration, the training is not as difficult as that in high-level applications since only a bias is needed to be learned to revise the corrupted pixel. In this situation, utilizing neighborhood information in the mapping stage is more important, since the desired value for a pixel should be predicted from its neighbor pixels. However, using larger filter size inevitably increases the complexity (e.g., filter size of 9Ã—9 is 9 times more complex as 3Ã—3) and training time.</p><p>For the training patch size, we set the filter number to be 64, filter size as 3Ã—3, skip connection step size as 2. Then we test different training patch sizes of 25Ã—25, 50Ã—50, 75Ã—75, 100Ã—100, as shown in <ref type="figure" target="#fig_0">Figure 11</ref>.</p><p>Better performance is achieved with larger training patch size. The reason can be two folds. First of all, since the network essentially performs pixel-wise prediction, if the number of training patches are the same, larger size of training patch results in more pixels to be used, which is equivalent to using more training data. Secondly, the corruptions in image restoration tasks can be described as some types of latent distributions. Larger size of training patch contains more pixels that better capture the latent distributions to be learned, which consequently helps the network to fit the corruptions better. As we can see, the "width" of the network is as crucial as the "depth" in training a network with satisfactory image restoration performance. However, one should always make a trade-off between the performance and speed.</p><p>We also provide the experiments of different step sizes of shortcuts, as shown in <ref type="figure" target="#fig_0">Figure 12</ref>. A smaller step size of shortcuts achieves better performance than a larger one. We believe that a smaller step size of shortcuts makes it easier to back-propagate the gradient to bottom layers, thus tackle the gradient vanishing issue better. Meanwhile, a small step size of shortcuts essentially passes more direct information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image denoising</head><p>Image denoising experiments are performed on two datasets: 14 common benchmark images <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, as show in <ref type="figure" target="#fig_0">Figure 13</ref> and the BSD dataset.</p><p>As a common experimental setting in the literature, additive Gaussian noises with zero mean and standard deviation Ïƒ are added to the image to test the performance of denoising methods. In this paper we test noise level Ïƒ   of 10, 30, 50 and 70. BM3D <ref type="bibr" target="#b29">[30]</ref>, NCSR <ref type="bibr" target="#b2">[3]</ref>, EPLL <ref type="bibr" target="#b5">[6]</ref>, PCLR <ref type="bibr" target="#b7">[8]</ref>, PGPD <ref type="bibr" target="#b8">[9]</ref> and WMMN <ref type="bibr" target="#b9">[10]</ref> are compared with our method. For these methods, we use the source code released by their authors and test on the images with their default parameters. Evaluation on the 14 images <ref type="table" target="#tab_3">Table 1</ref> presents the PSNR and SSIM results of Ïƒ 10, 30, 50, and 70. We can make some observations from the results. First of all, the 10 layer convolutional and deconvolutional network has already achieved better results than the state-of-the-art methods, which demonstrates that combining convolution and deconvolution for denoising works well, even without any skip connections.</p><p>Moreover, when the network goes deeper, the skip connections proposed in this paper help to achieve even better denoising performance, which exceeds the existing best method WNNM <ref type="bibr" target="#b9">[10]</ref> by 0.32dB, 0.43dB, 0.49dB and 0.51dB on noise levels of Ïƒ being 10, 30, 50 and 70 respectively. While WNNM is only slightly better than the second best existing method PCLR <ref type="bibr" target="#b7">[8]</ref> by 0.01dB, 0.06dB, 0.03dB and 0.01dB respectively, which shows the large improvement of our model.</p><p>Last, we can observe that the more complex the noise is, the more improvement our model achieves than other methods. Similar observations can be made on the evaluation of SSIM.</p><p>Evaluation on BSD200 For the BSD dataset, 300 images are used for training and the remaining 200 images are used for denoising to show more experimental results. For efficiency, we convert the images to gray-scale and resize them to smaller images. Then all the methods are run on the dataset to get average PSNR and SSIM results of Ïƒ 10, 30, 50, and 70, as shown in <ref type="table">Table 2</ref>. For existing methods, their denoising performance does not differ much, while our model achieves 0.38dB, 0.47dB, 0.49dB and 0.42dB higher of PSNR over WNNM <ref type="bibr" target="#b9">[10]</ref>.</p><p>Blind denoising We also perform blind denoising to show the superior performance of our network. In blind denoising, the training set consists of image patches of different levels of noises, and a 30-layer network is trained on this training set. In the testing phase, we test noisy images with Ïƒ of 10, 30, 50 and 70 using this model. The evaluation results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Although training with different levels of corruption, we can observe that the performance of our network degrades comparing to the case in which using separate models for denoising. This is reasonable because the network has to fit much more complex mappings. However, it still beats the existing methods. For PSNR evaluation, our blind denoising model achieves the same performance as WNNM [10] on Ïƒ = 10, and outperforms WNNM [10] by 0.35dB, 0.43dB and 0.40dB on Ïƒ = 30, 50 and 70 respectively, which is still marginal improvements. For SSIM evaluation, our network is 0.0005, 0.0141, 0.0199 and 0.0182 higher than WNNM. The performance improvement is more obvious on BSD dataset. The 30-layer network outperforms the second best method WNNM <ref type="bibr" target="#b9">[10]</ref> by 0.13dB, 0.4dB, 0.43dB, 0.41dB for PSNR and 0.0036, 0.0173, 0.0191, 0.0198 for SSIM. Visual results Some visual results are shown in <ref type="figure" target="#fig_0">Figure  14</ref>. We highlight some details of the clean image and the recovered ones by different methods. The first observation is that our method better recovers the image details, as we can see from the third and fourth rows, which is due to the high PSNR we achieve by minimizing the pixel-wise Euclidean loss.</p><p>Moreover, we can observe from the first and second rows that our network obtains more visually smooth results than other methods. This may due to the testing strategy which average the output of different orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Image super-resolution</head><p>For super-resolution, The high-resolution image is first down-sampled with scaling factor parameters of 2, 3 and 4 respectively. Since the size of the input and output of our network are the same, we up-sample the low-resolution image to its original size as the input of our network. We compare our network with SRCNN <ref type="bibr" target="#b27">[28]</ref>, NBSRF <ref type="bibr" target="#b52">[53]</ref>, CSCN <ref type="bibr" target="#b16">[17]</ref>, CSC <ref type="bibr" target="#b15">[16]</ref>, TSE <ref type="bibr" target="#b53">[54]</ref> and ARFL+ <ref type="bibr" target="#b54">[55]</ref> on three dataset: Set5, Set14 and BSD100. The results of the compared methods are either cited from their original papers or obtained using the released source code by the authors.</p><p>Evaluation on Set 5 The evaluation on Set5 is shown in <ref type="table" target="#tab_5">Table 4</ref>. In general, our 10-layer network already outperforms the compared methods, and we achieve better performance with deeper networks.</p><p>The second best method is CSCN, which is also a recently proposed neural network based method. Compared to CSCN, our 30-layer network exceeds it by 0.52dB, 0.56dB, 0.47dB on PSNR and 0.0032, 0.0063, 0.0094 on SSIM respectively.</p><p>The larger scaling parameter is, the better improvement our method can make, which demonstrates that our network is better at fitting complex corruptions than other methods.</p><p>Evaluation on Set 14 The evaluation on Set14 is shown in <ref type="table" target="#tab_6">Table 5</ref>. The improvement on Set14 in not as significant as that on Set5, but we can still observe that the 30-layer network achieves higher PSNR and SSIM than the second best CSCN for 0.23dB, 0.06dB, 0.1dB and 0.0049, 0.0070, 0.0098. The performance on 10-layer, 20-layer and 30-layer RED-Net also does not improve that much as on Set5, which may imply that Set14 is more difficult to perform image super-resolution.</p><p>Evaluation on BSD 100 We also evaluate superresolution results on BSD100, as shown in <ref type="table" target="#tab_7">Table 6</ref>. The overall results are very similar than those on Set5. CSCN   is still the second best method and outperforms other compared methods by large margin, but its performance is not as good as our 10-layer network. Our deeper networks obtain performance gains. Compared to CSCN, the 30-layer network achieves higher PSNR for 0.45dB, 0.38dB, 0.29dB and higher SSIM for 0.0066, 0.0084, 0.0099.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with VDSR [56] and DRCN [57]</head><p>Concurrent to our work <ref type="bibr" target="#b57">[58]</ref>, networks <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> which incorporate residual learning for image super-resolution are proposed. In <ref type="bibr" target="#b55">[56]</ref>, a fully convolutional network termed VDSR is proposed to learn the residual image for image super-resolution. The loss layer takes three inputs: residual estimate, low-resolution input and ground truth highresolution image, and Euclidean loss is computed between the reconstructed image (the sum of network input and output) and ground truth. DRCN <ref type="bibr" target="#b56">[57]</ref> proposed to use a recursive convolutional block, which does not increase the Firstly, both VDSR and DRCN use one path of connections between the input and output, which actually models the corruptions. In VDSR, the network itself is standard fully convolutional. DRCN uses recursive convolutional layers that lead to multiple losses, which is different from VDSR. The skip connections in VDSR and DRCN model the superresolution problem as learning the residual image, which actually learns the corruption as in image restoration. In other words, the residual learning is only conducted in the inputoutput level (low-resolution and high-resolution images) in VDSR and DRCN. In contrast, our network uses multiple skip connections that divide the network into multiple blocks for residual learning. Secondly, our skip connections pass image abstraction of different levels from multiple convolutional layers forwardly. No such information is used in VDSR and DRCN. In VDSR and DRCN, the skip connection only pass the input image. However, in our network, different levels of image abstraction are obtained after the convolutional layers, and they are passed to the deconvolutional layers for reconstruction. At last, our skip connections help to backpropagate gradients in different layers. In VDSR and DCRN, the skip connections do not involve in back-propagating gradients since they connect the input and output, and there are no weights to be updated for the input low-resolution image. The image super-resolution comparisons of VDSR, DRCN and our network on Set5, Set14 and BSD100 are provided in <ref type="table">Table 7</ref>.</p><p>Blind super-resolution The results of blind superresolution are shown in <ref type="table" target="#tab_8">Table 8</ref>. Among the compared methods, CSCN can also deal with different scaling parameters by repeatedly enlarging the image by a smaller scaling factor.</p><p>Our method is different from CSCN. Given a lowresolution image as input and the output size, we first upsample the input image to the desired size, resulting in an image with poor details. Then the image is fed into our network. The output is an image of the same size with fine details. The training set consists of image patches of different scaling parameters and a single model is trained. Except that CSCN works slightly better on Set 14 with scaling factors 3 and 4, our network outperforms the existing methods, showing that our network works much better in image super-resolution even using only one single model to deal with complex corruptions. Visual results Some visual results in grey-scale images are shown in <ref type="figure" target="#fig_0">Figure 15</ref>. Note that it is straightforward to perform super-resolution on color images.</p><p>We can observe from the second and third rows that our network is better at obtaining high resolution edges and text. Meanwhile, our results seem much more smooth than others. For faces such as the fourth row, out network still obtains better visually results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">JPEG deblocking</head><p>Lossy compression, such as JPEG, introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. In this section, we carry out deblocking experiments to recover high quality images from their JPEG compression. As in other compression artifacts reduction methods, standard JPEG compression schemes of JPEG quality settings q = 10 and q = 20 in MATLAB JPEG encoder are used. The LIVE1 dataset is used for evaluation, and we have compared our method with AR-CNN <ref type="bibr" target="#b20">[21]</ref>, SA-DCT <ref type="bibr" target="#b21">[22]</ref> and deeper SRCNN <ref type="bibr" target="#b20">[21]</ref>.</p><p>The results are shown in <ref type="table" target="#tab_10">Table 9</ref>. We can observe that since the Euclidean loss favors a high PSNR, our network outperforms other methods. Compared to AR-CNN, the 30layer network exceeds it by 0.37dB and 0.44dB on compression quality of 10 and 20. Meanwhile, we can see that <ref type="bibr">TABLE 7</ref> Comparisons between RED30 (ours), VDSR <ref type="bibr" target="#b55">[56]</ref> and DRCN <ref type="bibr" target="#b56">[57]</ref>: Average PSNR and SSIM results of scaling 2, 3 and 4 on Set5, Set14 and BSD100. compared to shallow networks, using significantly deeper networks does improve the deblocking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Non-blind deblurring</head><p>We mainly follow the experimental protocols as in <ref type="bibr" target="#b58">[59]</ref> for evaluation of non-blind deblurring. The performance on deblurring "disk", "motion" and "gaussian" kernels are compared, as shown in <ref type="table" target="#tab_3">Table 10</ref>. We generate blurred image patches with the corresponding kernels, and train end-to-end mapping with pairs of blurred and non-blurred image patches. As we can see from the results, our network outperforms those compared methods with significant improvements. <ref type="figure" target="#fig_0">Figure 16</ref> shows some visual comparisons. We can observe from the visual examples that our network works better than the compared methods on recovering the image details, as well as achieving visually more appealing results on low frequency image contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Image inpainting</head><p>In this section, we conduct text removal for experiments of image inpainting. Text is added to the original image from the LIVE1 dataset with font size of 10 and 20. We have compared our method with FoE <ref type="bibr" target="#b18">[19]</ref>. For our model, we extract image patches with text on them and learn a mapping from them to the original patches. For FoE, we provide both images with text and masks indicating which pixel is corrupted. The average PSNR and SSIM for font size 10 and 20 on LIVE are: 38.24dB, 0.9869 and 34.99dB, 0.9828 using 30-layer RED-Net, and they are much better than those of FoE, which are 34.59dB, 0.9762 and 31.10dB, 0.9510. For scratch removal, we randomly draw scratch on the clean image and test with our network and FoE. The PSNR and SSIM for our network are 39.41dB and 0.9923, which is much better than 32.92dB and 0.9686 of FoE. <ref type="figure" target="#fig_0">Figure 17</ref> shows some visual comparisons of our method between FoE. We can observe from the examples that our network is better at recovering text, logos, faces and edges in the natural images. Looking on the first example, one may wonder why the text in the original image is not eliminated. For traditional methods such as FoE, this problem is addressed by providing a mask, which indicates the location of corrupted pixels. While our network is trained on specific distributions of corruptions, i.e., the text of font sizes 10 and 20 that are added. It is equivalent to distinguishing corrupted and non-corrupted pixels of different distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper we have proposed a deep encoding and decoding framework for image restoration. Convolution and deconvolution are combined, modeling the restoration problem by extracting primary image content and recovering details.</p><p>More importantly,we propose to use skip connections, which helps on recovering clean images and tackles the optimization difficulty caused by gradient vanishing, and thus obtains performance gains when the network goes deeper. Experimental results and our analysis show that our network achieves better performance than state-of-the-art methods on image denoising, image super-resolution, JPEG deblocking and image inpainting. <ref type="figure" target="#fig_0">Fig. 17</ref>. Visual results of our method and FoE. Images from left to right are: Corrupted images, the inpainting results of FoE and the inpainting results of our method. We see better recovered details as shown in the zoomed patches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall architecture of our proposed network. The network contains layers of symmetric convolution (encoder) and deconvolution (decoder). Skip shortcuts are connected every a few (in our experiments, two) layers from convolutional feature maps to their mirrored deconvolutional feature maps. The response from a convolutional layer is directly propagated to the corresponding mirrored deconvolutional layer, both forwardly and backwardly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>PSNR values on the validation set during training. Our model exhibits better PSNR than the compared ones upon convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Visualization of the 10-layer fully convolutional network. The images from top-left to bottom-right are: clean image, noisy image, output of conv-2, output of conv-5, output of conv-8 and output of conv-10, where "conv-i" stands for the i-th convolutional layer; (b) Visualization of the 10-layer convolutional and deconvolutional network. The images from top-left to bottom-right are: clean image, noisy image, output of conv-2, output of conv-5, output of deconv-3 and output of deconv-5, where "deconv-i" stands for the i-th deconvolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>An example of a building block in the proposed framework. The rectangle in solid and dotted lines denote convolution and deconvolution respectively. âŠ• denotes element-wise sum of feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Recovering image details using deconvolution and skip connections. Skip connections are beneficial in recovering image details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 . 2 -</head><label>62</label><figDesc>The training loss on the training set during training. He et al. Block-4-RED, Ours Block-4-He et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Comparisons of skip connections in [1] and our model, where "Block-i-RED" is the connections in our model with block size i and "Block-i-He et al." is the connections in He et al. [1] with block size i; The PSNR values on the validation set during training: the PSNR at the last iteration for the curves are: 25.08, 24.59, 25.30 and 25.21.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>The PSNR values on the validation set during training with different number of filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>The PSNR values on the validation set during training with different size of filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 . 7 Fig. 12 .</head><label>11712</label><figDesc>The PSNR values on the validation set during training with different size of training patch. The PSNR values on the validation set during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>The 14 testing images for denoising.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Visual results of image denoising. Images from left to right column are: clean image; the recovered image of RED30, BM3D, EPLL, NCSR, PCLR, PGPD, WNNM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 Introduction 2 2 Related work 2 3 Very deep convolutional auto-encoder for image restoration</head><label></label><figDesc>Architecture . . . . . . . . . . . . . . . . 3 3.2 Deconvolution decoder . . . . . . . . . . 3 3.3 Skip connections . . . . . . . . . . . . . 4 3.4 Training . . . . . . . . . . . . . . . . . . 5 3.5 Testing . . . . . . . . . . . . . . . . . . . 6 Analysis on the architecture . . . . . . . 6 4.2 Gradient back-propagation . . . . . . . . Comparison with the deep residual network [1] . . . . . . . . . . . . . . . . . . 7 4.5 Testing efficiency . . . . . . . . . . . . . 7</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3</cell></row><row><cell></cell><cell>3.1</cell><cell></cell></row><row><cell>4</cell><cell cols="2">Discussions</cell><cell>6</cell></row><row><cell></cell><cell>4.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>6</cell></row><row><cell></cell><cell>4.3</cell><cell cols="2">Training with symmetric skip connections 6</cell></row><row><cell></cell><cell>4.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Average PSNR and SSIM results of Ïƒ 10, 30, 50, 70 on 14 images. Average PSNR and SSIM results of Ïƒ 10, 30, 50, 70 on BSD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BM3D</cell><cell>EPLL</cell><cell>NCSR</cell><cell>PCLR</cell><cell cols="5">PGPD WNNM RED10 RED20 RED30</cell></row><row><cell>Ïƒ = 10</cell><cell>34.18</cell><cell>33.98</cell><cell>34.27</cell><cell>34.48</cell><cell>34.22</cell><cell>34.49</cell><cell>34.62</cell><cell>34.74</cell><cell>34.81</cell></row><row><cell>Ïƒ = 30</cell><cell>28.49</cell><cell>28.35</cell><cell>28.44</cell><cell>28.68</cell><cell>28.55</cell><cell>28.74</cell><cell>28.95</cell><cell>29.10</cell><cell>29.17</cell></row><row><cell>Ïƒ = 50</cell><cell>26.08</cell><cell>25.97</cell><cell>25.93</cell><cell>26.29</cell><cell>26.19</cell><cell>26.32</cell><cell>26.51</cell><cell>26.72</cell><cell>26.81</cell></row><row><cell>Ïƒ = 70</cell><cell>24.65</cell><cell>24.47</cell><cell>24.36</cell><cell>24.79</cell><cell>24.71</cell><cell>24.80</cell><cell>24.97</cell><cell>25.23</cell><cell>25.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ïƒ = 10</cell><cell>0.9339</cell><cell cols="4">0.9332 0.9342 0.9366 0.9309</cell><cell>0.9363</cell><cell>0.9374</cell><cell>0.9392</cell><cell>0.9402</cell></row><row><cell>Ïƒ = 30</cell><cell>0.8204</cell><cell cols="4">0.8200 0.8203 0.8263 0.8199</cell><cell>0.8273</cell><cell>0.8327</cell><cell>0.8396</cell><cell>0.8423</cell></row><row><cell>Ïƒ = 50</cell><cell>0.7427</cell><cell cols="4">0.7354 0.7415 0.7538 0.7442</cell><cell>0.7517</cell><cell>0.7571</cell><cell>0.7689</cell><cell>0.7733</cell></row><row><cell>Ïƒ = 70</cell><cell>0.6882</cell><cell cols="4">0.6712 0.6871 0.6997 0.6913</cell><cell>0.6975</cell><cell>0.7012</cell><cell>0.7177</cell><cell>0.7206</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>BM3D</cell><cell>EPLL</cell><cell>NCSR</cell><cell>PCLR</cell><cell cols="5">PGPD WNNM RED10 RED20 RED30</cell></row><row><cell>Ïƒ = 10</cell><cell>33.01</cell><cell>33.01</cell><cell>33.09</cell><cell>33.30</cell><cell>33.02</cell><cell>33.25</cell><cell>33.49</cell><cell>33.59</cell><cell>33.63</cell></row><row><cell>Ïƒ = 30</cell><cell>27.31</cell><cell>27.38</cell><cell>27.23</cell><cell>27.54</cell><cell>27.33</cell><cell>27.48</cell><cell>27.79</cell><cell>27.90</cell><cell>27.95</cell></row><row><cell>Ïƒ = 50</cell><cell>25.06</cell><cell>25.17</cell><cell>24.95</cell><cell>25.30</cell><cell>25.18</cell><cell>25.26</cell><cell>25.54</cell><cell>25.67</cell><cell>25.75</cell></row><row><cell>Ïƒ = 70</cell><cell>23.82</cell><cell>23.81</cell><cell>23.58</cell><cell>23.94</cell><cell>23.89</cell><cell>23.95</cell><cell>24.13</cell><cell>24.33</cell><cell>24.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ïƒ = 10</cell><cell>0.9218</cell><cell cols="4">0.9255 0.9226 0.9261 0.9176</cell><cell>0.9244</cell><cell>0.9290</cell><cell>0.9310</cell><cell>0.9319</cell></row><row><cell>Ïƒ = 30</cell><cell>0.7755</cell><cell cols="4">0.7825 0.7738 0.7827 0.7717</cell><cell>0.7807</cell><cell>0.7918</cell><cell>0.7993</cell><cell>0.8019</cell></row><row><cell>Ïƒ = 50</cell><cell>0.6831</cell><cell cols="4">0.6870 0.6777 0.6947 0.6841</cell><cell>0.6928</cell><cell>0.7032</cell><cell>0.7117</cell><cell>0.7167</cell></row><row><cell>Ïƒ = 70</cell><cell>0.6240</cell><cell cols="4">0.6168 0.6166 0.6336 0.6245</cell><cell>0.6346</cell><cell>0.6367</cell><cell>0.6521</cell><cell>0.6551</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 Average</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">PSNR and SSIM results for image denoising using a single</cell></row><row><cell></cell><cell cols="3">30-layer network.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">14 images</cell><cell></cell></row><row><cell></cell><cell>Ïƒ = 10</cell><cell>Ïƒ = 30</cell><cell>Ïƒ = 50</cell><cell>Ïƒ = 70</cell></row><row><cell>PSNR</cell><cell>34.49</cell><cell>29.09</cell><cell>26.75</cell><cell>25.20</cell></row><row><cell>SSIM</cell><cell>0.9368</cell><cell>0.8414</cell><cell>0.7716</cell><cell>0.7157</cell></row><row><cell></cell><cell></cell><cell cols="2">BSD200</cell><cell></cell></row><row><cell></cell><cell>Ïƒ = 10</cell><cell>Ïƒ = 30</cell><cell>Ïƒ = 50</cell><cell>Ïƒ = 70</cell></row><row><cell>PSNR</cell><cell>33.38</cell><cell>27.88</cell><cell>25.69</cell><cell>24.36</cell></row><row><cell>SSIM</cell><cell>0.9280</cell><cell>0.7980</cell><cell>0.7119</cell><cell>0.6544</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Average PSNR and SSIM results of scaling 2, 3 and 4 on Set5.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SRCNN NBSRF CSCN</cell><cell>CSC</cell><cell>TSE</cell><cell cols="4">ARFL+ RED10 RED20 RED30</cell></row><row><cell>s = 2</cell><cell>36.66</cell><cell>36.76</cell><cell>37.14</cell><cell>36.62</cell><cell>36.50</cell><cell>36.89</cell><cell>37.43</cell><cell>37.62</cell><cell>37.66</cell></row><row><cell>s = 3</cell><cell>32.75</cell><cell>32.75</cell><cell>33.26</cell><cell>32.66</cell><cell>32.62</cell><cell>32.72</cell><cell>33.43</cell><cell>33.80</cell><cell>33.82</cell></row><row><cell>s = 4</cell><cell>30.49</cell><cell>30.44</cell><cell>31.04</cell><cell>30.36</cell><cell>30.33</cell><cell>30.35</cell><cell>31.12</cell><cell>31.40</cell><cell>31.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2</cell><cell>0.9542</cell><cell>0.9552</cell><cell>0.9567</cell><cell cols="2">0.9549 0.9537</cell><cell>0.9559</cell><cell>0.9590</cell><cell>0.9597</cell><cell>0.9599</cell></row><row><cell>s = 3</cell><cell>0.9090</cell><cell>0.9104</cell><cell>0.9167</cell><cell cols="2">0.9098 0.9094</cell><cell>0.9094</cell><cell>0.9197</cell><cell>0.9229</cell><cell>0.9230</cell></row><row><cell>s = 4</cell><cell>0.8628</cell><cell>0.8632</cell><cell>0.8775</cell><cell cols="2">0.8607 0.8623</cell><cell>0.8583</cell><cell>0.8794</cell><cell>0.8847</cell><cell>0.8869</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Average PSNR and SSIM results of scaling 2, 3 and 4 on Set14.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SRCNN NBSRF CSCN</cell><cell>CSC</cell><cell>TSE</cell><cell cols="4">ARFL+ RED10 RED20 RED30</cell></row><row><cell>s = 2</cell><cell>32.45</cell><cell>32.45</cell><cell>32.71</cell><cell>32.31</cell><cell>32.23</cell><cell>32.52</cell><cell>32.77</cell><cell>32.87</cell><cell>32.94</cell></row><row><cell>s = 3</cell><cell>29.30</cell><cell>29.25</cell><cell>29.55</cell><cell>29.15</cell><cell>29.16</cell><cell>29.23</cell><cell>29.42</cell><cell>29.61</cell><cell>29.61</cell></row><row><cell>s = 4</cell><cell>27.50</cell><cell>27.42</cell><cell>27.76</cell><cell>27.30</cell><cell>27.40</cell><cell>27.41</cell><cell>27.58</cell><cell>27.80</cell><cell>27.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2</cell><cell>0.9067</cell><cell>0.9071</cell><cell>0.9095</cell><cell cols="2">0.9070 0.9036</cell><cell>0.9074</cell><cell>0.9125</cell><cell>0.9138</cell><cell>0.9144</cell></row><row><cell>s = 3</cell><cell>0.8215</cell><cell>0.8212</cell><cell>0.8271</cell><cell cols="2">0.8208 0.8197</cell><cell>0.8201</cell><cell>0.8318</cell><cell>0.8343</cell><cell>0.8341</cell></row><row><cell>s = 4</cell><cell>0.7513</cell><cell>0.7511</cell><cell>0.7620</cell><cell cols="2">0.7499 0.7518</cell><cell>0.7483</cell><cell>0.7654</cell><cell>0.7697</cell><cell>0.7718</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 Average</head><label>6</label><figDesc>The second proposal is to use a skip-connection from input to the output. During training, the network has D outputs, in which the dth output y d = x + Rec(H d ).x is the input low-resolution image, Rec() denotes the reconstruction layer and H d is the output of dth recursive layer. The final loss includes three parts: (a) the Euclidean loss between the ground truth and each y d ; (b) the Euclidean loss between the ground truth and the weighted sum of all y d ; and (c) the L2 regularization on the network weights. Although skip connections are used in our network, VDSR and DCRN to perform identity mapping, their differences are significant.</figDesc><table><row><cell></cell><cell></cell><cell cols="7">PSNR and SSIM results of scaling 2, 3 and 4 on BSD100</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SRCNN NBSRF CSCN</cell><cell>CSC</cell><cell>TSE</cell><cell cols="4">ARFL+ RED10 RED20 RED30</cell></row><row><cell>s = 2</cell><cell>31.36</cell><cell>31.30</cell><cell>31.54</cell><cell>31.27</cell><cell>31.18</cell><cell>31.35</cell><cell>31.85</cell><cell>31.95</cell><cell>31.99</cell></row><row><cell>s = 3</cell><cell>28.41</cell><cell>28.36</cell><cell>28.58</cell><cell>28.31</cell><cell>28.30</cell><cell>28.36</cell><cell>28.79</cell><cell>28.90</cell><cell>28.93</cell></row><row><cell>s = 4</cell><cell>26.90</cell><cell>26.88</cell><cell>27.11</cell><cell>26.83</cell><cell>26.85</cell><cell>26.86</cell><cell>27.25</cell><cell>27.35</cell><cell>27.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2</cell><cell>0.8879</cell><cell>0.8876</cell><cell>0.8908</cell><cell cols="2">0.8876 0.8855</cell><cell>0.8885</cell><cell>0.8953</cell><cell>0.8969</cell><cell>0.8974</cell></row><row><cell>s = 3</cell><cell>0.7863</cell><cell>0.7856</cell><cell>0.7910</cell><cell cols="2">0.7853 0.7843</cell><cell>0.7851</cell><cell>0.7975</cell><cell>0.7993</cell><cell>0.7994</cell></row><row><cell>s = 4</cell><cell>0.7103</cell><cell>0.7110</cell><cell>0.7191</cell><cell cols="2">0.7101 0.7108</cell><cell>0.7091</cell><cell>0.7238</cell><cell>0.7268</cell><cell>0.7290</cell></row><row><cell cols="5">number of parameters while increasing the depth of the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">network. To ease the training, firstly each recursive layer is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">supervised to reconstruct the target high-resolution image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(HR).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>Average PSNR and SSIM results for image super-resolution using a single 30 layer network.</figDesc><table><row><cell></cell><cell></cell><cell>Set5</cell><cell></cell></row><row><cell></cell><cell>s = 2</cell><cell>s = 3</cell><cell>s = 4</cell></row><row><cell>PSNR</cell><cell>37.56</cell><cell>33.70</cell><cell>31.33</cell></row><row><cell>SSIM</cell><cell cols="3">0.9595 0.9222 0.8847</cell></row><row><cell></cell><cell></cell><cell>Set14</cell><cell></cell></row><row><cell></cell><cell>s = 2</cell><cell>s = 3</cell><cell>s = 4</cell></row><row><cell>PSNR</cell><cell>32.81</cell><cell>29.50</cell><cell>27.72</cell></row><row><cell>SSIM</cell><cell cols="3">0.9135 0.8334 0.7698</cell></row><row><cell></cell><cell></cell><cell>BSD100</cell><cell></cell></row><row><cell></cell><cell>s = 2</cell><cell>s = 3</cell><cell>s = 4</cell></row><row><cell>PSNR</cell><cell>31.96</cell><cell>28.88</cell><cell>27.35</cell></row><row><cell>SSIM</cell><cell cols="3">0.8972 0.7993 0.7276</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Scale VDSR (PSNR/SSIM) DRCN (PSNR/SSIM) RED30 (PSNR/SSIM) Set5 Fig. 15. Visual results of image super-resolution. Images from left to right column are: High resolution image; the recovered image of RED30, ARFL+, CSC, CSCN, NBSRF, SRCNN, TSE.</figDesc><table><row><cell></cell><cell>Ã—2</cell><cell>37.53/0.9587</cell><cell>37.63/0.9588</cell><cell>37.66/0.9599</cell></row><row><cell></cell><cell>Ã—3</cell><cell>33.66/0.9213</cell><cell>33.82/0.9226</cell><cell>33.82/0.9230</cell></row><row><cell></cell><cell>Ã—4</cell><cell>31.35/0.8838</cell><cell>31.53/0.8854</cell><cell>31.51/0.8869</cell></row><row><cell></cell><cell>Ã—2</cell><cell>33.03/0.9124</cell><cell>33.04/0.9118</cell><cell>32.94/0.9144</cell></row><row><cell>Set14</cell><cell>Ã—3</cell><cell>29.77/0.8314</cell><cell>29.76/0.8311</cell><cell>29.61/0.8341</cell></row><row><cell></cell><cell>Ã—4</cell><cell>28.01/0.7674</cell><cell>28.02/0.7670</cell><cell>27.86/0.7718</cell></row><row><cell></cell><cell>Ã—2</cell><cell>31.90/0.8960</cell><cell>31.85/0.8942</cell><cell>31.99/0.8974</cell></row><row><cell>BSD100</cell><cell>Ã—3</cell><cell>28.82/0.7976</cell><cell>28.80/0.7963</cell><cell>28.93/0.7994</cell></row><row><cell></cell><cell>Ã—4</cell><cell>27.29/0.7251</cell><cell>27.23/0.7233</cell><cell>27.40/0.7290</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>JPEG compression deblock: average PSNR results of LIVE1.SA-DCT Deeper SRCNN AR-CNN RED10 RED20 RED30 QualityTABLE 10 PSNR results on non-blind deblurring. kernel tpye Krishnan et al. [60] Levin et al. [61] Cho et al. [62] Schuler et al. [63] Xu et al. Fig. 16. Visual comparisons on non-blind deblurring. Images from left to right are: blurred images, the results of Cho [62], Krishnan [60], Levin [61], Schuler [63], Xu [59] and our method.</figDesc><table><row><cell></cell><cell>= 10</cell><cell>28.65</cell><cell>28.92</cell><cell>28.98</cell><cell>29.24</cell><cell>29.33</cell><cell>29.35</cell></row><row><cell></cell><cell>Quality = 20</cell><cell>30.81</cell><cell>-</cell><cell>31.29</cell><cell>31.63</cell><cell>31.71</cell><cell>31.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[59] RED30</cell></row><row><cell>disk</cell><cell>25.94</cell><cell>24.54</cell><cell></cell><cell>23.97</cell><cell>24.67</cell><cell></cell><cell>26.01</cell><cell>32.13</cell></row><row><cell>motion</cell><cell>30.34</cell><cell>37.80</cell><cell></cell><cell>33.25</cell><cell>-</cell><cell></cell><cell>-</cell><cell>38.84</cell></row><row><cell>gaussian</cell><cell>27.90</cell><cell>32.34</cell><cell></cell><cell>30.09</cell><cell>30.97</cell><cell></cell><cell>-</cell><cell>34.49</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonlocal sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascades of regression tree fields for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising via adaptive soft-thresholding based on non-local samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">External patch prior guided internal clustering for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A+: adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comp. Vis</title>
		<meeting>Asian Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image super-resolution using deformable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2917" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling deformable gradient compositions for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bonev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5417" to="5425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditioned regression models for non-blind single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>RÃ¼ther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional sparse coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1823" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="229" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tour of modern image filtering: New insights and methods, both practical and theoretical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning super-resolution jointly from external and internal examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4359" to="4371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clustering-based denoising with locally learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1438" to="1451" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recent developments in total variation image restoration,&quot; in In Mathematical Models of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Esedoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive total variation image deblurring: a majorization-minimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2479" to="2493" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1838" to="1857" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image restoration using a multilayer perceptron with a multilevel sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">B</forename><surname>Desai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2018" to="2022" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Naive bayes super-resolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image superresolution</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyper-laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1033" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Handling outliers in non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="495" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
