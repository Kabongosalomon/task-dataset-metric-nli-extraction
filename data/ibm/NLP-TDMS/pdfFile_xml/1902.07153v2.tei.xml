<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
						</author>
						<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref> are an efficient variant of Convolutional Neural Networks (CNNs) on graphs. GCNs stack layers of learned first-order spectral filters followed by a nonlinear activation function to learn graph representations. Recently, GCNs and subsequent variants have achieved state-of-the-art results in various application areas, including but not limited to citation networks <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref>, social networks , applied chemistry <ref type="bibr" target="#b29">(Liao et al., 2019)</ref>, natural language processing <ref type="bibr" target="#b51">(Yao et al., 2019;</ref><ref type="bibr" target="#b16">Han et al., 2012;</ref><ref type="bibr" target="#b55">Zhang et al., 2018c)</ref>, and computer vision <ref type="bibr" target="#b21">Kampffmeyer et al., 2018)</ref>.</p><p>Historically, the development of machine learning algo-* Equal contribution 1 Cornell University 2 Federal Institute of Ceara (Brazil).</p><p>Correspondence to: Felix Wu &lt;fw245@cornell.edu&gt;, Tianyi Zhang &lt;tz58@cornell.edu&gt;. rithms has followed a clear trend from initial simplicity to need-driven complexity. For instance, limitations of the linear Perceptron <ref type="bibr" target="#b37">(Rosenblatt, 1958)</ref> motivated the development of the more complex but also more expressive neural network (or multi-layer Perceptrons, MLPs) <ref type="bibr" target="#b38">(Rosenblatt, 1961)</ref>. Similarly, simple pre-defined linear image filters <ref type="bibr" target="#b40">(Sobel &amp; Feldman, 1968;</ref><ref type="bibr" target="#b18">Harris &amp; Stephens, 1988)</ref> eventually gave rise to nonlinear CNNs with learned convolutional kernels <ref type="bibr" target="#b44">(Waibel et al., 1989;</ref><ref type="bibr" target="#b25">LeCun et al., 1989)</ref>. As additional algorithmic complexity tends to complicate theoretical analysis and obfuscates understanding, it is typically only introduced for applications where simpler methods are insufficient. Arguably, most classifiers in real world applications are still linear (typically logistic regression), which are straight-forward to optimize and easy to interpret.</p><p>However, possibly because GCNs were proposed after the recent "renaissance" of neural networks, they tend to be a rare exception to this trend. GCNs are built upon multi-layer neural networks, and were never an extension of a simpler (insufficient) linear counterpart.</p><p>In this paper, we observe that GCNs inherit considerable complexity from their deep learning lineage, which can be burdensome and unnecessary for less demanding applications. Motivated by the glaring historic omission of a simpler predecessor, we aim to derive the simplest linear model that "could have" preceded the GCN, had a more "traditional" path been taken. We reduce the excess complexity of GCNs by repeatedly removing the nonlinearities between GCN layers and collapsing the resulting function into a single linear transformation. We empirically show that the final linear model exhibits comparable or even superior performance to GCNs on a variety of tasks while being computationally more efficient and fitting significantly fewer parameters. We refer to this simplified linear model as Simple Graph Convolution (SGC).</p><p>In contrast to its nonlinear counterparts, the SGC is intuitively interpretable and we provide a theoretical analysis from the graph convolution perspective. Notably, feature extraction in SGC corresponds to a single fixed filter applied to each feature dimension. <ref type="bibr" target="#b23">Kipf &amp; Welling (2017)</ref> empirically observe that the "renormalization trick", i.e. adding self-loops to the graph, improves accuracy, and we demon-Y SGC = softmax X ⇥ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; Predictions -1 +1 0</p><p>Feature Value: Class +1:</p><p>Class -1: Feature Vector:</p><p>K-step Feature Propagation X S K X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGC</head><p>Input Graph</p><p>x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><formula xml:id="formula_0">H (0) = X = [x 1 , . . . , x n ]</formula><p>&gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Graph</head><p>x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><p>x 7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; X = [x 1 , . . . , x n ] &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;</p><formula xml:id="formula_1">H (k) ReLU(H (k) )</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; <ref type="figure">Figure 1</ref>. Schematic layout of a GCN v.s. a SGC. Top row: The GCN transforms the feature vectors repeatedly throughout K layers and then applies a linear classifier on the final representation. Bottom row: the SGC reduces the entire procedure to a simple feature propagation step followed by standard logistic regression.</p><p>strate that this method effectively shrinks the graph spectral domain, resulting in a low-pass-type filter when applied to SGC. Crucially, this filtering operation gives rise to locally smooth features across the graph <ref type="bibr" target="#b7">(Bruna et al., 2014)</ref>.</p><p>Through an empirical assessment on node classification benchmark datasets for citation and social networks, we show that the SGC achieves comparable performance to GCN and other state-of-the-art graph neural networks. However, it is significantly faster, and even outperforms Fast-GCN  by up to two orders of magnitude on the largest dataset (Reddit) in our evaluation. Finally, we demonstrate that SGC extrapolates its effectiveness to a wide-range of downstream tasks. In particular, SGC rivals, if not surpasses, GCN-based approaches on text classification, user geolocation, relation extraction, and zero-shot image classification tasks. The code is available on Github 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Simple Graph Convolution</head><p>We follow <ref type="bibr" target="#b23">Kipf &amp; Welling (2017)</ref> to introduce GCNs (and subsequently SGC) in the context of node classification.</p><p>Here, GCNs take a graph with some labeled nodes as input and generate label predictions for all graph nodes. Let us formally define such a graph as G = (V, A), where V represents the vertex set consisting of nodes {v 1 , . . . , v n }, and A ∈ R n×n is a symmetric (typically sparse) adjacency matrix where a ij denotes the edge weight between nodes 1 https://github.com/Tiiiger/SGC v i and v j . A missing edge is represented through a ij = 0. We define the degree matrix D = diag(d 1 , . . . , d n ) as a diagonal matrix where each entry on the diagonal is equal to the row-sum of the adjacency matrix d i = j a ij .</p><p>Each node v i in the graph has a corresponding ddimensional feature vector x i ∈ R d . The entire feature matrix X ∈ R n×d stacks n feature vectors on top of one another, X = [x 1 , . . . , x n ] . Each node belongs to one out of C classes and can be labeled with a C-dimensional one-hot vector y i ∈ {0, 1} C . We only know the labels of a subset of the nodes and want to predict the unknown labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Convolutional Networks</head><p>Similar to CNNs or MLPs, GCNs learn a new feature representation for the feature x i of each node over multiple layers, which is subsequently used as input into a linear classifier. For the k-th graph convolution layer, we denote the input node representations of all nodes by the matrix H (k−1) and the output node representations H (k) . Naturally, the initial node representations are just the original input features:</p><formula xml:id="formula_2">H (0) = X,<label>(1)</label></formula><p>which serve as input to the first GCN layer.</p><p>A K-layer GCN is identical to applying a K-layer MLP to the feature vector x i of each node in the graph, except that the hidden representation of each node is averaged with its neighbors at the beginning of each layer. In each graph convolution layer, node representations are updated in three stages: feature propagation, linear transformation, and a pointwise nonlinear activation (see <ref type="figure">Figure 1</ref>). For the sake of clarity, we describe each step in detail.</p><p>Feature propagation is what distinguishes a GCN from an MLP. At the beginning of each layer the features h i of each node v i are averaged with the feature vectors in its local neighborhood,</p><formula xml:id="formula_3">h (k) i ← 1 d i + 1 h (k−1) i + n j=1 a ij (d i + 1)(d j + 1) h (k−1) j .</formula><p>(2) More compactly, we can express this update over the entire graph as a simple matrix operation. Let S denote the "normalized" adjacency matrix with added self-loops,</p><formula xml:id="formula_4">S =D − 1 2ÃD − 1 2 ,<label>(3)</label></formula><p>whereÃ = A + I andD is the degree matrix ofÃ. The simultaneous update in Equation 2 for all nodes becomes a simple sparse matrix multiplication</p><formula xml:id="formula_5">H (k) ← SH (k−1) .<label>(4)</label></formula><p>Intuitively, this step smoothes the hidden representations locally along the edges of the graph and ultimately encourages similar predictions among locally connected nodes.</p><p>Feature transformation and nonlinear transition. After the local smoothing, a GCN layer is identical to a standard MLP. Each layer is associated with a learned weight matrix Θ (k) , and the smoothed hidden feature representations are transformed linearly. Finally, a nonlinear activation function such as ReLU is applied pointwise before outputting feature representation H (k) . In summary, the representation updating rule of the k-th layer is:</p><formula xml:id="formula_6">H (k) ← ReLU H (k) Θ (k) .<label>(5)</label></formula><p>The pointwise nonlinear transformation of the k-th layer is followed by the feature propagation of the (k + 1)-th layer.</p><p>Classifier. For node classification, and similar to a standard MLP, the last layer of a GCN predicts the labels using a softmax classifier. Denote the class predictions for n nodes asŶ ∈ R n×C whereŷ ic denotes the probability of node i belongs to class c. The class predictionŶ of a K-layer GCN can be written as:</p><formula xml:id="formula_7">Y GCN = softmax SH (K−1) Θ (K) ,<label>(6)</label></formula><p>where softmax(x) = exp(x)/ C c=1 exp(x c ) acts as a normalizer across all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Simple Graph Convolution</head><p>In a traditional MLP, deeper layers increase the expressivity because it allows the creation of feature hierarchies, e.g. features in the second layer build on top of the features of the first layer. In GCNs, the layers have a second important function: in each layer the hidden representations are averaged among neighbors that are one hop away. This implies that after k layers a node obtains feature information from all nodes that are k−hops away in the graph. This effect is similar to convolutional neural networks, where depth increases the receptive field of internal features <ref type="bibr" target="#b17">(Hariharan et al., 2015)</ref>. Although convolutional networks can benefit substantially from increased depth <ref type="bibr" target="#b20">(Huang et al., 2016)</ref>, typically MLPs obtain little benefit beyond 3 or 4 layers.</p><p>Linearization. We hypothesize that the nonlinearity between GCN layers is not critical -but that the majority of the benefit arises from the local averaging. We therefore remove the nonlinear transition functions between each layer and only keep the final softmax (in order to obtain probabilistic outputs). The resulting model is linear, but still has the same increased "receptive field" of a K-layer GCN,</p><formula xml:id="formula_8">Y = softmax S . . . SSXΘ (1) Θ (2) . . . Θ (K) . (7)</formula><p>To simplify notation we can collapse the repeated multiplication with the normalized adjacency matrix S into a single matrix by raising S to the K-th power, S K . Further, we can reparameterize our weights into a single matrix Θ = Θ (1) Θ (2) . . . Θ <ref type="bibr">(K)</ref> . The resulting classifier becomeŝ</p><formula xml:id="formula_9">Y SGC = softmax S K XΘ ,<label>(8)</label></formula><p>which we refer to as Simple Graph Convolution (SGC).</p><p>Logistic regression. Equation 8 gives rise to a natural and intuitive interpretation of SGC: by distinguishing between feature extraction and classifier, SGC consists of a fixed (i.e., parameter-free) feature extraction/smoothing compo-nentX = S K X followed by a linear logistic regression classifierŶ = softmax(XΘ). Since the computation ofX requires no weight it is essentially equivalent to a feature pre-processing step and the entire training of the model reduces to straight-forward multi-class logistic regression on the pre-processed featuresX.</p><p>Optimization details. The training of logistic regression is a well studied convex optimization problem and can be performed with any efficient second order method or stochastic gradient descent <ref type="bibr" target="#b6">(Bottou, 2010)</ref>. Provided the graph connectivity pattern is sufficiently sparse, SGD naturally scales to very large graph sizes and the training of SGC is drastically faster than that of GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spectral Analysis</head><p>We now study SGC from a graph convolution perspective. We demonstrate that SGC corresponds to a fixed filter on the graph spectral domain. In addition, we show that adding self-loops to the original graph, i.e. the renormalization trick <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref>, effectively shrinks the underlying graph spectrum. On this scaled domain, SGC acts as a lowpass filter that produces smooth features over the graph. As a result, nearby nodes tend to share similar representations and consequently predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries on Graph Convolutions</head><p>Analogous to the Euclidean domain, graph Fourier analysis relies on the spectral decomposition of graph Laplacians. The graph Laplacian ∆ = D − A (as well as its normalized version ∆ sym = D −1/2 ∆D −1/2 ) is a symmetric positive semidefinite matrix with eigendecomposition ∆ = UΛU , where U ∈ R n×n comprises orthonormal eigenvectors and Λ = diag(λ 1 , . . . , λ n ) is a diagonal matrix of eigenvalues. The eigendecomposition of the Laplacian allows us to define the Fourier transform equivalent on the graph domain, where eigenvectors denote Fourier modes and eigenvalues denote frequencies of the graph. In this regard, let x ∈ R n be a signal defined on the vertices of the graph. We define the graph Fourier transform of x asx = U x, with inverse operation given by x = Ux. Thus, the graph convolution operation between signal x and filter g is</p><formula xml:id="formula_10">g * x = U (U g) (U x) = UĜU x,<label>(9)</label></formula><p>whereĜ = diag (ĝ 1 , . . . ,ĝ n ) denotes a diagonal matrix in which the diagonal corresponds to spectral filter coefficients.</p><p>Graph convolutions can be approximated by k-th order polynomials of Laplacians</p><formula xml:id="formula_11">UĜU x ≈ k i=0 θ i ∆ i x = U k i=0 θ i Λ i U x,<label>(10)</label></formula><p>where θ i denotes coefficients. In this case, filter coefficients correspond to polynomials of the Laplacian eigenvalues, i.e., <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref> employ an affine approximation (k = 1) of Equation 10 with coefficients θ 0 = 2θ and θ 1 = −θ from which we attain the basic GCN convolution operation</p><formula xml:id="formula_12">G = i θ i Λ i or equivalentlyĝ(λ j ) = i θ i λ i j . Graph Convolutional Networks (GCNs)</formula><formula xml:id="formula_13">g * x = θ(I + D −1/2 AD −1/2 )x.<label>(11)</label></formula><p>In their final design, <ref type="bibr" target="#b23">Kipf &amp; Welling (2017)</ref> replace the matrix I + D −1/2 AD −1/2 by a normalized versioñ D −1/2ÃD−1/2 whereÃ = A + I and consequentlỹ D = D + I, dubbed the renormalization trick. Finally, by generalizing the convolution to work with multiple filters in a d-channel input and layering the model with nonlinear activation functions between each layer, we have the GCN propagation rule as defined in Equation 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SGC and Low-Pass Filtering</head><p>The initial first-order Chebyshev filter derived in GCNs corresponds to the propagation matrix S 1-order = I + D −1/2 AD −1/2 (see <ref type="figure">Equation 11</ref>). Since the normalized Laplacian is ∆ sym = I − D −1/2 AD −1/2 , then S 1-order = 2I − ∆ sym . Therefore, feature propagation with S K 1-order implies filter coefficientsĝ i =ĝ(λ i ) = (2 − λ i ) K , where λ i denotes the eigenvalues of ∆ sym . <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the filtering operation related to S 1-order for a varying number of propagation steps K ∈ {1, . . . , 6}. As one may observe, high powers of S 1-order lead to exploding filter coefficients and undesirably over-amplify signals at frequencies λ i &lt; 1.</p><p>To tackle potential numerical issues associated with the first-order Chebyshev filter, <ref type="bibr" target="#b23">Kipf &amp; Welling (2017)</ref> propose the renormalization trick. Basically, it consists of replacing S 1-order by the normalized adjacency matrix after adding self-loops for all nodes. We call the resulting propagation matrix the augmented normalized adjacency matrixS adj =D −1/2ÃD−1/2 , whereÃ = A + I and D = D + I. Correspondingly, we define the augmented normalized Laplacian∆ sym = I −D −1/2ÃD−1/2 . Thus, we can describe the spectral filters associated withS adj as a polynomial of the eigenvalues of the underlying Laplacian, i.e.,ĝ(λ i ) = (1 −λ i ) K , whereλ i are eigenvalues of∆ sym .</p><p>We now analyze the spectrum of∆ sym and show that adding self-loops to graphs shrinks the spectrum (eigenvalues) of the corresponding normalized Laplacian.</p><p>Theorem 1. Let A be the adjacency matrix of an undirected, weighted, simple graph G without isolated nodes and with corresponding degree matrix D. LetÃ = A + γI, such that γ &gt; 0, be the augmented adjacency matrix with corresponding degree matrixD. Also, let λ 1 and λ n denote the smallest and largest eigenvalues of ∆ sym = I−D −1/2 AD −1/2 ; similarly, letλ 1 andλ n be the smallest and largest eigenvalues of∆ sym = I −D −1/2ÃD−1/2 . We have that</p><formula xml:id="formula_14">0 = λ 1 =λ 1 &lt;λ n &lt; λ n .<label>(12)</label></formula><p>Theorem 1 shows that the largest eigenvalue of the normalized graph Laplacian becomes smaller after adding selfloops γ &gt; 0 (see supplementary materials for the proof). <ref type="figure" target="#fig_1">Figure 2</ref> depicts the filtering operations associated with the normalized adjacency S adj = D −1/2 AD −1/2 and its augmented variantS adj =D −1/2ÃD−1/2 on the Cora dataset <ref type="bibr" target="#b39">(Sen et al., 2008)</ref>. Feature propagation with S adj corresponds to filters g(λ i ) = (1 − λ i ) K in the spectral range  [0, 2]; therefore odd powers of S adj yield negative filter coefficients at frequencies λ i &gt; 1. By adding self-loops (S adj ), the largest eigenvalue shrinks from 2 to approximately 1.5 and then eliminates the effect of negative coefficients. Moreover, this scaled spectrum allows the filter defined by taking powers K &gt; 1 ofS adj to act as a low-pass-type filters. In supplementary material, we empirically evaluate different choices for the propagation matrix. <ref type="bibr" target="#b7">Bruna et al. (2014)</ref> first propose a spectral graph-based extension of convolutional networks to graphs. In a followup work, ChebyNets <ref type="bibr" target="#b11">(Defferrard et al., 2016)</ref> define graph convolutions using Chebyshev polynomials to remove the computationally expensive Laplacian eigendecomposition. GCNs <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref> further simplify graph convolutions by stacking layers of first-order Chebyshev polynomial filters with a redefined propagation matrix S. <ref type="bibr" target="#b10">Chen et al. (2018)</ref> propose an efficient variant of GCN based on importance sampling, and <ref type="bibr" target="#b15">Hamilton et al. (2017)</ref> propose a framework based on sampling and aggregation. <ref type="bibr" target="#b1">Atwood &amp; Towsley (2016)</ref>, <ref type="bibr" target="#b0">Abu-El-Haija et al. (2018)</ref>, <ref type="bibr" target="#b29">and Liao et al. (2019)</ref> exploit multi-scale information by raising S to higher order. <ref type="bibr" target="#b48">Xu et al. (2019)</ref> study the expressiveness of graph neural networks in terms of their ability to distinguish any two graphs and introduce Graph Isomorphism Network, which is proved to be as powerful as the Weisfeiler-Lehman test for graph isomorphism. <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> separate the non-linear transformation from propagation by using a neural network followed by a personalized random walk. There are many other graph neural models <ref type="bibr" target="#b32">(Monti et al., 2017;</ref><ref type="bibr" target="#b12">Duran &amp; Niepert, 2017;</ref><ref type="bibr" target="#b27">Li et al., 2018)</ref>; we refer to <ref type="bibr" target="#b57">Zhou et al. (2018)</ref>; <ref type="bibr" target="#b2">Battaglia et al. (2018)</ref>; <ref type="bibr" target="#b47">Wu et al. (2019)</ref> for a more comprehensive review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph Neural Networks</head><p>Previous publications have pointed out that simpler, sometimes linear models can be effective for node/graph classification tasks. <ref type="bibr" target="#b41">Thekumparampil et al. (2018)</ref> empirically show that a linear version of GCN can perform competitively and propose an attention-based GCN variant. <ref type="bibr" target="#b8">Cai &amp; Wang (2018)</ref> propose an effective linear baseline for graph classification using node degree statistics. <ref type="bibr" target="#b14">Eliav &amp; Cohen (2018)</ref> show that models which use linear feature/label propagation steps can benefit from self-training strategies. <ref type="bibr" target="#b28">Li et al. (2019)</ref> propose a generalized version of label propagation and provide a similar spectral analysis of the renormalization trick.</p><p>Graph Attentional Models learn to assign different edge weights at each layer based on node features and have achieved state-of-the-art results on several graph learning tasks <ref type="bibr" target="#b42">(Velickovic et al., 2018;</ref><ref type="bibr" target="#b41">Thekumparampil et al., 2018;</ref><ref type="bibr" target="#b52">Zhang et al., 2018a;</ref><ref type="bibr" target="#b21">Kampffmeyer et al., 2018)</ref>. However, the attention mechanism usually adds significant overhead to computation and memory usage. We refer the readers to <ref type="bibr" target="#b26">Lee et al. (2018)</ref> for further comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Other Works on Graphs</head><p>Graph methodologies can roughly be categorized into two approaches: graph embedding methods and graph laplacian regularization methods. Graph embedding methods <ref type="bibr" target="#b46">(Weston et al., 2008;</ref><ref type="bibr" target="#b33">Perozzi et al., 2014;</ref><ref type="bibr" target="#b50">Yang et al., 2016;</ref><ref type="bibr" target="#b43">Velikovi et al., 2019)</ref> represent nodes as high-dimensional feature vectors. Among them, DeepWalk <ref type="bibr" target="#b33">(Perozzi et al., 2014)</ref> and Deep Graph Infomax (DGI) <ref type="bibr" target="#b43">(Velikovi et al., 2019)</ref> use unsupervised strategies to learn graph embeddings. DeepWalk relies on truncated random walk and uses a skip-gram model to generate embeddings, whereas DGI trains a graph convolutional encoder through maximizing mutual information. Graph Laplacian regularization <ref type="bibr" target="#b58">(Zhu et al., 2003;</ref><ref type="bibr" target="#b56">Zhou et al., 2004;</ref><ref type="bibr" target="#b3">Belkin &amp; Niyogi, 2004;</ref><ref type="bibr" target="#b4">Belkin et al., 2006)</ref> introduce a regularization term based on graph structure which forces nodes to have similar labels to their neighbors. Label Propagation <ref type="bibr" target="#b58">(Zhu et al., 2003)</ref> makes predictions by spreading label information from labeled nodes to their neighbors until convergence.  <ref type="figure">Figure 3</ref>. Performance over training time on Pubmed and Reddit. SGC is the fastest while achieving competitive performance. We are not able to benchmark the training time of GaAN and DGI on Reddit because the implementations are not released. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Discussion</head><p>We first evaluate SGC on citation networks and social networks and then extend our empirical analysis to a wide range of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Citation Networks &amp; Social Networks</head><p>We evaluate the semi-supervised node classification performance of SGC on the Cora, Citeseer, and Pubmed citation network datasets <ref type="table" target="#tab_2">(Table 2)</ref>  <ref type="bibr" target="#b39">(Sen et al., 2008)</ref>. We supplement our citation network analysis by using SGC to inductively predict community structure on Reddit <ref type="table">(Table 3)</ref>, which consists of a much larger graph. Dataset statistics are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Datasets and experimental setup. On the citation networks, we train SGC for 100 epochs using Adam <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2015)</ref> with learning rate 0.2. In addition, we use weight decay and tune this hyperparameter on each dataset using hyperopt <ref type="bibr" target="#b5">(Bergstra et al., 2015)</ref> for 60 iterations on the public split validation set. Experiments on citation networks are conducted transductively. On the Reddit dataset, we train SGC with L-BFGS <ref type="bibr" target="#b30">(Liu &amp; Nocedal, 1989)</ref> using no regularization, and remarkably, training converges in 2 steps. We evaluate SGC inductively by following <ref type="bibr" target="#b10">Chen et al. (2018)</ref>: we train SGC on a subgraph comprising only training nodes and test with the original graph. On all datasets, we tune the number of epochs based on both convergence behavior and validation accuracy. Baselines. For citation networks, we compare against GCN <ref type="bibr" target="#b23">(Kipf &amp; Welling, 2017)</ref> GAT <ref type="bibr" target="#b42">(Velickovic et al., 2018)</ref> FastGCN  LNet, AdaLNet <ref type="bibr" target="#b29">(Liao et al., 2019)</ref> and DGI <ref type="bibr" target="#b43">(Velikovi et al., 2019)</ref> using the publicly released implementations. Since GIN is not initially evaluated on citation networks, we implement GIN following <ref type="bibr" target="#b48">Xu et al. (2019)</ref> and use hyperopt to tune weight decay and learning rate for 60 iterations. Moreover, we tune the hidden dimension by hand.</p><p>For Reddit, we compare SGC to the reported performance of GaAN <ref type="bibr" target="#b52">(Zhang et al., 2018a)</ref>, supervised and unsupervised variants of GraphSAGE <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>, FastGCN, and DGI. <ref type="table">Table 3</ref> also highlights the setting of the feature extraction step for each method. We note that SGC involves no learning because the feature extraction step, S K X, has no parameter. Both unsupervised and no-learning approaches train logistic regression models with labels afterward.</p><p>Performance. Based on results in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table">Table 3</ref>, we conclude that SGC is very competitive. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of SGC can match the performance of GCN and state-of-the-art graph networks on citation networks. In particular on Citeseer, SGC is about 1% better than GCN, and we reason this performance boost is caused by SGC having fewer parameters and therefore suffering less from overfitting. Remarkably, GIN performs slight worse because of overfitting. Also, both LNet and AdaLNet are unstable on citation networks. On Reddit, <ref type="table">Table 3</ref> shows that SGC outperforms the previous sampling-based GCN variants, SAGE-GCN and FastGCN by more than 1%.</p><p>Notably, <ref type="bibr" target="#b43">Velikovi et al. (2019)</ref> report that the performance of a randomly initialized DGI encoder nearly matches that of a trained encoder; however, both models underperform SGC on Reddit. This result may suggest that the extra weights and nonlinearities in the DGI encoder are superfluous, if not outright detrimental.</p><p>Efficiency. In <ref type="figure">Figure 3</ref>, we plot the performance of the state-of-the-arts graph networks over their training time relative to that of SGC on the Pubmed and Reddit datasets. In particular, we precompute S K X and the training time of SGC takes into account this precomputation time. We measure the training time on a NVIDIA GTX 1080 Ti GPU and present the benchmark details in supplementary materials.</p><p>On large graphs (e.g. Reddit), GCN cannot be trained due to excessive memory requirements. Previous approaches tackle this limitation by either sampling to reduce neighborhood size <ref type="bibr" target="#b15">Hamilton et al., 2017)</ref> or limiting their model sizes <ref type="bibr" target="#b43">(Velikovi et al., 2019)</ref>. By applying a fixed filter and precomputing S K X, SGC minimizes memory usage and only learns a single weight matrix during training. Since S is typically sparse and K is usually small, we can exploit fast sparse-dense matrix multiplication to compute S K X. <ref type="figure">Figure 3</ref> shows that SGC can be trained up to two orders of magnitude faster than fast sampling-based methods while having little or no drop in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Downstream Tasks</head><p>We extend our empirical evaluation to 5 downstream applications -text classification, semi-supervised user geolocation, relation extraction, zero-shot image classification, and graph classification -to study the applicability of SGC. We describe experimental setup in supplementary materials.</p><p>Text classification assigns labels to documents. <ref type="bibr" target="#b51">Yao et al. (2019)</ref> use a 2-layer GCN to achieve state-of-the-art results by creating a corpus-level graph which treats both documents and words as nodes in a graph. Word-word edge weights are pointwise mutual information (PMI) and worddocument edge weights are normalized TF-IDF scores. <ref type="table" target="#tab_3">Table 4</ref> shows that an SGC (K = 2) rivals their model on 5 benchmark datasets, while being up to 83.6× faster.</p><p>Semi-supervised user geolocation locates the "home" position of users on social media given users' posts, connections among users, and a small number of labelled users. <ref type="bibr" target="#b34">Rahimi et al. (2018)</ref> apply GCNs with highway connections on this task and achieve close to state-of-the-art results. Ta- <ref type="table">Table 6</ref>. Test Accuracy (%) on Relation Extraction. The numbers are averaged over 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TACRED</head><p>Test Accuracy ↑ C-GCN <ref type="bibr" target="#b55">(Zhang et al., 2018c)</ref> 66.4 C-GCN 66.4 ± 0.4 C-SGC 67.0 ± 0.4 <ref type="table">Table 7</ref>. Top-1 accuracy (%) averaged over 10 runs in the 2hop and 3-hop setting of the zero-shot image task on ImageNet. ADGPM <ref type="bibr" target="#b21">(Kampffmeyer et al., 2018)</ref>   Relation extraction involves predicting the relation between subject and object in a sentence. <ref type="bibr" target="#b55">Zhang et al. (2018c)</ref> propose C-GCN which uses an LSTM (Hochreiter &amp; Schmidhuber, 1997) followed by a GCN and an MLP. We replace GCN with SGC (K = 2) and call the resulting model C-SGC. <ref type="table">Table 6</ref> shows that C-SGC sets new state-ofthe-art on TACRED <ref type="bibr" target="#b54">(Zhang et al., 2017)</ref>.</p><p>Zero-shot image classification consists of learning an image classifier without access to any images or labels from the test categories. GCNZ  uses a GCN to map the category names -based on their relations in WordNet <ref type="bibr" target="#b31">(Miller, 1995)</ref> -to image feature domain, and find the most similar category to a query image feature vector. <ref type="table">Table 7</ref> shows that replacing GCN with an MLP followed by SGC can improve performance while reducing the number of parameters by 55%. We find that an MLP feature extractor is necessary in order to map the pretrained GloVe vectors to the space of visual features extracted by a ResNet-50. Again, this downstream application demonstrates that learned graph convolution filters are superfluous; similar to <ref type="bibr" target="#b9">Changpinyo et al. (2018)</ref>'s observation that GCNs may not be necessary.</p><p>Graph classification requires models to use graph structure to categorize graphs. <ref type="bibr" target="#b48">Xu et al. (2019)</ref> theoretically show that GCNs are not sufficient to distinguish certain graph structures and show that their GIN is more expressive and achieves state-of-the-art results on various graph classification datasets. We replace the GCN in DCGCN <ref type="bibr" target="#b53">(Zhang et al., 2018b)</ref> with an SGC and get 71.0% and 76.2% on NCI1 and COLLAB datasets <ref type="bibr" target="#b49">(Yanardag &amp; Vishwanathan, 2015)</ref> respectively, which is on par with an GCN counterpart, but far behind GIN. Similarly, on QM8 quantum chemistry dataset <ref type="bibr" target="#b35">(Ramakrishnan et al., 2015)</ref>, more advanced AdaL-Net and LNet <ref type="bibr" target="#b29">(Liao et al., 2019)</ref> get 0.01 MAE on QM8, outperforming SGC's 0.03 MAE by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In order to better understand and explain the mechanisms of GCNs, we explore the simplest possible formulation of a graph convolutional model, SGC. The algorithm is almost trivial, a graph based pre-processing step followed by standard multi-class logistic regression. However, the performance of SGC rivals -if not surpasses -the performance of GCNs and state-of-the-art graph neural network models across a wide range of graph learning tasks. Moreover by precomputing the fixed feature extractor S K , training time is reduced to a record low. For example on the Reddit dataset, SGC can be trained up to two orders of magnitude faster than sampling-based GCN variants.</p><p>In addition to our empirical analysis, we analyze SGC from a convolution perspective and manifest this method as a low-pass-type filter on the spectral domain. Low-pass-type filters capture low-frequency signals, which corresponds with smoothing features across a graph in this setting. Our analysis also provides insight into the empirical boost of the "renormalization trick" and demonstrates how shrinking the spectral domain leads to a low-pass-type filter which underpins SGC.</p><p>Ultimately, the strong performance of SGC sheds light onto GCNs. It is likely that the expressive power of GCNs originates primarily from the repeated graph propagation (which SGC preserves) rather than the nonlinear feature extraction (which it doesn't.)</p><p>Given its empirical performance, efficiency, and interpretability, we argue that the SGC should be highly beneficial to the community in at least three ways: (1) as a first model to try, especially for node classification tasks;</p><p>(2) as a simple baseline for comparison with future graph learning models; (3) as a starting point for future research in graph learning -returning to the historic machine learning practice to develop complex from simple models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simplifying Graph Convolutional Networks (Supplementary Material)</head><p>A. The spectrum of∆ sym</p><p>The normalized Laplacian defined on graphs with self-loops, ∆ sym , consists of an instance of generalized graph Laplacians and hold the interpretation as a difference operator, i.e. for any signal x ∈ R n it satisfies</p><formula xml:id="formula_15">(∆ sym x) i = jã ij √ d i + γ x i √ d i + γ − x j d j + γ .</formula><p>Here, we prove several properties regarding its spectrum.</p><p>Lemma 1. (Non-negativity of∆ sym ) The augmented normalized Laplacian matrix is symmetric positive semidefinite.</p><p>Proof. The quadratic form associated with∆ sym is</p><formula xml:id="formula_16">x ∆ sym x = i x 2 i − i jã ij x i x j (d i + γ)(d j + γ) = 1 2   i x 2 i + j x 2 j − i j 2ã ij x i x j (d i + γ)(d j + γ)   = 1 2   i jã ij x 2 i d i + γ + j iã ij x 2 j d j + γ − i j 2ã ij x i x j (d i + γ)(d j + γ)   = 1 2 i jã ij x i √ d i + γ − x j d j + γ 2 ≥ 0<label>(13)</label></formula><p>Lemma 2. 0 is an eigenvalue of both ∆ sym and∆ sym .</p><p>Proof. First, note that v = [1, . . . , 1] is an eigenvector of ∆ associated with eigenvalue 0, i.e., ∆v = (D − A)v = 0.</p><p>Also, we have</p><formula xml:id="formula_17">that∆ sym =D −1/2 (D −Ã)D −1/2 = D −1/2 ∆D −1/2 . Denote v 1 =D 1/2 v, theñ ∆ sym v 1 =D −1/2 ∆D −1/2D1/2 v =D −1/2 ∆v = 0.</formula><p>Therefore, v 1 =D 1/2 v is an eigenvector of∆ sym associated with eigenvalue 0, which is then the smallest eigenvalue from the non-negativity of∆ sym . Likewise, 0 can be proved to be the smallest eigenvalues of ∆ sym .</p><p>Lemma 3. Let β 1 ≤ β 2 ≤ · · · ≤ β n denote eigenvalues of D −1/2 AD −1/2 and α 1 ≤ α 2 ≤ · · · ≤ α n be the eigenvalues ofD −1/2 AD −1/2 . Then,</p><formula xml:id="formula_18">α 1 ≥ max i d i γ + max i d i β 1 , α n ≤ min i d i γ + min i d i .<label>(14)</label></formula><p>Proof. We have shown that 0 is an eigenvalue of ∆ sym . Since D −1/2 AD −1/2 = I − ∆ sym , then 1 is an eigenvalue of D −1/2 AD −1/2 . More specifically, β n = 1. In addition, by combining the fact that Tr(D −1/2 AD −1/2 ) = 0 = i β i with β n = 1, we conclude that β 1 &lt; 0. By choosing x such that x = 1 and y = D 1/2D−1/2 x, we have that y 2 = i di di+γ x 2 i and mini di γ+mini di ≤ y 2 ≤ maxi di γ+maxi di . Hence, we use the Rayleigh quotient to provide a lower bound to α 1 :</p><formula xml:id="formula_19">α 1 = min x =1 x D −1/2 AD −1/2 x = min x =1 y D −1/2 AD −1/2 y (by replacing variable) = min x =1 y D −1/2 AD −1/2 y y 2 y 2 ≥ min x =1 y D −1/2 AD −1/2 y y 2 max x =1 y 2 (∵ min(AB) ≥ min(A) max(B) if min(A) &lt; 0, ∀B &gt; 0,</formula><p>and min</p><p>x =1</p><formula xml:id="formula_20">y D −1/2 AD −1/2 y y 2 = β 1 &lt; 0) = β 1 max x =1 y 2 ≥ max i d i γ + max i d i β 1 .</formula><p>One may employ similar steps to prove the second inequality in Equation 14.</p><p>Proof of Theorem 1. Note that∆ sym = I − γD −1 − D −1/2 AD −1/2 . Using the results in Lemma 3, we show that the largest eigenvalueλ n of∆ sym is</p><formula xml:id="formula_21">λ n = max x =1 x (I − γD −1 −D −1/2 AD −1/2 )x ≤ 1 − min x =1 γx D −1 x − min x =1 x D −1/2 AD −1/2 x = 1 − γ γ + max i d i − α 1 ≤ 1 − γ γ + max i d i − max i d i γ + max i d i β 1 &lt; 1 − max i d i γ + max i d i β 1 (γ &gt; 0 and β 1 &lt; 0) &lt; 1 − β 1 = λ n<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Details</head><p>Node Classification. We empirically find that on Reddit dataset for SGC, it is crucial to normalize the features into zero mean and univariate.</p><p>Training Time Benchmarking. We hereby describe the experiment setup of <ref type="figure">Figure 3</ref>. <ref type="bibr" target="#b10">Chen et al. (2018)</ref> benchmark the training time of FastGCN on CPU, and as a result, it is difficult to compare numerical values across reports. Moreover, we found the performance of FastGCN improved with a smaller early stopping window (10 epochs); therefore, we could decrease the model's training time. We provide the data underpinning <ref type="figure">Figure 3</ref> in <ref type="table" target="#tab_5">Table 8 and Table 9</ref>. Text Classification. <ref type="bibr" target="#b51">Yao et al. (2019)</ref> use one-hot features for the word and document nodes. In training SGC, we normalize the features to be between 0 and 1 after propagation and train with L-BFGS for 3 steps. We tune the only hyperparameter, weight decay, using hyperopt <ref type="bibr" target="#b5">(Bergstra et al., 2015)</ref> for 60 iterations. Note that we cannot apply this feature normalization for TextGCN because the propagation cannot be precomputed.</p><p>Semi-supervised User Geolocation. We replace the 4layer, highway-connection GCN with a 3rd degree propagation matrix (K = 3) SGC and use the same set of hyperparameters as <ref type="bibr" target="#b34">Rahimi et al. (2018)</ref>. All experiments on the GEOTEXT dataset are conducted on a single Nvidia GTX-1080Ti GPU while the ones on the TWITTER-NA and TWITTER-WORLD datasets are excuded with 10 cores of the Intel(R) Xeon(R) Silver 4114 CPU (2.20GHz). Instead of collapsing all linear transformations, we keep two of them which we find performing slightly better possibly due to . Despite of this subtle variation, the model is still linear.</p><p>Relation Extraction. We replace the 2-layer GCN with a 2nd degree propagation matrix (K = 2) SGC and remove the intermediate dropout. We keep other hyperparameters unchanged, including learning rate and regularization. Similar to <ref type="bibr" target="#b55">Zhang et al. (2018c)</ref>, we report the best validation accuracy with early stopping.</p><p>Zero-shot Image Classification. We replace the 6-layer <ref type="bibr">GCN (hidden size: 2048</ref><ref type="bibr">GCN (hidden size: , 2048</ref><ref type="bibr">GCN (hidden size: , 1024</ref><ref type="bibr">GCN (hidden size: , 1024</ref><ref type="bibr">GCN (hidden size: , 512, 2048</ref> baseline with an 6-layer MLP (hidden size: 512, 512, 512, 1024, 1024, 2048) followed by a SGC with K = 6. Following , we only apply dropout to the output of SGC. Due to the slow evaluation of this task, we do not tune the dropout rate or other hyperparameters. Rather, we follow the GCNZ code and use learning rate of 0.001, weight decay of 0.0005, and dropout rate of 0.5. We also train the models with ADAM <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2015)</ref> for 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiments</head><p>Random Splits for Citation Networks. Possibly due to their limited size, the citation networks are known to be unstable. Accordingly, we conduct an additional 10 experiments on random splits of the training set while maintaining the same validation and test sets.</p><p>Propagation choice. We conduct an ablation study with different choices of propagation matrix, namely:</p><p>Normalized Adjacency: S adj = D −1/2 AD −1/2 Random Walk Adjacency S rw = D −1 A Simplifying Graph Convolutional Networks  Aug. Normalized AdjacencyS adj =D −1/2ÃD−1/2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aug. Random WalkS rw =D −1Ã</head><p>First-Order Cheby S 1-order = (I + D −1/2 AD −1/2 )</p><p>We investigate the effect of propagation steps K ∈ {2..10} on validation set accuracy. We use hyperopt to tune L2-regularization and leave all other hyperparameters unchanged. <ref type="figure" target="#fig_3">Figure 4</ref> depicts the validation results achieved by varying the degree of different propagation matrices.</p><p>We see that augmented propagation matrices (i.e. those with self-loops) attain higher accuracy and more stable performance across various propagation depths. Specifically, the accuracy of S 1-order tends to deteriorate as the power K increases, and this results suggests using large filter coefficients on low frequencies degrades SGC performance on semi-supervised tasks.</p><p>Another pattern is that odd powers of K cause a significant performance drop for the normalized adjacency and random walk propagation matrices. This demonstrates how odd powers of the un-augmented propagation matrix use negative filter coefficients on high frequency information. Adding self-loops to the propagation matrix shrinks the spectrum such that the largest eigenvalues decrease from ≈ 2 to ≈ 1.5 on the citation network datasets. By effectively shrinking the spectrum, the effect of negative filter coefficients on high frequencies is minimized, and as a result, using odd-powers of K does not degrade the performance of augmented propagation matrices. For non-augmented propagation matriceswhere the largest eigenvalue is approximately 2 -negative coefficients significantly distort the signal, which leads to decreased accuracy. Therefore, adding self-loops constructs a better domain in which fixed filters can operate. Data amount. We also investigated the effect of training dataset size on accuracy. As demonstrated in <ref type="table" target="#tab_1">Table 11</ref>, SGC continues to perform similarly to GCN as the training dataset size is reduced, and even outperforms GCN when there are fewer than 5 training samples. We reason this study demonstrates SGC has at least the same modeling capacity as GCN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Feature (red) and filters (blue) spectral coefficients for different propagation matrices on Cora dataset (3rd feature).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>that SGC outperforms GCN with highway connections on GEOTEXT<ref type="bibr" target="#b13">(Eisenstein et al., 2010)</ref>, TWITTER-US<ref type="bibr" target="#b36">(Roller et al., 2012)</ref>, and TWITTER-WORLD<ref type="bibr" target="#b16">(Han et al., 2012)</ref> under<ref type="bibr" target="#b34">Rahimi et al. (2018)</ref>'s framework, while saving 30+ hours on TWITTER-WORLD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Validation accuracy with SGC using different propagation matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics of the citation networks and Reddit.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Nodes # Edges Train/Dev/Test Nodes</cell></row><row><cell cols="2">Cora Citeseer Pubmed 19, 717 2, 708 3, 327</cell><cell>5, 429 4, 732 44, 338</cell><cell>140/500/1, 000 120/500/1, 000 60/500/1, 000</cell></row><row><cell>Reddit</cell><cell>233K</cell><cell>11.6M</cell><cell>152K/24K/55K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test accuracy (%) averaged over 10 runs on citation networks. † We remove the outliers (accuracy &lt; 75/65/75%) when calculating their statistics due to high variance.</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell cols="3">Numbers from literature: GCN 81.5 GAT 83.0 ± 0.7 GLN 81.2 ± 0.1 AGNN 83.1 ± 0.1 LNet 79.5 ± 1.8 AdaLNet 80.4 ± 1.1 DeepWalk 70.7 ± 0.6 DGI 82.3 ± 0.6</cell><cell>70.3 72.5 ± 0.7 70.9 ± 0.1 71.7 ± 0.1 66.2 ± 1.9 68.7 ± 1.0 51.4 ± 0.5 71.8 ± 0.7</cell><cell>79.0 79.0 ± 0.3 78.9 ± 0.1 79.9 ± 0.1 78.3 ± 0.3 78.1 ± 0.4 76.8 ± 0.6 76.8 ± 0.6</cell></row><row><cell cols="3">Our experiments: GCN 81.4 ± 0.4 GAT 83.3 ± 0.7 FastGCN 79.8 ± 0.3 GIN 77.6 ± 1.1</cell><cell>70.9 ± 0.5 72.6 ± 0.6 68.8 ± 0.6 66.1 ± 0.9</cell><cell>79.0 ± 0.4 78.5 ± 0.3 77.4 ± 0.3 77.0 ± 1.2</cell></row><row><cell>LNet</cell><cell cols="2">80.2 ± 3.0  †</cell><cell>67.3 ± 0.5</cell><cell>78.3 ± 0.6  †</cell></row><row><cell>AdaLNet DGI SGC</cell><cell cols="4">81.9 ± 1.9  † 70.6 ± 0.8  † 77.8 ± 0.7  † 82.5 ± 0.7 71.6 ± 0.7 78.4 ± 0.7 81.0 ± 0.0 71.9 ± 0.1 78.9 ± 0.0</cell></row><row><cell cols="5">Table 3. Test Micro F1 Score (%) averaged over 10 runs on Red-</cell></row><row><cell cols="5">dit. Performances of models are cited from their original papers.</cell></row><row><cell cols="2">OOM: Out of memory.</cell><cell></cell><cell></cell></row><row><cell>Setting</cell><cell></cell><cell>Model</cell><cell></cell><cell>Test F1</cell></row><row><cell cols="2">Supervised</cell><cell cols="2">GaAN SAGE-mean SAGE-LSTM SAGE-GCN FastGCN GCN</cell><cell>96.4 95.0 95.4 93.0 93.7 OOM</cell></row><row><cell cols="2">Unsupervised</cell><cell cols="2">SAGE-mean SAGE-LSTM SAGE-GCN DGI</cell><cell>89.7 90.7 90.8 94.0</cell></row><row><cell cols="2">No Learning</cell><cell cols="3">Random-Init DGI 93.3 SGC 94.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Test Accuracy (%) on text classification datasets. The numbers are averaged over 10 runs.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Model Test Acc. ↑ Time (seconds) ↓</cell></row><row><cell>20NG</cell><cell>GCN SGC</cell><cell cols="2">87.9 ± 0.2 88.5 ± 0.1</cell><cell>1205.1 ± 144.5 19.06 ± 0.15</cell></row><row><cell>R8</cell><cell>GCN SGC</cell><cell cols="2">97.0 ± 0.2 97.2 ± 0.1</cell><cell>129.6 ± 9.9 1.90 ± 0.03</cell></row><row><cell>R52</cell><cell>GCN SGC</cell><cell cols="2">93.8 ± 0.2 94.0 ± 0.2</cell><cell>245.0 ± 13.0 3.01 ± 0.01</cell></row><row><cell>Ohsumed</cell><cell>GCN SGC</cell><cell cols="2">68.2 ± 0.4 68.5 ± 0.3</cell><cell>252.4 ± 14.7 3.02 ± 0.02</cell></row><row><cell>MR</cell><cell>GCN SGC</cell><cell cols="2">76.3 ± 0.3 75.9 ± 0.3</cell><cell>16.1 ± 0.4 4.00 ± 0.04</cell></row><row><cell cols="5">Table 5. Test accuracy (%) within 161 miles on semi-supervised</cell></row><row><cell cols="5">user geolocation. The numbers are averaged over 5 runs.</cell></row><row><cell>Dataset</cell><cell></cell><cell>Model</cell><cell cols="2">Acc.@161↑</cell><cell>Time ↓</cell></row><row><cell>GEOTEXT</cell><cell></cell><cell>GCN+H SGC</cell><cell cols="2">60.6 ± 0.2 61.1 ± 0.1</cell><cell>153.0s 5.6s</cell></row><row><cell>TWITTER-US</cell><cell></cell><cell>GCN+H SGC</cell><cell cols="2">61.9 ± 0.2 62.5 ± 0.1</cell><cell>9h 54m 4h 33m</cell></row><row><cell cols="2">TWITTER-WORLD</cell><cell>GCN+H SGC</cell><cell cols="2">53.6 ± 0.2 2d 05h 17m 54.1 ± 0.2 22h 53m</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Training time (seconds) of graph neural networks on Citation Networks. Numbers are averaged over 10 runs.</figDesc><table><row><cell>Models</cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell></row><row><cell cols="2">GCN GAT FastGCN GIN LNet AdaLNet 10.15 0.49 63.10 2.47 2.09 15.02 DGI 21.24 SGC 0.13</cell><cell>0.59 118.10 3.96 4.47 49.16 31.80 21.06 0.14</cell><cell>8.31 121.74 1.77 26.15 266.47 222.21 76.20 0.29</cell></row><row><cell cols="4">Table 9. Training time (seconds) on Reddit dataset.</cell></row><row><cell cols="2">Model</cell><cell cols="2">Time(s) ↓</cell></row><row><cell cols="3">SAGE-mean SAGE-LSTM 486.53 78.54 SAGE-GCN 86.86 FastGCN 270.45 SGC 2.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>Test accuracy (%) on citation networks (random splits). We remove the outliers (accuracy &lt; 0.7/0.65/0.75) when calculating their statistics due to high variance. LNet 74.23 ± 4.50 † 67.26 ± 0.81 † 77.20 ± 2.03 † AdaLNet 72.68 ± 1.45 † 71.04 ± 0.95 † 77.53 ± 1.76 † GAT 82.29 ± 1.16 72.6 ± 0.58 78.79 ± 1.41 SGC 80.62 ± 1.21 71.40 ± 3.92 77.02 ± 1.62</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Ours: GCN GIN</cell><cell>80.53 ± 1.40 76.94 ± 1.24</cell><cell>70.67 ± 2.90 66.56 ± 2.27</cell><cell>77.09 ± 2.95 74.46 ± 2.19</cell></row></table><note>†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 .</head><label>11</label><figDesc>Validation Accuracy (%) when SGC and GCN are trained with different amounts of data on Cora. The validation accuracy is averaged over 10 random training splits such that each class has the same number of training examples.</figDesc><table><row><cell cols="2"># Training Samples SGC GCN</cell></row><row><cell>1 5 10 20 40 80</cell><cell>33.16 32.94 63.74 60.68 72.04 71.46 80.30 80.16 85.56 85.38 90.08 90.44</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is supported in part by grants from the National Science Foundation (III-1618134, III-1526012,  IIS1149882, IIS-1724282, and TRIPODS-1740822), the Office of Naval Research DOD (N00014-17-1-2175), Bill and Melinda Gates Foundation, and Facebook Research. We are thankful for generous support by SAP America Inc. Amauri Holanda de Souza Jr. thanks CNPq (Brazilian Council for Scientific and Technological Development) for the financial support. We appreciate the discussion with Xiang Fu, Shengyuan Hu, Shangdi Yu, Wei-Lun Chao and Geoff Pleiss as well as the figure design support from Boyi Li.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>N-Gcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08888</idno>
		<title level="m">Multi-scale graph convolution for semi-supervised node classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="209" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 19th International Conference on Computational Statistics</title>
		<meeting>19th International Conference on Computational Statistics</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2014)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for non-attribute graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Classifier and exemplar synthesis for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06423</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fastgcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">;</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A latent variable model for geographic lexical variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1277" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrapped graph diffusions: Exposing the power of nonlinearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eliav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<idno>10:1-10:19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Measurement and Analysis of Computing Systems</title>
		<meeting>the ACM on Measurement and Analysis of Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">;</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geolocation prediction in social media data by finding location indicative words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1045" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Alvey Vision Conference</title>
		<meeting>the 4th Alvey Vision Conference</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking knowledge graph propagation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11724</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2019)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention models in graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1801.07606</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Label efficient semi-supervised learning via graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2019)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD&apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD&apos;14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised user geolocation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2009" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Electronic spectra from TDDFT and machine learning in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84111</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised text-based geolocation using language models on an adaptive grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speriosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rallapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1500" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Principles of neurodynamics: Perceptrons and the theory of brain mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
			<publisher>Cornell Aeronautical Lab Inc</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A 3x3 isotropic gradient operator for image processing. A talk at the Stanford Artificial Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="page" from="271" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Graph InfoMax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2019)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech. and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;2019)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI&apos;19)</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence (AAAI&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI&apos;2018)</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI&apos;2018)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Thrun, S., Saul, L. K., and Schölkopf, B.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
