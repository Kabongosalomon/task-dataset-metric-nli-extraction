<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Essentials for Class Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Galesso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Essentials for Class Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contemporary neural networks are limited in their ability to learn from evolving streams of training data. When trained sequentially on new or evolving tasks, their accuracy drops sharply, making them unsuitable for many real-world applications. In this work, we shed light on the causes of this well known yet unsolved phenomenon -often referred to as catastrophic forgetting -in a classincremental setup. We show that a combination of simple components and a loss that balances intra-task and intertask learning can already resolve forgetting to the same extent as more complex measures proposed in literature. Moreover, we identify poor quality of the learned representation as another reason for catastrophic forgetting in class-IL. We show that performance is correlated with secondary class information (dark knowledge) learned by the model and it can be improved by an appropriate regularizer. With these lessons learned, class-incremental learning results on CIFAR-100 and ImageNet improve over the state-of-the-art by a large margin, while keeping the approach simple.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to learn from continuously evolving data is important for many real-world applications. Latest machine learning models, especially artificial neural networks, have shown great ability to learn the task at hand, but when confronted with a new task, they tend to override the previous concepts. Deep networks suffer heavily from this catastrophic forgetting <ref type="bibr" target="#b18">[19]</ref> when trained with a sequence of tasks, impeding continual or lifelong learning.</p><p>In this work, we focus on class-incremental learning (class-IL) <ref type="bibr" target="#b22">[23]</ref>. It is one of the three scenarios of continual learning as described in <ref type="bibr" target="#b26">[27]</ref>, where the objective is to learn a unified classifier over incrementally occurring sets of classes. Since all the incremental data cannot be retained for unified training, the major challenge is to avoid forgetting previous classes while learning new ones.</p><p>The three crucial components of a class-IL algorithm include a memory buffer to store few exemplars from old classes, a forgetting constraint to keep previous knowledge while learning new tasks, and a learning system that balances old and new classes. Although several methods have been proposed to address each of these components, there is not yet a common understanding of best practices.</p><p>Prabhu et al. <ref type="bibr" target="#b21">[22]</ref> provides an overview over current state of continual learning methods for classification. It shows that a simple greedy balanced sampler-based approach (GDumb) can outperform various specialized formulations in most of the continual learning settings, however, it finds class-IL particularly challenging. In this work, we propose a complementary approach to <ref type="bibr" target="#b21">[22]</ref> for class-IL, where softmax outputs are masked appropriately with data balancing to outperform previous sophisticated approaches.</p><p>Contributions. We propose a compositional class-IL (CCIL) model that isolates the underlying reasons for catastrophic forgetting in class-IL and combines the most simple and most effective components to build a robust base model. It employs plain knowledge distillation <ref type="bibr" target="#b10">[11]</ref> as a forgetting constraint and selects exemplar samples simply randomly. For the loss evaluation, we propose important changes in the output normalization. This base model already exceeds state-of-the-art results by a good margin.</p><p>In addition, we study the influence of the learned representation's properties on forgetting and show that the degree of feature specialization (overfitting) correlates with the degree of forgetting. We study some common regularization techniques and show that only those that keep, or even improve, the so-called secondary class information -also referred as dark knowledge by <ref type="bibr" target="#b10">[11]</ref> -have a positive influence on class-incremental learning, whereas others make things much worse. The source code of this paper is available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>iCaRL was the first approach that formally introduced the class-IL problem <ref type="bibr" target="#b22">[23]</ref>. iCaRL is a decoupled approach for feature representation learning and classifier learning. It alleviates catastrophic forgetting via knowledge distillation and a replay-based approach. Later Castro et al. <ref type="bibr" target="#b2">[3]</ref> extended it to an end-to-end learning model based on a com-bination of distillation and cross-entropy loss to show improved results over iCaRL. Successive works usually dedicated their contribution to one of the three components in class-IL.</p><p>Exemplar selection: Replay-based approaches have been shown to be quite effective in mitigating catastrophic forgetting. Typically, a memory buffer is allocated to store exemplar samples of old classes, which are replayed while learning a new task to mitigate forgetting. Many works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref> use herding heuristics <ref type="bibr" target="#b28">[29]</ref> for exemplar selection. Herding selects and retains samples closest to the mean sample for each class. Liu et al. <ref type="bibr" target="#b16">[17]</ref> parameterized the exemplars to optimize them jointly with the model. Iscen et al. <ref type="bibr" target="#b12">[13]</ref> introduced a memory efficient approach to store feature descriptors instead of images. In our work, we simply sample from each class randomly to compile the exemplar set.</p><p>Forgetting-constraint: Knowledge distillation (KD) was first introduced by Li et al. <ref type="bibr" target="#b15">[16]</ref> for multi-task incremental learning. Thereafter, various works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref> have adopted it in class-IL to restore previous knowledge. Lately, several works have proposed new forgetting constraints with an objective to preserve the structure of old-class embeddings. Hou et al. <ref type="bibr" target="#b11">[12]</ref> proposed the usage of feature-level distillation by penalizing change is the feature representation from the old model. Yu et al. <ref type="bibr" target="#b31">[32]</ref> utilized an embedding network to rectify the semantic drift, Tao et al. <ref type="bibr" target="#b24">[25]</ref> proposed a Hebbian graph-based approach to retain the topology of the feature space. In this work, we utilize plain knowledge distillation, which is based on logits to avoid forgetting.</p><p>Bias removal methods: Various works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> have pointed out that class-imbalance between old and new classes creates a bias in the class weight vectors in the last linear layer, due to which the network predictions are biased towards new classes. To rectify this bias, Wu et al. <ref type="bibr" target="#b29">[30]</ref> trained an extra bias-correction layer using the validation set, Belouadah et al. <ref type="bibr" target="#b1">[2]</ref> proposed to rectify the final activations using the statistics of the old task predictions, Zhao et al. <ref type="bibr" target="#b33">[34]</ref> adjusted the norm of new class-weight vectors to those of the old class-weight vectors, and Hou et al. <ref type="bibr" target="#b11">[12]</ref> applied cosine normalization in the last layer. The focus of these works is limited to the bias in the last layer, but ultimately catastrophic forgetting is an issue that affects the entire network: class imbalance causes the model to overfit to the new task, deteriorating the performance on the old ones. Some works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> also fine-tune the model to avoid overfitting to the current task. We propose a learning system that resolves this bias without the need of any post-processing, by fixing the underlying issues; see Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Class-Incremental Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>The objective of class-incremental learning (class-IL) is to learn a unified classifier from a sequence of data from different classes. Data arrives incrementally as a batch of per-class sets X i.e. (X 1 , X 2 , ..., X t ), where X y contains all images from class y. Learning from a batch of classes can be considered as a task T . At each incremental step, the data for the new task T i arrives, which contains samples of the new set of classes. At each step, complete data is only available for new classes X i.e. (X s+1 , ..., X t ). Only a small amount of exemplar data P old i.e. (P 1 , ..., P s ) from previous classes i.e. (X 1 , ..., X s ) is retained in a memory buffer of limited size. The model is expected to classify all the classes seen so far.</p><p>The problem definition with strictly separated batches may appear a bit specific. In many practical applications, the data will arrive in a more mixed-up fashion. However, this strict protocol allows the comparison of techniques and it covers the key issues with class-incremental learning. Improvements on this protocol also serve less strict applied settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Metrics for Class-IL</head><p>Class-IL models are evaluated using three metrics: average incremental accuracy, forgetting rate and feature retention. After each incremental step, all classes seen so far are evaluated using the latest model. After N incremental tasks, the accuracy A n over all (N + 1) steps is averaged and reported. It is termed as average incremental accuracy (Avg Acc), introduced by Rebuffi et al. <ref type="bibr" target="#b22">[23]</ref>. We also evaluate the forgetting rate F proposed by Liu et al. <ref type="bibr" target="#b16">[17]</ref>. The forgetting rate measures the performance drop on the first task. It is the accuracy difference on the classes of the first task X 1:s test , using Θ 0 and Θ N . Therefore, it is independent of the absolute performance on the initial task T 0 . We introduce another metric, referred as R φ , to measure retention in the feature extractor φ(·). It measures how much information is retained in the feature extractor while learning the tasks incrementally as compared to a jointly trained model. To measure R φ : after the final incremental step, parameters of the feature extractor are frozen and the last linear layer is learned using all the data from all the classes. R φ is the accuracy difference between this model and a model where the whole network is trained on all the classes with complete data access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Basic Class-IL Framework</head><p>The network model Θ consists of a feature extractor φ(·) and a fully-connected layer f c(·) for classification. Similar to a standard multi-class classifier, the output logits o are processed through a softmax activation function σ(·) before Exemplar selection: We compile the exemplar set by randomly selecting an equal number of samples (m) for each class. The samples are sorted in ascending order according to the distance from the mean of the feature vectors µ i for each class separately. Since the size of the limited memory is fixed (K), some samples of old classes are re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: CCIL: IncrementalStep</head><p>Input: X = (X s+1 , ..., X t ), P s = (P 1 , ..., P s ) // new classes data, old exemplar sets Input: K, Θ s ,Θ s // memory size, current model, frozen current model Output: Θ t // model trained on t classes 1 m ← K/t // number of exemplars per class 2 Θ t ← Θ s // add output nodes for new classes 3 P ← UpdateExemplarSets(X ; P s , m, Θ s ) 4 for (x, y) ∈ X do // update for mini-batch data in X Forgetting constraint: Our model uses knowledge distillation as the forgetting constraint. Knowledge distillation penalizes the change with respect to the output of the old model (Θ s ) using KL-divergence, thus preserving the network's knowledge about the old classes. The distillation loss (L KD ) is computed for the exemplar sets (P) as well as for samples from the new classes (X ). The final loss for our CCIL model is a combination of cross-entropy loss L CE for classification and distillation loss L KD for mitigating catastrophic forgetting as shown in Algorithm 1-Line 15.</p><p>Learning system: We propose a new compositional learning system which addresses the weight-bias issue in class-IL. The proposed loss isolates inter-task and intra-task learning for a balanced processing of data by appropriately normalizing the output logits. The task-agnostic parts are shared to yield improved efficiency. The details are presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Compositional Learning System</head><p>For each gradient update, the CCIL model receives data in separate batches from the set of new classes X and the set of exemplars P. P is the updated exemplar set which also includes equal size of exemplars from the current new classes. (see Algorithm 1-Line 3) Instead of merging the batches, we propose to compute two separate losses for X and P mini-batches:</p><formula xml:id="formula_0">L X = L CE X + λ * L KD X (1) L P = L CE P + λ * L KD P<label>(2)</label></formula><p>Intra-task Learning: The classification loss for the new classes (L CE X ) is computed using a dedicated softmax function σ new comprising logits of new classes only ( <ref type="figure" target="#fig_0">Figure 1b</ref>) computed as:</p><formula xml:id="formula_1">L CE X = − t i=s+1 y[i] · log(p new [i])<label>(3)</label></formula><p>for</p><formula xml:id="formula_2">(x, y) ∈ X , where p new = σ new (o new ), o = Θ t (x)</formula><p>and output logits comprise o = {o old , o new }. This allows the classifier weights for the new classes to be learned independently of the previous classes -while sharing the feature extractor, thus effectively eliminating the weight bias. Distillation loss (L KD X ) is always computed using σ old (see <ref type="figure" target="#fig_0">Figure 1b</ref>), since output of new network p old = σ old (o old ) are compared against the output of previous modelp = σ old (Θ s (x)) as:</p><formula xml:id="formula_3">L KD X = D KL (p||p old )<label>(4)</label></formula><p>In case of a unified softmax, the weights of the old classes are suppressed by the larger amount of new class samples during training. A similar analysis has been shown by <ref type="bibr" target="#b0">[1]</ref> for a fine-tuning setup.</p><p>Inter-task Learning: The separate softmax helps intratask learning for the new classes, but this does not yet discriminate the new from the old classes. For inter-task learning, we plan a balanced interaction between the samples of old and new classes. We compile an exemplar set P which contains equal numbers of samples from all classes including old and new classes. However small, such exemplar set enables the model to capture the inter-task relationship through the loss L CE P , which uses a combined softmax function σ evaluated on all classes (see <ref type="figure" target="#fig_0">Figure 1b</ref>).</p><formula xml:id="formula_4">L CE P = − t i=1 y [i] · log(q[i])<label>(5)</label></formula><p>for (x , y ) ∈ P, where q = σ(o ) and o = Θ t (x ). The distillation loss is computed similar to Eq. 4,</p><formula xml:id="formula_5">L KD P = D KL (q||q old )<label>(6)</label></formula><p>whereq = σ old (Θ s (x )) and q old = σ old (o old ). This exemplar set is compiled before learning the incremental task, contrary to previous works, where it is always compiled after the incremental step. <ref type="figure" target="#fig_0">Figure 1</ref> shows how the loss terms are calculated using a separate softmax function 1b and also compares it to the unified softmax 1a used in previous works.</p><p>Transfer Learning: We observed that a separate softmax does not remove the bias completely. Another cause for unbalanced class-weight vectors, and catastrophic forgetting in general, is the change in the data distribution between different tasks. We hypothesize that the effect of this distribution shift in the training data is more harmful to the previous knowledge when the transfer learning from old to new classes is poor, resulting in strong alteration of the parameters of the network. We propose to reduce the learning rate for the incremental steps as a simple way to improve transfer learning and mitigate the adverse effect of distribution shift. This further helps reduce the weight bias. The reduced learning rate on incremental steps depends on the scale and relevance of features learned in the base task, therefore it is determined experimentally. Although lowering the learning rate is a standard technique when finetuning a network on a new dataset, its importance is underestimated and often missing in incremental learning works. We experimentally show its importance in ablation studies (Section 6.2).  Intuitively, poorly transferable embeddings will force the model to alter its parameters significantly in order to learn new concepts. This destroys the knowledge accumulated for the previous tasks. In this section, we explore this novel direction-aiming to learn robust representations that are transferable to a new task and effectively retain previous knowledge in class-IL. In particular, we study the detrimental effects of overfitting and loss of secondary class information. We find that: 1) both phenomena strongly correlate with catastrophic forgetting; 2) regularization methods can significantly improve robustness against forgetting, but only as long as they enhance the secondary class information of the learned model.</p><formula xml:id="formula_6">for i = s + 1, ..., t do 5 P i ← (p 1 , ..., p m ) ⊂ X i ) // randomly pick m samples 6 µ i ← 1 m m j=1 φ(p j ) // mean feature / * sort exemplars based on distance from µ i * / 7 for k = 1, ..., m do 8 p k ← arg min ||µ i − φ(p k )||</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Measuring the Quality of Secondary Logits</head><p>Secondary information captures semantic relationship between the target and non-target classes. In literature, the term secondary information is interchangeably used to de- note the non-target and non-maximum scores of a classifier <ref type="bibr" target="#b30">[31]</ref>. Here, for evaluation purposes, the term denotes the non-maximum scores produced by the networks. When applying the maximum operation to the scores predicted by a classifier, part of the information produced by the model is discarded. For each individual sample this information represents the model's belief about the semantic nature of the image, in relation to the other classes. It is important to learn this secondary information, such that the model can re-use it to learn new classes with least modification to previous concepts. We argue that semantically similar classes should lie closer in the representation space as compared to the dissimilar classes since they share more features, and higher secondary information is an indicator of such an efficient non-redundant feature space. Appendix includes an analysis on feature representations to support this argument. No proper annotations exist for secondary information, therefore we define a proxy evaluation objective, exploiting the coarse-labeling of the CIFAR-100 dataset, which partitions the 100 fine-classes into 20 superclasses. The 5 classes belonging to each superclass are mostly semantically related, and have been previously used for evaluating secondary information <ref type="bibr" target="#b30">[31]</ref>. As a proxy evaluation measure for secondary class information we propose to use the classification performance on the superclasses, restricting the network output to the non-maximum logits. We define two new metrics for this purpose: Secondary Superclass NLL and Secondary Superclass Accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Secondary Superclass-NLL (SS-NLL): Negative Log</head><p>Likelihood is a commonly used cost function for classification, also known as Cross-Entropy Loss. Here we compute the NLL induced by the secondary (non-maximum) logits on the superclass classification problem. Given a set of superclasses S, we can group the fine-grained classes into sub- sets C according to their coarse-label, and compute:</p><formula xml:id="formula_7">Epoch SS- SS- F ↓ R φ ↓ NLL ↓ Acc</formula><formula xml:id="formula_8">SS-N LL(x, y) = − j∈S 1 Cj (y) log k∈Cjσ k f (x) ,<label>(7)</label></formula><p>where 1 Cj (y) is an indicator function which evaluates to 1 if the true class y belongs to superclass j,σ is a softmax function over the secondary fine-logits (i.e. it suppresses the maximum logit). The network prediction (logits) is denoted as f (x). A lower SS-NLL indicates better superclass classification and thus higher secondary information quality.</p><p>Secondary Superclass-Accuracy (SS-Acc): Secondary superclass accuracy computes the percentage of correct superclass predictions. As for SS-NLL, the largest logit score is excluded from the prediction to focus the measure on the quality of secondary information. Higher SS-Acc values indicate higher quality of the secondary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Forgetting starts before the incremental step</head><p>In this section, we study how the quality of the representations learned during the initial base task correlates with incremental learning performance. We experimentally show how a decline in quality of the learned features -measured as overfitting and loss of secondary information -leads to higher catastrophic forgetting, motivating our following search for a suitable regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment details:</head><p>We set up a standard class-IL experiment (with 5 incremental tasks) on CIFAR-100, using a ResNet-32 model. The initial base network is trained for up to 500 epochs. We employ a SGD optimizer with a base learning of 1e-1, weight decay of 5e-4 and momentum 0.9. We use a step learning rate schedule, where the learning rate is divided by 10 at 60 th and 90 th epochs.  Analysis: <ref type="figure" target="#fig_3">Figure 2</ref> shows that the validation loss (red curve) starts increasing after about 100 epochs, showing an overfitting effect. Thereafter, we perform five different class-IL experiments, each based on a different snapshot of the base network (every 100 th epoch). As the validation loss of the snapshot increases, incremental learning performance of the corresponding class-IL model drops (green curve), and both forgetting rate (F ) and feature retention metric (R φ ) worsen ( <ref type="table" target="#tab_1">Table 1</ref>). The worsening R φ metric indicates that the issue is rooted in the feature representations, and cannot be mitigated by acting on the last layer bias. Along with these metrics, we observe that overfitting causes the quality of secondary information to deteriorate (SS-Acc decreases and the SS-NLL increases, <ref type="table" target="#tab_1">Table 1</ref>). This loss of secondary information could also be linked to increasing overconfidence of the network, measured as Expected Calibration Error (ECE) <ref type="bibr" target="#b8">[9]</ref> (details in Appendix A).</p><p>These results indicate that: 1) the quality of the features learned during the first base task influences the performance of the class-IL model, and as such it should be expressly addressed. 2) secondary information can be considered as an indicator of the features' quality and their fitness for incremental learning. In the next section we will show experimental evidence in support of these hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analyzing Catastrophic Forgetting with Regularization</head><p>Having established a link between early feature quality and catastrophic forgetting, we hypothesize that the application of adequate regularization techniques can improve model performance on the task at hand. We apply four common regularization techniques to our CCIL model: self-distillation <ref type="bibr" target="#b6">[7]</ref>, data-augmentation (including cropping, cutout <ref type="bibr" target="#b5">[6]</ref> and an extended set of AutoAugment <ref type="bibr" target="#b3">[4]</ref> policies), label smoothing <ref type="bibr" target="#b23">[24]</ref>, and mixup <ref type="bibr" target="#b32">[33]</ref>. All these regularizers have been shown to improve generalization on the held-out validation data. We report details about the application of said regularization methods in Appendix A.</p><p>Self-distillation Self-distillation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref> is a form of knowledge distillation in which the teacher and student networks have the same architecture. It can be applied iteratively, in generations: at each generation a copy of the current student becomes the new teacher, with proven positive effects on generalization.</p><p>Data augmentation Augmentation is one of the most widespread regularization techniques for neural networks, especially in computer vision. A well designed data augmentation routine is key to obtaining good results on the held-out dataset. We sample randomly from a pool of augmentation policies which contain pairs of different geometric and color transformations, similarly to <ref type="bibr" target="#b3">[4]</ref>.</p><p>Label smoothing Label smoothing <ref type="bibr" target="#b23">[24]</ref> acts on the crossentropy loss for classification by interpolating the one-hot labels with a uniform distribution over the possible classes. This technique has been shown to improve generalization and reducing overconfidence of classification models <ref type="bibr" target="#b23">[24]</ref>.</p><p>Mixup Mixup <ref type="bibr" target="#b32">[33]</ref> is an operation that generates training samples for classification by linearly combining pairs of existing samples -image and label. Mixup has successfully been used as a form of data augmentation in image classification, improving generalization and calibration <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>We analyse above discussed metrics for each of these regularization techniques. <ref type="table" target="#tab_0">Table 2</ref> shows the Average Accuracy after finishing the last incremental step, secondary information quality of the first task model, forgetting rate, feature retention (Section 3.2) and expected calibration error <ref type="bibr" target="#b8">[9]</ref>. We can divide the regularization methods into two groups: the ones which improve class-IL performance (selfdistillation, augmentation) and the ones which harm it (label smoothing, mixup). The first group also shows consistent improvements in secondary information and reduction in forgetting, with augmentation performing the best across L2-norm of weights  all metrics -by a significant margin. In the second group, label smoothing harms secondary information the most. It has been observed that label smoothing encourages representations to be closer to their respective class centroid and equidistant to the other class centroids <ref type="bibr" target="#b20">[21]</ref>, and this comes at the expense of inter-class sample relationships, i.e., secondary information. Mixup also harms the quality of secondary information: we believe this is because it artificially forces arbitrary distances between classes, which modifies the natural output distribution -similarly to label smoothing. Interestingly, all regularizers improve network calibration, but ECE is not a good indicator of class-IL performance, unlike secondary information, shown in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>In summary, label smoothing and mixup -despite their proven regularization effects -harm secondary class information and have clear negative consequences for classincremental learning. On the other hand, regularization methods that enhance secondary class information (self distillation and data augmentation) boost the average incremental accuracy. Analogously to the analysis of Section 5.2 we show that the quality of secondary information negatively correlates to the forgetting rate <ref type="table" target="#tab_0">(Table 2)</ref>, further indicating the importance of secondary class information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Training Details Datasets</head><p>We conduct experiments on CIFAR100 <ref type="bibr" target="#b13">[14]</ref>, ImageNet-100 Subset <ref type="bibr" target="#b4">[5]</ref> and full ImageNet datasets. CIFAR-100 contains 60K images from 100 classes of size 32 × 32, with 50K images for training and 10K for evaluation. The ImageNet-100 dataset has 100 randomly sampled classes (using Numpy seed:1993) from ImageNet. The base CCIL model uses default data augmentation including random cropping and horizontal flipping for CIFAR-100, and resized-random cropping and horizontal flipping for Ima-geNet datasets. All the randomization seeds are selected following the experiments in previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Benchmark protocol We follow the protocol used in previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. The protocol involves learning of 1 initial base task followed by N incremental tasks. We evaluate with two incremental settings: where the model learns N = 5 and N = 10 incremental tasks. For CIFAR-100 and ImageNet-100, 50 classes are selected as the base classes for the initial task and the remaining classes are equally divided over the incremental steps. A similar format is followed for ImageNet with 500 base classes. Exemplar memory size is set to K = 2k for 100 class datasets and K = 20k for the full ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We use a 32-layer ResNet <ref type="bibr" target="#b9">[10]</ref> for CIFAR-100 dataset, and a 18-layer ResNet for ImageNet-100 and ImageNet datasets. The last layer is cosine normalized following the recommendations of <ref type="bibr" target="#b11">[12]</ref>. We will publish the code after acceptance. Hyperparameter details are included in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Studies</head><p>Elements of the compositional learning system We evaluate the contributions of each element in the proposed learning system by training multiple class-IL models featuring them. The incremental learning in these experiments is conducted in two settings -in a simple fine-tuning setup (without distillation), in order to single out the effects of the proposed changes and with knowledge distillation loss included. In <ref type="figure" target="#fig_5">Figure 3a &amp; 3b</ref> we compare the average L2 norm of the class weight vectors for old and new classes after 5 incremental training steps, while in <ref type="figure" target="#fig_5">Figure 3c</ref> we provide the average accuracies of the respective models. We notice a major difference in the weight norms of old and new classes for the default combined softmax (Comb) setting <ref type="figure" target="#fig_0">(Figure 1a</ref>). Using separate-softmax (Sep) substantially reduces this difference and improves class-IL performance, but does not resolve the problem completely. Lower learning rate (Comb+LowLR) also removes the bias and improves the performance, although to a lesser extent. When   <ref type="table">Table 4</ref>: Comparing average incremental accuracy computed using different methods on CIFAR-100, ImageNet-100 and ImageNet dataset. *as reported in <ref type="bibr" target="#b11">[12]</ref> both approaches are combined (Sep+Low-LR), this bias is largely resolved and the best class-IL results are produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Drawing parallels with iCaRL We compare different components of our CCIL model with the first baseline approach (iCaRL) proposed by <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_6">Table 3</ref> summarizes these changes. We first isolate the contributions of some follow-up methods by creating another baseline as iCaRL++. It consists of a (1) cosine-normalized layer (cos) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>, where the features and class-weight vectors in the final layer are normalized to lie in a high-dimensional sphere. It helps in removing the remaining weight bias during inference, and (2) adaptive weighting (AW), where the weight of the distillation loss increases with incremental steps. AW was previously introduced in <ref type="bibr" target="#b11">[12]</ref>, which helps in adaptive balancing of classification and distillation loss (more details are included in the Appendix). The last row shows that replacing the combined-softmax (comb) with the proposed separate-softmax (sep) and reducing the learning rate (LowLR) yields a major improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison to SOTA</head><p>Results for CIFAR-100, ImageNet-100 and ImageNet datasets are shown in <ref type="table">Table 4</ref>. We report the upper bound 'Joint-training', where at every incremental step all the data for the classes seen until then is accessible. The simple CCIL model compares favorably to previous results on all datasets, especially on larger datasets like ImageNet-1k. The regularized CCIL-SD closes the gap to joint training further and achieves state-of-the-art performance across all datasets. Since the CCIL model is based only on simple components, we believe that the application of advanced methods for mitigating forgetting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> and more informative exemplar selection <ref type="bibr" target="#b16">[17]</ref> can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We presented a straightforward class-incremental learning system that focuses on the essential components and already exceeds the state of the art without integrating sophisticated modules. This makes it a good base model for future research on advancing class-incremental learning.</p><p>Moreover, we showed that countering catastrophic forgetting during the incremental step is not enough: the quality of the feature representation prior to the incremental step considerably determines the amount of forgetting. This suggests that representation learning is a promising direction to maximize also incremental performance. In this regard we showed that boosting secondary information is key to improve the transferability of features from old to new tasks without forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Details</head><p>This section includes details concerning experiments included in the paper: Adapting weighting (AW) details, hyperparameter details and standard deviation for all the results. We report all the evaluation metrics averaged over 3 run trials (unless mentioned otherwise) to capture the variance in the class-IL training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Adaptive Weighting (AW)</head><p>In each incremental step, training a network comprises a classification loss and a distillation loss to preserve knowledge about previous classes. Our baseline contains an adaptive weighting function λ (similar to <ref type="bibr" target="#b11">[12]</ref>) between two losses:</p><formula xml:id="formula_9">λ = λ base C n + C o C n 2/3<label>(8)</label></formula><p>,where C n denotes number of new classes, C o denotes number of old classes, λ base is fixed constant for each method. It dynamically increases weightage on preserving old knowledge as incremental training continues. It improves the baseline model by 0.45% for 5 task experiment on CIFAR-100. λ base = 5 is set for CIFAR-100, λ base = 20 for ImageNet-100 and λ base = 600 for Ima-geNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experiment Details</head><p>Dataset: CIFAR-100 classes are shuffled using a fixed seed (Numpy <ref type="bibr" target="#b27">[28]</ref> seed:1993) across all methods for fair comparison. The ImageNet-100 dataset has 100 randomly sampled classes (using Numpy seed:1993) from ImageNet and further shuffled (using Numpy seed:1993). It contains around 128K images of size 224 × 224 for training and 5K images for evaluation. ImageNet-1k classes are also shuffled using a Numpy seed:1993.</p><p>Optimizer: On CIFAR-100, the base network is trained for 120 epochs using a cosine learning rate schedule, where the base learning rate is 1e-1. Subsequent N tasks are trained for 240 epochs with a base learning rate of 1e-2. The learning rate is decayed until 1e-4. We use a batch size of 100 for CIFAR-100 experiments. Networks for CIFAR-100 dataset is optimized using the SGD optimizer with a momentum of 0.9 and weight decay of 5e-4.</p><p>For ImageNet-100, the network is trained for 70 epochs using a step learning rate schedule, where the base learning rate is 1e-1 for the base task and 1e-2 for the subsequent N tasks. The base learning rate is divided by 10 at {30, 60} epochs. For ImageNet, base task is trained for 70 epochs following a step learning rate, where the base learning is 1e-1. The base learning rate is divided by 10 at {30, 60} epochs.</p><p>The incremental task is trained for 40 epochs following a step learning rate, where the base learning rate starts from 1e-2. The base learning rate is divided by 10 at {25, 35} epochs. Networks for ImageNet datasets are optimized using the SGD optimizer with a momentum of 0.9 and weight decay of 1e-4. We use a batch size of 128 for both ImageNet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Overfitting Experiment</head><p>Results with standard deviation <ref type="table" target="#tab_8">Table 5</ref> shows class-IL performance using average accuracy and forgetting rate, and quality of secondary information using SS-NLL and SS-Acc for each class-IL runs using increasingly overfitted model snapshots. Average incremental accuracy and forgetting rate is computed for class-IL model trained over different snapshots (every 100 th ) from the above run. <ref type="table" target="#tab_8">Table 5</ref> also shows expected calibration error (ECE) with standard deviation for different snapshots of the overfitted model. It shows that ECE monotonically increases with the number of training epochs. Tables includes values averaged over 5 runs with respective standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Regularization</head><p>All the regularizers are applied at base and all incremental steps, however major improvement is observed due its usage in the initial base task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-distillation</head><p>In the experiments, self-distillation is conducted over 4 generations (optimized using validation performance) for CIFAR-100 and ImageNet-100 dataset, and over 2 generations for ImageNet dataset. In the beginning of each self-distillation generation, the network snapshot (student) becomes the teacher network and the student continues to train (fine-tuned) with a combination of classification and distillation loss.</p><p>For CIFAR-100 experiments, the first base model is trained for 120 epochs following a cosine learning rate schedule, decaying from a learning rate 1e-1 to 1e-4. For self-distillation generations, the model is trained for 70 epochs with a decaying (cosine) learning rate from 1e-1 to 1e-3. All other optimizer settings are the same as the baseline model.</p><p>For ImageNet-100 experiments, first base model is trained for 70 epochs following a step learning rate schedule. For self-distillation generations, the model is trained for 30 epochs each where base learning rate is 1e-2 and it is divided by 10 at 10, 20 epochs.</p><p>For ImageNet experiments, the first base model is trained for 70 epochs following a step learning rate schedule. For self-distillation generations, the model is trained for 15 epochs each where base learning rate is 1e-2 and it is divided by 10 at 8, 12 epochs.   <ref type="table">Table 6</ref>: Effect of regularization on class-IL performance and secondary information. All the metrics are evaluated on the network trained on the first task. ↓ and ↑ in the column headings indicate that lower and higher values are better respectively. Values that are better than our baseline method (CCIL ) are marked in green whereas the worse ones are marked in red. SD:self-distillation, LS:label-smoothing.  <ref type="table">Table 6</ref> shows the effect of different regularization on the quality of secondary class information and the effect of different regularization on class-IL performance in terms of average incremental accuracy and forgetting rate. All experiments are conducted on CIFAR-100 dataset.</p><formula xml:id="formula_10">Epoch SS-NLL ↓ SS-Acc ↑ Avg Acc ↑ F ↓ R φ ↓ ECE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results with standard deviations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Representations: Qualitative Analysis</head><p>This section provides a qualitative analysis on the effect of different regularizers on the feature representations (penultimate-layer activations). We analyze the representations of the network trained on 50 classes (first task) of CIFAR-100 dataset using ResNet-32 network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Class-mean Representations</head><p>We argue that the classes which are semantically similar must be closer in the representation space as compared to the dissimilar classes since they share more features. Based on this argument we analyze the effect of different regularization methods on the relative distances between classmean representations. We utilize the fine-and coarse-label structure of the CIFAR-100 dataset to compare the effect on the distance between semantically similar and dissimilar classes relative to the default baseline model. Classes associated with the same coarse label or superclass are considered as similar classes, whereas dissimilar classes are picked from different superclasses. L2 distance is used as the distance metric. <ref type="figure" target="#fig_6">Figure 4</ref> show this qualitative analysis for two classes: cup and tulip. For example cup and can are semantically similar classes. When self-distillation and augmentation are used as regularizers, the relative distance reduces to 0.9 and 0.8 respectively, whereas when label-smoothing and mixup are applied, the relative distance increases to 1.2 and 1.1 respectively. Other similar classes follow a similar trend, whereas dissimilar pairs show an opposite behavior. Overall we find that regularizers: self-distillation and heavy dataaugmentation reduce the relative distance between the similar classes (marked in bold) while not affecting or increasing distance between dissimilar classes. Whereas mixup and label smoothing increase the relative distance between similar classes and reduce the relative distance between dissimilar classes. We notice that these observations agree with the findings on secondary class information presented in the main paper.</p><p>Earlier in the main paper, we argued that labelsmoothing and mixup regularization deteriorate secondary class information since they dismantle the natural output distribution. This qualitative analysis supports our argument showing how they conversely hamper the distances between similar and dissimilar classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The comparison between a (a) standard loss system and our proposed (b) compositional loss system (right). σ shows the softmax function span over all the network output logits. σ old and σnew shows softmax span over the set of old and new class logits respectively. cross-entropy loss L CE is evaluated corresponding to the correct class. For the initial base task T 0 , the model Θ s learns a standard classifier for the first (y ∈ y[1 : s]) classes. In the incremental step, the f c layer is adapted to learn new classes (y ∈ y[s + 1 : t]) by adding new output nodes, whereas the other part of the network remains unchanged, resulting into a new model Θ t . The three main elements of class-IL are set up as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 o 6 softmax over new class logits σ new (o new ) 7 compute classification loss L CE X (Eq. 3) 8 softmax over old class logits σ old (o old ) 9 compute 15 L</head><label>5678915</label><figDesc>= Θ t (x) // o = {o old , o new } = (L CE X + L CE P ) + λ * (L KD X + L KD P ) 16 endmoved to accommodate exemplars from new classes. Samples with larger distances to the mean vector are removed first. Detailed steps are shown in Algorithm 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Feature Representations for Incremental Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The effect of overfitting on class-IL performance on the CIFAR-100 dataset. Figure shows the overfitting behavior on the initial base task. The validation loss (red curve) starts increasing monotonically after the 100 th epoch. The green curve shows the average incremental accuracy (right y-axis) for class-IL experiments performed over different snapshots at every 100 th epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>(a) &amp; (b) compares the average L2 norm of the classification weight vectors for old and new classes for class-IL experiments without (w/o) and with (w/) KD respectively. We evaluate standard combined softmax (Comb) against proposed separate softmax (Sep) and we assess the effect of reduced learning rate (LowLR). (c) contains the corresponding class-IL results without distillation (w/o KD) and with distillation (w/ KD) in terms of average incremental accuracy. All experiments use the linear classification layer. Results shown on CIFAR-100 for 5-task experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Effect of regularizers on the distance between mean class representations. The numbers shown in the plot are the ratios between the class means distances of each method and of the default CCIL model. Similar classes are marked in bold. Dotted circle at 1.0 depicts distances between classes in the baseline CCIL model and other distances are depicted relative to the baseline model. Positive and negative cases indicate similar and dissimilar classes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 2 :</head><label>2</label><figDesc>UpdateExemplarSets Input: X , P old // new class data, old exemplar set Input: Θ s , m // old model, new exemplar size per class Output: P new // new Exemplar sets 1 for i = 1, ..., s do 2 P i ← (p 1 , ..., p m ) // keep first m samples</figDesc><table><row><cell>3 end</cell><cell></cell></row><row><cell>/ * add new class exemplars</cell><cell>* /</cell></row><row><cell>4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The effect of overfitting on class-IL performance and its correlation with secondary information, on the CIFAR-100 dataset.Table showsthe performance of the snapshots taken at every 100 th epoch and the corresponding class-IL model. SS-Acc decreases and SS-NLL increases as more overfitted models are evaluated. Forgetting rate F and feature retention metric R φ also correlate with overfitting. Results are averaged over 5 runs, standard deviation is reported in Appendix.</figDesc><table><row><cell></cell><cell></cell><cell>↑</cell></row><row><cell>100</cell><cell>2.54</cell><cell>38.68 16.03 9.04</cell></row><row><cell>200</cell><cell>2.89</cell><cell>32.88 16.04 9.27</cell></row><row><cell>300</cell><cell>3.03</cell><cell>30.09 16.94 9.51</cell></row><row><cell>400</cell><cell>3.09</cell><cell>29.04 18.38 9.68</cell></row><row><cell>500</cell><cell>3.11</cell><cell>27.97 18.57 10.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Effect of regularization class-IL average accuracy, secondary information (on the first-task model), forgetting rate and feature retention (5 tasks), on CIFAR-100. All the values are averaged over 3 runs. ↓ and ↑ in the column headings indicate that lower and higher values are better respectively. Values that are better than the CCIL baseline are marked in green whereas the worse ones are marked in red.</figDesc><table /><note>SD:self-distillation, LS:label-smoothing, H-Aug:heavy data augmentation. Standard deviation in Appendix A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Drawing parallels between iCaRL and our proposed model. Average accuracy is reported for 5-task class-IL experiments on CIFAR-100 dataset. Last row highlights our proposed changes. All methods use random exemplar selection as used in this work, Dot: linear layer, KD: knowledge distillation, NME: nearest-mean-of-exemplars (used in<ref type="bibr" target="#b22">[23]</ref>)</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR-100</cell><cell cols="2">ImageNet-100</cell><cell cols="2">ImageNet</cell></row><row><cell>No. of incremental tasks →</cell><cell>5</cell><cell>10</cell><cell>5</cell><cell>10</cell><cell>5</cell><cell>10</cell></row><row><cell>iCaRL  *  [23]</cell><cell>57.17</cell><cell>52.57</cell><cell>65.04</cell><cell>59.53</cell><cell>51.50</cell><cell>46.89</cell></row><row><cell>BIC [30]</cell><cell>59.36</cell><cell>54.20</cell><cell>70.07</cell><cell>64.96</cell><cell>62.65</cell><cell>58.72</cell></row><row><cell>WA [34]</cell><cell>63.25</cell><cell>58.57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LUCIR [12]</cell><cell>63.12</cell><cell>60.14</cell><cell>70.47</cell><cell>68.09</cell><cell>64.34</cell><cell>61.28</cell></row><row><cell>Mnemonics [17]</cell><cell>63.34</cell><cell>62.28</cell><cell>72.58</cell><cell>71.37</cell><cell>64.54</cell><cell>63.01</cell></row><row><cell>TPCIL [25]</cell><cell>65.34</cell><cell>63.58</cell><cell>76.27</cell><cell>74.81</cell><cell>64.89</cell><cell>62.88</cell></row><row><cell>CCIL (ours)</cell><cell>66.44</cell><cell>64.86</cell><cell>77.99</cell><cell>75.99</cell><cell>67.53</cell><cell>65.61</cell></row><row><cell>CCIL-SD (ours)</cell><cell>67.17</cell><cell>65.86</cell><cell>79.44</cell><cell>76.77</cell><cell>68.04</cell><cell>66.25</cell></row><row><cell>Joint-training</cell><cell>74.12</cell><cell>73.80</cell><cell>84.72</cell><cell>84.67</cell><cell>69.72</cell><cell>69.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>100 2.54 ± 0.04 38.68 ± 0.89 65.42 ± 0.06 16.03 ± 0.36 9.04 ± 0.24 0.093±0.003 200 2.89 ± 0.06 32.88 ± 0.59 65.05 ± 0.08 16.04 ± 0.26 9.27 ± 0.42 0.118±0.003 300 3.03 ± 0.06 30.09 ± 0.53 64.72 ± 0.07 16.94 ± 0.61 9.51 ± 0.23 0.126±0.004 400 3.09 ± 0.07 29.04 ± 0.68 64.3 ± 0.12 18.38 ± 0.19 9.68 ± 0.17 0.131±0.005 500 3.11 ± 0.03 27.97 ± 0.54 62.92 ± 0.11 18.57 ± 0.39 10.00 ± 0.20 0.137±0.002</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The effect of overfitting on class-IL performance and its correlation with secondary information. Table shows the performance of the network snapshots taken at every 100 th epoch. Accuracy decreases and SS-NLL increases, both monotonically, as more severely overfitted models are evaluated. Forgetting rate F also correlates with overfitting. Results are computed over 5 runs.</figDesc><table><row><cell>Model</cell><cell cols="2">Avg. Acc. ↑</cell><cell>Forgetting (5 tasks)</cell><cell>Retention</cell><cell cols="2">SS Metrics (5 tasks)</cell></row><row><cell></cell><cell>5 tasks</cell><cell>10 tasks</cell><cell>F ↓</cell><cell>R φ ↓</cell><cell>SS-NLL ↓</cell><cell>SS-Acc. ↑</cell></row><row><cell>CCIL</cell><cell cols="2">66.44 ± 0.31 64.86 ± 0.40</cell><cell>17.13 ± 1.12</cell><cell cols="3">9.70 ± 0.15 2.784 ± 0.014 34.83 ± 0.654</cell></row><row><cell>CCIL + SD</cell><cell cols="2">67.17 ± 0.14 65.86 ± 0.29</cell><cell>16.81 ± 0.25</cell><cell cols="3">8.88 ± 0.35 2.675 ± 0.037 37.26 ± 0.251</cell></row><row><cell cols="3">CCIL + H-Aug 71.66 ± 0.23 69.88 ± 0.36</cell><cell>13.37 ± 0.60</cell><cell cols="3">6.73 ± 0.45 2.051 ± 0.013 47.69 ± 0.590</cell></row><row><cell>CCIL + LS</cell><cell cols="2">63.08 ± 0.21 61.99 ± 0.30</cell><cell>18.79 ± 0.29</cell><cell cols="3">12.83 ± 0.41 3.103 ± 0.013 24.25 ± 0.278</cell></row><row><cell cols="3">CCIL + Mixup 62.31 ± 0.46 57.75 ± 1.64</cell><cell>24.56 ± 2.52</cell><cell cols="3">16.01 ± 0.16 2.791 ± 0.006 31.57 ± 0.256</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This study was supported by the German Federal Ministry of Education and Research via the project Deep-PTL.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A simple class decision balancing for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13947,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Il2m: Class incremental learning with dual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eden</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic fewshot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memory-efficient incremental learning through feature adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with unlabeled data in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mnemonics training: Multi-class incremental learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohe</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Catastrophic interference in connectionist networks: The sequential learning problem. The Psychology of Learning and Motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-distillation amplifies regularization in hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno>arXiv:12002.05715</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gdumb: A simple approach that questions our progress in continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topology-preserving classincremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The numpy array: A structure for efficient numerical computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science Engineering</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge distillation in generations: More tolerant teachers educate better students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic drift compensation for class-incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangling</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maintaining discrimination and fairness in class incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
