<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperbolic Graph Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-30">October 30, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
							<email>chami@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
							<email>rexying@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperbolic Graph Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-30">October 30, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCNs operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Convolutional Neural Networks (GCNs) are state-of-the-art models for representation learning in graphs, where nodes of the graph are embedded into points in Euclidean space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>. However, many real-world graphs, such as protein interaction networks and social networks, often exhibit scale-free or hierarchical structure <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> and Euclidean embeddings, used by existing GCNs, have a high distortion when embedding such graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>. In particular, scale-free graphs have tree-like structure and in such graphs the graph volume, defined as the number of nodes within some radius to a center node, grows exponentially as a function of radius. However, the volume of balls in Euclidean space only grows polynomially with respect to the radius, which leads to high distortion embeddings <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, while in hyperbolic space, this volume grows exponentially.</p><p>Hyperbolic geometry offers an exciting alternative as it enables embeddings with much smaller distortion when embedding scale-free and hierarchical graphs. However, current hyperbolic embedding techniques only account for the graph structure and do not leverage rich node features. For instance, Poincaré embeddings <ref type="bibr" target="#b28">[29]</ref> capture the hyperbolic properties of real graphs by learning shallow embeddings with hyperbolic distance metric and Riemannian optimization. Compared to deep alternatives such as GCNs, shallow embeddings do not take into account features of nodes, lack scalability, and lack inductive capability. Furthermore, in practice, optimization in hyperbolic space is challenging.</p><p>While extending GCNs to hyperbolic geometry has the potential to lead to more faithful embeddings and accurate models, it also poses many hard challenges: <ref type="bibr" target="#b0">(1)</ref>  optimally use them as inputs to hyperbolic neural networks; (2) It is not clear how to perform set aggregation, a key step in message passing, in hyperbolic space; And (3) one needs to choose hyperbolic spaces with the right curvature at every layer of the GCN.</p><p>Here we solve the above challenges and propose Hyperbolic Graph Convolutional Networks (HGCN) 1 , a class of graph representation learning models that combines the expressiveness of GCNs and hyperbolic geometry to learn improved representations for real-world hierarchical and scale-free graphs in inductive settings: <ref type="bibr" target="#b0">(1)</ref> We derive the core operations of GCNs in the hyperboloid model of hyperbolic space to transform input features which lie in Euclidean space into hyperbolic embeddings; <ref type="bibr" target="#b1">(2)</ref> We introduce a hyperbolic attention-based aggregation scheme that captures hierarchical structure of networks; (3) At different layers of HGCN we apply feature transformations in hyperbolic spaces of different trainable curvatures to learn low-distortion hyperbolic embeddings.</p><p>The transformation between different hyperbolic spaces at different layers allows HGCN to find the best geometry of hidden layers to achieve low distortion and high separation of class labels. Our approach jointly trains the weights for hyperbolic graph convolution operators, layer-wise curvatures and hyperbolic attention to learn inductive embeddings that reflect hierarchies in graphs.</p><p>Compared to Euclidean GCNs, HGCN offers improved expressiveness for hierarchical graph data. We demonstrate the efficacy of HGCN in link prediction and node classification tasks on a wide range of open graph datasets which exhibit different extent of hierarchical structure. Experiments show that HGCN significantly outperforms Euclideanbased state-of-the-art graph neural networks on scale-free graphs and reduces error from 11.5% up to 47.5% on node classification tasks and from 28.2% up to 63.1% on link prediction tasks. Furthermore, HGCN achieves new state-of-the-art results on the standard PUBMED benchmark. Finally, we analyze the notion of hierarchy learned by HGCN and show how the embedding geometry transforms from Euclidean features to hyperbolic embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The problem of graph representation learning belongs to the field of geometric deep learning. There exist two major types of approaches: transductive shallow embeddings and inductive GCNs. Transductive, shallow embeddings. The first type of approach attempts to optimize node embeddings as parameters by minimizing a reconstruction error. In other words, the mapping from nodes in a graph to embeddings is an embedding look-up. Examples include matrix factorization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> and random walk methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. Shallow embedding methods have also been developed in hyperbolic geometry <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> for reconstructing trees <ref type="bibr" target="#b34">[35]</ref> and graphs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref>, or embedding text <ref type="bibr" target="#b38">[39]</ref>. However, shallow (Euclidean and hyperbolic) embedding methods have three major downsides: (1) They fail to leverage rich node feature information, which can be crucial in tasks such as node classification. <ref type="bibr" target="#b1">(2)</ref> These methods are transductive, and therefore cannot be used for inference on unseen graphs. And, (3) they scale poorly as the number of model parameters grows linearly with the number of nodes. (Euclidean) Graph Neural Networks. Instead of learning shallow embeddings, an alternative approach is to learn a mapping from input graph structure as well as node features to embeddings, parameterized by neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. While various Graph Neural Network architectures resolve the disadvantages of shallow embeddings, they generally embed nodes into a Euclidean space, which leads to a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Our work builds on GNNs and extends them to hyperbolic geometry. Hyperbolic Neural Networks. Hyperbolic geometry has been applied to neural networks, to problems of computer vision or natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. More recently, hyperbolic neural networks <ref type="bibr" target="#b9">[10]</ref> were proposed, where core neural network operations are in hyperbolic space. In contrast to previous work, we derive the core neural network operations in a more stable model of hyperbolic space, and propose new operations for set aggregation, which enables HGCN to perform graph convolutions with attention in hyperbolic space with trainable curvature. After NeurIPS 2019 announced accepted papers, we also became aware of the concurrently developed HGNN model <ref type="bibr" target="#b25">[26]</ref> for learning GNNs in hyperbolic space. The main difference with our work is how our HGCN defines the architecture for neighborhood aggregation and uses a learnable curvature. Additionally, while <ref type="bibr" target="#b25">[26]</ref> demonstrates strong performance on graph classification tasks and provides an elegant extension to dynamic graph embeddings, we focus on link prediction and node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Problem setting. Without loss of generality we describe graph representation learning on a single graph. Let G = (V, E) be a graph with vertex set V and edge set E, and let (x 0,E i ) i∈V be d-dimensional input node features where 0 indicates the first layer. We use the superscript E to indicate that node features lie in a Euclidean space and use H to denote hyperbolic features. The goal in graph representation learning is to learn a mapping f which maps nodes to embedding vectors:</p><formula xml:id="formula_0">f : (V, E, (x 0,E i ) i∈V ) → Z ∈ R |V|×d , where d</formula><p>|V|. These embeddings should capture both structural and semantic information and can then be used as input for downstream tasks such as node classification and link prediction. Graph Convolutional Neural Networks (GCNs). Let N (i) = {j : (i, j) ∈ E} denote a set of neighbors of i ∈ V, (W , b ) be weights and bias parameters for layer , and σ(·) be a non-linear activation function. General GCN message passing rule at layer for node i then consists of:</p><formula xml:id="formula_1">h ,E i = W x −1,E i + b (feature transform) (1) x ,E i = σ(h ,E i + j∈N (i) w ij h ,E j ) (neighborhood aggregation)<label>(2)</label></formula><p>where aggregation weights w ij can be computed using different mechanisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. Message passing is then performed for multiple layers to propagate messages over network neighborhoods. Unlike shallow methods, GCNs leverage node features and can be applied to unseen nodes/graphs in inductive settings. The hyperboloid model of hyperbolic space. We review basic concepts of hyperbolic geometry that serve as building blocks for HGCN. Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature, where curvature measures how a geometric object deviates from a flat plane (cf. <ref type="bibr" target="#b32">[33]</ref> for an introduction to differential geometry). Here, we work with the hyperboloid model for its simplicity and its numerical stability <ref type="bibr" target="#b29">[30]</ref>. We review results for any constant negative curvature, as this allows us to learn curvature as a model parameter, leading to more stable optimization (cf. Section 4.5 for more details). Hyperboloid manifold. We first introduce our notation for the hyperboloid model of hyperbolic space. Let ., . L :</p><formula xml:id="formula_2">R d+1 × R d+1 → R denote the Minkowski inner product, x, y L := −x 0 y 0 + x 1 y 1 + . . . + x d y d .</formula><p>We denote H d,K as the hyperboloid manifold in d dimensions with constant negative curvature −1/K (K &gt; 0), and T x H d,K the (Euclidean) tangent space centered at point x</p><formula xml:id="formula_3">H d,K := {x ∈ R d+1 : x, x L = −K, x 0 &gt; 0} T x H d,K := {v ∈ R d+1 : v, x L = 0}.<label>(3)</label></formula><p>Figure 2: HGCN neighborhood aggregation (Eq. 9) first maps messages/embeddings to the tangent space, performs the aggregation in the tangent space, and then maps back to the hyperbolic space.</p><formula xml:id="formula_4">Now for v and w in T x H d,K , g K x (v, w) := v, w L is a Riemannian metric tensor [33] and (H d,K , g K x )</formula><p>is a Riemannian manifold with negative curvature −1/K. T x H d,K is a local, first-order approximation of the hyperbolic manifold at x and the restriction of the Minkowski inner product to T x H d,K is positive definite. T x H d,K is useful to perform Euclidean operations undefined in hyperbolic space and we denote ||v|| L = v, v L as the norm of v ∈ T x H d,K . Geodesics and induced distances. Next, we introduce the notion of geodesics and distances in manifolds, which are generalizations of shortest paths in graphs or straight lines in Euclidean geometry <ref type="figure" target="#fig_0">(Figure 1</ref>). Geodesics and distance functions are particularly important in graph embedding algorithms, as a common optimization objective is to minimize geodesic distances between connected nodes. Let x ∈ H d,K and u ∈ T x H d,K , and assume that u is unit-speed, i.e. u, u L = 1, then we have the following result:</p><formula xml:id="formula_5">Proposition 3.1. Let x ∈ H d,K , u ∈ T x H d,K be unit-speed. The unique unit-speed geodesic γ x→u (·) such that γ x→u (0) = x,γ x→u (0) = u is γ K x→u (t) = cosh t √ K x + √ Ksinh t √</formula><p>K u, and the intrinsic distance function between two points x, y in H d,K is then:</p><formula xml:id="formula_6">d K L (x, y) = √ Karcosh(− x, y L /K).<label>(4)</label></formula><p>Exponential and logarithmic maps. Mapping between tangent space and hyperbolic space is done by exponential and logarithmic maps. Given x ∈ H d,K and a tangent vector v ∈ T x H d,K , the exponential map exp K</p><formula xml:id="formula_7">x : T x H d,K → H d,K assigns to v the point exp K x (v) := γ(1), where γ is the unique geodesic satisfying γ(0) = x andγ(0) = v.</formula><p>The logarithmic map is the reverse map that maps back to the tangent space at x such that log K</p><formula xml:id="formula_8">x (exp K x (v)) = v.</formula><p>In general Riemannian manifolds, these operations are only defined locally but in the hyperbolic space, they form a bijection between the hyperbolic space and the tangent space at a point. We have the following direct expressions of the exponential and the logarithmic maps, which allow us to perform operations on points on the hyperboloid manifold by mapping them to tangent spaces and vice-versa: <ref type="bibr">K</ref> and y ∈ H d,K such that v = 0 and y = x, the exponential and logarithmic maps of the hyperboloid model are given by:</p><formula xml:id="formula_9">Proposition 3.2. For x ∈ H d,K , v ∈ T x H d,</formula><formula xml:id="formula_10">exp K x (v) = cosh ||v||L √ K x + √ Ksinh ||v||L √ K v ||v||L , log K x (y) = d K L (x, y) y + 1 K x, y Lx ||y + 1 K x, y Lx||L .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hyperbolic Graph Convolutional Networks</head><p>Here we introduce HGCN, a generalization of inductive GCNs in hyperbolic geometry that benefits from the expressiveness of both graph neural networks and hyperbolic embeddings. First, since input features are often Euclidean, we derive a mapping from Euclidean features to hyperbolic space. Next, we derive two components of graph convolution:</p><p>The analogs of Euclidean feature transformation and feature aggregation (Equations 1, 2) in the hyperboloid model. Finally, we introduce the HGCN algorithm with trainable curvature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mapping from Euclidean to hyperbolic spaces</head><p>HGCN first maps input features to the hyperboloid manifold via the exp map. Let x 0,E ∈ R d denote input Euclidean features. For instance, these features could be produced by pre-trained Euclidean neural networks. Let o := { √ K, 0, . . . , 0} ∈ H d,K denote the north pole (origin) in H d,K , which we use as a reference point to perform tangent space operations. We have (0, x 0,E ), o = 0. Therefore, we interpret (0, x 0,E ) as a point in T o H d,K and use Proposition 3.2 to map it to H d,K with:</p><formula xml:id="formula_11">x 0,H = exp K o ((0, x 0,E )) = √ Kcosh ||x 0,E || 2 √ K , √ Ksinh ||x 0,E || 2 √ K x 0,E ||x 0,E || 2 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature transform in hyperbolic space</head><p>The feature transform in Equation 1 is used in GCN to map the embedding space of one layer to the next layer embedding space and capture large neighborhood structures. We now want to learn transformations of points on the hyperboloid manifold. However, there is no notion of vector space structure in hyperbolic space. We build upon Hyperbolic Neural Network (HNN) <ref type="bibr" target="#b9">[10]</ref> and derive transformations in the hyperboloid model. The main idea is to leverage the exp and log maps in Proposition 3.2 so that we can use the tangent space T o H d,K to perform Euclidean transformations. Hyperboloid linear transform. Linear transformation requires multiplication of the embedding vector by a weight matrix, followed by bias translation. To compute matrix vector multiplication, we first use the logarithmic map to project hyperbolic points x H to T o H d,K . Thus the matrix representing the transform is defined on the tangent space, which is Euclidean and isomorphic to R d . We then project the vector in the tangent space back to the manifold using the exponential map. Let W be a d × d weight matrix. We define the hyperboloid matrix multiplication as:</p><formula xml:id="formula_12">W ⊗ K x H := exp K o (W log K o (x H )),<label>(6)</label></formula><p>where log K o (·) is on H d,K and exp K o (·) maps to H d ,K . In order to perform bias addition, we use a result from the HNN model and define b as an Euclidean vector located at T o H d,K . We then parallel transport b to the tangent space of the hyperbolic point of interest and map it to the manifold. If P K o→x H (·) is the parallel transport from T o H d ,K to T x H H d ,K (c.f. Appendix A for details), the hyperboloid bias addition is then defined as:</p><formula xml:id="formula_13">x H ⊕ K b := exp K x H (P K o→x H (b)).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neighborhood aggregation on the hyperboloid manifold</head><p>Aggregation <ref type="formula" target="#formula_1">(Equation 2</ref>) is a crucial step in GCNs as it captures neighborhood structures and features. Suppose that x i aggregates information from its neighbors (x j ) j∈N (i) with weights (w j ) j∈N (i) . Mean aggregation in Euclidean GCN computes the weighted average j∈N (i) w j x j . An analog of mean aggregation in hyperbolic space is the Fréchet mean <ref type="bibr" target="#b8">[9]</ref>, which, however, has no closed form solution. Instead, we propose to perform aggregation in tangent spaces using hyperbolic attention. Attention based aggregation. Attention in GCNs learns a notion of neighbors' importance and aggregates neighbors' messages according to their importance to the center node. However, attention on Euclidean embeddings does not take into account the hierarchical nature of many real-world networks. Thus, we further propose hyperbolic attention-based aggregation. Given hyperbolic embeddings (x H i , x H j ), we first map x H i and x H j to the tangent space of the origin to compute attention weights w ij with concatenation and Euclidean Multi-layer Percerptron (MLP). We then propose a  hyperbolic aggregation to average nodes' representations:</p><formula xml:id="formula_14">w ij = SOFTMAX j∈N (i) (MLP(log K o (x H i )||log K o (x H j ))) (8) AGG K (x H ) i = exp K x H i j∈N (i) w ij log K x H i (x H j ) .<label>(9)</label></formula><p>Note that our proposed aggregation is directly performed in the tangent space of each center point x H i , as this is where the Euclidean approximation is best (cf. <ref type="figure">Figure 2</ref>). We show in our ablation experiments (cf. <ref type="table" target="#tab_3">Table 2</ref>) that this local aggregation outperforms aggregation in tangent space at the origin (AGG o ), due to the fact that relative distances have lower distortion in our approach. Non-linear activation with different curvatures. Analogous to Euclidean aggregation (Equation 2), HGCN uses a non-linear activation function, σ(·) such that σ(0) = 0, to learn non-linear transformations. Given hyperbolic curvatures −1/K −1 , −1/K at layer − 1 and respectively, we introduce a hyperbolic non-linear activation σ ⊗ K −1 ,K with different curvatures. This step is crucial as it allows us to smoothly vary curvature at each layer. More concretely, HGCN applies the Euclidean non-linear activation in T o H d,K −1 and then maps back to H d,K :</p><formula xml:id="formula_15">σ ⊗ K −1 ,K (x H ) = exp K o (σ(log K −1 o (x H ))).<label>(10)</label></formula><p>Note that in order to apply the exponential map, points must be located in the tangent space at the north pole. Fortunately, tangent spaces of the north pole are shared across hyperboloid manifolds of the same dimension that have different curvatures, making Equation 10 mathematically correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">HGCN architecture</head><p>Having introduced all the building blocks of HGCN, we now summarize the model architecture. Given a graph G = (V, E) and input Euclidean features (x 0,E ) i∈V , the first layer of HGCN maps from Euclidean to hyperbolic space as detailed in Section 4.1. HGCN then stacks multiple hyperbolic graph convolution layers. At each layer HGCN transforms and aggregates neighbour's embeddings in the tangent space of the center node and projects the result to a hyperbolic space with different curvature. Hence the message passing in a HGCN layer is:</p><formula xml:id="formula_16">h ,H i = (W ⊗ K −1 x −1,H i ) ⊕ K −1 b (hyperbolic feature transform)<label>(11)</label></formula><formula xml:id="formula_17">y ,H i = AGG K −1 (h ,H ) i (attention-based neighborhood aggregation)<label>(12)</label></formula><formula xml:id="formula_18">x ,H i = σ ⊗ K −1 ,K (y ,H i ) (non-linear activation with different curvatures)<label>(13)</label></formula><p>where −1/K −1 and −1/K are the hyperbolic curvatures at layer − 1 and respectively. Hyperbolic embeddings (x L,H ) i∈V at the last layer can then be used to predict node attributes or links.  <ref type="table">Table 1</ref>: ROC AUC for Link Prediction (LP) and F1 score for Node Classification (NC) tasks. For inductive datasets, we only evaluate inductive methods since shallow methods cannot generalize to unseen nodes/graphs. We report graph hyperbolicity values δ (lower is more hyperbolic).</p><p>For link prediction, we use the Fermi-Dirac decoder <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, a generalization of sigmoid, to compute probability scores for edges:</p><formula xml:id="formula_19">p((i, j) ∈ E|x L,H i , x L,H j ) = e (d K L L (x L,H i ,x L,H j ) 2 −r)/t + 1 −1 ,<label>(14)</label></formula><p>where d K L L (·, ·) is the hyperbolic distance and r and t are hyper-parameters. We then train HGCN by minimizing the cross-entropy loss using negative sampling.</p><p>For node classification, we map the output of the last HGCN layer to the tangent space of the origin with the logarithmic map log K L o (·) and then perform Euclidean multinomial logistic regression. Note that another possibility is to directly classify points on the hyperboloid manifold using the hyperbolic multinomial logistic loss <ref type="bibr" target="#b9">[10]</ref>. This method performs similarly to Euclidean classification (cf. <ref type="bibr" target="#b9">[10]</ref> for an empirical comparison). Finally, we also add a link prediction regularization objective in node classification tasks, to encourage embeddings at the last layer to preserve the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Trainable curvature</head><p>We further analyze the effect of trainable curvatures in HGCN. Theorem 4.1 (proof in Appendix B) shows that assuming infinite precision, for the link prediction task, we can achieve the same performance for varying curvatures with an affine invariant decoder by scaling embeddings. However, despite the same expressive power, adjusting curvature at every layer is important for good performance in practice due to factors of limited machine precision and normalization. First, with very low or very high curvatures, the scaling factor K K in Theorem 4.1 becomes close to 0 or very large, and limited machine precision results in large error due to rounding. This is supported by <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="table" target="#tab_3">Table 2</ref> where adjusting and training curvature lead to significant performance gain. Second, the norms of hidden layers that achieve the same local minimum in training also vary by a factor of √ K. In practice, however, optimization is much more stable when the values are normalized <ref type="bibr" target="#b15">[16]</ref>. In the context of HGCN, trainable curvature provides a natural way to learn embeddings of the right scale at each layer, improving optimization. <ref type="figure" target="#fig_4">Figure 4</ref> shows the effect of decreasing curvature (K = +∞ is the Euclidean case) on link prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We comprehensively evaluate our method on a variety of networks, on both node classification (NC) and link prediction (LP) tasks, in transductive and inductive settings. We compare performance of HGCN against a variety of shallow and GNN-based baselines. We further use visualizations to investigate the expressiveness of HGCN in link prediction tasks, and also demonstrate its ability to learn embeddings that capture the hierarchical structure of many real-world networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Datasets. We use a variety of open transductive and inductive datasets that we detail below (more details in Appendix). We compute Gromov's δ−hyperbolicity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref>, a notion from group theory that measures how tree-like a graph is. The lower δ, the more hyperbolic is the graph dataset, and δ = 0 for trees. We conjecture that HGCN works better on graphs with small δ-hyperbolicity.</p><p>1. Citation networks. CORA <ref type="bibr" target="#b35">[36]</ref> and PUBMED <ref type="bibr" target="#b26">[27]</ref> are standard benchmarks describing citation networks where nodes represent scientific papers, edges are citations between them, and node labels are academic (sub)areas. CORA contains 2,708 machine learning papers divided into 7 classes while PUBMED has 19,717 publications in the area of medicine grouped in 3 classes. 2. Disease propagation tree. We simulate the SIR disease spreading model <ref type="bibr" target="#b1">[2]</ref>, where the label of a node is whether the node was infected or not. Based on the model, we build tree networks, where node features indicate the susceptibility to the disease. We build transductive and inductive variants of this dataset, namely DISEASE and DISEASE-M (which contains multiple tree components). 3. Protein-protein interactions (PPI) networks. PPI is a dataset of human PPI networks <ref type="bibr" target="#b36">[37]</ref>. Each human tissue has a PPI network, and the dataset is a union of PPI networks for human tissues. Each protein has a label indicating the stem cell growth rate after 19 days <ref type="bibr" target="#b39">[40]</ref>, which we use for the node classification task. The 16-dimensional feature for each node represents the RNA expression levels of the corresponding proteins, and we perform log transform on the features. 4. Flight networks. AIRPORT is a transductive dataset where nodes represent airports and edges represent the airline routes as from OpenFlights.org. Compared to previous compilations <ref type="bibr" target="#b48">[49]</ref>, our dataset has larger size (2,236 nodes). We also augment the graph with geographic information (longitude, latitude and altitude), and GDP of the country where the airport belongs to. We use the population of the country where the airport belongs to as the label for node classification.</p><p>Baselines. For shallow methods, we consider Euclidean embeddings (EUC) and Poincaré embeddings (HYP) <ref type="bibr" target="#b28">[29]</ref>. We conjecture that HYP will outperform EUC on hierarchical graphs. For a fair comparison with HGCN which leverages node features, we also consider EUC-MIXED and HYP-MIXED baselines, where we concatenate the corresponding shallow embeddings with node features, followed by a MLP to predict node labels or links. For state-of-the-art Euclidean GNN models, we consider GCN <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE (SAGE) <ref type="bibr" target="#b14">[15]</ref>, Graph Attention Networks (GAT) <ref type="bibr" target="#b40">[41]</ref> and Simplified Graph Convolution (SGC) <ref type="bibr" target="#b43">[44]</ref> 2 . We also consider feature-based approaches: MLP and its hyperbolic variant (HNN) <ref type="bibr" target="#b9">[10]</ref>, which does not utilize the graph structure.</p><p>Training. For all methods, we perform a hyper-parameter search on a validation set over initial learning rate, weight decay, dropout 3 , number of layers, and activation functions. We measure performance on the final test set over 10 random parameter initializations. For fairness, we also control the number of dimensions to be the same <ref type="bibr" target="#b15">(16)</ref> for all methods. We optimize all models with Adam <ref type="bibr" target="#b18">[19]</ref>, except Poincaré embeddings which are optimized with RiemannianSGD <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>. Further details can be found in Appendix. We open source our implementation <ref type="bibr" target="#b3">4</ref>     standard splits <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46]</ref> with 20 train examples per class for CORA and PUBMED. One of the main advantages of HGCN over related hyperbolic graph embedding is its inductive capability. For inductive tasks, the split is performed across graphs. All nodes/edges in training graphs are considered the training set, and the model is asked to predict node class or unseen links for test graphs. Following previous works, we evaluate link prediction by measuring area under the ROC curve on the test set and evaluate node classification by measuring F1 score, except for CORA and PUBMED, where we report accuracy as is standard in the literature. <ref type="table">Table 1</ref> reports the performance of HGCN in comparison to baseline methods. HGCN works best in inductive scenarios where both node features and network topology play an important role. The performance gain of HGCN with respect to Euclidean GNN models is correlated with graph hyperbolicity. HGCN achieves an average of 45.4% (LP) and 12.3% (NC) error reduction compared to the best deep baselines for graphs with high hyperbolicity (low δ), suggesting that GNNs can significantly benefit from hyperbolic geometry, especially in link prediction tasks. Furthermore, the performance gap between HGCN and HNN suggests that neighborhood aggregation has been effective in learning node representations in graphs. For example, in disease spread datasets, both Euclidean attention and hyperbolic geometry lead to significant improvement of HGCN over other baselines. This can be explained by the fact that in disease spread trees, parent nodes contaminate their children. HGCN can successfully model these asymmetric and hierarchical relationships with hyperbolic attention and improves performance over all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>On the CORA dataset with low hyperbolicity, HGCN does not outperform Euclidean GNNs, suggesting that Euclidean geometry is better for its underlying graph structure. However, for small dimensions, HGCN is still significantly more effective than GCN even with CORA. <ref type="figure" target="#fig_2">Figure 3c</ref> shows 2-dimensional HGCN and GCN embeddings trained with LP objective, where colors denote the label class. HGCN achieves much better label class separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>Ablations. We further analyze the effect of proposed components in HGCN, namely hyperbolic attention (ATT) and trainable curvature (C) on AIRPORT and DISEASE datasets in <ref type="table" target="#tab_3">Table 2</ref>. We observe that both attention and trainable curvature lead to performance gains over HGCN with fixed curvature and no attention. Furthermore, our attention model ATT outperforms ATT o (aggregation in tangent space at o), and we conjecture that this is because the local Euclidean average is a better approximation near the center point rather than near o. Finally, the addition of both ATT and C improves performance even further, suggesting that both components are important in HGCN. Visualizations. We first visualize the GCN and HGCN embeddings at the first and last layers in <ref type="figure" target="#fig_2">Figure 3</ref>. We train HGCN with 3-dimensional hyperbolic embeddings and map them to the Poincaré disk which is better for visualization. In contrast to GCN, tree structure is preserved in HGCN, where nodes close to the center are higher in the hierarchy of the tree. This way HGCN smoothly transforms Euclidean features to Hyperbolic embeddings that preserve node hierarchy. <ref type="figure" target="#fig_5">Figure 5</ref> shows the attention weights in the 2-hop neighborhood of a center node (red) for the DISEASE dataset. The red node is the node where we compute attention. The darkness of the color for other nodes denotes their hierarchy. The attention weights for nodes in the neighborhood are visualized by the intensity of edges. We observe that in HGCN the center node pays more attention to its (grand)parent. In contrast to Euclidean GAT, our aggregation with attention in hyperbolic space allows us to pay more attention to nodes with high hierarchy. Such attention is crucial to good performance in DISEASE, because only sick parents will propagate the disease to their children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced HGCN, a novel architecture that learns hyperbolic embeddings using graph convolutional networks. In HGCN, the Euclidean input features are successively mapped to embeddings in hyperbolic spaces with trainable curvatures at every layer. HGCN achieves new state-of-the-art in learning embeddings for real-world hierarchical and scale-free graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Review of Differential Geometry</head><p>We first recall some definitions of differential and hyperbolic geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Differential geometry</head><p>Manifold. An d−dimensional manifold M is a topological space that locally resembles the topological space R d near each point. More concretely, for each point x on M, we can find a homeomorphism (continuous bijection with continuous inverse) between a neighbourhood of x and R d . The notion of manifold is a generalization of surfaces in high dimensions. In what follows, we review the Poincaré and the hyperboloid models of hyperbolic space as well as connections between these two models. where λ x := 2 1−||x|| 2 2 and I d is the identity matrix. The induced distance between two points (x, y) in D d,1 can be computed as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Poincaré ball model</head><formula xml:id="formula_20">d 1 D (x, y) = arcosh 1 + 2 ||x − y|| 2 2 (1 − ||x|| 2 2 )(1 − ||y|| 2 2 )</formula><p>. The induced distance between two points (x, y) in H d,1 can be computed as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Hyperboloid model</head><formula xml:id="formula_21">Hyperboloid model. Let ., . L : R d+1 × R d+1 → R denote</formula><formula xml:id="formula_22">d 1 L (x, y) = arcosh(− x, y L ).</formula><p>Geodesics. We recall a result that gives the unit speed geodesics in the hyperboloid model with curvature −1 <ref type="bibr" target="#b32">[33]</ref>. This result can be used to show Propositions 3.1 and 3.2 for the hyperboloid manifold with negative curvature −1/K, and then learn K as a model parameter in HGCN.  Parallel Transport. If two points x and y on the hyperboloid H d,1 are connected by a geodesic, then the parallel transport of a tangent vector v ∈ T x H d,1 to the tangent space T y H d,1 is:</p><formula xml:id="formula_23">P x→y (v) = v − log x (y), v L d 1 L (x, y) 2 (log x (y) + log y (x)).<label>(15)</label></formula><p>Projections. Finally, we recall projections to the hyperboloid manifold and its corresponding tangent spaces. A point x = (x 0 , x 1:d ) ∈ R d+1 can be projected on the hyperboloid manifold H d,1 with:</p><formula xml:id="formula_24">Π R d+1 →H d,1 (x) := ( 1 + ||x 1:d || 2 2 , x 1:d ).<label>(16)</label></formula><p>Similarly, a point v ∈ R d+1 can be projected on T x H d,1 with:</p><formula xml:id="formula_25">Π R d+1 →TxH d,1 (v) := v + x, v L x.<label>(17)</label></formula><p>In practice, these projections are very useful for optimization purposes as they constrain embeddings and tangent vectors to remain on the manifold and tangent spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Connection between the Poincaré ball model and the hyperboloid model</head><p>While the hyperboloid model tends to be more stable for optimization than the Poincaré model <ref type="bibr" target="#b29">[30]</ref>, the Poincaré model is very interpretable and embeddings can be directly visualized on the Poincaré disk. Fortunately, these two models are isomorphic (cf. <ref type="figure" target="#fig_10">Figure 7</ref>) and there exist a diffeomorphism Π H d,1 →D d,1 (·) mapping one space onto the other:</p><formula xml:id="formula_26">Π H d,1 →D d,1 (x 0 , . . . , x d ) = (x 1 , . . . , x d ) x 0 + 1<label>(18)</label></formula><p>and</p><formula xml:id="formula_27">Π D d,1 →H d,1 (x 1 , . . . , x d ) = (1 + ||x|| 2 2 , 2x 1 , . . . , 2x d ) 1 − ||x|| 2 2 .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyperboloid model of hyperbolic space</head><p>For completeness, we re-derive results of hyperbolic geometry for any arbitrary curvature. Similar derivations can be found in the literature <ref type="bibr" target="#b42">[43]</ref>.</p><p>By identification, this leads to</p><formula xml:id="formula_28">γ K x→v (t) = cosh ||v|| L √ K t x + √ Ksinh ||v|| L √ K t v ||v|| L .</formula><p>We can use this result to derive exponential and logarthimic maps on the hyperboloid model. We know that exp K x (v) = γ K x→v <ref type="bibr" target="#b0">(1)</ref>. Therefore we get,</p><formula xml:id="formula_29">exp K x (v) = cosh ||v|| L √ K x + √ Ksinh ||v|| L √ K v ||v|| L . Now let y = exp K x (v).</formula><p>We have x, y L = −Kcosh ||v|| L √ K as x, x L = −K and x, v L = 0. Therefore Proof. For any hyperbolic embedding x = (x 0 , x 1 , . . . , x d ) ∈ H d,K we have the identity:</p><formula xml:id="formula_30">y + 1 K x, y L x = √ Ksinh ||v|| L √ K v ||v|| L and we get v = √ Karsinh ||y + 1 K x, y L x|| L √ K y + 1 K x, y L x ||y + 1 K x, y L x|| L , where ||y + 1 K x, y L || L is well defined since y + 1 K x, y L x ∈ T x H d,K . Note that, ||y + 1 K x, y L x|| L = y, y L + 2 K x, y 2 L + 1 K 2 x, y 2 L x, x L = −K + 1 K x, y 2 L = √ K x √ K , y √ K 2 L − 1 = √ Ksinh arcosh − x √ K , y √ K L as x √ K , y √ K L ≤ −1.</formula><formula xml:id="formula_31">x, x L = −x 2 0 + d i=1 x 2 i = −K.</formula><p>For any hyperbolic curvature −1/K &lt; 0, consider the mapping φ(x) = K K x. Then we have the identity φ(x), φ(x) L = −K and therefore φ(</p><formula xml:id="formula_32">x) ∈ H d,K . For any pair (u, v), φ(u), φ(v) L = K K −u 0 v 0 + d i=1 u i v i = K K u, v L .</formula><p>The factor K K only depends on curvature, but not the specific embeddings.</p><p>Lemma 1 implies that given a set of embeddings learned in hyperbolic space H d,K , we can find embeddings in another hyperbolic space with different curvature, H d,K , such that the Minkowski inner products for all pairs of embeddings are scaled by the same factor K K . For link prediction tasks, Theorem 4.1 shows that with infinite precision, the expressive power of hyperbolic spaces with varying curvatures is the same. <ref type="table" target="#tab_3">Node features  CORA  2708  5429  7  1433  PUBMED 19717 88651  3  500  HUMAN PPI 17598 5429  4  17  AIRPORT  3188 18631  4  4  DISEASE  1044  1043  2  1000  DISEASE-M 43193 43102</ref> 2 1000 , H = {h i |h i = K K h i }, such that the reconstructed graph from H via the Fermi-Dirac decoder is the same as the reconstructed graph from H, with different decoder parameters (r, t) and (r , t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name Nodes Edges Classes</head><p>Proof. The Fermi-Dirac decoder predicts that there exists a link between node i and j iif e (d K L (hi,hj )−r)/t + 1</p><formula xml:id="formula_33">−1 ≥ b,</formula><p>where b ∈ (0, 1) is the threshold for determining existence of links. The criterion is equivalent to d K L (h i , h j ) ≤ r + t log <ref type="formula">(</ref>  </p><formula xml:id="formula_34">d K L (φ(h i ), φ(h j )) = √ K arcosh − K K h i , h j L /K = K K d K L (h i , h j ).<label>(20)</label></formula><p>Due to linearity, we can find decoder parameter, r and t that satisfy</p><formula xml:id="formula_35">r + t log( 1−b b ) = K K (r + t log( 1−b b )). With such r , t , the criterion d K L (h i , h j ) ≤ r + t log( 1−b b ) is equivalent to d K L (φ(h i ), φ(h j )) ≤ r + t log( 1−b b )</formula><p>. Therefore, the reconstructed graph G H based on the set of embeddings H is identical to G H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Dataset statistics</head><p>We detail the dataset statistics in <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Training details</head><p>Here we present details of HGCN's training pipeline, with optimization and incorporation of DropConnect <ref type="bibr" target="#b41">[42]</ref>. Parameter optimization. Recall that linear transformations and attention are defined on the tangent space of points. Therefore the linear layer and attention parameters are Euclidean. For bias, there are two options: one can either define parameters in hyperbolic space, and use hyperbolic addition operation <ref type="bibr" target="#b9">[10]</ref>, or define parameters in Euclidean space, and use Euclidean addition after transforming the points into the tangent space. Through experiments we find that Euclidean optimization is much more stable, and gives slightly better test performance compared to Riemannian optimization, if we define parameters such as bias in hyperbolic space. Hence different from shallow hyperbolic embeddings, although our model and embeddings are hyperbolic, the learnable graph convolution parameters can be optimized via Euclidean optimization (Adam Optimizer <ref type="bibr" target="#b18">[19]</ref>), thanks to exponential and logarithmic maps. Note that to train shallow Poincaré embeddings, we use Riemannian Stochastic Gradient Descent <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48]</ref>, since its model parameters are hyperbolic. We use early stopping based on validation set performance with a patience of 100 epochs. Drop connection. Since rescaling vectors in hyperbolic space requires exponential and logarithmic maps, and is conceptually not tied to the inverse dropout rate in terms of re-normalizing L1 norm, Dropout cannot be directly applied in HGCN. However, as a result of using Euclidean parameters in HGCN, DropConnect <ref type="bibr" target="#b41">[42]</ref>, the generalization of Dropout, can be used as a regularization. DropConnect randomly zeros out the neural network connections, i.e. elements of the Euclidean parameters during training time, improving the generalization of HGCN. Projections. Finally, we apply projections similar to Equations 16 and 17 for the hyperboloid model H d,K after each feature transform and log or exp map, to constrain embeddings and tangent vectors to remain on the manifold and tangent spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Input node features are usually Euclidean, and it is not clear how to Left: Poincaré disk geodesics (shortest path) connecting x and y for different curvatures. As curvature (−1/K) decreases, the distance between x and y increases, and the geodesics lines get closer to the origin. Center: Hyperbolic distance vs curvature. Right: Poincaré geodesic lines. x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) GCN layers.(b) HGCN layers.(c) GCN (left), HGCN (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of embeddings for LP on DISEASE and NC on CORA (visualization on the Poincaré disk for HGCN). (a) GCN embeddings in first and last layers for DISEASE LP hardly capture hierarchy (depth indicated by color). (b) In contrast, HGCN preserves node hierarchies. (c) On CORA NC, HGCN leads to better class separation (indicated by different colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 4 . 1 .</head><label>41</label><figDesc>For any hyperbolic curvatures −1/K, −1/K &lt; 0, for any node embeddings H = {h i } ⊂ H d,K of a graph G, we can find H ⊂ H d,K , H = {h i |h i = K K h i }, such that the reconstructed graph from H via the Fermi-Dirac decoder is the same as the reconstructed graph from H, with different decoder parameters (r, t) and (r , t ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Decreasing curvature (−1/K) improves link prediction performance on DISEASE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Attention: Euclidean GAT (left), HGCN (right). Each graph represents a 2-hop neighborhood of the DISEASE-M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Tangent space. Intuitively, if we think of M as a d−dimensional manifold embedded in R d+1 , the tangent space T x M at point x on M is a d−dimensional hyperplane in R d+1 that best approximates M around x. Another possible interpretation for T x M is that it contains all the possible directions of curves on M passing through x. The elements of T x M are called tangent vectors and the union of all tangent spaces is called the tangent bundle T M = ∪ x∈M T x M. Riemannian manifold. A Riemannian manifold is a pair (M, g), where M is a smooth manifold and g = (g x ) x∈M is a Riemannian metric, that is a family of smoothly varying inner products on tangent spaces, g x : T x M × T x M → R. Riemannian metrics can be used to measure distances on manifolds. Distances and geodesics. Let (M, g) be a Riemannian manifold. For v ∈ T x M, define the norm of v by ||v|| g := g x (v, v). Suppose γ : [a, b] → M is a smooth curve on M. Define the length of γ by: L(γ) := b a ||γ (t)|| g dt. Now with this definition of length, every connected Riemannian manifold becomes a metric space and the distance d : M × M → [0, ∞) is defined as: d(x, y) := inf γ {L(γ) : γ is a continuously differentiable curve joining x and y}. Geodesic distances are a generalization of straight lines (or shortest paths) to non-Euclidean geometry. A curve γ : [a, b] → M is geodesic if d(γ(t), γ(s)) = L(γ| [t,s] )∀(t, s) ∈ [a, b](t &lt; s). Parallel transport. Parallel transport is a generalization of translation to non-Euclidean geometry. Given a smooth manifold M, parallel transport P x→y (·) maps a vector v ∈ T x M to P x→y (v) ∈ T y M. In Riemannian geometry, parallel transport preserves the Riemannian metric tensor (norm, inner products...). Curvature. At a high level, curvature measures how much a geometric object such as surfaces deviate from a flat plane. For instance, the Euclidean space has zero curvature while spheres have positive curvature. We illustrate the concept of curvature in Figure 6. A.2 Hyperbolic geometry Hyperbolic space. The hyperbolic space in d dimensions is the unique complete, simply connected d−dimensional Riemannian manifold with constant negative sectional curvature. There exist several models of hyperbolic space such as the Poincaré model or the hyperboloid model (also known as the Minkowski model or the Lorentz model).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Let ||.|| 2 Figure 6 :</head><label>26</label><figDesc>be the Euclidean norm. The Poincaré ball model with unit radius and constant negative curvature −1 in d dimensions is the Riemannian manifold (D d,1 , (g x ) x ) where D d,1 := {x ∈ R d : ||x|| 2 &lt; 1}, and g x = λ 2 x I d , From left to right: a surface of negative curvature, a surface of zero curvature, and a surface of positive curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>the Minkowski inner product, x, y L := −x 0 y 0 + x 1 y 1 + . . . + x d y d . The hyperboloid model with unit imaginary radius and constant negative curvature −1 in d dimensions is defined as the Riemannian manifold (H d,1 , (g x ) x ) where H d,1 := {x ∈ R d+1 : x, x L = −1, x 0 &gt; 0},</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Theorem A. 1 .</head><label>1</label><figDesc>Let x ∈ H d,1 and u ∈ T x H d,1 unit-speed (i.e. u, u L = 1). The unique unit-speed geodesic γ x→u : [0, 1] → H d,1 such that γ x→u (0) = x andγ x→u (0) = u is given by: γ x→u (t) = cosh(t)x + sinh(t)u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of the hyperboloid model (top) in 3 dimensions and its connection to the Poincaré disk (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>1−b b ). Given H = {h 1 , . . . , h n }, the graph G H reconstructed with the Fermi-Dirac decoder has the edge set E H = (i, j)|d K L (h i , h j ) ≤ r + t log( 1−b b ) . Consider the mapping to H d,K , φ(x) := K K x. Let H = {φ(h 1 ), . . . , φ(h n )}. By Lemma 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>± 0.0 60.9 ± 3.4 83.3 ± 0.1 48.2 ± 0.7 82.5 ± 0.3 23.8 ± 0.7 HYP<ref type="bibr" target="#b28">[29]</ref> 63.5 ± 0.<ref type="bibr" target="#b5">6</ref> 45.5 ± 3.3 ----94.5 ± 0.0 70.2 ± 0.1 87.5 ± 0.1 68.5 ± 0.3 87.6 ± 0.2 22.0 ± 1.5 EUC-MIXED 49.6 ± 1.1 35.2 ± 3.4 ----91.5 ± 0.1 68.3 ± 2.3 86.0 ± 1.3 63.0 ± 0.3 84.4 ± 0.2 46.1 ± 0.4 HYP-MIXED 55.1 ± 1.3 56.9 ± 1.5 ----93.3 ± 0.0 69.6 ± 0.1 83.8 ± 0.3 73.9 ± 0.2 85.6 ± 0.5 45.9 ± 0.3 NN MLP 72.6 ± 0.6 28.8 ± 2.5 55.3 ± 0.5 55.9 ± 0.3 67.8 ± 0.2 55.3±0.4 89.8 ± 0.5 68.6 ± 0.6 84.1 ± 0.9 72.4 ± 0.2 83.1 ± 0.5 51.5 ± 1.0 HNN[10] 75.1 ± 0.3 41.0 ± 1.8 60.9 ± 0.4 56.2 ± 0.3 72.9 ± 0.3 59.3 ± 0.4 90.8 ± 0.2 80.5 ± 0.5 94.9 ± 0.1 69.8 ± 0.4 89.0 ± 0.1 54.6 ± 0.4 ±0.5 69.7 ± 0.4 66.0 ± 0.8 59.4 ± 3.4 77.0 ± 0.5 69.7 ± 0.3 89.3 ± 0.4 81.4 ± 0.6 91.1 ± 0.5 78.1 ± 0.2 90.4 ± 0.2 81.3 ± 0.3 GAT [41] 69.8 ±0.3 70.4 ± 0.4 69.5 ± 0.4 62.5 ± 0.7 76.8 ± 0.4 70.5 ± 0.4 90.5 ± 0.3 81.5 ± 0.3 91.2 ± 0.1 79.0 ± 0.3 93.7 ± 0.1 83.0 ± 0.7 SAGE [15] 65.9 ± 0.3 69.1 ± 0.6 67.4 ± 0.5 61.3 ± 0.4 78.1 ± 0.6 69.1 ± 0.3 90.4 ± 0.5 82.1 ± 0.5 86.2 ± 1.0 77.4 ± 2.2 85.5 ± 0.6 77.9 ± 2.4 SGC [44] 65.1 ± 0.2 69.5 ± 0.2 66.2 ± 0.2 60.5 ± 0.3 76.1 ± 0.2 71.3 ± 0.1 89.8 ± 0.3 80.6 ± 0.1 94.1 ± 0.0 78.9 ± 0.0 91.5 ± 0.1 81.0 ± 0.1 Ours HGCN 90.8 ± 0.3 74.5 ± 0.9 78.1 ± 0.4 72.2 ± 0.5 84.5 ± 0.4 74.6 ± 0.3 96.4 ± 0.1 90.6 ± 0.2 96.3 ± 0.0 80.3 ± 0.3 92.9 ± 0.1 79.9 ± 0.2</figDesc><table><row><cell>Dataset</cell><cell>DISEASE</cell><cell></cell><cell cols="2">DISEASE-M</cell><cell cols="2">HUMAN PPI</cell><cell cols="2">AIRPORT</cell><cell cols="2">PUBMED</cell><cell>CORA</cell><cell></cell></row><row><cell>Hyperbolicity δ</cell><cell>δ = 0</cell><cell></cell><cell>δ = 0</cell><cell></cell><cell>δ = 1</cell><cell></cell><cell>δ = 1</cell><cell></cell><cell cols="2">δ = 3.5</cell><cell>δ = 11</cell><cell></cell></row><row><cell>Method</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell></row><row><cell cols="8">Shallow 92.0 GNN EUC 59.8 ± 2.0 32.5 ± 1.1 ----GCN[21] 64.7 (%) ERR RED -63.1% -13.8% -28.2% -25.9% -29.2% -11.5% -60.9%</cell><cell>-47.5%</cell><cell>-27.5%</cell><cell>-6.2%</cell><cell>+12.7%</cell><cell>+18.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>of HGCN and baselines. Evaluation metric. In transductive LP tasks, we randomly split edges into 85/5/10% for training, validation and test sets. For transductive NC, we use 70/15/15% splits for AIRPORT, 30/10/60% splits for DISEASE, and we use</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 0.2 94.9 ± 0.3 HGCN-ATT-C 90.8 ± 0.3 96.4 ± 0.1</figDesc><table><row><cell>Method</cell><cell>DISEASE</cell><cell>AIRPORT</cell></row><row><cell>HGCN</cell><cell cols="2">78.4 ± 0.3 91.8 ± 0.3</cell></row><row><cell>HGCN-ATTo</cell><cell cols="2">80.9 ± 0.4 92.3 ± 0.3</cell></row><row><cell>HGCN-ATT</cell><cell cols="2">82.0 ± 0.2 92.5 ± 0.2</cell></row><row><cell>HGCN-C</cell><cell>89.1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>ROC AUC for link prediction on AIRPORT and DISEASE datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>For any hyperbolic spaces with constant curvatures −1/K, −1/K &gt; 0, and any pair of hyperbolic points (u, v) embedded in H d,K , there exists a mapping φ : H d,K → H d,K to another pair of corresponding hyperbolic points in H d,K , (φ(u), φ(v)) such that the Minkowski inner product is scaled by a constant factor.</figDesc><table><row><cell cols="3">Therefore, we finally have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>log K x (y) =</cell><cell>√</cell><cell>Karcosh −</cell><cell>x √ K</cell><cell>,</cell><cell>y √ K</cell><cell>L</cell><cell>y + 1 K x, y L x ||y + 1 K x, y L x|| L</cell><cell>.</cell></row><row><cell>B.2 Curvature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lemma 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Benchmarks' statisticsTheorem 4.1. For any hyperbolic curvatures −1/K, −1/K &lt; 0, for any node embeddings H = {h i } ⊂ H d,K of a graph G, we can find H ⊂ H d,K</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Project website with code and data: http://snap.stanford.edu/hgcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The equivalent of GCN in link prediction is GAE<ref type="bibr" target="#b19">[20]</ref>. We did not compare link prediction GNNs based on shallow embeddings such as<ref type="bibr" target="#b48">[49]</ref> since they are not inductive.<ref type="bibr" target="#b2">3</ref> HGCN uses DropConnect<ref type="bibr" target="#b41">[42]</ref>, as described in Appendix C.<ref type="bibr" target="#b3">4</ref> Code available at http://snap.stanford.edu/hgcn. We provide HGCN implementations for hyperboloid and Poincaré models. Empirically, both models give similar performance but hyperboloid model offers more stable optimization, because Poincaré distance is numerically unstable<ref type="bibr" target="#b29">[30]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proposition 3.1. Let x ∈ H d,K , u ∈ T x H d,K be unit-speed. The unique unit-speed geodesic γ x→u (·) such that γ x→u (0) = x,γ x→u (0) = u is γ K x→u (t) = cosh t √ K x + √ Ksinh t √ K u, and the intrinsic distance function between two points x, y in H d,K is then:</p><p>Proof. Using theorem A.1, we know that the unique unit-speed geodesic γ y→u (.) in H d,1 must satisfy γ y→u (0) = y andγ y→u (0) = u and d dt γ y→u (t),γ y→u (t) L = 0 ∀t, and is given by</p><p>and since γ K x→u (.) is the unique unit-speed geodesic in H d,K , we also have <ref type="bibr" target="#b0">1</ref> and we get φ y→u (t) = cosh(t)y + sinh(t)u.</p><p>Finally, this leads to </p><p>Proof. We use a similar reasoning to that in Corollary 1.1 in <ref type="bibr" target="#b10">[11]</ref>. Let γ K x→v (.) be the unique geodesic such that</p><p>Therefore φ K x→u (.) is a unit-speed geodesic in H d,K and we get φ K x→u (t) = cosh(</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree-like structure in large social and information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 13th International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Infectious diseases of humans: dynamics and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>May</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Benjamin Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10359</idno>
		<title level="m">Neural embeddings of graphs in hyperbolic space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the hyperbolicity of small-world and treelike random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangda</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="434" to="491" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical structure and the prediction of missing links in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7191</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Embedding text in hyperbolic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Les éléments aléatoires de nature quelconque dans un espace distancié</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Fréchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;institut Henri Poincaré</title>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperbolic entailment cones for learning hierarchical embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1632" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning mixed-curvature representations in product spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hyperbolic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaled gromov hyperbolic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Jonckheere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poonsuk</forename><surname>Lohsoonthorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bonahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Theory</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyla</forename><surname>Mirvakhabova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02239</idno>
		<title level="m">Hyperbolic image embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geographic routing using hyperbolic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Communications</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kruskal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
	<note type="report_type">Psychometrika</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale curvature of networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onuttom</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iraj</forename><surname>Saniee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66108</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the lorentz model of hyperbolic geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3776" to="3785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical organization in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erzsébet</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dietmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salamon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Introduction to differential geometry. ETH, Lecture Notes, preliminary version</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4457" to="4466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Low distortion delaunay embedding of trees in hyperbolic plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Collective classification in network data. AI magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The string database in 2017: quality-controlled protein-protein association networks, made broadly accessible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nadezhda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Doncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hyperbolic representation learning for fast and efficient neural question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Poincaré glove: Hyperbolic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cortecon: a temporal transcriptome analysis of in vitro human cerebral cortex development from human embryonic stem cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Van De Leemput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Thomas R Kiehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patty</forename><surname>Corneo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vilas</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Refugio</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">L</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spherical and hyperbolic embeddings of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elżbieta</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert Pw</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2255" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Riemannian svrg: Fast stochastic optimization on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4592" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evolution of resilience in protein interactomes across the tree of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="4426" to="4433" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
