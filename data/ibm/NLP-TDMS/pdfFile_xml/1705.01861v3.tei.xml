<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Tubelet Detector for Spatio-Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
						</author>
						<title level="a" type="main">Action Tubelet Detector for Spatio-Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level that are then linked or tracked across time. In this paper, we leverage the temporal continuity of videos instead of operating at the frame level. We propose the ACtion Tubelet detector (ACT-detector) that takes as input a sequence of frames and outputs tubelets, i.e., sequences of bounding boxes with associated scores. The same way state-of-theart object detectors rely on anchor boxes, our ACT-detector is based on anchor cuboids. We build upon the SSD framework <ref type="bibr" target="#b17">[18]</ref>. Convolutional features are extracted for each frame, while scores and regressions are based on the temporal stacking of these features, thus exploiting information from a sequence. Our experimental results show that leveraging sequences of frames significantly improves detection performance over using individual frames. The gain of our tubelet detector can be explained by both more accurate scores and more precise localization. Our ACT-detector outperforms the state-of-the-art methods for frame-mAP and video-mAP on the J-HMDB [12] and UCF-101 <ref type="bibr" target="#b29">[30]</ref> datasets, in particular at high overlap thresholds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action localization is one of the key elements to video understanding. It has been an active research topic for the past years due to various applications, e.g. video surveillance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> or video captioning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>. Action localization focuses both on classifying the actions present in a video and on localizing them in space and time. Action localization task faces significant challenges, e.g. intra-class variability, cluttered background, low quality video data, occlusion, changes in viewpoint. Recently, Convolutional Neural Networks (CNNs) have proven well adapted for action localization, as they provide robust representations of video frames. Indeed, most state-of-the-art action localization approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> are based on CNN object detectors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> that detect human actions at the 1 Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. <ref type="bibr" target="#b1">2</ref> University of Edinburgh 3 Naver Labs Europe standing up? si,ng down? <ref type="figure">Figure 1</ref>. Understanding an action from a single frame can be ambiguous, e.g. sitting down or standing up; the action becomes clear when looking at a sequence of frames. frame level. Then, they either link frame-level detections or track them over time to create spatio-temporal tubes. Although these action localization methods have achieved remarkable results <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>, they do not exploit the temporal continuity of videos as they treat the video frames as a set of independent images on which a detector is applied independently. Processing frames individually is not optimal, as distinguishing actions from a single frame can be ambiguous, e.g. person sitting down or standing up <ref type="figure">(Figure 1</ref>). In this paper, we propose to surpass this limitation and treat a video as a sequence of frames. Modern object detectors for images, such as Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> and Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b17">[18]</ref>, proceed by classifying and regressing a set of anchor boxes to the ground-truth bounding box of the object. In this paper, we introduce a spatio-temporal tubelet extension of this design. Our Action Tubelet detector (ACT-detector) takes as input a short sequence of a fixed number of frames and outputs tubelets, i.e., sequences of bounding boxes over time ( <ref type="figure" target="#fig_0">Figure 2</ref>). Our method considers densely sampled anchors of cuboid shape with various sizes and aspect ratios. At test time, we generate for each anchor cuboid a score for a given action and regressed coordinates transforming it into a tubelet. Importantly, the score and regression are based on convolutional feature maps from all frames in the sequence. While the anchor cuboids have fixed spatial extent across time, the tubelets change size, location and aspect ratio over time, following the actors. Here we build upon the SSD framework, but the proposed tubelet extension is applicable to other detectors based on anchor boxes, such as Faster R-CNN. Overview of our ACT-detector. Given a sequence of frames, we extract convolutional features with weights shared between frames. We stack the features from subsequent frames to predict scores and regress coordinates for the anchor cuboids (middle figure, blue color). Depending on the size of the anchors, the features come from different convolutional layers (left figure, color coded: yellow, red, purple, green). As output, we obtain tubelets (right figure, yellow color).</p><p>Our experiments show that taking as input a sequence of frames improves: (a) action scoring, because the ambiguity between different actions reduces and (b) localization accuracy, because frames in a cuboid are regressed jointly and hence, they share information about the location of the actor in neighboring frames, see <ref type="figure">Figure 1</ref>. Our ACT-detector obtains state-of-the-art frame-mAP and video-mAP performance on the J-HMDB <ref type="bibr" target="#b11">[12]</ref> and UCF-101 <ref type="bibr" target="#b29">[30]</ref> datasets, in particular at high overlap thresholds. In summary, we make the following contributions:</p><p>• We introduce the ACT-detector, an action tubelet detector that proceeds by scoring and regressing anchor cuboids.</p><p>• We demonstrate that anchor cuboids can handle moving actors for sequences up to around 10 frames.</p><p>• We provide an extensive analysis demonstrating the clear benefit of leveraging sequences of frames instead of operating at the frame level.</p><p>The code of our ACT-detector is available at http://thoth.inrialpes.fr/src/ACTdetector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Almost all recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> for action localization build on CNN object detectors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. In the following, we review recent CNN object detectors and then state-of-the-art action localization approaches. Object detection with CNNs. Recent state-of-the-art object detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> are based on CNNs. R-CNN <ref type="bibr" target="#b7">[8]</ref> casts the object detection task as a region-proposal classification problem. Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> extends this approach by generating bounding box proposals with a fullyconvolutional Region Proposal Network (RPN). RPN considers a set of densely sampled anchor boxes, that are scored and regressed. Moreover, it shares convolutional features with proposal classification and regression branches. These branches operate on fixed-size features obtained using a Region-of-Interest (RoI) pooling layer. In a similar spirit, YOLO <ref type="bibr" target="#b22">[23]</ref> and SSD <ref type="bibr" target="#b17">[18]</ref> also use a set of anchor boxes, which are directly classified and regressed without a RoI pooling layer. In YOLO, all scores and regressions are computed from the last convolutional feature maps, whereas SSD adapts the features to the size of the boxes. Features for predicting small-sized boxes come from early layers, and features for big boxes come from the latter layers, with larger receptive fields. All these object detectors rely on anchor boxes. In our work, we extend them to anchor cuboids leading to significant improvement for action localization. Action localization. Initial approaches for spatio-temporal action localization are extensions of the sliding window scheme <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, requiring strong assumptions such as a cuboid shape, i.e., a fixed spatial extent of the actor across frames. Other methods extend object proposals to videos. Hundreds of action proposals are extracted per video given low-level cues, such as super-voxels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref> or dense trajectories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref>. They then cast action localization as a proposal classification problem. More recently, some approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref> rely on an actionness measure <ref type="bibr" target="#b3">[4]</ref>, i.e., a pixel-wise probability of containing any action. To estimate actionness, they use low-level cues such as optical flow <ref type="bibr" target="#b36">[37]</ref>, CNNs with a two-stream fully-convolutional architecture <ref type="bibr" target="#b32">[33]</ref> or recurrent neural networks <ref type="bibr" target="#b32">[33]</ref>. They extract action tubes either by thresholding <ref type="bibr" target="#b15">[16]</ref> the actionness score or by using a maximum set coverage formulation <ref type="bibr" target="#b36">[37]</ref>. This, however, outputs only a rough localization of the action as it is based on noisy pixel-level maps.</p><p>Most recent approaches rely on object detectors trained to discriminate human action classes at the frame level. Gkioxari and Malik <ref type="bibr" target="#b8">[9]</ref> extend the R-CNN framework to a two-stream variant <ref type="bibr" target="#b26">[27]</ref>, processing RGB and flow data separately. The resulting per-frame detections are then linked using dynamic programming with a cost function based on detection scores of the boxes and overlap between detections of consecutive frames. Weinzaepfel et al. <ref type="bibr" target="#b34">[35]</ref> replace the linking algorithm by a tracking-by-detection method. More recently, two-stream Faster R-CNN was introduced by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. Saha et al. <ref type="bibr" target="#b25">[26]</ref> fuse the scores of both streams based on overlap between the appearance and the motion RPNs. Peng and Schmid <ref type="bibr" target="#b21">[22]</ref> combine proposals extracted from the two streams and then classify and regress them with fused RGB and multi-frame optical flow features. They also use multiple regions inside each action proposal and then link the detections across a video based on spatial overlap and classification score. Singh et al. <ref type="bibr" target="#b28">[29]</ref> perform action localization in real-time using (a) the efficient SSD detector, (b) a fast method <ref type="bibr" target="#b12">[13]</ref> to estimate the optical flow for the motion stream, and (c) an online linking algorithm. All these approaches rely on detections at the frame level. In contrast, we build our ACT-detector by taking as input sequences of frames and demonstrate improved action scores and location accuracy over frame-level detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ACtion Tubelet (ACT) detector</head><p>We introduce the ACtion Tubelet detector (ACTdetector), an action tubelet approach for action localization in videos. The ACT-detector takes as input a sequence of K frames f 1 , ..., f K and outputs a list of spatio-temporal detections, each one being a tubelet, i.e., a sequence of bounding boxes, with one confidence score per action class. The idea of such an extension to videos could be applied on top of various state-of-the-art object detectors. Here, we apply our method on top of SSD, as it has lower runtime than other detectors, which makes it suitable for large video datasets. In this section, we first describe our fdafd4dfposed ACT-detector (Section 3.1), and then our full framework for video detection (Section 3.2). Finally, Section 3.3 describes our method for constructing action tubes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ACT-detector</head><p>In this paper, we claim that action localization benefits from predicting tubelets taking as input a sequence of frames instead of operating at the frame level. Indeed, the appearance and even the motion may be ambiguous for a single frame. Considering more frames for predicting the scores reduces this ambiguity ( <ref type="figure">Figure 1</ref>). Moreover, this allows to perform regression jointly over consecutive frames. Our ACT-detector builds upon SSD, see <ref type="figure" target="#fig_0">Figure 2</ref> for an overview of the approach. In the following we review the SSD detector in details and then present our ACT-detector. SSD detector. The SSD detector (Single Shot MultiBox Detector) <ref type="bibr" target="#b17">[18]</ref> performs object detection by considering a set of anchor boxes at different positions, scales and aspect ratios. Each of them is (a) scored for each object class and for a background class, and (b) regressed to better fit the object extent. SSD uses a fully convolutional architecture, without any object proposal step, enabling fast computation. Classification and regression are performed using different convolutional layers depending on the scale of the anchor box. Note that the receptive field of a neuron used to predict the classification scores and the regression of a given anchor box remains significantly larger than the box.</p><p>ACT-detector. Given a sequence of K frames, the ACTdetector computes convolutional features for each one. The weights of these convolutional features are shared among all input frames. We extend the anchor boxes of SSD to anchor cuboids by assuming that the spatial extent is fixed over time along the K frames. We then stack the corresponding convolutional features from each of K frames ( <ref type="figure" target="#fig_0">Figure 2)</ref>. The stacked features are the input of two convolutional layers, one for scoring action classes and one for regressing the anchor cuboids. For instance, when considering an anchor cuboid for which the prediction is based on the 'red' feature maps of <ref type="figure" target="#fig_0">Figure 2</ref>, the classification and regression are performed with convolutional layers that take as input the 'red' stacked feature maps from the K frames. The classification layer outputs for each anchor cuboid C + 1 scores: one per action class plus one for the background. This means that the tubelet classification is done based on the sequence of frames. The regression outputs 4 × K coordinates (4 for each of the K frames) for each anchor cuboid. Note that although all boxes in a tubelet are regressed jointly, they result in a different regression for each frame.</p><p>The initial anchor cuboids have a fixed spatial extent over time. In Section 4.2 we show experimentally that such anchor cuboids can handle moving actors for short sequences of frames. Note that the receptive field of the neurons used to score and regress an anchor cuboid is larger than its spatial extent. This allows us to base the prediction also on the context around the cuboid, i.e., with knowledge for actors that may move outside the cuboid. Moreover, the regression significantly deforms the cuboid shape. Even though anchor cuboids have fixed spatial extent, the tubelets obtained after regressing the 4×K coordinates do not. We display two examples in <ref type="figure" target="#fig_1">Figure 3</ref> with the anchor cuboid (cyan boxes) and the resulting regressed tubelet (yellow boxes). Note how the regression outputs an accurate localization despite the change in aspect ratio of the action boxes across time. Training loss. For training, we consider only sequences of frames in which all frames contain the ground-truth action. As we want to learn action tubes, all positive and negative training data come from sequences in which actions occur. We exclude sequences in which the action starts or ends. Let A be the set of anchor cuboids. We denote by P the set of anchor cuboids for which at least one groundtruth tubelet has an overlap over 0.5, and by N the complementary set. Overlap between tubelets is measured by averaging the Intersection over Union (IoU) between boxes over K frames. Each anchor cuboid from P is assigned to ground-truth boxes with IoU over 0.5. More precisely, let x y ij ∈ {0, 1} be the binary variable whose value is 1 if and only if the anchor cuboid a i is assigned to the ground-truth tubelet g j of label y. The training loss L is defined as:</p><formula xml:id="formula_0">L = 1 N Lconf + Lreg ,<label>(1)</label></formula><p>with N = i,j,y x y ij the number of positive assignments and Lconf (resp. Lreg) the confidence (resp. regression) loss as defined below. The confidence loss is defined using a softmax loss. Let c y i be the predicted confidence score (after softmax) of an anchor a i for class y. The confidence loss is:</p><formula xml:id="formula_1">Lconf = − i∈P x y ij log (ĉ y i ) − i∈N log ĉ 0 i .<label>(2)</label></formula><p>The regression loss is defined using a Smooth-L1 loss between the predicted regression and the ground-truth target. We regress an offset for the center (x, y) of each box in the tubelet, as well as for the width w and the height h. The regression loss is averaged over K frames. More precisely, letr x k i be the predicted regression for the x coordinate of anchor a i at frame f k and let g j be the ground-truth target. The regression loss is defined as:</p><formula xml:id="formula_2">Lreg = 1 K i∈P c∈{x,y,w,h} x y ij K k=1 SmoothL1 r c k i − g c k ij , with g x k ij = g x k j − a x k i a w k i g y k ij = g y k j − a y k i a h k i , g w k ij = log g w k j a w k i g h k ij = log   g h k j a h k i   .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two-stream ACT-detector</head><p>Following standard practice for action localization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>, we use a two-stream detector. We train an appearance detector, for which the input is a sequence of K consecutive RGB frames, and a motion detector, which takes as input the flow images <ref type="bibr" target="#b0">[1]</ref> obtained following <ref type="bibr" target="#b8">[9]</ref>.</p><p>Each stream outputs a set of regressed tubelets with confidence scores, originating from the same set of anchor cuboids. For combining the two streams at test time we compare two approaches: union fusion and late fusion. For the union fusion <ref type="bibr" target="#b28">[29]</ref>, we consider the set union of the outputs from both streams: the tubelets from the RGB stream with their associated scores and the tubelets from the flow stream with their scores. For the late fusion <ref type="bibr" target="#b5">[6]</ref>, we average the scores from both streams for each anchor cuboid, as the set of anchors is the same for both streams. We keep the regressed tubelet from the RGB stream, as appearance is more relevant for regressing boxes, in particular for actions with limited motion. Our experiments show that late fusion outperforms the union fusion (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">From action tubelets to spatio-temporal tubes</head><p>For constructing action tubes, we build upon the frame linking algorithm of <ref type="bibr" target="#b28">[29]</ref>, as it is robust to missed detections and can generate tubes spanning different temporal extents of the video. We extend their algorithm from frame linking to tubelet linking and propose a temporal smoothing to build action tubes from the linked tubelets. The method is online and proceeds by iteratively adding tubelets to a set of links while processing the frames. In the following, t is a tubelet and L a link, i.e., a sequence of tubelets. Input tubelets. Given a video, we extract tubelets for each sequence of K frames. This means that consecutive tubelets overlap by K −1 frames. The computation of overlapping tubelets can be performed at an extremely low cost as the weights of the convolutional features are shared. We compute the convolutional features for each frame only once. For each sequence of frames, only the last layers that predict scores and regressions, given the stacked convolutional features <ref type="figure" target="#fig_0">(Figure 2</ref>), remain to be computed. For linking, we keep only the N = 10 highest scored tubelets for each class after non-maximum suppression (NMS) at a threshold 0.3 in each sequence of frames. Overlap between a link and a tubelet. Our linking algorithm relies on an overlap measure ov(L, t) between a link L and a tubelet t that temporally overlaps with the end of the link. We define the overlap between L and t as the overlap between the last tubelet of the link L and t. The overlap between two tubelets is defined as the average IoU between their boxes over overlapping frames. Initialization. In the first frame, a new link is started for each of the N tubelets. At a given frame, new links start from tubelets that are not associated to any existing link. Linking tubelets. Given a new frame f , we extend one by one in descending order of scores each of the existing links with one of the N tubelet candidates starting at this frame. The score of a link is defined as the average score of its tubelets. To extend a link L, we pick the tubelet candidate t  . The yellow color represents the detections and their scores for the classes shown, the red color highlights errors due to missed detections (first row) or wrong labeling (third row) and the green color corresponds to correct labels. Our ACT-detector outputs one class label with one score per tubelet, we thus display it once.</p><p>that meets the following criteria: (i) is not already selected by another link, (ii) has the highest score, and (iii) verifies ov(L, t) τ , with τ a given threshold. In our experiments we use τ = 0.2. Termination. Links stop when these criteria are not met for more than K − 1 consecutive frames. Temporal smoothing: from tubelet links to action tubes.</p><p>For each link L, we build an action tube, i.e., a sequence of bounding boxes. The score of a tube is set to the score of the link, i.e., the average score over the tubelets in the link.</p><p>To set the bounding boxes, note that we have multiple box candidates per frame as the tubelets are overlapping. One can simply use the box of the highest scored tubelet. Instead, we propose a temporal smoothing strategy. For each frame, we average the box coordinates of tubelets that pass through that frame. This allows us to build smooth tubes. Temporal detection. The initialization and termination steps result in tubes spanning different temporal extents of the video. Each tube determines, thus, the start and end in time of the action it covers. No further processing is required for temporal localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section we study the effectiveness of our ACTdetector. After presenting the datasets used in our experiments (Section 4.1), we provide an analysis of our ACTdetector: we validate anchor cuboids (Section 4.2), evaluate input modalities (RGB and flow) and their fusion (Section 4.3), examine the impact of the length K of the se-quence of frames (Section 4.4), and present an error analysis (Section 4.5). We finally compare our method to the state of the art (Section 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets, metrics and implementation details</head><p>Datasets. The UCF-Sports dataset <ref type="bibr" target="#b24">[25]</ref> contains 150 videos from 10 sports classes such as diving or running. The videos are trimmed to the action. We use the train/test split of <ref type="bibr" target="#b13">[14]</ref>.</p><p>The J-HMDB dataset <ref type="bibr" target="#b11">[12]</ref> contains 928 videos with 21 actions, including brush hair and climb stairs. The videos are trimmed to the action. We report results averaged on the three splits defined in <ref type="bibr" target="#b11">[12]</ref>, unless stated otherwise.</p><p>The UCF-101 dataset <ref type="bibr" target="#b29">[30]</ref> contains spatio-temporal annotations for 24 sports classes in 3207 videos. The videos are not trimmed. Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>, we report results for the first split only. Metrics. We use metrics at both frame and video level. Frame-level metrics allow us to compare the quality of the detections independently of the linking strategy. Metrics at the video level are the same as the ones at the frame level, replacing the Intersection-over-Union (IoU) between boxes by a spatio-temporal overlap between tubes, i.e., an average across time of the per-frame IoU <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>. To measure our performance at the frame level, we take into account the boxes originating from all tubelets that pass through the frame with their individual scores and perform NMS. In all cases, we only keep the detections with a score above 0.01.</p><p>We report frame and video mean Average Precision (mAP). A detection is correct if its IoU with a ground-truth   box or tube is greater than 0.5 and its action label is correctly predicted <ref type="bibr" target="#b4">[5]</ref>. For each class, we compute the average precision (AP) and report the average over all classes.</p><p>To evaluate the localization accuracy of the detections, we report MABO (Mean Average Best Overlap) <ref type="bibr" target="#b30">[31]</ref>. We compute the IoU between each ground-truth box (or tube) and our detections. For each ground truth box (or tube), we keep the overlap of the best overlapping detection (BO) and, for each class, we average over all boxes (or tubes) (ABO). The mean is computed over all classes (MABO).</p><p>To evaluate the quality of the detections in terms of scoring, we also measure classification accuracy. In each frame, assuming that the ground-truth localization is known, we compute class scores for each ground-truth box by averaging the scores from the detected boxes or tubelets (after regression) whose overlap with the ground-truth box of this frame is greater than 0.7. We then assign the class having the highest score to each of these boxes and measure the ratio of boxes that are correctly classified. Implementation details. We use VGG <ref type="bibr" target="#b27">[28]</ref> with ImageNet pre-training for both appearance and motion streams <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>. Our frames are resized to 300 × 300. We use the same hard negative mining strategy as SSD <ref type="bibr" target="#b17">[18]</ref>, i.e., to avoid an unbalanced factor between positive and negative samples, only the hardest negatives up to a ratio of 3 negatives for 1 positive are kept in the loss. We perform data augmentation to the whole sequence of frames: photometric transformation, rescaling and cropping. Given the K parallel streams, the gradient of the shared convolutional layers is the sum over the K streams. We find that dividing the learning rate of the shared convolutional layers by K helps convergence, as it prevents large gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Validation of anchor cuboids</head><p>This section demonstrates that an anchor cuboid can handle moving actions. We first measure how much the actors move in the training sets of the three action localization datasets by computing the mean motion overlap. For each box in a ground-truth tube, we measure its motion overlap: the overlap between this box and the ground-truth box n frames later for varying n. For each class, we compute the average motion overlap over all frames and we report the mean over all classes in <ref type="figure" target="#fig_4">Figure 5 (a)</ref>. We observe that the motion overlap reduces as n increases, especially for UCF-Sports and UCF-101 for which the motion overlap for a gap of n = 10 frames is around 60%. This implies that there is still overlap between the ground-truth boxes that are separated by n = 10 frames. It also means that in many cases, this overlap is below 50% due to the motion of the actor.</p><p>In practice, we want to know if we have positive training anchor cuboids. Positive cuboids are the ones that have an overlap of at least 50% with a ground-truth tubelet; the overlap being the average IoU between boxes over the K frames in the sequence. Such cuboids are required for training the classifier and the regressor. Thus, we consider all possible training sequences and compute for each class the recall of the anchor cuboids with respect to the ground-truth tubelets, i.e., the ratio of ground-truth tubelets for which at least one anchor cuboid has an overlap over 0.5. We report the mean recall over the classes for varying IoU thresholds for the three datasets in <ref type="figure" target="#fig_4">Figure 5 (b-d)</ref>. For all datasets, the recall at IoU= 0.5 remains 98% up to K = 6 and over 95% for K = 10. This confirms that cuboid-shaped anchors can be used in case of moving actors. When increasing K, for instance to 32, the recall starts dropping significantly.</p><p>Given that sequences of up to K = 10 frames result in high recall of the anchor cuboids, we examine in Sections 4.3 and 4.4 the performance of our tubelet detector for sequences of length ranging between K = 1 and K = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Tubelet modality</head><p>In this section, we examine the impact of the RGB and flow modalities and their fusion on the performance of our ACT-detector. For all datasets, we examine the frame-mAP when using (i) only RGB data, (ii) only flow data, (iii) union of RGB + flow data <ref type="bibr" target="#b25">[26]</ref>, and (iv) late fusion of RGB + flow data for varying sequence length from 1 to 10 frames.</p><p>For all datasets and for all K, the RGB stream (blue line) outperforms the flow stream (red line), showing that appearance information is on average a more distinctive cue than motion ( <ref type="figure">Figure 6</ref>). In all cases, using both modalities (green and black lines) improves the detection performance com-  <ref type="figure">Figure 6</ref>. Frame-mAP of our ACT-detector on the three datasets when varying K for RGB data (blue line), flow (red line), union and late fusion of RGB + flow data (black and green lines, resp.).</p><p>pared to using only one. We observe that late fusion of the scores (green line) performs consistently better than union fusion (black line), with a gain between 1% and 4% in terms of frame-mAP. This can be explained by the fact that union fusion considers a bigger set of detections without taking into account the similarity between appearance and motion detections. Instead, the late fusion re-scores every detection by taking into account both RGB and flow scores. Given that late fusion delivers the best performance, we use it in the remainder of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Tubelet length</head><p>In this section, we examine the impact of K. We consider K = 1 as the baseline, and we report results for our method with K = 2, 4, 6, 8, 10. We quantify the impact of K by measuring (i) the localization accuracy (MABO), (ii) the classification accuracy, (iii) the detection performance (frame-mAP), and (iv) the motion speed of actors. MABO. MABO allows us to examine the localization accuracy of the per-frame detections when varying K. Results are reported in <ref type="figure" target="#fig_6">Figure 7</ref> (top). For all three datasets we observe that using sequences of frames (K &gt; 1) leads to a significant improvement. In particular, MABO increases up to K = 4, and then remains almost constant up to K = 8 frames. For instance, MABO increases by 5% on UCF-Sports, 2% on J-HMDB and 5% on UCF-101 when using K = 6 instead of K = 1. This clearly demonstrates that performing detection at the sequence level results in more accurate localization, see <ref type="figure" target="#fig_1">Figure 3</ref> for examples. Overall, we observe that K = 6 is one of the values for which MABO obtains excellent results for all datasets. Classification accuracy. We report classification accuracy on the three action localization datasets in <ref type="figure" target="#fig_6">Figure 7</ref> (bottom). Using sequences of frames (K &gt; 1) improves the classification accuracy of the detections for all datasets. For UCF-Sports, the accuracy keeps increasing with K, while for J-HMDB it remains almost constant after K = 6. For UCF-101, the accuracy increases when moving from K = 1 to K = 4 and after K = 8 it starts decreasing. Overall, using up to K = 10 frames improves performance over K = 1. This shows that the tubelet scoring improves the classification accuracy of the detections. Again, K = 6 is one of the values which results in excellent results for all datasets.  Frame-mAP. <ref type="figure">Figure 6</ref> shows the frame-mAP when training the ACT-detector with varying K. On all three datasets, we observe a gain up to 10% when increasing the tubelet length up to K = 6 or 8 frames depending on the dataset, compared to the standard baseline of per-frame detection. This result highlights the benefit of performing detection at the sequence level. For J-HMDB and UCF-101, we also observe a performance drop for K &gt; 8, because regressing from anchor cuboids is harder as (a) the required transformation is larger when the actor moves, and (b) there are more training parameters for less positive samples, given that the recall of anchor cuboids decreases (Section 4.2).</p><p>The above results show that K = 6 gives overall good results. We use this value in the following sections. <ref type="figure" target="#fig_2">Figure 4</ref> shows some qualitative examples comparing the performance for K = 1 and K = 6. We observe that our tubelets lead to less missed detections and to more accurate localization compared to per-frame detection (first and second rows). Moreover, our ACT-detector reduces labeling mistakes when one frame is not enough to disambiguate between classes. For instance, in the last row we predict the correct label catch, whereas in the third row there is a big variance in the labels (swing basketball, kick ball, catch).</p><p>Handling moving actors. To validate that our ACTdetector can handle moving actors, we measure frame-mAP with respect to the speed of the actor. We group actors into three categories (slow, medium, fast) with 1/3 of the data in each category. Speed is computed using the IoU of an actor with its instances in ±10 neighboring frames. <ref type="table">Table 1</ref> reports the frame-mAP at IoU = 0.5 for the three categories. For all datasets there is a clear gain between K = 1 and K = 6 for all speeds. In particular, for actors with fast motion the gain is +8% for UCF-Sports, +9% for J-HMDB, and +3% for UCF-101. This confirms that our tubelets can successfully handle large displacements. A potential explanation is the fact that the receptive fields are significantly larger than the the spatial extent of the anchor cuboid. <ref type="figure">Figure 8</ref>. Error analysis of our ACT-detector for K = 1 and K = 6 on three action localization datasets. We show frame-mAP and different sources of error, see Section 4.5 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Error breakdown analysis</head><p>In this section, we examine the cause of errors in frame-mAP to better understand the reasons why our tubelets improve detection performance. More precisely, we consider five mutually exclusive factors and analyze which percentage of the mAP is lost due to each of them:</p><p>1. localization error E L : the detection is in a frame containing the correct class, but the localization is wrong, i.e., IoU &lt; 0.5 with the ground-truth box. 2. classification error E C : the detection has IoU 0.5 with the ground-truth box of another action class. 3. time error E T : the detection is in an untrimmed video for the correct class, but the temporal extent of the action does not cover this frame. 4. other errors E O : the detection appears in a frame without the class, and has IoU &lt; 0.5 with ground-truth boxes of any other class. 5. missed detections E M : we do not have a detection for a ground-truth box. The first four factors are categories of false positive detections, while E M refers to the ones we did not detect at all. For the first four factors, we follow the frame-mAP computation and measure the area under the curve when plotting the percentage of each category at all recall values. The missed detections (E M ) factor is computed by measuring the percentage of missed detections, i.e., ratio of groundtruth boxes for which there are no correct detections. <ref type="figure">Figure 8</ref> shows the percentage that each of these factors contributes to errors in the mAP for K = 1 and K = 6 with late fusion of RGB and flow as input modalities. For all datasets, we observe that when going from K = 1 to K = 6 there is almost no change in E L or in E O . In particular, for UCF-Sports and J-HMDB their values are extremely small even for K = 1. We also observe a significant decrease of E C between K = 1 and K = 6, in particular on the UCF-Sports and J-HMDB datasets. This highlights that including more frames facilitates the action classification task <ref type="figure">(Figure 1</ref>). This drop is lower on the UCF-101 dataset. This can be explained by the fact that most errors in this dataset come from false detections outside the tem- poral extent of the actions (E T ). Note that E T = 0 on UCF-Sports and J-HMDB, as these datasets are trimmed. For all datasets, a big gain comes from the missed detections E M : for K = 6 the percentage of missed detections drops significantly compared to K = 1. For instance, on J-HMDB the percentage of missed detections is reduced by a factor of 2. This clearly shows the ability of our proposed ACT-detector not only to better classify and localize (E C and MABO) actions but also to detect actions missed by the single-frame detector (see <ref type="figure" target="#fig_2">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison to the state of the art</head><p>We compare our ACT-detector to the state of the art. Note that our results reported in this section are obtained by stacking 5 consecutive flow images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> as input to the motion stream, instead of just 1 for each of the K = 6 input frames. This variant brings about +1% frame-mAP. Frame-mAP. We report frame-mAP on the three datasets in <ref type="table" target="#tab_3">Table 2</ref>. We compare our performance with late fusion of RGB+5flows when K = 6 to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>, that use a twostream R-CNN, and to <ref type="bibr" target="#b32">[33]</ref>, which is based on actionness. We also compare to Peng and Schmid <ref type="bibr" target="#b21">[22]</ref> that build upon a two-stream Faster R-CNN with multiscale training and testing. We report results of <ref type="bibr" target="#b21">[22]</ref> with and without their multi-region approach. The latter case can be seen as the baseline Faster R-CNN with multiscale training and testing for K = 1. Our ACT-detector (i.e., with K = 6) brings a clear gain in frame-mAP, outperforming the state of the art on UCF-Sports, J-HMDB and UCF-101. We also observe that overall the performance of the baseline SSD (K = 1) is somewhat lower (by around 3 to 5%) than Faster R-CNN used by the state of the art <ref type="bibr" target="#b21">[22]</ref>, see <ref type="figure">Figure 6</ref>. SSD, however, is much faster than Faster R-CNN, and therefore more suitable for large video datasets. Video-mAP. <ref type="table">Table 3</ref> reports the video-mAP results for our method and the state of the art at various IoU thresholds (0.2, 0.5, and 0.75). We also report results with the protocol 0.5:0.95 <ref type="bibr" target="#b16">[17]</ref>, which averages over multiple IoU thresholds, i.e., over 10 IoU thresholds between 0.5 and 0.95 with a step of 0.05. At rather low IoU thresholds (0.2, 0.5) on UCF-Sports and J-HMDB the performance of our ACTdetector is comparable to the state-of-the-art methods that rely on Faster R-CNN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref> or on SSD <ref type="bibr" target="#b28">[29]</ref>. At higher overlap thresholds we significantly outperform them. For instance on UCF-Sports and J-HMDB at IoU = 0.75 we detector method UCF-Sports J-HMDB (all splits) UCF-101 (split 1) 0.2 0.5 0.75 0.  <ref type="table">Table 3</ref>. Comparison of video-mAP to the state of the art at various detection thresholds. The columns 0.5:0.95 correspond to the average video-mAP for thresholds with step 0.05 in this range. For <ref type="bibr" target="#b21">[22]</ref>, we report the results with and without their multi-region (+MR) approach.</p><p>outperform <ref type="bibr" target="#b21">[22]</ref> by 31% and 4%. In particular, our performance drops slower than the state of the art as the IoU threshold increases. This highlights the high localization accuracy of our tubelets and, therefore of our tubes. On UCF-101, we significantly outperform the state of the art at all overlap thresholds, with a larger gap at high thresholds. For instance, we outperform <ref type="bibr" target="#b28">[29]</ref> by 5% at IoU = 0.5, and by 7.5% at IoU = 0.75. To validate our tubelet linking strategy (Section 3.3), we experiment with an approach that transforms tubelets into individual boxes and links them with <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. We observe a consistent gain of 1% on all datasets. As a summary, our ACT-detector improves over the state of the art, especially at high thresholds.</p><p>Runtime. We compare our runtime using two streams (appearance and flow) to the frame-based SSD approach of Singh et al. <ref type="bibr" target="#b28">[29]</ref> and to frame-based Faster R-CNN approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. We report runtime on a single GPU without flow computation. Faster R-CNN based approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref> run at 4fps and the SSD-based method <ref type="bibr" target="#b28">[29]</ref> at 25-30fps.</p><p>Our ACT-detector also runs at 25-30fps (K = 6). Computing tubelets has a low overhead, since the convolutional features are computed once per frame due to the parallel architecture with shared weights. The post-processing is extremely fast (∼300fps) for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We introduced the ACT-detector, a tubelet detector that leverages the temporal continuity of video frames. It takes as input a sequence of frames and outputs tubelets instead of operating on single frames, as is the case with previous state-of-the-art methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Our method builds upon SSD and introduces anchor cuboids that are scored and regressed over sequences of frames. An extensive experimental analysis shows the benefits of our ACT-detector for both classification and localization. It achieves state-ofthe-art results, in particular for high overlap thresholds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of our ACT-detector. Given a sequence of frames, we extract convolutional features with weights shared between frames. We stack the features from subsequent frames to predict scores and regress coordinates for the anchor cuboids (middle figure, blue color). Depending on the size of the anchors, the features come from different convolutional layers (left figure, color coded: yellow, red, purple, green). As output, we obtain tubelets (right figure, yellow color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples of regressed tubelets (yellow) from cuboids (cyan) in our ACT-detector. Note the accurate localization of the tubelet, despite the fact that the aspect ratio of the cuboid is changing over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples when comparing per-frame (K = 1) and tubelet detections (K = 6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) Motion overlap: Mean motion overlap between a box in a ground-truth tube and its box n frames later for varying n.(b-d) Recall of the anchor cuboids for various IoU thresholds on the training set of three action localization datasets. The numbers in parenthesis indicate the recall at IoU = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>MABO (top)  and classification accuracy (bottom) of our ACT-detector on the three datasets when varying K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of frame-mAP to the state of the art. For<ref type="bibr" target="#b21">[22]</ref>, we report the results with and without their multi-region (+MR).</figDesc><table><row><cell>detector</cell><cell>method</cell><cell cols="3">UCF-Sports J-HMDB UCF-101</cell></row><row><cell cols="2">actionness [33]</cell><cell>-</cell><cell>39.9</cell><cell>-</cell></row><row><cell>R-CNN</cell><cell>[9] [35]</cell><cell>68.1 71.9</cell><cell>36.2 45.8</cell><cell>-35.8</cell></row><row><cell>Faster</cell><cell>[22] w/o MR</cell><cell>82.3</cell><cell>56.9</cell><cell>64.8</cell></row><row><cell>R-CNN</cell><cell>[22] with MR</cell><cell>84.5</cell><cell>58.5</cell><cell>65.7</cell></row><row><cell>SSD</cell><cell>ours</cell><cell>87.7</cell><cell>65.7</cell><cell>67.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by the ERC grants ALLEGRO and VisCul, the MSR-Inria joint project, a Google research award, a Facebook gift, an Intel gift and an Amazon research award. We gratefully acknowledge the support of NVIDIA with the donation of GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-dataset action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action detection by implicit intentional motion clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">APT: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on visual surveillance of object motion and behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast optical flow using dense inverse search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative figure-centric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieving actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised tube extraction using transductive learning and dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Marian</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A large-scale benchmark dataset for event recognition in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatiotemporal object detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-region two-stream R-CNN for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action MACH: A spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for detecting multiple space-time action tubes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Online real time multiple spatiotemporal action localisation and prediction on a single platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1611.08563</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
