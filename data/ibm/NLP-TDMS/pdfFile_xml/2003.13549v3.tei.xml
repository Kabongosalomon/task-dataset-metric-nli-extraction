<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Haase</surname></persName>
							<email>daniel.haase@zeiss.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Amthor</surname></persName>
							<email>manuel.amthor@zeiss.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ZEISS Microscopy</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ZEISS Microscopy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce blueprint separable convolutions (BSConv) as highly efficient building blocks for CNNs. They are motivated by quantitative analyses of kernel properties from trained models, which show the dominance of correlations along the depth axis. Based on our findings, we formulate a theoretical foundation from which we derive efficient implementations using only standard layers. Moreover, our approach provides a thorough theoretical derivation, interpretation, and justification for the application of depthwise separable convolutions (DSCs) in general, which have become the basis of many modern network architectures. Ultimately, we reveal that DSC-based architectures such as MobileNets implicitly rely on cross-kernel correlations, while our BSConv formulation is based on intrakernel correlations and thus allows for a more efficient separation of regular convolutions. Extensive experiments on large-scale and fine-grained classification datasets show that BSConvs clearly and consistently improve MobileNets and other DSC-based architectures without introducing any further complexity. For fine-grained datasets, we achieve an improvement of up to 13.7 percentage points. In addition, if used as drop-in replacement for standard architectures such as ResNets, BSConv variants also outperform their vanilla counterparts by up to 9.5 percentage points on ImageNet. Code and models are available under https: //github.com/zeiss-microscopy/BSConv.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b26">26]</ref> have become the basis of practically all state-of-the-art approaches for image classification, object detection <ref type="bibr" target="#b5">[5]</ref>, semantic segmentation <ref type="bibr" target="#b30">[30]</ref>, and many other applications. In the past, improvements of CNNs were mainly driven by increasing the model capacity, while at the same time ensuring a proper training behavior <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. Recently, this has led to the development of models with half a billion parameters <ref type="bibr" target="#b18">[18]</ref>. * Authors contributed equally <ref type="figure">Figure 1</ref>. We introduce blueprint separable convolutions (BSConv) as highly efficient building block for CNNs. BSConv exploits correlations of CNN kernels along their depth axis. In consequence, BSConv represents each filter kernel using one 2d blueprint kernel which is distributed along the depth axis using a weight vector.</p><p>In practical applications, however, the computational capacity is often limited, especially in mobile and automotive contexts. This fact has led to another important research direction which focuses on improving model efficiency. The most prominent approaches are based on depthwise separable convolutions (DSCs) <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b16">16]</ref>-building blocks which are motivated by exploiting redundancies of filter weights.</p><p>Based on quantitative and qualitative analyses of trained CNNs, in Section 3 we propose blueprint separable convolutions (BSConv), which follow this path of research. The main idea of BSConv is to exploit that kernels of CNNs usually show high redundancies along their depth axis (i.e., intra-kernel correlations). Thus, BSConv represents each filter kernel using one 2d blueprint which is distributed along the depth axis using a weight vector (see <ref type="figure">Figure 1</ref>).</p><p>Although DSCs are also motivated by intra-kernel correlations <ref type="bibr" target="#b38">[37]</ref>, in Section 4 we show that their derived order of operations contradicts this assumption and is reversed compared to our BSConv solution. In fact, the DSC result is equivalent to assuming redundancies between kernels (i.e., cross-kernel correlations, see <ref type="figure" target="#fig_8">Figure 5</ref>), which was shown <ref type="bibr">Figure 2</ref>. Exemplary filter weights of a vanilla VGG-19, Inception v2, and ResNet-50 CNN trained on ImageNet. For each of the three CNN architectures, one filter kernel of size M × K × K = 128 × 3 × 3 is visualized, split into 128 images. The weights of each filter kernel are highly correlated along the depth axis. Concretely, most slices show the same filter-specific 3 × 3 'blueprint', only scaled by different factors (including negative factors which cause 'inverted' versions of the blueprint). While only three slices are highlighted for each kernel, the correlation is visible for far more slices (52/128 for the ResNet-50 example). This observation is the motivation for our proposal of blueprint separable convolutions (BSConv), which inherently represent a filter kernel of size M × K × K using one blueprint kernel of size K × K and a set of M multiplicative factors used to distribute the blueprint across the depth axis.</p><p>to be less effective when separating convolutions <ref type="bibr" target="#b7">[7]</ref>. In addition, a natural extension of BSConv leads to an interpretation and justification for DSCs with linear bottlenecks <ref type="bibr" target="#b36">[35]</ref>, which are extensively used in many recent models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b42">41]</ref>. Our solution, however, directly implies the use of an additional regularization loss to improve the subspace transform implicitly performed by these bottlenecks.</p><p>In Section 5, we thoroughly demonstrate that BSConv consistently outperforms DSC-based architectures such as MobileNets on a wide variety of large-scale and finegrained datasets at the same parameter and time complexity. Furthermore, BSConv can be used as drop-in replacement for standard convolution layers and can be applied to other architectures as well, leading to performance gains while drastically increasing model efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Numerous approaches exist to improve the efficiency of CNNs. One example is model pruning, where filters and/or connections are removed from CNNs during or after model training <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b29">29]</ref>. Closely related and often combined are quantization <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b19">19]</ref> and compression techniques <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b8">8]</ref> to accelerate model inference.</p><p>Another relevant research area is efficiency-driven CNN architecture search. It can be performed manually <ref type="bibr" target="#b12">[12]</ref> or automatically, e.g. via genetic algorithms <ref type="bibr" target="#b44">[43]</ref> or via reinforcement learning in the form of neural architecture search <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b1">1]</ref>. The latter is the basis for the most recent advances in building efficient models such as MnasNet [40], Mo-bileNetV3 <ref type="bibr" target="#b15">[15]</ref>, and EfficientNet <ref type="bibr" target="#b42">[41]</ref>.</p><p>Building blocks typically consist of activation, normalization, and convolution operations, where the latter bear the greatest potential for efficiency optimizations. Redundancies in convolution weights of trained CNNs are ana-lyzed in <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b7">7]</ref>. Approaches to reduce these redundancies are for instance low-rank approximations of the filter kernels <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> and the usage of grouped convolutions <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b45">44]</ref>. In <ref type="bibr" target="#b38">[37]</ref>, DSCs are introduced, which form the basis for practically all recent efficient network architectures. Their direct application can for instance be found in MobileNetV1 <ref type="bibr" target="#b16">[16]</ref>, factorized CNNs <ref type="bibr" target="#b43">[42]</ref>, and Xception <ref type="bibr" target="#b2">[2]</ref>. An extension of DSCs are inverted residual bottlenecks which are introduced in MobileNetV2 <ref type="bibr" target="#b36">[35]</ref>. They are used in state-of-the-art efficient architectures, including Mnas-Net [40], MobileNetV3 <ref type="bibr" target="#b15">[15]</ref>, and EfficientNet <ref type="bibr" target="#b42">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Blueprint Separable Convolutions (BSConv)</head><p>In standard CNNs, each convolution layer transforms an input tensor U of size M × Y × X into an output tensor V of size N × Y × X by applying convolution kernels F (1) , . . . , F (N ) , each of size M × K × K such that V n,:,: = U * F (n) <ref type="bibr" target="#b1">(1)</ref> with n ∈ {1, . . . , N } (see <ref type="figure">Figure 1</ref>, top row). The entries (or 'weights') of these N kernels are optimized during the training stage of CNNs via backpropagation and may take arbitrary values. However, in the following we show that in practice these weights often converge towards a state in which they show a substantial amount of correlation. We analyze these correlations qualitatively and quantitatively, and then derive a new, parameter-and time-efficient version of convolution layers for CNNs based on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Intra-Kernel Correlations in Standard CNNs</head><p>In this paper we focus on intra-kernel correlations and their potential for the design of parameter-and timeefficient CNNs. We start with a qualitative analysis of intrakernel correlations by visualizing filters of trained CNNs.  <ref type="figure">Figure 2</ref> shows exemplary filter kernels for three established CNN architectures trained on the ImageNet datasetnamely VGG-19 <ref type="bibr" target="#b39">[38]</ref>, Inception v2 <ref type="bibr" target="#b20">[20]</ref>, and ResNet-50 <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>. From these visualizations, it can be seen that intrakernel correlations exist along the depth axis. Concretely, it often seems that for a filter F (n) , its slices F show the same filter-specific K ×K 'blueprint', only scaled by different factors (including negative factors which cause 'inverted' versions of the blueprint).</p><p>While <ref type="figure">Figure 2</ref> shows only a small subset of filter kernels, the described property of filter slices being based on a 'blueprint' are by no means an exception. According to our observations, it occurs consistently across different CNN architectures, training settings, and datasets. To systematically quantify to which extent filter kernels show this behavior, we analyze several trained CNNs in the following way: For each individual filter of the CNN, we (i) split the M × K × K kernel into M samples of size K × K, (ii) perform principal component analysis (PCA) on the set of M samples, and (iii) determine the variance of the filter kernel which is explained by the first principal component (PC1). Using this approach, we can quantify how well each filter is representable by a K × K filter-specific blueprint (which in this case corresponds to PC1) and M factors (which in this case are the 'scores' obtained via PCA). We aggregate these individual values into histograms, which are shown in <ref type="figure" target="#fig_0">Figure 3</ref> for the same vanilla CNNs used in <ref type="figure">Figure 2</ref>. It can be seen that on average, about 50% of each filter kernel's variance can be explained using this simple model, suggesting a large potential for efficiency improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From Correlations to BSConv</head><p>The analysis in Section 3.1 suggests that for trained vanilla CNNs, each M × K × K filter can be approximated using a K × K blueprint and M factors which 'distribute' the blueprint along the depth dimension. Even though it is by no means enforced during training, this approximation explains a large portion of the observed variance. This finding is the motivation for the introduction of blueprint separable convolutions (BSConv). They are defined in such a way that above approximation turns into an integral property of the filters of CNNs. Concretely, we define each filter kernel F (n) to be represented using a blueprint B (n) and the weights w n,1 , . . . , w n,M ∈ R via</p><formula xml:id="formula_0">F (n) m,:,: = w n,m · B (n) ,<label>(2)</label></formula><p>with m ∈ {1, . . . , M } and n ∈ {1, . . . , N } (see <ref type="figure">Figure 1</ref>, bottom row). While this definition poses a hard constraint on the filter kernels, in Section 5 we experimentally show that CNNs trained with BSConv can reach the same or even better quality in comparison to their vanilla counterparts. However, in contrast to standard convolution layers which have M ·N ·K 2 free parameters (see <ref type="figure">Figure 1</ref>), the BSConv variant only has N · K 2 parameters for the blueprints, and M · N parameters for the weights. As is discussed below, the latter can be reduced even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Variants and Implementations</head><p>A BSConv module consists of N filters, each having one blueprint and M weights. All M · N weights can be combined into the matrix W = (w n,m ). Depending on how W is learned in the training step, different BSConv variants can be derived. In the following, two variants are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Unconstrained BSConv (BSConv-U)</head><p>In the most general case, the weights W can vary without any constraints and are learned directly via backpropagation, just like the entries of the blueprint filter kernels.</p><p>As shown in <ref type="figure">Figure 1</ref>, a naive implementation can be achieved by constructing a full kernel from each blueprint and performing a regular convolution afterwards. However, to derive a much more efficient implementation for CNNs, we rewrite Equation (1) in the following way: Firstly, because the input data tensor U and the filter kernels F (1) , . . . , F (N ) have the same size M along their depth dimension, we can split each 3d convolution into a sum of M </p><p>Secondly, we can replace each filter with its BSConv representation as given in Equation <ref type="formula" target="#formula_0">(2)</ref>, and obtain</p><formula xml:id="formula_2">V n,:,: = M m=1 U m,:,: * w n,m · B (n) .<label>(4)</label></formula><p>Because (i) each filter blueprint B (n) is independent of the input channel m, and (ii) w n,m is a scalar, we can rearrange above equation into </p><p>If we further rearrange w n,1 , . . . , w n,M into a M × 1 × 1 arrayw n , the sum can be replaced by a convolution, giving V n,:,: = U * w n</p><p>V n,:,: = V n,:,: * B (n) .</p><p>For a concrete implementation, these Equations <ref type="formula" target="#formula_4">(6)</ref> and <ref type="formula" target="#formula_5">(7)</ref> can directly be translated into two tensor operations: (i) a 1 × 1 pointwise convolution with the kernelsw 1 , . . . ,w N , which is performed on the input data tensor U , and (ii) a K × K depthwise convolution <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b20">20]</ref> with kernels B (1) , . . . , B (N ) , which is applied to the result of the first step. A flowchart visualization of these steps is given in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Subspace BSConv (BSConv-S)</head><p>For CNNs trained with BSConv-U convolution layers, the free parameters to be estimated are the N blueprint ker-nel matrices and the weight matrix W = (w n,m ) of size N × M . When analyzing the structure of the weight matrices W of such CNNs, we observe that the rows of W are often highly correlated. This fact indicates the potential for a further regularization and parameter reduction. Concretely, we perform a low-rank approximation of W by factorizing it into the N × M matrix To minimize redundancies in the low-rank subspace, we want the basis defined by W B to be orthonormal. This can be achieved via the regularization loss</p><formula xml:id="formula_6">W A = (w A n,m ) and the M × M matrix W B = (w B m ,m ) as W = W A · W B ,<label>(8)</label></formula><formula xml:id="formula_7">L ortho = W B W B T − I F ,<label>(9)</label></formula><p>where I is the identity matrix and · F the Frobenius norm of a matrix. The regularization loss is added to the classification loss L class with the weighting factor α, resulting in the joint loss L = L class + αL ortho .</p><p>To derive an efficient implementation of BSConv-S, we can substitute w n,m =   </p><p>Again, Equations <ref type="formula" target="#formula_0">(12)</ref> to <ref type="bibr" target="#b14">(14)</ref> directly translate into tensor operations, thus a concrete implementation of BSConv-S is a three-step process: (i) the input tensor is projected into a M -dimensional subspace via a 1×1 pointwise convolution with kernelsw B 1 , . . . ,w B M , (ii) another 1×1 pointwise convolution with kernelsw A 1 , . . . ,w A N is applied to the result of the first step, and (iii) a K × K depthwise convolution with kernels B <ref type="bibr" target="#b1">(1)</ref> , . . . , B (N ) is applied to the result of step two (see <ref type="figure" target="#fig_2">Figure 4</ref> for a visualization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Rethinking Depthwise Separable Convolutions</head><p>In the following, we show how the derived variants of BSConv are related to both most important building blocks for mobile models, i.e. depthwise separable convolutions and linear inverted residual bottlenecks. Moreover, we demonstrate how current model architectures can easily be equipped with our improved building blocks. As we will see in our experiments in Section 5, BSConv variants substantially outperform their vanilla counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">BSConv-U is a Reversed Depthwise Separable Convolution</head><p>Although the derivation of DSCs in <ref type="bibr" target="#b38">[37]</ref> is based on the same observation of kernel correlations along the depth axis, they obtain a reversed order of depthwise and pointwise convolution layers compared to BSConv (see <ref type="figure" target="#fig_2">Figure 4)</ref>. This is because DSC in fact enforces cross-kernel correlations instead of intra-kernel correlations. Using our formulation in Section 3.3, this can be verified by setting Equation <ref type="formula" target="#formula_0">(2)</ref> to F (n) m,:,: = w n,m · B m,:,: . This case corresponds to having a single 3d M × K × K blueprint kernel B , which is replicated along the width axis, i.e. across kernels (see <ref type="figure" target="#fig_8">Figure 5</ref>). While both cross-kernel and intrakernel correlations are valid assumptions, in <ref type="bibr" target="#b7">[7]</ref> it is shown that the latter dominate and thus have a larger potential for an efficient separation. This becomes even more obvious given that natural images are inherently correlated along the depth axis, which propagates through all layers.</p><p>The MobileNetV1 architecture can be translated to a BSConv-U model by simply substituting all DSCs by BSConv-U building blocks, which effectively means switching the order depthwise and pointwise convolutions. However, the inversion of the layer order should have no substantial effect on the middle flow of the network, since we already have alternating point-and depthwise convolutions. The main difference comes from the entry flow: with our approach, feature maps from the first regular convolution can be fully utilized by the depthwise convolution via the preceding pointwise distribution. In contrast, each kernel of the first depthwise convolution of the original MobileNetV1 model can only benefit from a single feature map, leading to limited expressiveness. Following our derivation, for the BSConv-U version of MobileNetV1, no activation nor normalization is applied after the pointwise convolutions, since it is essential to allow the weights w n,m to be negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">BSConv-S is a Shifted Linear Bottleneck with Orthonormal Regularization</head><p>Linear bottlenecks with inverted residual skips were first introduced in <ref type="bibr" target="#b36">[35]</ref> as a highly efficient building block providing impressive expressiveness at a minimum amount of required operations. It follows the idea of regularizing bottlenecks in very large ResNets and became the de facto standard in most of the current state-of-the-art mobile architectures such as MobileNetV2 and MobileNetV3. A single block consists of a cascade of a pointwise, a depthwise, and a pointwise convolution where the bottleneck is placed between blocks. Considering BSConv-S, we can see the close relationship when shifting our cascade of two pointwise and one depthwise convolution within a block to obtain a pointwise, a depthwise, and a pointwise convolution. Thus, Mo-bileNetV2 and V3 can be reinterpreted as a BSConv model with subspace transforms. As found by <ref type="bibr" target="#b36">[35]</ref>, the shift of residual skips into the bottleneck provides superior model performance and we keep that idea also for our implementation of MobileNets equipped with BSConv-S. Following our derivation in Section 3.3.2, the linear bottleneck, i.e. without the use of an activation function, comes quite natural since negative components are equally essential as positive ones for the subspace transform. Note that this implies that the first bottleneck block does not apply a subspace transform while the last feature map before the final classification layer is in fact compressed. Most importantly, BSConv-S models greatly benefit from our theoretical findings concerning the application of the orthonormal regularization during training. Note that the conversion to a BSConv-S model as described above is also suited for other architectures which use linear bottleneck blocks, such as Ef-ficientNet <ref type="bibr" target="#b42">[41]</ref> and MnasNet [40].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our approach of blueprint separable convolutions based on a variety of commonly used benchmark datasets. We provide a comprehensive analysis of the MobileNet family and their modified counterparts according to our findings in Section 4. Furthermore, we demonstrate how our approach can be used as a drop-in substitution for regular convolution layers in standard models like ResNets to drastically reduce the number of model parameters and operations, while keeping or even gaining accuracy.</p><p>To allow for a fair comparison, we train all modelsincluding the baseline networks-with exactly the same training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">CIFAR10 and CIFAR100</head><p>The CIFAR10/100 datasets <ref type="bibr" target="#b25">[25]</ref> consist of 50k training and 10k test images of size 32 px × 32 px and comprise 10 and 100 classes, respectively. As suggested in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b46">45]</ref>, we train for 200 epochs for both datasets. We use SGD with momentum set to 0.9 and a weight decay of 10 −4 . The initial learning rate is set to 0.1 and decayed by a factor of 0.1 at epochs 100, 150, and 180. Images are augmented by random horizontal flips and random shifts by up to 4 px to prevent models from overfitting <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b46">45]</ref>.</p><p>MobileNets. As first experiment, we evaluate our improvements of MobileNetV1-V3 <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b15">15]</ref> as derived in Section 4. To make MobileNets applicable to CIFAR, we removed the first and second pooling operation to obtain a final feature map of size 4 × 4. With this modification, we achieve state-of-the-art performance for the baseline models (see <ref type="table">Table 1</ref>).</p><p>As described in Section 4, we use BSConv-U for MobileNetV1 and BSConv-S for MobileNetV2/V3. For the BSConv-S models, we use a subspace compression ratio of p = 1 6 to have exactly the same parameter count as the vanilla model. The weighting coefficient α for the orthonormal regularization loss was set to 0.01.</p><p>The results are shown in <ref type="table">Table 1</ref>. We can state that all BSConv variants outperform their corresponding Mo-bileNet baselines. For MobileNetV1, this can be explained by (i) the inversion of point-and depthwise convolutions and (ii) the absence of ReLU activations for the pointwise convolutions (see discussion in Section 4.1). For MobileNetsV2 and V3, the fact that BSConv-S always outperforms the baseline models clearly confirms the advantage of our proposed orthonormal regularization loss.</p><p>ResNets and WideResNets. In addition to the improvements with respect to MobileNets, we can use our BSConv approach as drop-in substitution for regular convolution layers in standard networks. In the following, we consider ResNets <ref type="bibr" target="#b13">[13]</ref> and WideResNets <ref type="bibr" target="#b46">[45]</ref> as two state-of-the-art models for the CIFAR datasets. In both cases, we use the BSConv-U variant. We increase the depth and width factor of each BSConv model such that its parameter count matches the parameter count of the corresponding baseline model. We apply the same training protocol and augmentation techniques as described above.</p><p>In <ref type="table" target="#tab_0">Table 2</ref> we compare the original networks with their modified BSConv versions. For ResNets, we can improve accuracy by up to 3.1 percentage points for CIFAR100, while having slightly fewer parameters and computational costs. For WideResNets, we can gain accuracy of up to 2.1 percentage points for CIFAR100, while having fewer parameters and computational costs. This clearly shows the effectiveness of our approach as an drop-in substitution of regular convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet</head><p>To assess the performance of BSConv models in largescale classification scenarios, we conduct experiments on the ImageNet dataset (ILSVRC2012, <ref type="bibr" target="#b35">[34]</ref>). It contains about 1.3M images for training and 50k images for testing which are drawn from 1000 object categories.</p><p>We employ a common training protocol and train for 100 epochs with an initial learning rate of 0.1 which is decayed by a factor of 0.1 at epochs 30, 60, and 90. We use SGD with momentum 0.9 and a weight decay of 10 −4 . To allow for a fair comparison and to investigate the effect of our approach, we train own baseline models with exactly the same training setup as used for BSConv models. The images are resized such that their short side has a length of 256 px. We use the well-established Inception-like scale augmentation <ref type="bibr" target="#b40">[39]</ref>, horizontal flips, and color jitter <ref type="bibr" target="#b26">[26]</ref>.</p><p>MobileNets. As for the CIFAR experiments, we compare MobileNets to their corresponding BSConv variants. Again, BSConv-U is used for MobileNetV1, and BSConv-S is used for MobileNetV2/V3. The subspace compression ratio for BSConv-S is p = 1 6 just like for the CIFAR experiments. The weighting coefficient α for the orthonormal regularization loss was set to 0.1.</p><p>The results are presented in <ref type="table" target="#tab_1">Table 3</ref>. Again, it can be seen that the BSConv variants of MobileNets outperform their corresponding baseline models. However, the relative improvements are no longer as large as for the CIFAR experiments. This effect can be explained by the regularization impact of the dataset itself. Considering the Network CIFAR10 CIFAR100 Parameters FLOPs Accuracy Parameters FLOPs Accuracy ResNet-20 <ref type="bibr" target="#b13">[13]</ref> 272  ants are ResNet-10, ResNet-18, ResNet-34, ResNet-68, and ResNet-102. Again, we use the same training protocol and augmentation techniques as described above.</p><p>The results are shown in <ref type="figure" target="#fig_9">Figure 6</ref>, split by parameter count and computational complexity. It can be seen that the BSConv-U variants of ResNets significantly outperform the baseline models. ResNet-10 and ResNet-68+BSConv-U, for instance, have similar parameter counts, while us-ing BSConv leads to an accuracy gain of 9.5 percentage points. Another interesting example is ResNet-18 vs. ResNet-34+BSConv-U: both have a comparable accuracy, while the BSConv model has only about one fifth of the baseline model parameter count. weight decay of 10 −4 . The initial learning rate is set to 0.1 and linearly decayed at every epoch such that it approaches zero after a total of 100 epochs.</p><p>MobileNets. We use the same model setup as for the CIFAR and ImageNet experiments discussed above. The results are shown in <ref type="table">Table 1</ref>. Again, all BSConv models substantially outperform their baseline counterparts. In contrast to the CIFAR results, the margin is even larger. Therefore, the interpretation of the CIFAR results applies here as well.</p><p>Other Architectures. We further evaluate the effect of our approach for a variety of state-of-the-art models. We replace regular convolution layers in standard networks such as VGG <ref type="bibr" target="#b39">[38]</ref> and DenseNet <ref type="bibr" target="#b17">[17]</ref>.</p><p>In <ref type="table">Table 4</ref> we can see that all models greatly benefit from the application of BSConv. Accuracy for BSConv-U can be improved by at least 2 percentage points, while having up to 8.5× less parameters and a substantial reduction of computational complexity. Most of the recently proposed model architectures utilize residual linear bottlenecks <ref type="bibr" target="#b36">[35]</ref>, which can also be easily equipped with our BSConv-S approach in the same way as for MobileNetV2/V3 (see Section 4.2). As can be seen in <ref type="table">Table 4</ref>, our subspace model clearly outperforms the original EfficientNet-B0 [41] by 6.5 percentage points and MnasNet [40] by 5 percentage points with the same number of parameters and computational complexity. This shows the effectiveness of our proposed orthonormal regularization of the BSConv-S subspace transform.</p><p>Influence of the Orthonormal Regularization. To evaluate the influence of the proposed orthonormal regularization loss for BSConv-S models, we conduct an ablation study using MobileNetV3-large. In particular, several identical models are trained on the Stanford Dogs dataset using weighting coefficients α in the range of 10 −5 , . . . , 10 0 .</p><p>As can be seen in <ref type="figure" target="#fig_11">Figure 7</ref>, by regularizing the subspace components to be orthonormal, model performance can be substantially improved by over 5 percentage points. An optimum is reached for a weighting coefficient of α = 0.1.</p><p>For smaller values, the influence of the regularization decreases, until it is no longer effective and converges towards the baseline performance. Larger values, however, decrease model performance since the optimization is mainly driven by rapidly reaching a solution with an orthonormal basis independently of creating a beneficial joint representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Accuracy VGG-16 (BN) <ref type="bibr" target="#b39">[38]</ref> 60.5 VGG-16 (BN) (BSConv-U) 62.4 DenseNet-121 <ref type="bibr" target="#b17">[17]</ref> 56.9 DenseNet-121 (BSConv-U) 59.4 Xception* <ref type="bibr" target="#b2">[2]</ref> 59.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We introduced blueprint separable convolutions (BSConv) as highly efficient building blocks for CNNs. Our formulation provided an interpretation and justification for depthwise separable convolutions. By using BSConv, we clearly and consistently improved established models such as MobileNets, MnasNets, EfficientNets, and ResNets. Code and trained models are available under https: //github.com/zeiss-microscopy/BSConv.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Histogram of the variance along the depth axis of filter kernels which can be explained using only one principal component per filter. The filters are grouped by convolution stages (stage 1: blue, stage 2: orange, stage 3: green, stage 4: red). These quantitative results show that a large portion of CNN filters can potentially be represented using our BSConv formulation. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Computational graphs for the efficient implementation of the different BSConv variants (see Sections 3.3.1 and 3.3.2). 2d convolutions, yielding V n,:,: = U * F (n) = M m=1 U m,:,: * F (n) m,:,: .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>V</head><label></label><figDesc>n,:,: = M m=1 U m,:,: · w n,m V n,:,: * B (n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>with M = p · M , while p ∈ (0.0, 1.0) specifies the size of the subspace M in relation to the size M of the original space. The matrix W B can be thought of as set of M basis vectors, while W A is the subspace version of W . Instead of N · M weights as for the case of BSConv-U, this method reduces the parameter count to N · M + M · M , as only W A and W B have to be learned via backpropagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>· w B m ,m * B (n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 10 )</head><label>10</label><figDesc>Using the same arguments as in Section 3.3.1, we can rearrange this equation into V n,:,: = V n,:,: * B (n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 11 )</head><label>11</label><figDesc>By rearranging the weights w B m ,1 , . . . , w B m ,M into the M × 1 × 1 arrayw B m and the weights w A n,1 , . . . , w A n,M into the M × 1 × 1 arrayw A n , the sums can be replaced by convolutions in the same way as in Section 3.3.1, and we obtainU m ,:,: = U * w B m (12) V n,:,: = U * w A n (13) V n,:,: = V n,:,: * B (n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Interpretations for DSC and BSConv. DSCs implicitly assume one 3d blueprint which is used for all kernels, while BSConv relies on individual 2d blueprints for each kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>ResNets on ImageNet. For the baseline models, we use ResNet-10/18/26. The BSConv variants are ResNet-10/18/34/68/102.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Table 4 .</head><label>4</label><figDesc>Results of various architectures and their BSConv counterparts for the Stanford Dogs dataset. BSConv-U CNNs have fewer parameters and a smaller computational complexity compared to their baseline models. BSConv-S CNNs have the same parameter count and computational complexity as their counterparts. * Commonly used implementation based on DSCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Influence of the orthonormal regularization loss on the accuracy for the BSConv-S variant of MobileNetV3-large (red solid line) on Stanford Dogs. The baseline MobileNetV3-large model without BSConv-S is indicated by the black dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>ResNets and WideResNets on CIFAR10 and CIFAR100. We increase the depth and width factor of each BSConv model such that its parameter count matches the parameter count of the corresponding baseline model.</figDesc><table><row><cell></cell><cell></cell><cell>.5K</cell><cell>41.3M</cell><cell>92.2</cell><cell>278.3K</cell><cell>41.3M</cell><cell>67.7</cell></row><row><cell cols="2">ResNet-110 (BSConv-U)</cell><cell>239.0K</cell><cell>41.1M</cell><cell>92.9</cell><cell>244.8K</cell><cell>41.1M</cell><cell>70.8</cell></row><row><cell cols="2">WideResNet-40-3 [45]</cell><cell>5.0M</cell><cell>735.0M</cell><cell>94.9</cell><cell>5.0M</cell><cell>735.0M</cell><cell>75.5</cell></row><row><cell cols="2">WideResNet-40-8 (BSConv-U)</cell><cell>4.2M</cell><cell>671.6M</cell><cell>95.2</cell><cell>4.3M</cell><cell>671.6M</cell><cell>77.6</cell></row><row><cell>Network</cell><cell cols="3">Original BSConv (ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV1 (x0.25)</cell><cell>51.8</cell><cell>53.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV1 (x0.5)</cell><cell>63.5</cell><cell>64.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV1 (x0.75)</cell><cell>68.2</cell><cell>69.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV1 (x1.0)</cell><cell>70.8</cell><cell>71.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV2 (x1.0)</cell><cell>69.7</cell><cell>69.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV3-small (x1.0)</cell><cell>64.4</cell><cell>64.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV3-large (x1.0)</cell><cell>71.5</cell><cell>71.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>MobileNets on ImageNet. BSConv-U is used for Mo-bileNetV1, and BSConv-S is used for MobileNetV2/V3. Note that BSConv does not introduce additional parameters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MobileNetV3-large results, we note that even if the orthonormal regularization loss seems to be no longer effective, it has no negative influence on the training.ResNets. As noted before, it is possible to directly substitute regular convolution layers in standard networks by BSConv variants. To this end, we analyze the effectiveness of our approach when applied to ResNets on largescale image databases. For the baseline models, we use ResNet-10, ResNet-18, and ResNet-26. The BSConv vari-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">.3. Fine-grained RecognitionApart from large-scale object recognition, we are interested in the task of fine-grained classification, as those datasets usually have no inherent regularization. The following experiments are conducted on three well-established benchmark datasets for fine-grained recognition, namely Stanford Dogs<ref type="bibr" target="#b23">[23]</ref>, Stanford Cars<ref type="bibr" target="#b24">[24]</ref>, and Oxford 102 Flowers<ref type="bibr" target="#b32">[32]</ref>. We train all models from scratch, since parts of these datasets are a subset of ImageNet. In contrast to the ImageNet training protocol, we do not use aggressive data augmentation, since we observed that it severely affects model performance. We only augment data via random crops, horizontal flips, and random gamma transform.We use the same training protocol for all three datasets. In particular, we use SGD with momentum set to 0.9 and a</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Table 1. MobileNet results for various datasets. The columns &apos;orig&apos; refer to the baseline MobileNet models. The columns &apos;ours&apos; refer to BSConv-U for MobileNetV1 and BSConv-S for MobileNetV2/V3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<title level="m">Compressing deep convolutional networks using vector quantization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05517</idno>
		<title level="m">Network decoupling: From regular to depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eie: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5353" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<title level="m">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy-Oukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6869" to="6898" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Flattened convolutional neural networks for feedforward acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05270</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2217" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Factorized convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="545" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
