<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Richer Convolutional Features for Edge Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20191939">AUGUST 2019 1939</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Wang</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
						</author>
						<title level="a" type="main">Richer Convolutional Features for Edge Detection</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">41</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20191939">AUGUST 2019 1939</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Edge detection</term>
					<term>deep learning</term>
					<term>richer convolutional features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Edge detection is a fundamental problem in computer vision. Recently, convolutional neural networks (CNNs) have pushed forward this field significantly. Existing methods which adopt specific layers of deep CNNs may fail to capture complex data structures caused by variations of scales and aspect ratios. In this paper, we propose an accurate edge detector using richer convolutional features (RCF). RCF encapsulates all convolutional features into more discriminative representation, which makes good usage of rich feature hierarchies, and is amenable to training via backpropagation. RCF fully exploits multiscale and multilevel information of objects to perform the image-to-image prediction holistically. Using VGG16 network, we achieve state-of-the-art performance on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of 0.811 while retaining a fast speed (8 FPS). Besides, our fast version of RCF achieves ODS F-measure of 0.806 with 30 FPS. We also demonstrate the versatility of the proposed method by applying RCF edges for classical image segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E DGE detection can be viewed as a method to extract visually salient edges and object boundaries from natural images. Due to its far-reaching applications in many high-level applications including object detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, object proposal generation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and image segmentation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, edge detection is a core low-level problem in computer vision.</p><p>The fundamental scientific question here is what is the appropriate representation which is rich enough for a predictor to distinguish edges/boundaries from the image data. To answer this, traditional methods first extract the local cues of brightness, color, gradient and texture, or other manually designed features like Pb <ref type="bibr" target="#b7">[8]</ref> and gPb <ref type="bibr" target="#b8">[9]</ref>, then sophisticated learning paradigms <ref type="bibr" target="#b9">[10]</ref> are used to classify edge and non-edge pixels. Although low-level features based edge detectors are somehow promising, their limitations are obvious as well. For example, edges and boundaries are often defined to be semantically meaningful, however, it is difficult to use low-level cues to represent high-level information. Recently, convolutional neural networks (CNNs) have become popular in computer vision <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Since CNNs have a strong capability to automatically learn the highlevel representations for natural images, there is a recent trend of using CNNs to perform edge detection. Some well-known CNNbased methods have pushed forward this field significantly, such as DeepEdge <ref type="bibr" target="#b12">[13]</ref>, N 4 -Fields <ref type="bibr">[14]</ref>, DeepContour <ref type="bibr">[15]</ref>, and HED <ref type="bibr" target="#b13">[16]</ref>. Our algorithm falls into this category as well.</p><p>As illustrated in <ref type="figure">Fig. 1</ref>, we build a simple network to produce side outputs of intermediate layers using VGG16 <ref type="bibr" target="#b10">[11]</ref> with HED architecture <ref type="bibr" target="#b13">[16]</ref>. We can see that the information obtained by different convolution (i.e. conv ) layers gradually becomes coarser. More importantly, intermediate conv layers contain essential fine details. However, previous CNN architectures only use the final conv layer or the layers before the pooling layers of neural networks, but ignore the intermediate layers. On the other hand, since richer convolutional features are highly effective for many vision tasks, many researchers make efforts to develop deeper networks <ref type="bibr" target="#b14">[17]</ref>. However, it is difficult to get the networks to converge when going deeper because of vanishing/exploding gradients and training data University of Science and Technology, Nanjing 210094, China. • A preliminary version of this work has been published in CVPR 2017 <ref type="bibr" target="#b0">[1]</ref>.</p><formula xml:id="formula_0">(a) original image (b) ground truth (c) conv3 1 (d) conv3 2 (e) conv3 3 (f) conv4 1 (g) conv4 2 (h) conv4 3 Fig. 1:</formula><p>We build a simple network based on VGG16 <ref type="bibr" target="#b10">[11]</ref> to produce side outputs (c-h). One can see that convolutional features become coarser gradually, and the intermediate layers (c,d,f,g) contain essential fine details that do not appear in other layers.</p><p>shortage (e.g. for edge detection). So why don't we make full use of the CNN features we have now? Based on these observations, we propose richer convolutional features (RCF), a novel deep structure fully exploiting the CNN features from all the conv layers, to perform the pixel-wise prediction for edge detection in an image-to-image fashion. RCF can automatically learn to combine complementary information from all layers of CNNs and thus can obtain accurate representations for objects or object parts in different scales. The evaluation results demonstrate RCF performs very well on edge detection. After the publication of the conference version <ref type="bibr" target="#b0">[1]</ref>, our proposed RCF edges have been widely used in weakly supervised semantic segmentation <ref type="bibr" target="#b15">[18]</ref>, style transfer <ref type="bibr" target="#b16">[19]</ref>, and stereo matching <ref type="bibr" target="#b17">[20]</ref>. Besides, the idea of utilizing all the conv layers in a unified framework can be potentially generalized to other vision tasks. This has been demonstrated in skeleton detection <ref type="bibr" target="#b18">[21]</ref>, medial axis detection <ref type="bibr" target="#b19">[22]</ref>, people detection <ref type="bibr" target="#b20">[23]</ref>, and surface fatigue crack identification <ref type="bibr" target="#b21">[24]</ref>.</p><p>When evaluating our method on BSDS500 dataset <ref type="bibr" target="#b8">[9]</ref> for edge detection, we achieve a good trade-off between effectiveness and efficiency with the ODS F-measure of 0.811 and the speed of 8 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1612.02103v3 [cs.CV] 3 Jul 2019</head><p>It even outperforms human perception (ODS F-measure 0.803). In addition, a fast version of RCF is also presented, which achieves ODS F-measure of 0.806 with 30 FPS. When applying our RCF edges to classic image segmentation, we can obtain high-quality perceptual regions as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As one of the most fundamental problem in computer vision, edge detection has been extensively studied for several decades. Early pioneering methods mainly focus on the utilization of intensity and color gradients, such as Canny <ref type="bibr" target="#b22">[25]</ref>. However, these early methods are usually not accurate enough for real-life applications. To this end, feature learning based methods have been proposed. These methods, such as Pb <ref type="bibr" target="#b7">[8]</ref>, gPb <ref type="bibr" target="#b8">[9]</ref>, and SE <ref type="bibr" target="#b9">[10]</ref>, usually employ sophisticated learning paradigms to predict edge strength with low-level features such as intensity, gradient, and texture. Although these methods are shown to be promising in some cases, these handcrafted features have limited ability to represent high-level information for semantically meaningful edge detection.</p><p>Deep learning based algorithms have made vast inroads into many computer vision tasks. Under this umbrella, many deep edge detectors have been introduced recently. Ganin et al. <ref type="bibr">[14]</ref> proposed N 4 -Fields that combines CNNs with the nearest neighbor search. Shen et al.</p><p>[15] partitioned contour data into subclasses and fitted each subclass by learning the model parameters. Recently, Xie et al. <ref type="bibr" target="#b13">[16]</ref> developed an efficient and accurate edge detector, HED, which performs image-to-image training and prediction. This holisticallynested architecture connects their side output layers, which is composed of one conv layer with kernel size 1, one deconv layer, and one softmax layer, to the last conv layer of each stage in VGG16 <ref type="bibr" target="#b10">[11]</ref>. Moreover, Liu et al. <ref type="bibr" target="#b23">[26]</ref> used relaxed labels generated by bottom-up edges to guide the training process of HED. Wang et al. <ref type="bibr" target="#b24">[27]</ref> leveraged a top-down backward refinement pathway to effectively learn crisp boundaries. Xu et al. <ref type="bibr" target="#b25">[28]</ref> introduced a hierarchical deep model to robustly fuse the edge representations learned at different scales. Yu et al. <ref type="bibr" target="#b26">[29]</ref> extended the success in edge detection to semantic edge detection which simultaneously detected and recognized the semantic categories of edge pixels.</p><p>Although these aforementioned CNN-based models have pushed the state of the arts to some extent, they all turn out to be lacking in our view because that they are not able to fully exploit the rich feature hierarchies from CNNs. These methods usually adopt CNN features only from the last layer of each conv stage. To address this, we propose a fully convolutional network to combine features from all conv layers efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RICHER CONVOLUTIONAL FEATURES (RCF)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>We take inspirations from existing work <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[16]</ref> and embark on the VGG16 network <ref type="bibr" target="#b10">[11]</ref>. VGG16 network composes of 13 conv layers and 3 fully connected layers. Its conv layers are divided into five stages, in which a pooling layer is connected after each stage. The useful information captured by each conv layer becomes coarser with its receptive field size increasing. Detailed receptive field sizes of different layers can be found in <ref type="bibr" target="#b13">[16]</ref>. The use of this rich hierarchical information is hypothesized to help edge detection. The starting point of our network design lies here.</p><p>The novel network introduced by us is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Compared with VGG16, our modifications can be summarized as following:</p><p>• We cut all the fully connected layers and the pool5 layer. On the one side, we remove the fully connected layers to have a fully convolutional network for an image-to-image prediction. On the other hand, adding pool5 layer will increase the stride by two times, which usually leads to degeneration of edge localization. • Each conv layer in VGG16 is connected to a conv layer with kernel size 1 × 1 and channel depth 21. And the resulting feature maps in each stage are accumulated using an eltwise layer to attain hybrid features.</p><p>• An 1 × 1 − 1 conv layer follows each eltwise layer. Then, a deconv layer is used to up-sample this feature map.</p><p>• A cross-entropy loss/sigmoid layer is connected to the upsampling layer in each stage.</p><p>• All the up-sampling layers are concatenated. Then an 1 × 1 conv layer is used to fuse feature maps from each stage. At last, a cross-entropy loss/sigmoid layer is followed to get the fusion loss/output.</p><p>In RCF, features from all conv layers are well-encapsulated into a final representation in a holistic manner which is amenable to training by back-propagation. As receptive field sizes of conv layers in VGG16 are different from each other, RCF endows a better mechanism than existing ones to learn multiscale information coming from all levels of convolutional features which we believe are all pertinent for edge detection. In RCF, high-level features are coarser and can obtain strong response at the larger object or object part boundaries as illustrated in <ref type="figure">Fig. 1</ref> while features from lower-part of CNNs are still beneficial in providing complementary fine details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotator-robust Loss Function</head><p>Edge datasets in this community are usually labeled by several annotators using their knowledge about the presence of objects or object parts. Though humans vary in cognition, these human-labeled edges for the same image share high consistency <ref type="bibr" target="#b7">[8]</ref> For each image, we average all the ground truth to generate an edge probability map, which ranges from 0 to 1. Here, 0 means no annotator labeled at this pixel, and 1 means all annotators have labeled at this pixel. We consider the pixels with edge probabilities higher than η as positive samples and the pixels with edge probabilities equal to 0 as negative samples. Otherwise, if a pixel is marked by fewer than η of the annotators, this pixel may be semantically controversial to be an edge point. Thus, regarding those pixels as either positive or negative samples may confuse the networks. Hence we ignore them, but HED tasks them as negative samples and uses a fix η of 0.5. We compute the loss of each pixel with respect to its label as</p><formula xml:id="formula_1">l(Xi; W ) =      α · log (1 − P (Xi; W )) if yi = 0 0 if 0 &lt; yi ≤ η β · log P (Xi; W ) otherwise,<label>(1)</label></formula><p>in which</p><formula xml:id="formula_2">α = λ · |Y + | |Y + | + |Y − | β = |Y − | |Y + | + |Y − | .<label>(2)</label></formula><p>Y + and Y − denote the positive sample set and the negative sample set, respectively. The hyper-parameter λ is used to balance the number of positive and negative samples. The activation value (CNN feature vector) and ground truth edge probability at pixel i are presented by Xi and yi, respectively. P (X) is the standard sigmoid function, and W denotes all the parameters that will be learned in our architecture. Therefore, our improved loss function can be formulated as</p><formula xml:id="formula_3">L(W ) = |I| i=1 K k=1 l(X (k) i ; W ) + l(X f use i ; W ) ,<label>(3)</label></formula><p>where X (k) i is the activation value from stage k while X f use i is from the fusion layer. |I| is the number of pixels in image I, and K is the number of stages (equals to 5 here).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiscale Hierarchical Edge Detection</head><p>In single scale edge detection, we feed an original image into our fine-tuned RCF network, then, the output is an edge probability map. To further improve the quality of edges, we use image pyramids during the test phase. Specifically, we resize an image to construct an image pyramid, and each of these images is fed into our singlescale detector separately. Then, all resulting edge probability maps are resized to the original image size using bilinear interpolation. At last, these maps are fused to get the final prediction map. We adopt simple average fusion in this study although other advanced strategies are also applicable. In this way, our preliminary version <ref type="bibr" target="#b0">[1]</ref> firstly demonstrates multiscale testing is still beneficial for edge detection although RCF itself is able to encode multiscale information. Considering the trade-off between accuracy and speed, we use three scales 0.5, 1.0, and 1.5 in this paper. When evaluating RCF on BSDS500 <ref type="bibr" target="#b8">[9]</ref> dataset, this simple multiscale strategy improves the ODS F-measure from 0.806 to 0.811 with the speed of 8 FPS which we believe is good enough for real-life applications. See Sec. 4.1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison With HED</head><p>The most obvious differences between our RCF and HED <ref type="bibr" target="#b13">[16]</ref> lie in the three following aspects.</p><p>First, HED only considers the last conv layer in each stage of VGG16, in which lots of helpful information for edge detection is missed. In contrast to it, RCF uses richer features from all the conv layers, making it more possible to capture more object or object-part boundaries across a larger range of scales.</p><p>Second, a novel loss function is proposed to treat training examples properly. We consider the edge pixels that η of the annotators labeled as positive samples and the pixels that no annotator labeled as negative samples. Besides, we ignore edge pixels that are marked by a few annotators because of their confusing attributes. In contrast, HED view edge pixels that are marked by less than half of the annotators as negative samples, which may confuse the network training because these pixels are not true non-edge points. Our new loss have been used in <ref type="bibr" target="#b24">[27]</ref>.</p><p>Thirdly, our preliminary version <ref type="bibr" target="#b0">[1]</ref> first proposes the multiscale test for edge detection. Recent edge detectors such as HED usually use multiscale network features, but we demonstrate the simple multiscale test is still helpful to edge detection. This idea is also accepted by recent work <ref type="bibr" target="#b24">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS ON EDGE DETECTION</head><p>We implement our network using the Caffe framework <ref type="bibr" target="#b28">[31]</ref>. The default setting using VGG16 <ref type="bibr" target="#b10">[11]</ref> backbone net, and we also test ResNet <ref type="bibr" target="#b14">[17]</ref> backbone net. In RCF training, the weights of 1 × 1 conv layers in stage 1-5 are initialized from zero-mean Gaussian distributions with standard deviation 0.01 and the biases are initialized to 0. The weights of the 1 × 1 conv layer in the fusion stage are initialized to 0.2 and the biases are initialized to 0. The weights of other layers are initialized using pre-trained ImageNet models. Stochastic gradient descent (SGD) minibatch samples 10 images randomly in each iteration. For other SGD hyper-parameters, the global learning rate is set to 1e-6 and will be divided by 10 after every 10k iterations. The momentum and weight decay are set to 0.9 and 0.0002, respectively. We run SGD for 40k iterations totally. The parameters η and λ in loss function are set depending on the training data. All experiments in this paper are finished using a NVIDIA TITAN X GPU.</p><p>Given an edge probability map, a threshold is needed to produce the binary edge map. There are two choices to set this threshold. The first one is referred as optimal dataset scale (ODS) which employs a fixed threshold for all images in a dataset. The second is called optimal image scale (OIS) which selects an optimal threshold for each image. We report the F-measure ( 2·P recision·Recall P recision+Recall ) of both ODS and OIS in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BSDS500 Dataset</head><p>BSDS500 <ref type="bibr" target="#b8">[9]</ref> is a widely used dataset in edge detection. It is composed of 200 training, 100 validation and 200 test images, each of which is labeled by 4 to 9 annotators. We utilize the training and validation sets for fine-tuning, and test set for evaluation. Data augmentation is the same as <ref type="bibr" target="#b13">[16]</ref>. Inspired by the previous work <ref type="bibr" target="#b23">[26]</ref>, <ref type="bibr" target="#b29">[32]</ref>, <ref type="bibr" target="#b30">[33]</ref>, we mix the augmented data of BSDS500 with flipped VOC Context dataset <ref type="bibr" target="#b31">[34]</ref> as training data. When training, we set loss parameters η and λ to 0.5 and 1.1, respectively. When evaluating, standard non-maximum suppression (NMS) <ref type="bibr" target="#b9">[10]</ref> is applied to thin detected edges. We compare our method with some non-deeplearning algorithms, including Canny <ref type="bibr" target="#b22">[25]</ref>, Pb <ref type="bibr" target="#b7">[8]</ref>, SE <ref type="bibr" target="#b9">[10]</ref>, and OEF <ref type="bibr" target="#b32">[35]</ref>, and some recent deep learning based approaches, including DeepContour [15], DeepEdge <ref type="bibr" target="#b12">[13]</ref>, HED <ref type="bibr" target="#b13">[16]</ref>, HFL <ref type="bibr" target="#b33">[36]</ref>, MIL+G-DSN+MS+NCuts <ref type="bibr" target="#b30">[33]</ref>, CASENet <ref type="bibr" target="#b26">[29]</ref>, AMH <ref type="bibr" target="#b25">[28]</ref>, CED <ref type="bibr" target="#b24">[27]</ref> and etc. <ref type="figure">Fig. 3a</ref> shows the evaluation results. The performance of human eye in edge detection is known as 0.803 ODS F-measure. Both single-scale and multiscale (MS) versions of RCF get better results than average human performance. When comparing with HED <ref type="bibr" target="#b13">[16]</ref>,  <ref type="figure">Fig. 4</ref>: The comparison with some competitors on the BSDS500 <ref type="bibr" target="#b8">[9]</ref> dataset. † means GPU time.</p><p>We show statistic comparison in <ref type="figure">Fig. 4</ref>. From RCF to RCF-MS, the ODS F-measure increases from 0.806 to 0.811, though the speed drops from 30 FPS to 8 FPS. It proves the validity of our multiscale strategy. RCF with ResNet101 <ref type="bibr" target="#b14">[17]</ref> achieves a state-of-the-art 0.819 ODS F-measure. We also observe an interesting phenomenon in which the RCF curves are not as long as other methods when evaluated using the default parameters in BSDS500 benchmark. It may suggest that RCF tends to only remain very confident edges. Our methods also achieve better results than recent edge detectors, such as AMH <ref type="bibr" target="#b25">[28]</ref> and CED <ref type="bibr" target="#b24">[27]</ref>. Note that AMH and CED use complex networks with more weights than our simple RCF. Our RCF network only adds some 1 × 1 conv layers to HED, so the time consumption is on par with HED. We can see that RCF achieves a good trade-off between effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NYUD Dataset</head><p>NYUD <ref type="bibr" target="#b27">[30]</ref> dataset is composed of 1449 densely labeled pairs of aligned RGB and depth images captured from indoor scenes. Recently, many works have conducted edge evaluation on it, such as <ref type="bibr" target="#b9">[10]</ref>. Gupta et al. <ref type="bibr" target="#b34">[37]</ref> split NYUD dataset into 381 training, 414 validation, and 654 test images. We follow their settings and train RCF using the training and validation sets as in HED <ref type="bibr" target="#b13">[16]</ref>.</p><p>We utilize depth information by using HHA <ref type="bibr" target="#b35">[38]</ref>, in which depth information is encoded into three channels: horizontal disparity, height above ground, and angle with gravity. Thus HHA features can be represented as a color image by normalization. Then, two models for RGB images and HHA feature images are trained separately. In the training process, λ is set to 1.2 for both RGB and HHA. Since NYUD only has one ground truth for each image, η is useless here. Other network settings are the same as used for BSDS500. At the test phase, the final edge predictions are defined by averaging the outputs of RGB model and HHA model. Since there is already an average operation, the multiscale test is not evaluated here. When evaluating, we increase localization tolerance, which controls the maximum allowed distance in matches between predicted edges and ground truth, from 0.0075 to 0.011, because images in NYUD dataset are larger than images in BSDS500 dataset.</p><p>We compare our single-scale version of RCF with some wellestablished competitors. OEF <ref type="bibr" target="#b32">[35]</ref> only uses RGB images, while other methods employ both depth and RGB information. The precision-recall curves are shown in <ref type="figure">Fig. 3b</ref>. RCF achieves competitive performance on NYUD dataset, and it is significantly better than HED. <ref type="figure">Fig. 5</ref> shows the statistical comparison. We can see that RCF outperforms HED not only on separate HHA or RGB data, but also on the merged RGB-HHA data. For HHA and RGB data, ODS F-measure of RCF is 2.2% and 2.6% higher than HED, respectively. For merging RGB-HHA data, RCF is 2.4% higher than HED. Furthermore, HHA edges perform worse than RGB, but averaging HHA and RGB edges achieves much higher results. It suggests that combining different types of information is very useful for edge detection, and this may explain why OEF performs much</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ODS OIS FPS OEF <ref type="bibr" target="#b32">[35]</ref> 0.651 0.667 1/2 gPb+NG <ref type="bibr" target="#b34">[37]</ref> 0.687 0.716 1/375 SE <ref type="bibr" target="#b9">[10]</ref> 0.695 0.708 5 SE+NG+ <ref type="bibr" target="#b35">[38]</ref> 0.706 0.734 1/15 HED-HHA <ref type="bibr" target="#b13">[16]</ref> 0.681 0.695 20 † HED-RGB <ref type="bibr" target="#b13">[16]</ref> 0.717 0.732 20 † HED-RGB-HHA <ref type="bibr" target="#b13">[16]</ref> 0.741 0.757 10 † RCF-HHA 0.703 0.717 20 † RCF-RGB 0.743 0.757 20 † RCF-RGB-HHA 0.765 0.780 10 † RCF-ResNet50-RGB-HHA 0.781 0.793 7 † <ref type="figure">Fig. 5</ref>: The comparison with some competitors on the NYUD dataset <ref type="bibr" target="#b27">[30]</ref>. †means GPU time.</p><p>worse than other methods. RCF with ResNet50 [17] improves a 1.6% ODS F-measure when compared with RCF with VGG16 <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multicue Dataset</head><p>Multicue dataset is proposed by Mély et al. <ref type="bibr" target="#b37">[40]</ref> to study psychophysics theory for boundary detection. It is composed of short binocular video sequences of 100 challenging natural scenes captured by a stereo camera. Each scene contains a left and a right view short (10-frame) color sequences. The last frame of the left images for each scene is labeled for two annotations: object boundaries and low-level edges. Unlike people who usually use boundary and edge interchangeably, they strictly defined boundary and edge according to visual perception at different stages. Thus, boundaries are referred to the boundary pixels of meaningful objects, and edges are abrupt pixels at which the luminance, color, or stereo changes sharply. In this subsection, we use boundary and edge as defined by Mély et al. <ref type="bibr" target="#b37">[40]</ref> while considering boundary and edge having the same meaning in previous sections. As done in Mély et al. <ref type="bibr" target="#b37">[40]</ref> and HED <ref type="bibr" target="#b13">[16]</ref>, we randomly split these human-labeled images into 80 training and 20 test samples, and average the scores of three independent trials as final results. When training on Multicue, λ is set to 1.1, and η is set to 0.4 for boundary task and 0.3 for edge task. For boundary detection task, we use learning rate 1e-6 and run SGD for 2k iterations. For edge detection task, we use learning rate 1e-7 and run SGD for 4k iterations. Since the image resolution of Multicue is very high, we randomly crop 500 × 500 patches from original images at each iteration.</p><p>We use VGG16 <ref type="bibr" target="#b10">[11]</ref> as the backbone net. The evaluation results are summarized in <ref type="figure">Fig. 9</ref>. Our proposed RCF achieves substantially higher results than HED. For boundary task, RCF-MS is 1.1% ODS F-measure higher and 1.4% OIS F-measure higher than HED. For edge task, RCF-MS is 0.9% ODS F-measure higher than HED. Note that the fluctuation of RCF is also smaller than HED, which suggests RCF is more robust over different kinds of images. Some qualitative results are shown in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Network Discussion</head><p>To further explore the effectiveness of our network architecture, we implement some mixed networks using VGG16 <ref type="bibr" target="#b10">[11]</ref> by connecting our richer feature side outputs to some convolution stages while connecting side outputs of HED to the other stages. With training only on BSDS500 <ref type="bibr" target="#b8">[9]</ref> dataset and testing on the single scale, evaluation results of these mixed networks are shown in <ref type="figure">Fig. 10</ref>. The last two lines of this table correspond to HED and RCF, respectively. We can observe that all of these mixed networks perform better than HED and worse than RCF that is fully connected to RCF side outputs. It clearly demonstrates the importance of our strategy of richer convolutional features.</p><p>In order to investigate whether including additional nonlinearity helps, we connecting ReLU layer after 1 × 1 − 21 or 1 × 1 − 1 conv layers in each stage. However, the network performs worse. Especially, when we attempt to add nonlinear layers to 1 × 1 − 1 conv layers, the network can not converge properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS ON IMAGE SEGMENTATION</head><p>The predicted edges of natural images are often used in another low-level vision technique, image segmentation, which aims to cluster similar pixels to form perceptual regions. To transform a predicted edge map into a segmentation, Arbeláez <ref type="bibr" target="#b8">[9]</ref> introduced the Ultrametric Contour Map (UCM) that can generate different image partitions when thresholding this hierarchical contour map at various contour probability values. MCG <ref type="bibr" target="#b5">[6]</ref> develops a fast normalized cuts algorithm to accelerate <ref type="bibr" target="#b8">[9]</ref> and makes effective use of multiscale information to generate an accurate hierarchical segmentation tree. Note that MCG needs edge orientations as input. These orientations are usually computed using simple morphological operations. COB <ref type="bibr" target="#b43">[46]</ref> simultaneously predicts the magnitudes and orientations of edges using HED-based CNNs, and then applies MCG framework to convert these predictions to UCM. Since much more accurate edge orientations are used, COB achieves the state-of-the-art segmentation results.</p><p>In order to demonstrate the versatility of the proposed method, here we evaluate the edges of RCF in the context of image segmentation. Specifically, we apply the COB framework but replacing the HED edges with our RCF edges to perform image segmentation. We evaluate the resulting segmenter on the BSDS500 <ref type="bibr" target="#b8">[9]</ref> and NYUD <ref type="bibr" target="#b27">[30]</ref> datasets. Note that COB uses ResNet50 as its backbone net, so we also test RCF with ResNet for fair comparison. Besides the boundary measure (F b ) <ref type="bibr" target="#b7">[8]</ref> used in Sec. 4, we also use the evaluation metric of precision-recall for objects and parts (Fop) <ref type="bibr" target="#b36">[39]</ref> to evaluate the region similarity between the segmentation and the corresponding ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BSDS500 Dataset</head><p>On the challenging BSDS500 dataset <ref type="bibr" target="#b8">[9]</ref>, we compare RCF with some well-known generic image segmenters, including NCut <ref type="bibr" target="#b38">[41]</ref>, MShift <ref type="bibr" target="#b39">[42]</ref>, EGB <ref type="bibr" target="#b40">[43]</ref>, gPb-UCM <ref type="bibr" target="#b8">[9]</ref>, ISCRA <ref type="bibr" target="#b41">[44]</ref>, MCG <ref type="bibr" target="#b5">[6]</ref>, LEP <ref type="bibr" target="#b42">[45]</ref>, and COB <ref type="bibr" target="#b43">[46]</ref>. The evaluation results are shown in <ref type="figure">Fig. 7</ref>. RCF achieves the new state of the art on this dataset, both in terms of boundary and region quality. COB <ref type="bibr" target="#b43">[46]</ref> gets the second place. We show the numeric comparison in <ref type="figure">Fig. 11</ref>. For the boundary measure, both the ODS and OIS F-measure of RCF are 1.3% higher than COB. For the region measure, the ODS and OIS F-measure of RCF are 2.4% and 3.0% higher than COB, respectively. Using the ResNet as the backbone net, RCF can further improve performance. Since COB uses the edges produced by HED <ref type="bibr" target="#b13">[16]</ref>, these results demonstrate the effectiveness of RCF architecture. From <ref type="figure">Fig. 7</ref>, we can also see that although the boundary measure of RCF segmentation have reached human performance, the region measure is still far away from the human performance. It indicates that better perception regions should be the main pursuit of classical image segmentation in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYUD Dataset</head><p>On the RGB-D dataset of NYUD <ref type="bibr" target="#b27">[30]</ref>, we compare not only with some RGB based methods, e.g. gPb-UCM <ref type="bibr" target="#b8">[9]</ref> and MCG <ref type="bibr" target="#b5">[6]</ref>, but also with some RGB-D based methods, e.g. gPb+NG <ref type="bibr" target="#b34">[37]</ref>, SE+NG+ <ref type="bibr" target="#b35">[38]</ref>, and COB <ref type="bibr" target="#b43">[46]</ref>. The precision-recall curves of boundary and region measures are displayed in <ref type="figure">Fig. 8</ref>. The numeric comparison is summarized in <ref type="figure" target="#fig_1">Fig. 12</ref>. Our RCF with VGG16 achieves higher F-measure score than COB on the region measure, while performs slightly worse than original COB on the boundary measure. With ResNet50 as the backbone net, RCF achieves similar performance with COB on the boundary measure but 1.6% higher on the region <ref type="figure">Fig. 6</ref>: Some examples of RCF. Top two rows: BSDS500 <ref type="bibr" target="#b8">[9]</ref>. Bottom two rows: NYUD <ref type="bibr" target="#b27">[30]</ref>. From Left to Right: origin image, ground truth, RCF edge map, RCF UCM map. measure. Moreover, both COB and RCF outperform traditional methods by a large margin, which demonstrates the importance of accurate edges in the classic image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we introduce richer convolutional features (RCF), a novel CNN architecture which makes good usage of feature hierarchies in CNNs, for edge detection. RCF encapsulates both semantic and fine detail features by leveraging all convolutional features. RCF is both accurate and efficient, making it promising to be applied in other vision tasks. We also achieve competitive results when applying RCF edges for classical image segmentation. RCF architecture can be seen as a development direction of fully convolutional networks, like FCN <ref type="bibr" target="#b11">[12]</ref> and HED <ref type="bibr" target="#b13">[16]</ref>. It would be interesting to explore the effectiveness of our network architecture in other hot topics <ref type="bibr" target="#b18">[21]</ref>, <ref type="bibr" target="#b19">[22]</ref>, <ref type="bibr" target="#b20">[23]</ref>, <ref type="bibr" target="#b21">[24]</ref>. Source code is available at https://mmcheng.net/rcfedge/.  <ref type="figure">Fig. 7</ref>: The precision-recall curves for the evaluation of boundary measure (F b <ref type="bibr" target="#b7">[8]</ref>) and region measure (Fop <ref type="bibr" target="#b36">[39]</ref>) of classical image segmentation on the BSDS500 test set <ref type="bibr" target="#b8">[9]</ref>.  <ref type="figure">Fig. 8</ref>: The precision-recall curves for the evaluation of boundary measure (F b <ref type="bibr" target="#b7">[8]</ref>) and region measure (Fop <ref type="bibr" target="#b36">[39]</ref>) of classical image segmentation on the NYUD test set <ref type="bibr" target="#b27">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method ODS OIS</head><p>Human-Boundary <ref type="bibr" target="#b37">[40]</ref> 0.760 (0.017) -Multicue-Boundary <ref type="bibr" target="#b37">[40]</ref> 0.720 (0.014) -HED-Boundary <ref type="bibr" target="#b13">[16]</ref> 0.814 (0.011) 0.822 (0.008) RCF-Boundary 0.817 (0.004) 0.825 (0.005) RCF-MS-Boundary 0.825 (0.008) 0.836 (0.007) Human-Edge <ref type="bibr" target="#b37">[40]</ref> 0.750 (0.024) -Multicue-Edge <ref type="bibr" target="#b37">[40]</ref> 0.830 (0.002) -HED-Edge <ref type="bibr" target="#b13">[16]</ref> 0.851 (0.014) 0.864 (0.011) RCF-Edge 0.857 (0.004) 0.862 (0.004) RCF-MS-Edge 0.860 (0.005) 0.864 (0.004) <ref type="figure">Fig. 9</ref>: The comparisons on the Multicue dataset <ref type="bibr" target="#b37">[40]</ref>. The numbers in the parentheses mean standard deviations.  <ref type="bibr" target="#b38">[41]</ref> 0.641 0.674 0.213 0.270 MShift <ref type="bibr" target="#b39">[42]</ref> 0.601 0.644 0.229 0.292 EGB <ref type="bibr" target="#b40">[43]</ref> 0.636 0.674 0.158 0.240 gPb-UCM <ref type="bibr" target="#b8">[9]</ref> 0.726 0.760 0.348 0.385 ISCRA <ref type="bibr" target="#b41">[44]</ref> 0.724 0.752 0.352 0.418 MCG <ref type="bibr" target="#b5">[6]</ref> 0.747 0.779 0.380 0.433 LEP <ref type="bibr" target="#b42">[45]</ref> 0.757 0.793 0.417 0.468 COB <ref type="bibr" target="#b43">[46]</ref> 0.793 0.820 0.415 0.466 RCF 0.806 0.833 0.439 0.496 RCF-ResNet50 0.808 0.833 0.441 0.500 RCF-ResNet101 0.810 0.836 0.440 0.501 <ref type="figure">Fig. 11</ref>: Evaluation results of boundaries (F b <ref type="bibr" target="#b7">[8]</ref>) and regions (Fop <ref type="bibr" target="#b36">[39]</ref>) on the BSDS500 test set <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Boundaries (F b ) Regions (Fop) ODS OIS ODS OIS gPb-UCM <ref type="bibr" target="#b8">[9]</ref> 0.631 0.661 0.242 0.283 MCG <ref type="bibr" target="#b5">[6]</ref> 0.651 0.681 0.264 0.300 gPb+NG <ref type="bibr" target="#b34">[37]</ref> 0.687 0.716 0.286 0.324 SE+NG+ <ref type="bibr" target="#b35">[38]</ref> 0.706 0.734 0.319 0.359 COB <ref type="bibr" target="#b43">[46]</ref> 0  <ref type="bibr" target="#b27">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>M.M. Cheng is the corresponding author. URL: http://mmcheng.net/rcfedge/ • Y. Liu, M.M. Cheng, and J.W. Bian, are with the College of Computer Science, Nankai University, Tianjin 300350, China. • L. Zhang is with the Advanced Digital Sciences Center. • X. Bai is with Huazhong University of Science and Technology. • J. Tang is with School of Computer Science and Engineering, Nanjing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Our RCF network architecture. The input is an image with arbitrary sizes, and our network outputs an edge possibility map in the same size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comput. Vis. Pattern Recog., 2015, pp. 4380-4389. [14] Y. Ganin and V. Lempitsky, "N 4 -Fields: Neural network nearest neighbor fields for image transforms," in ACCV, 2014, pp. 536-551. [15] W. Shen, X. Wang, Y. Wang, X. Bai, and Z. Zhang, "DeepContour: A deep convolutional feature learned by positive-sharing loss for contour</figDesc><table><row><cell>Methods</cell><cell>Boundaries (F b ) ODS OIS</cell><cell>Regions (Fop) ODS OIS</cell><cell></cell></row><row><cell>NCut</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>RCF Stage</cell><cell>HED Stage</cell><cell>ODS</cell><cell>OIS</cell></row><row><cell></cell><cell></cell><cell>1, 2</cell><cell>3, 4, 5</cell><cell>0.792 0.810</cell></row><row><cell></cell><cell></cell><cell>2, 4</cell><cell>1, 3, 5</cell><cell>0.795 0.812</cell></row><row><cell></cell><cell></cell><cell>4, 5</cell><cell>1, 2, 3</cell><cell>0.790 0.810</cell></row><row><cell></cell><cell></cell><cell>1, 3, 5</cell><cell>2, 4</cell><cell>0.794 0.810</cell></row><row><cell></cell><cell></cell><cell>3, 4, 5</cell><cell>1, 2</cell><cell>0.796 0.812</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>1, 2, 3, 4, 5</cell><cell>0.788 0.808</cell></row><row><cell></cell><cell></cell><cell>1, 2, 3, 4, 5</cell><cell>-</cell><cell>0.798 0.815</cell></row><row><cell></cell><cell></cell><cell cols="3">Fig. 10: Results of some thought networks.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by NSFC (NO. 61620106008, 61572264), the national youth talent support program, Tianjin Natural Science Foundation for Distinguished Young Scholars (NO. 17JCJQJC43700), Huawei Innovation Research Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5872" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition by linear combinations of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="992" to="1006" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequential optimization for efficient high-quality object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HFS: Hierarchical feature selection for efficient image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="867" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1558" to="1570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepEdge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. detection,&quot; in IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Webseg: Learning semantic segmentation from web searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09859</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth-aware neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Non-Photorealistic Animation and Rendering</title>
		<meeting>the Symposium on Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">EdgeStereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05196</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hi-Fi: Hierarchical feature integration for skeleton detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">RSRN: Rich side-output residual network for medial axis detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1739" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">People detection in crowded scenes using hierarchical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Surface fatigue crack identification in steel box girder of bridges by a deep fusion convolutional neural network based on consumer-grade camera images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Health Monitoring</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning relaxed deep supervision for better edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep crisp boundaries: From boundaries to higher-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02439</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep structured multi-scale features using attention-gated CRFs for contour prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="3961" to="3970" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CASENet: Deep category-aware semantic edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Oriented edge forests for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1732" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-for-low and low-forhigh: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised evaluation of image segmentation and object proposal techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1465" to="1478" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A systematic comparison between visual cues for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mély</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image segmentation by cascaded region agglomeration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Segmenting natural images with the least effort as humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno>110.1-110.12</idno>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="580" to="596" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
