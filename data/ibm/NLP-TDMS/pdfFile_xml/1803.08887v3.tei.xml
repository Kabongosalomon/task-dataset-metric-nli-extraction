<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-15">15 Dec 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Gan ; Tuan-Anh</forename><surname>Dist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">ST Electronics -SUTD Cyber Security Laboratory</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<title level="m">An Improved GAN using Distance Constraints Ngoc-Trung Tran</title>
						<imprint>
							<date type="published" when="2018-12-15">15 Dec 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Generative Adversarial Networks · image generation · dis- tance constraints · autoencoders</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce effective training algorithms for Generative Adversarial Networks (GAN) to alleviate mode collapse and gradient vanishing. In our system, we constrain the generator by an Autoencoder (AE). We propose a formulation to consider the reconstructed samples from AE as "real" samples for the discriminator. This couples the convergence of the AE with that of the discriminator, effectively slowing down the convergence of discriminator and reducing gradient vanishing. Importantly, we propose two novel distance constraints to improve the generator. First, we propose a latent-data distance constraint to enforce compatibility between the latent sample distances and the corresponding data sample distances. We use this constraint to explicitly prevent the generator from mode collapse. Second, we propose a discriminator-score distance constraint to align the distribution of the generated samples with that of the real samples through the discriminator score. We use this constraint to guide the generator to synthesize samples that resemble the real ones. Our proposed GAN using these distance constraints, namely Dist-GAN, can achieve better results than state-of-the-art methods across benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and STL-10 datasets. Our code is published here 1 for research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Network <ref type="bibr" target="#b9">[12]</ref> (GAN) has become a dominant approach for learning generative models. It can produce very visually appealing samples with few assumptions about the model. GAN can produce samples without explicitly estimating data distribution, e.g. in analytical forms. GAN has two main components which compete against each other, and they improve through the competition. The first component is the generator G, which takes low-dimensional random noise z ∼ P z as an input and maps them into highdimensional data samples, x ∼ P x . The prior distribution P z is often uniform or normal. Simultaneously, GAN uses the second component, a discriminator D, to distinguish whether samples are drawn from the generator distribution P G or data distribution P x . Training GAN is an adversarial process: while the discriminator D learns to better distinguish the real or fake samples, the generator G learns to confuse the discriminator D into accepting its outputs as being real. The generator G uses discriminator's scores as feedback to improve itself over time, and eventually can approximate the data distribution.</p><p>Despite the encouraging results, GAN is known to be hard to train and requires careful designs of model architectures <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b21">24]</ref>. For example, the imbalance between discriminator and generator capacities often leads to convergence issues, such as gradient vanishing and mode collapse. Gradient vanishing occurs when the gradient of discriminator is saturated, and the generator has no informative gradient to learn. It occurs when the discriminator can distinguish very well between "real" and "fake" samples, before the generator can approximate the data distribution. Mode collapse is another crucial issue. In mode collapse, the generator is collapsed into a typical parameter setting that it always generates small diversity of samples.</p><p>Several GAN variants have been proposed <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b1">4,</ref><ref type="bibr" target="#b26">29]</ref> to solve these problems. Some of them are Autoencoders (AE) based GAN. AE explicitly encodes data samples into latent space and this allows representing data samples with lower dimensionality. It not only has the potential for stabilizing GAN but is also applicable for other applications, such as dimensionality reduction. AE was also used as part of a prominent class of generative models, Variational Autoencoders (VAE) <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b3">6]</ref>, which are attractive for learning inference/generative models that lead to better log-likelihoods <ref type="bibr" target="#b25">[28]</ref>. These encouraged many recent works following this direction. They applied either encoders/decoders as an inference model to improve GAN training <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b16">19]</ref>, or used AE to define the discriminator objectives <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b2">5]</ref> or generator objectives <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b24">27]</ref>. Others have proposed to combine AE and GAN <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b15">18]</ref>.</p><p>In this work, we propose a new design to unify AE and GAN. Our design can stabilize GAN training, alleviate the gradient vanishing and mode collapse issues, and better approximate data distribution. Our main contributions are two novel distance constraints to improve the generator. First, we propose a latentdata distance constraint. This enforces compatibility between latent sample distances and the corresponding data sample distances, and as a result, prevents the generator from producing many data samples that are close to each other, i.e. mode collapse. Second, we propose a discriminator-score distance constraint. This aligns the distribution of the fake samples with that of the real samples and guides the generator to synthesize samples that resemble the real ones. We propose a novel formulation to align the distributions through the discriminator score. Comparing to state of the art methods using synthetic and benchmark datasets, our method achieves better stability, balance, and competitive standard scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The issue of non-convergence remains an important problem for GAN research, and gradient vanishing and mode collapse are the most important problems <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b0">3]</ref>. Many important variants of GAN have been proposed to tackle these issues. Improved GAN <ref type="bibr" target="#b23">[26]</ref> introduced several techniques, such as feature matching, mini-batch discrimination, and historical averaging, which drastically reduced the mode collapse. Unrolled GAN <ref type="bibr" target="#b19">[22]</ref> tried to change optimization process to address the convergence and mode collapse. <ref type="bibr" target="#b1">[4]</ref> analyzed the convergence properties for GAN. Their proposed GAN variant, WGAN, leveraged the Wasserstein distance and demonstrated its better convergence than Jensen Shannon (JS) divergence, which was used previously in vanilla GAN <ref type="bibr" target="#b9">[12]</ref>. However, WGAN required that the discriminator must lie on the space of 1-Lipschitz functions, therefore, it had to enforce norm critics to the discriminator by weight-clipping tricks. WGAN-GP <ref type="bibr" target="#b10">[13]</ref> stabilized WGAN by alternating the weight-clipping by penalizing the gradient norm of the interpolated samples. Recent work SN-GAN <ref type="bibr" target="#b20">[23]</ref> proposed a weight normalization technique, named as spectral normalization, to slow down the convergence of the discriminator. This method controls the Lipschitz constant by normalizing the spectral norm of the weight matrices of network layers.</p><p>Other work has integrated AE into the GAN. AAE <ref type="bibr" target="#b18">[21]</ref> learned the inference by AE and matched the encoded latent distribution to given prior distribution by the minimax game between encoder and discriminator. Regularizing the generator with AE loss may cause the blurry issue. This regularization can not assure that the generator is able to approximate well data distribution and overcome the mode missing. VAE/GAN <ref type="bibr" target="#b15">[18]</ref> combined VAE and GAN into one single model and used feature-wise distance for the reconstruction. Due to depending on VAE <ref type="bibr" target="#b14">[17]</ref>, VAEGAN also required re-parameterization tricks for back-propagation or required access to an exact functional form of prior distribution. InfoGAN <ref type="bibr" target="#b5">[8]</ref> learned the disentangled representation by maximizing the mutual information for inducing latent codes. EBGAN <ref type="bibr" target="#b27">[30]</ref> introduced the energy-based model, in which the discriminator is considered as energy function minimized via reconstruction errors. BEGAN <ref type="bibr" target="#b2">[5]</ref> extended EBGAN by optimizing Wasserstein distance between AE loss distributions. ALI <ref type="bibr" target="#b7">[10]</ref> and BiGAN <ref type="bibr" target="#b6">[9]</ref> encoded the data into latent and trained jointly the data/latent samples in GAN framework. This model can learn implicitly encoder/decoder models after training. MDGAN <ref type="bibr" target="#b4">[7]</ref> required two discriminators for two separate steps: manifold and diffusion. The manifold step tended to learn a good AE, and the diffusion objective is similar to the original GAN objective, except that the constructed samples are used instead of real samples.</p><p>In the literature, VAEGAN and MDGAN are most related to our work in term of using AE to improve the generator. However, our design is remarkably different: (1) VAEGAN combined KL divergence and reconstruction loss to train the inference model. With this design, it required an exact form of prior distribution and re-parameterization tricks for solving the optimization via back-propagation. In contrast, our method constrains AE by the data and  latent sample distances. Our method is applicable to any prior distribution. (2) Unlike MDGAN, our design does not require two discriminators. (3) VAEGAN considered the reconstructed samples as "fake", and MDGAN adopts this similarly in its manifold step. In contrast, we use them as "real" samples, which is important to restrain the discriminator in order to avoid gradient vanishing, therefore, reduce mode collapse. (4) Two of these methods regularize G simply by reconstruction loss. This is inadequate to solve the mode collapse. We conduct an analysis and explain why additional regularization is needed for AE. Experiment results demonstrate that our model outperforms MDGAN and VAEGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>Mode collapse is an important issue for GAN. In this section, we first propose a new way to visualize the mode collapse. Based on the visualization results, we propose a new model, namely Dist-GAN, to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visualize mode collapse in latent space</head><p>Mode collapse occurs when "the generator collapses to a parameter setting where it always emits the same point. When collapse to a single mode is imminent, the gradient of the discriminator may point in similar directions for many similar points." <ref type="bibr" target="#b23">[26]</ref>. Previous work usually examines mode collapse by visualizing a few collapsed samples (generated from random latent samples of a prior distribution). <ref type="figure" target="#fig_0">Fig. 1a</ref> is an example. However, the data space is high-dimensional, therefore it is difficult to visualize points in the data space. On the other hand, the latent space is lower-dimensional and controllable, and it is possible to visualize the entire 2D/3D spaces. Thus, it could be advantageous to examine mode collapse in the latent space. However, the problem is that GAN is not invertible to map the data samples back to the latent space. Therefore, we propose the following method to visualize the samples and examine mode collapse in the latent space. We apply an off-the-shelf classifier. This classifier predicts labels of the generated samples. We visualize these class labels according to the latent samples, see <ref type="figure" target="#fig_0">Fig. 1b</ref>. This is possible because, for many datasets such as MNIST, pre-trained classifiers can achieve high accuracy, e.g. 0.04% error rate.  <ref type="figure" target="#fig_0">Fig. 1b</ref> clearly suggests the extent of mode collapse: many latent samples from large regions of latent space are collapsed into the same digit, e.g. '1'. Even some latent samples reside very far apart from each other, they map to the same digit. This suggests that a generator G θ with parameter θ has mode collapse when there are many latent samples mapped to small regions of the data space:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distance constraint: Motivation</head><formula xml:id="formula_0">x i = G θ (z i ), x j = G θ (z j ) : f (x i , x j ) &lt; δ x (1)</formula><p>Here {z i } are latent samples, and {x i } are corresponding synthesized samples by G θ . f is some distance metric in the data space, and δ x is a small threshold in the data space. Therefore, we propose to address mode collapse using a distance metric g in latent space, and a small threshold δ z of this metric, to restrain G θ as follows:</p><formula xml:id="formula_1">g(z i , z j ) &gt; δ z → f (x i , x j ) &gt; δ x<label>(2)</label></formula><p>However, determining good functions f, g for two spaces of different dimensionality and their thresholds δ x , δ z is not straightforward. Moreover, applying these constraints to GAN is not simple, because GAN has only one-way mapping from latent to data samples. In the next section, we will propose novel formulation to represent this constraint in latent-data distance and apply this to GAN. We have also tried to apply this visualization for two state-of-the-art methods: DCGAN <ref type="bibr" target="#b21">[24]</ref>, WGANGP <ref type="bibr" target="#b10">[13]</ref> on the MNIST dataset (using the code of <ref type="bibr" target="#b10">[13]</ref>). Note that all of our experiments were conducted in the unsupervised setting. The off-the-shelf classifier is used here to determine the labels of generated samples solely for visualization purpose. <ref type="figure" target="#fig_1">Fig. 2a</ref> and <ref type="figure" target="#fig_1">Fig. 2b</ref> represent the labels of the 55K latent variables of DCGAN and WGANGP respectively at iteration of 70K. <ref type="figure" target="#fig_1">Fig. 2a</ref> reveals that DCGAN is partially collapsed, as it generates very few digits '5' and '9' according to their latent variables near the bottom-right top-left corners of the prior distribution. In contrast, WGANGP does not have mode collapse, as shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>. However, for WGANGP, the latent variables corresponding to each digit are fragmented in many sub-regions. It is an interesting observation for WGANGP. We will investigate this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improving GAN using Distance Constraints</head><p>We apply the idea of Eqn. 2 to improve generator through an AE. We apply AE to encode data samples into latent variables and use these encoded latent variables to direct the generator's mapping from the entire latent space. First, we train an AE (encoder E ω and decoder G θ ), then we train the discriminator D γ and the generator G θ . Here, the generator is the decoder of AE and ω, θ, γ are the parameters of the encoder, generator, and discriminator respectively. Two main reasons for training an AE are: (i) to regularize the parameter θ at each training iteration, and (ii) to direct the generator to synthesize samples similar to real training samples. We include an additional latent-data distance constraint to train the AE: min</p><formula xml:id="formula_2">ω,θ L R (ω, θ) + λ r L W (ω, θ) (3) where L R (ω, θ) = ||x − G θ (E ω (x))|| 2 2 is the conventional AE objective. The latent-data distance constraint L W (ω, θ)</formula><p>is to regularize the generator and prevent it from being collapsed. This term will be discussed later. Here, λ r is the constant. The reconstructed samples G θ (E ω (x)) can be approximated by G θ (E ω (x)) = x + ε, where ε is the reconstruction error. Usually the capacity of E and G are large enough so that is small (like noise). Therefore, it is reasonable to consider those reconstructed samples as "real" samples (plus noise ε). The pixel-wise reconstruction may cause blurry. To circumvent this, we instead use feature-wise distance <ref type="bibr" target="#b15">[18]</ref> or similarly feature matching <ref type="bibr" target="#b23">[26]</ref>:</p><formula xml:id="formula_3">L R (ω, θ) = ||Φ(x)−Φ(G θ (E ω (x)))|| 2 2 . Here Φ(x)</formula><p>is the high-level feature obtained from some middle layers of deep networks. In our implementation, Φ(x) is the feature output from the last convolution layer of discriminator D γ . Note that in the first iteration, the parameters of discriminator are randomly initialized, and features produced from this discriminator is used to train the AE.</p><p>Our framework is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. We propose to train encoder E ω , generator G θ and discriminator D γ following the order: (i) fix D γ and train E ω and G θ to minimize the reconstruction loss Eqn. 3 (ii) fix E ω , G θ , and train D γ to minimize (Eqn. 5), and (iii) fix E ω , D γ and train G θ to minimize (Eqn. 4).</p><p>Generator and discriminator objectives When training the generator, maximizing the conventional generator objective E z σ(D γ (G θ (z))) <ref type="bibr" target="#b9">[12]</ref> tends to produce samples at high-density modes, and this leads to mode collapse easily. Here, σ denotes the sigmoid function and E denotes the expectation. Instead, we train the generator with our proposed "discriminator-score distance". We align the synthesized sample distribution to real sample distribution with the 1 distance. The alignment is through the discriminator score, see Eqn. 4. Ideally, the generator synthesizes samples similar to the samples drawn from the real distribution, and this also helps reduce missing mode issue.</p><formula xml:id="formula_4">min θ L G (θ) = |E x σ(D γ (x)) − E z σ(D γ (G θ (z)))| (4)</formula><p>The objective function of the discriminator is shown in Eqn. 5. It is different from original discriminator of GAN in two aspects. First, we indicate the reconstructed samples as "real", represented by the term L C = E x log σ(D γ (G θ (E ω (x)))). Considering the reconstructed samples as "real" can systematically slow down the convergence of discriminator, so that the gradient from discriminator is not saturated too quickly. In particular, the convergence of the discriminator is coupled with the convergence of AE. This is an important constraint. In contrast, if we consider the reconstruction as "fake" in our model, this speeds up the discriminator convergence, and the discriminator converges faster than both generator and encoder. This leads to gradient saturation of D γ . Second, we apply the gradient penalty L P = (||∇xD γ (x)|| 2 2 − 1) 2 for the discriminator objective (Eqn. 5), where λ p is penalty coefficient, andx = x + (1 − )G(z), is a uniform random number ∈ U [0, 1]. This penalty was used to enforce Lipschitz constraint of Wasserstein-1 distance <ref type="bibr" target="#b10">[13]</ref>. In this work, we also find this useful for JS divergence and stabilizing our model. It should be noted that using this gradient penalty alone cannot solve the convergence issue, similar to WGANGP. The problem is partially solved when combining this with our proposed generator objective in Eqn. 4, i.e., discriminator-score distance. However, the problem cannot be completely solved, e.g. mode collapse on MNIST dataset with 2D latent inputs as shown in <ref type="figure" target="#fig_1">Fig. 2c</ref>. Therefore, we apply the proposed latent-data distance constraints as additional regularization term for AE: L W (ω, θ), to be discussed in the next section.</p><formula xml:id="formula_5">min γ L D (ω, θ, γ) = −(E x log σ(D γ (x)) + E z log(1 − σ(D γ (G θ (z)))) + E x log σ(D γ (G θ (E ω (x)))) − λ p Ex(||∇xD γ (x)|| 2 2 − 1) 2 )<label>(5)</label></formula><p>Regularizing Autoencoders by Latent-Data Distance Constraint In this section, we discuss the latent-data distance constraint L W (ω, θ) to regularize AE in order to reduce mode collapse in the generator (the decoder in the AE). In particular, we use noise input to constrain encoder's outputs, and simultaneously reconstructed samples to constrain the generator's outputs. Mode collapse occurs when the generator synthesizes low diversity of samples in the data space given different latent inputs. Therefore, to reduce mode collapse, we aim to achieve: if the distance of any two latent variables g(z i , z j ) is small (large) in the latent space, the corresponding distance f (x i , x j ) in data space should be small (large), and vice versa. We propose a latent-data distance regularization L W (ω, θ):</p><formula xml:id="formula_6">L W (ω, θ) = ||f (x, G θ (z)) − λ w g(E ω (x), z)|| 2 2<label>(6)</label></formula><p>where f and g are distance functions computed in data and latent space. λ w is the scale factor due to the difference in dimensionality. It is not straight forward to compare distances in spaces of different dimensionality. Therefore, instead of using the direct distance functions, e.g. Euclidean, 1 -norm, etc, we propose to compare the matching score f (x, G θ (z)) of real and fake distributions, and the matching score g(E ω (x), z) of two latent distributions. We use means as the matching scores. Specifically:</p><formula xml:id="formula_7">f (x, G θ (z)) = M d (E x G θ (E ω (x)) − E z G θ (z)) (7) g(E ω (x), z) = M d (E x E ω (x) − E z z)<label>(8)</label></formula><p>where M d computes the average of all dimensions of the input. <ref type="figure" target="#fig_4">Fig. 4a</ref> illustrates 1D frequency density of 10000 random samples mapped by M d from [−1, 1] uniform distribution of different dimensionality. We can see that outputs of M d from high dimensional spaces have small values. Thus, we require λ w in (6) to account for the difference in dimensionality. Empirically, we found λ w = dz dx suitable, where d z and d x are dimensions of latent and data samples respectively. <ref type="figure" target="#fig_4">Fig. 4b</ref> shows the frequency density of a collapse mode case. We can observe that the 1D density of generated samples is clearly different from that of the real data. <ref type="figure" target="#fig_4">Fig. 4c</ref> compares 1D frequency densities of 55K MNIST samples generated by different methods. Our Dist-GAN method can estimate better 1D density than DCGAN and WGANGP measured by KL divergence (kldiv) between the densities of generated samples and real samples. The entire algorithm is presented in Algorithm. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic data</head><p>All our experiments are conducted using the unsupervised setting. First, we use synthetic data to evaluate how well our Dist-GAN can approximate the data distribution. We use a synthetic dataset of 25 Gaussian modes in grid layout similar to <ref type="bibr" target="#b7">[10]</ref>. Our dataset contains 50K training points in 2D, and we draw 2K generated samples for testing. For fair comparisons, we use equivalent Algorithm 1 Dist-GAN architectures and setup for all methods in the same experimental condition if possible. The architecture and network size are similar to <ref type="bibr" target="#b19">[22]</ref> on the 8-Gaussian dataset, except that we use one more hidden layer. We use fully-connected layers and Rectifier Linear Unit (ReLU) activation for input and hidden layers, sigmoid for output layers. The network size of encoder, generator and discriminator are presented in <ref type="table" target="#tab_1">Table 1</ref> of Supplementary Material, where d in = 2, d out = 2, d h = 128 are dimensions of input, output and hidden layers respectively. N h = 3 is the number of hidden layers. The output dimension of the encoder is the dimension of the latent variable. Our prior distribution is uniform [−1, 1]. We use Adam optimizer with learning rate lr = 0.001, and the exponent decay rate of first moment β 1 = 0.8. The learning rate is decayed every 10K steps with a base of 0.9. The mini-batch size is 128. The training stops after 500 epochs. To have fair comparison, we carefully fine-tune other methods (and use weight decay during training if this achieves better results) to ensure they achieve their best results on the synthetic data. For evaluation, a mode is missed if there are less than 20 generated samples registered into this mode, which is measured by its mean and variance of 0.01 <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b19">22]</ref>. A method has mode collapse if there are missing modes. In this experiment, we fix the parameters λ r = 0.1 (Eqn. 3), λ p = 0.1 (Eqn. 5), λ w = 1.0 (Eqn. 6). For each method, we repeat eight runs and report the average. The number of registered modes (a) and points (b) of our method with two different settings on the synthetic dataset. We compare our Dist-GAN to the baseline GAN <ref type="bibr" target="#b9">[12]</ref> and other methods on the same dataset measured by the number of registered modes (classes) (c) and points (d).</p><p>First, we highlight the capability of our model to approximate the distribution P x of synthetic data. We carry out the ablation experiment to understand the influence of each proposed component with different settings:</p><p>-Dist-GAN 1 : uses the "discriminator-score distance" for generator objective (L G ) and the AE loss L R but does not use data-latent distance constraint term (L W ) and gradient penalty (L P ). This setting has three different versions as using reconstructed samples (L C ) as "real", "fake" or "none" (not use it) in the discriminator objective. -Dist-GAN 2 : improves from Dist-GAN 1 (regarding reconstructed samples as "real") by adding the gradient penalty L P . <ref type="figure">Fig. 6</ref>. The mode balance obtained by different methods.</p><p>-Dist-GAN: improves the Dist-GAN 2 by adding the data-latent distance constraint L W . (See <ref type="table" target="#tab_3">Table 3</ref> in Supplementary Material for details).</p><p>The quantitative results are shown in <ref type="figure" target="#fig_5">Fig. 5. Fig. 5a</ref> is the number of registered modes changing over the training. Dist-GAN 1 misses a few modes while Dist-GAN 2 and Dist-GAN generates all 25 modes after about 50 epochs. Since they almost do not miss any modes, it is reasonable to compare the number of registered points as in <ref type="figure" target="#fig_5">Fig. 5b</ref>. Regarding reconstructed samples as "real" achieves better results than regarding them as "fake" or "none". It is reasonable that Dist-GAN 1 obtains similar results as the baseline GAN when not using the reconstructed samples in discriminator objective ("none" option). Other results show the improvement when adding the gradient penalty into the discriminator (Dist-GAN 2 ). Dist-GAN demonstrates the effectiveness of using the proposed latent-data constraints, when comparing with Dist-GAN 2 .</p><p>To highlight the effectiveness of our proposed "discriminator-score distance" for the generator, we use it to improve the baseline GAN <ref type="bibr" target="#b9">[12]</ref>, denoted by GAN 1 . Then, we propose GAN 2 to improve GAN 1 by adding the gradient penalty. We can observe that combination of our proposed generator objective and gradient penalty can improve stability of GAN. We compare our best setting (Dist-GAN) to previous work. ALI <ref type="bibr" target="#b7">[10]</ref> and DAN-2S <ref type="bibr" target="#b16">[19]</ref> are recent works using encoder/decoder in their model. VAE-GAN <ref type="bibr" target="#b15">[18]</ref> introduces a similar model. WGAN-GP <ref type="bibr" target="#b10">[13]</ref> is one of the current state of the art. The numbers of covered modes and registered points are presented in <ref type="figure" target="#fig_5">Fig. 5c and Fig 5d respectively</ref>. The quantitative numbers of last epochs are shown in <ref type="table">Table 2</ref> of Supplementary Material. In this table, we report also Total Variation scores to measure the mode balance. The result for each method is the average of eight runs. Our method outperforms GAN <ref type="bibr" target="#b9">[12]</ref>, DAN-2S <ref type="bibr" target="#b16">[19]</ref>, ALI <ref type="bibr" target="#b7">[10]</ref>, and VAE/GAN <ref type="bibr" target="#b15">[18]</ref> on the number of covered modes. While WGAN-GP sometimes misses one mode and diverges, our method (Dist-GAN) does not suffer from mode collapse in all eight runs. Furthermore, we achieve a higher number of registered samples than WGAN-GP and all others. Our method is also better than the rest with Total Variation (TV) <ref type="bibr" target="#b16">[19]</ref>. <ref type="figure">Fig. 6</ref> depicts the detail proportion of generated samples of <ref type="figure">Fig. 7</ref>. The real and our generated samples in one mini-batch. And the number of generated samples per class obtained by our method on the MNIST dataset. We compare our frequency of generated samples to the ground-truth via KL divergence: KL = 0.01. 25 modes. (More visualization of generated samples in Section 2 of Supplementary Material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MNIST-1K</head><p>For image datasets, we use Φ(x) instead x for the reconstruction loss and the latent-data distance constraint in order to avoid the blur. We fix the parameters λ p = 1.0, and λ r = 1.0 for all image datasets that work consistently well. The λ w is automatically computed from dimensions of features Φ(x) and latent samples. Our model implementation for MNIST uses the published code of WGAN-GP <ref type="bibr" target="#b10">[13]</ref>. <ref type="figure">Fig. 7</ref> from left to right are the real samples, the generated samples and the frequency of each digit generated by our method for standard MNIST. It demonstrates that our method can approximate well the MNIST digit distribution. Moreover, our generated samples look realistic with different styles and strokes that resemble the real ones. In addition, we follow the procedure in <ref type="bibr" target="#b19">[22]</ref> to construct a more challenging 1000-class MNIST (MNIST-1K) dataset. It has 1000 modes from 000 to 999. We create a total of 25,600 images. We compare methods by counting the number of covered modes (having at least one sample <ref type="bibr" target="#b19">[22]</ref>) and computing KL divergence. To be fair, we adopt the equivalent network architecture (low-capacity generator and two crippled discriminators K/4 and K/2) as proposed by <ref type="bibr" target="#b19">[22]</ref>. <ref type="table" target="#tab_1">Table 1</ref> presents the number of modes and KL divergence of compared methods. Results show that our method outperforms all others in the number of covered modes, especially with the low-capacity discriminator (K/4 architecture), where our method has 150 modes more than the second best. Our method reduces the gap between the two architectures (e.g. about 60 modes), which is smaller than other methods. For both architectures, we obtain better results for both KL divergence and the number of recovered modes. All results support that our proposed Dist-GAN handles better mode collapse, and is robust even in case of imbalance in generator and discriminator.</p><p>5 CelebA, CIFAR-10 and STL-10 datasets Furthermore, we use CelebA dataset and compare with DCGAN <ref type="bibr" target="#b21">[24]</ref> and WGAN-GP <ref type="bibr" target="#b10">[13]</ref>. Our implementation is based on the open source [2,1]. <ref type="figure">Fig. 8</ref> shows samples generated by DCGAN, WGANGP and our Dist-GAN. While DCGAN is slightly collapsed at epoch 50, and WGAN-GP sometimes generates broken faces. Our method does not suffer from such issues and can generate recognizable and realistic faces. We also report results for the CIFAR-10 dataset using DCGAN architecture <ref type="bibr" target="#b21">[24]</ref> of same published code <ref type="bibr" target="#b10">[13]</ref>. The generated samples with our method trained on this dataset can be found in Section 4 of Supplementary Material. For quantitative results, we report the FID scores <ref type="bibr" target="#b12">[15]</ref> for both datasets. FID can detect intra-class mode dropping, and measure the diversity and the quality of generated samples. We follow the experimental procedure and model architecture in <ref type="bibr" target="#b17">[20]</ref>. Our method outperforms others for both CelebA and CIFAR-10, as shown in the first and second rows of <ref type="table">Table 2</ref>. Here, the results of other GAN methods are from <ref type="bibr" target="#b17">[20]</ref>. We also report FID score of VAEGAN on these datasets. Our method is better than VAEGAN. Note that we have also tried MDGAN, but it has serious mode collapsed for both these datasets. Therefore, we do not report its result in our paper. Lastly, we compare our model with recent SN-GAN [23] on CIFAR-10 and STL-10 datasets with standard CNN architecture. Experimental setup is the same as <ref type="bibr" target="#b20">[23]</ref>, and FID is the score for the comparison. Results are presented in the third to fifth rows of <ref type="table">Table 2</ref>. In addition to settings reported using synthetic dataset, we have additional settings and ablation study for image datasets, which are reported in Section 5 of Supplementary Material. The results confirm the stability of our model, and our method outperforms SN-GAN on the CIFAR-10 dataset. Interestingly, when we replace "log" by "hinge loss" functions in the discriminator as in <ref type="bibr" target="#b20">[23]</ref>, our "hinge loss" version performs even better with FID = 22.95, compared to FID = 25.5 of SN-GAN. It is worth noting that our model is trained with the default parameters λ p = 1.0 and λ r = 1.0. Our generator <ref type="table">Table 2</ref>. Comparing FID score to other methods. First two rows (CelebA, CIFAR-10) follow the experimental setup of <ref type="bibr" target="#b17">[20]</ref>, and the second row follow the experimental setup of <ref type="bibr" target="#b20">[23]</ref>  requires about 200K iterations with the mini-batch size of 64. When we apply our "hinge loss" version on STL-10 dataset similar to <ref type="bibr" target="#b20">[23]</ref>, our model can achieve the FID score 36.19 for this dataset, which is also better than SN-GAN (FID = 43.2). We also compare our model to SN-GAN on CIFAR-10 with similar ResNet architecture as in <ref type="bibr" target="#b20">[23]</ref>. Our FID = 17.61 ± .30 significantly outperforms that of SN-GAN with FID = 21.70 ± .21. It again confirms the robustness and quality of our proposed GAN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a robust AE-based GAN model with novel distance constraints, called Dist-GAN, that can address the mode collapse and gradient vanishing effectively. Our model is different from previous work: (i) We propose a new generator objective using "discriminator-score distance". (ii) We propose to couple the convergence of the discriminator with that of the AE by considering reconstructed samples as "real" samples. (iii) We propose to regularize AE by "latent-data distance constraint" in order to prevent the generator from falling into mode collapse settings. Extensive experiments demonstrate that our method can approximate multi-modal distributions. Our method reduces drastically the mode collapse for MNIST-1K. Our model is stable and does not suffer from mode collapse for MNIST, CelebA, CIFAR-10 and STL-10 datasets. Furthermore, we achieve better FID scores than previous works. These demonstrate the effectiveness of the proposed Dist-GAN. Future work applies our proposed Dist-GAN to different computer vision tasks <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b11">14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">2D synthetic data</head><p>This section describes our model architecture <ref type="table" target="#tab_3">(Table 3)</ref>, additional quantitative results <ref type="table">(Table 4</ref>) and additional visualization of generated samples <ref type="figure">(Fig. 9</ref>) from different methods. <ref type="table">Table 4</ref>. Synthetic data results. Columns indicate the number of covered modes, and the number of registered samples among 2000 generated samples, and two types of Total Variation (TV). We evaluate three versions of Dist-GAN1 regarding reconstructed samples as "fake", "none" or "real" samples. 3. CelebA dataset <ref type="figure" target="#fig_0">Fig. 10</ref>. Samples generated by our method from interpolated latents.</p><p>Here we demonstrate that our model does not have over-fitting. Also, our model allows control using latent space samples. We interpolate the latent variables to generate faces as shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. The transitions in the generated faces are smooth, and interpolated faces are realistic, suggesting that our model can be used to generate smooth faces from latents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CIFAR-10 and STL-10 datasets</head><p>We visualize generated samples on CIFAR-10 and STL-10 datasets, <ref type="figure" target="#fig_0">Fig. 11</ref>. The output images are shape, and have sufficient variation. This is also shown with FID score in the main paper. <ref type="figure" target="#fig_0">Fig. 11</ref>. Generated samples using our method trained on CIFAR-10 (first row) and STL-10 (second row). Generated samples are on the left and real samples on the right.</p><p>We found that, in each iteration, training the generator twice can improve convergence and does not cause instability to our model. In particular, first, we train the generator as part of AE for realistic samples reconstruction. Then, we train the generator to confuse the discriminator in order to generate better images. The AE may converge to some local minimum, but training the generator again could direct the AE to converge to some saddle point, where the generator can generate quality images. The convergence of FID scores is shown in <ref type="figure" target="#fig_0">Fig. 12</ref> when training our model on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation study on CIFAR-10</head><p>This ablation study complements the synthetic data experiments. We remove each of the proposed components L G , L C , L W , L P (denote the generator ob- <ref type="figure" target="#fig_0">Fig. 12</ref>. The convergence of FID scores on CIFAR-10 dataset. <ref type="table">Table 5</ref>. Different configurations in our ablation study. LG, LC , LW , LP indicates the use of our proposed generator objective (discriminator score distance), reconstruction constraints, regularization of AE (latent-data distance) and gradient penalty respectively. : use, ×: no use. '*': collapsed.</p><formula xml:id="formula_8">Use Dist-GAN1 Dist-GAN2 Dist-GAN3 Dist-GAN4 Dist-GAN5 Dist-GAN6 Dist-GAN L G × L C × × L W × × L P × × ×</formula><p>jective, the reconstruction constraints of discriminator, the AE regularization and the gradient penalty, resp.) from our model and then evaluate on CIFAR-10 dataset. Ablation results ( <ref type="table" target="#tab_5">Table 6</ref>) suggests that the generator objective (discriminator score distance) can significantly reduce mode collapse (see Dist-GAN 3 ). Also, regularization of Autoencoder (latent-data distance) can reduce mode collapse (see Dist-GAN 2 ). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Mode collapse observed by data samples of the MNIST dataset, and (b) their corresponding latent samples of an uniform distribution. Mode collapse occurs frequently when the capacity of networks is small or the design of generator/discriminator networks is unbalance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Latent space visualization: The labels of 55K 2D latent variables obtained by (a) DCGAN, (b) WGANGP, (c) our Dist-GAN2 (without latent-data distance) and (d) our Dist-GAN3 (with our proposed latent-data distance). The Dist-GAN settings are defined in the section of Experimental Results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1b is</head><label></label><figDesc>the latent sample visualization using this technique, and the latent samples are uniformly distributed in a 2D latent space of [−1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of Dist-GAN includes Encoder (E), Generator (G) and Discriminator (D). Reconstructed samples are considered as "real". The input, reconstructed, and generated samples as well as the input noise and encoded latent are all used to form the latent-data distance constraint for AE (regularized AE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(a) The 1D frequency density of outputs using M d from uniform distribution of different dimensionality. (b) One example of the density when mode collapse occurs. (c) The 1D density of real data and generated data obtained by different methods: DCGAN (kldiv: 0.00979), WGANGP (kldiv: 0.00412), Dist-GAN2 (without data-latent distance constraint of AE, kldiv: 0.01027), and Dist-GAN (kldiv: 0.00073).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>From left to right figures: (a), (b), (c), (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Training the generator on x m , z m according to Eqn. 4.</figDesc><table><row><cell></cell><cell>, G θ</cell></row><row><cell cols="2">2: repeat</cell></row><row><cell>3:</cell><cell>x 3</cell></row><row><cell>6:</cell><cell>ω, θ ← min ω,θ LR(ω, θ) + λrLW (ω, θ)</cell></row><row><cell>7:</cell><cell>// Training discriminators according to Eqn. 5 on x m , z m</cell></row><row><cell>8:</cell><cell>γ ← minγ LD(ω, θ, γ)</cell></row><row><cell cols="2">9: // 10: θ ← min θ LG(θ)</cell></row><row><cell cols="2">11: until</cell></row><row><cell cols="2">12: return Eω, G θ , Dγ</cell></row></table><note>1: Initialize discriminators, encoder and generator Dγ, Eωm ← Random minibatch of m data points from dataset.4: zm ← Random m samples from noise distribution Pz 5: // Training encoder and generator using x m and z m by Eqn.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The comparison on MNIST-1K of methods. We follow the setup and network architectures from Unrolled GAN.</figDesc><table><row><cell>Architecture</cell><cell>GAN</cell><cell cols="2">Unrolled GAN WGAN-GP Dist-GAN</cell></row><row><cell>K/4, #</cell><cell>30.6 ± 20.7</cell><cell cols="2">372.2 ± 20.7 640.1 ± 136.3 859.5 ± 68.7</cell></row><row><cell>K/4, KL</cell><cell>5.99 ± 0.04</cell><cell>4.66 ± 0.46</cell><cell>1.97 ± 0.70 1.04 ± 0.29</cell></row><row><cell>K/2, #</cell><cell cols="3">628.0 ± 140.9 817.4 ± 39.9 772.4 ± 146.5 917.9 ± 69.6</cell></row><row><cell>K/2, KL</cell><cell>2.58 ± 0.75</cell><cell>1.43 ± 0.12</cell><cell>1.35 ± 0.55 1.06 ± 0.23</cell></row><row><cell cols="4">Fig. 8. Generated samples of DCGAN (50 epochs, results from [1]), WGAN-GP (50</cell></row><row><cell cols="4">epochs, results from [1]) and our Dist-GAN (50 epochs).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>with standard CNN architectures, and the last row is with ResNet architecture.</figDesc><table><row><cell></cell><cell cols="7">NS GAN LSGAN WGANGP BEGAN VAEGAN SN-GAN Dist-GAN</cell></row><row><cell>CelebA</cell><cell cols="5">58.0 ± 2.7 53.6 ± 4.2 26.8 ± 1.2 38.1 ± 1.1 27.5 ± 1.9</cell><cell>-</cell><cell>23.7 ± 0.3</cell></row><row><cell>CIFAR-10</cell><cell cols="5">58.6 ± 2.1 67.1 ± 2.9 52.9 ± 1.3 71.4 ± 1.1 58.1 ± 3.2</cell><cell>-</cell><cell>45.6 ± 1.2</cell></row><row><cell>CIFAR-10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.3</cell><cell>28.23</cell></row><row><cell>CIFAR-10 (hinge)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.5</cell><cell>22.95</cell></row><row><cell>STL-10 (hinge)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.2</cell><cell>36.19</cell></row><row><cell>CIFAR-10 (ResNet)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">21.70 ± .21 17.61 ± .30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Network structures for synthetic data experiment.Fig. 9. Visualizing samples generated by different methods on the synthetic dataset.</figDesc><table><row><cell></cell><cell>din dout N h d h</cell></row><row><cell>Encoder (E)</cell><cell>2 2 3 128</cell></row><row><cell>Generator (G)</cell><cell>2 2 3 128</cell></row><row><cell cols="2">Discriminator (D) 2 1 3 128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>± 52.81 1.25 ± 0.32 1.31 ± 0.44 Dist-GAN1 (none) 21.96 ± 1.96 1710.62 ± 51.65 1.04 ± 0.26 0.99 ± 0.35 Dist-GAN1 (real) 23.62 ± 1.05 1756.12 ± 31.69 0.87 ± 0.17 0.80 ± 0.22 Dist-GAN2 25.00 ± 0.00 1719.25 ± 49.77 0.31 ± 0.09 0.18 ± 0.06 Dist-GAN 25.00 ± 0.00 1827.38 ± 25.60 0.20 ± 0.03 0.12 ± 0.02</figDesc><table><row><cell>Method</cell><cell cols="3">#registered modes #registered points TV (True) TV (Differential)</cell></row><row><cell>GAN [12]</cell><cell>21.50 ± 2.00</cell><cell>1875.63 ± 42.53 1.00 ± 0.00</cell><cell>0.94 ± 0.02</cell></row><row><cell>DAN-2S [19]</cell><cell>14.07 ± 2.85</cell><cell>986.33 ± 160.80 0.99 ± 0.02</cell><cell>0.70 ± 0.09</cell></row><row><cell>VAE-GAN [18]</cell><cell>21.13 ± 2.33</cell><cell>1816.19 ± 61.23 0.98 ± 0.15</cell><cell>0.93 ± 0.21</cell></row><row><cell>ALI [10]</cell><cell>22.00 ± 2.63</cell><cell>1490.25 ± 194.38 0.93 ± 0.11</cell><cell>0.68 ± 0.15</cell></row><row><cell>WGAN-GP [13]</cell><cell>24.04 ± 1.16</cell><cell>1766.25 ± 79.48 0.54 ± 0.35</cell><cell>0.43 ± 0.31</cell></row><row><cell>GAN1</cell><cell>23.75 ± 0.71</cell><cell>1899.62 ± 26.48 0.90 ± 0.29</cell><cell>0.86 ± 0.27</cell></row><row><cell>GAN2</cell><cell>24.50 ± 1.07</cell><cell>1809.62 ± 113.54 0.42 ± 0.32</cell><cell>0.33 ± 0.27</cell></row><row><cell>Dist-GAN1 (fake)</cell><cell>17.67 ± 1.89</cell><cell>1678.50</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on CIFAR dataset with CNN architecture.</figDesc><table><row><cell></cell><cell cols="6">Dist-GAN Dist-GAN2 Dist-GAN3 Dist-GAN4 Dist-GAN5 Dist-GAN6</cell></row><row><cell>CIFAR</cell><cell>28.23</cell><cell>36.09</cell><cell>&gt;200*</cell><cell>29.80</cell><cell>28.50</cell><cell>30.37</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by both ST Electronics and the National Research Foundation(NRF), Prime Minister's Office, Singapore under Corporate Laboratory @ University Scheme (Programme Title: STEE Infosec -SUTD Corporate Laboratory).</p><p>We implement 1D demos of GAN, WGAN-GP, MDGAN, VAEGAN, and our Dist-GAN to illustrate the behavior of each method (See links of demo videos). In the implementation, we keep the same all parameters and network size (one input layer, one hidden layer with size = 4, and one output layer) for all methods. We observe that baseline GAN, MDGAN cannot approximate the data distribution and suffer from the gradient vanishing. WGAN-GP can approximate data distribution at some points, but diverges later. Moreover, WGAN-GP is sensitive to the penalty values as we show its results with different penalty values (0.1, 0.5, and 1.0). Using a smaller value may not sufficient to provide strong gradient so that the method can approximate real distribution. On the other hand, using large value may cause divergence. VAEGAN diverges from real data distribution and is slightly collapsed at the end. Our method converges to the data distribution and does not suffer from other issues. Note that in the demo videos, the discriminator's learning curve is blue, and the generator's one is green. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>1. 1D demo</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<title level="m">Towards principled methods for training generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">Began: Boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient and deep person re-identification using multi-level similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selective deep convolutional features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Le Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1600" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09549</idno>
		<title level="m">Distributional adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<title level="m">Spectral normalization for generative adversarial networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the quantitative analysis of decoder-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yazıcı</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04498</idno>
		<title level="m">The unusual effectiveness of averaging in gan training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
