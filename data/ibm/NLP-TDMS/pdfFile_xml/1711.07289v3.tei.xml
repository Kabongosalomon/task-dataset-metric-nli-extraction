<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Steerable Filters for Rotation Equivariant CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
							<email>m.weiler@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AMLab / QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">HCI/IWR</orgName>
								<orgName type="institution" key="instit2">University of Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
							<email>fred.hamprecht@iwr.uni-heidelberg.de</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">HCI/IWR</orgName>
								<orgName type="institution" key="instit2">University of Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Storath</surname></persName>
							<email>martin.storath@iwr.uni-heidelberg.de</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">HCI/IWR</orgName>
								<orgName type="institution" key="instit2">University of Heidelberg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Steerable Filters for Rotation Equivariant CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many machine learning tasks it is desirable that a model's prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He's weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks are extremely successful predictive models when the input data has spatial structure. One principal reason is that the convolution operation exhibits translational equivariance so that feature extraction is independent of the spatial position. For many types of images it is desirable to make feature extraction orientation independent as well. Typical examples are biomedical microscopy images or astronomical data which do not show a prevailing global orientation. Consequently, the output of a network processing such data should be equivariant w.r.t. the orientation of its input -if the input is rotated, the output should transform accordingly. Even when there is a predominant direction in an image as a whole, the low level features in the first layers such as edges usually appear in all orientations; see e.g. the filterbanks visualized in <ref type="bibr" target="#b0">[1]</ref>.</p><p>In both cases, conventional CNNs are compelled to learn rotated versions of the same filter, introducing redundant degrees of freedom and increasing the risk of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contribution</head><p>We propose a rotation-equivariant CNN architecture which shares weights over filter orientations to improve generalization and to reduce sample complexity. A key property of our network is that its filters are learned such that they are steerable. This approach avoids interpolation artifacts which can be severe at the small length scale of typical filter kernels. We accomplish the steerability of the learned filters by representing them as linear combinations of a fixed system of atomic steerable filters.</p><p>In all intermediate layers of the network, we utilize group convolutions to ensure an equivariant mapping of feature maps. Group-convolutional networks were proposed by Cohen and Welling <ref type="bibr" target="#b1">[2]</ref> who considered four filter orientations. An advantage of our construction is that we can achieve an arbitrary angular resolution w.r.t. the sampled filter orientations. Indeed, our experiments show that results improve significantly when using more than four orientations.</p><p>An important practical aspect of CNNs is a proper weight initialization. Since the weights to be learned serve as expansion coefficients for the steerable function space, common weight initialization schemes need to be adapted. Here, we generalize the results found by Glorot and Bengio <ref type="bibr" target="#b2">[3]</ref> and He et al. <ref type="bibr" target="#b3">[4]</ref> to networks which learn filters as a composition of (not necessarily steerable) atomic filters.</p><p>Our network achieves state-of-the-art results on two important rotation-equivariant/invariant recognition tasks: (i) The proposed approach is the first to obtain an accuracy higher than 99% on the rotated MNIST dataset, which is the standard benchmark for rotation-invariant classification. (ii) A processing pipeline based on the proposed SFCNN layers ranks among the top three entries in the ISBI 2012 electron microscopy segmentation challenge <ref type="bibr" target="#b4">[5]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steerable Filter CNN (SFCNN)</head><p>orientation-dependent feature maps learned complex filter coefficients w q <ref type="figure" target="#fig_0">Figure 1</ref>: Key concepts of the proposed Steerable Filter CNN: The filters are parameterized in a steerable function space with shared weights over filter orientations. Exact filter rotations are achieved by a phase manipulation of the expansion coefficients wq. All layers are designed to be jointly translation and rotation equivariant. The weights wq serve as expansion coefficients of a fixed filter basis {ψq}q rather than pixel values. Therefore, we adapt He's weight initialization scheme to this more general case which implies to normalize the basis filter energies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Equivariance properties of CNNs</head><p>Equivariance is the property of a function to commute with the actions of a symmetry group acting on its domain and codomain. Formally, given a transformation group G, a function f : X → Y is said to be equivariant if</p><formula xml:id="formula_0">f ϕ X g (x) = ϕ Y g (f (x)) ∀g ∈ G, x ∈ X,</formula><p>where ϕ (·) g denotes a group action in the corresponding space. A special case of equivariance is invariance for which ϕ Y g = id. In many machine learning tasks a set of transformations is known a-priori under which the prediction should transform in an equivariant way. Including such knowledge directly into the model can greatly facilitate learning by freeing up model capacity for other factors of variation. As an example consider a segmentation problem where the goal is to learn a mapping from an image space I to label images in L, which we formalize by a ground truth segmentation map S : I → L. The learning process involves fitting a model M : I → L to approximate the ground truth. For segmentation tasks, however, translations of the input image I ∈ I should typically lead to a translated segmentation map. Specifically, one has</p><formula xml:id="formula_1">S (T d I) = T d S(I) ∀d ∈ R 2 , I ∈ I,<label>(1)</label></formula><p>where T d is an action of the translation group T = (R 2 , +) which shifts the image or segmentation by d ∈ R 2 . The group action partitions the image space in equivalence classes T.I = {T d I | d ∈ R 2 } which are known as group orbits and comprise all images that are related by the action. Note that the translation equivariance (1) of the ground truth segmentation function implies a mapping of whole orbits in I to orbits in L. It is therefore possible to reformulate the ground truth as S : I/T → L/T , where (·)/T denotes the quotient space resulting from collapsing equivalent images in an orbit to a single element. Instead of fitting an unrestricted model M to S it is advantageous to incorporate the transformation behavior into the model by construction. The crucial consequence is that this reduces the hypothesis space to models M : I/T → L/T. CNN layers, which transform feature maps ζ by convolving them with filters Ψ, are by construction equivariant under translations, that is, (T d ζ) * Ψ = T d (ζ * Ψ) . Therefore, their hypothesis space is restricted to M . 1 As consequence, patterns learned at one specific location evoke the same response at each other location which leads to reduced sample complexity and enhanced generalization.</p><p>Besides translations, there are often further transformations like rotations, mirroring or dilations under which the model should be equivariant. Enforcing equivariance under an extended transformation group G leads to an enhanced generalization over larger orbits G.I and reduces the hypothesis space further to M : I/G → L/G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Steerable Filter CNNs</head><p>Here, we develop Steerable Filter CNNs (SFCNNs) which achieve equivariance under joint translations and discrete rotations. The key concept leading to translation equivariance of CNNs is translational weight sharing. We extend the transformation group under which our networks' layers are equivariant by additionally sharing weights over filter orientations. This implies to perform convolutions with several rotated versions of each filter. The rotational weight sharing leads to an improved sample complexity and to an enhanced generalization over orbits consisting of images connected by translations and discrete rotations.</p><p>In the following sections we introduce our parametrization of steerable filters, propose the network design in terms of these filters and derive a weight initialization scheme adapted to the filter parametrization. For the formal derivations we assume the images, feature maps and filters to be defined on the continuous domain R 2 . The effects resulting from a discretized implementation are investigated in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parametrization of the steerable filters</head><p>At the heart of convolutional neural networks lies the concept of learning filter kernels. Our construction demands for filters whose responses can be computed accurately and economically for several filter orientations. Simultaneously the filters should not be restricted in their expressive power, i.e. in the patterns to be learned. All of these requirements are met by learning linear combinations of a system of steerable filters. Here we describe a suitable construction of steerable filters for learning in CNNs.</p><p>A filter Ψ is rotationally steerable in the sense of Hel-Or and Teo <ref type="bibr" target="#b5">[6]</ref>, when its rotation by an arbitrary angle θ can be expressed in a function space spanned by a fixed set of atomic basis functions {ψ q } Q q=1 . This definition includes the classical formulation of steerability by Freeman and Adelson <ref type="bibr" target="#b6">[7]</ref> as a specific choice of basis. Formally, a steerable filter Ψ :</p><formula xml:id="formula_2">R 2 → R satisfies ρ θ Ψ(x) = Q q=1 κ q (θ)ψ q (x),<label>(2)</label></formula><p>for all angles θ ∈ (−π, π] and for angular expansion coefficient functions κ q . Here ρ θ denotes both the rotation operator defined by ρ θ Ψ(x) = Ψ(ρ −θ x) when acting on a function as well as a counterclockwise rotation by the angle θ when acting on a coordinate vector. As pointed out by Freeman and Adelson <ref type="bibr" target="#b6">[7]</ref>, the rotation by steerability is analytic and exact even for signals sampled on a grid. In contrast to rotations by interpolation the approach does not suffer from interpolation artifacts. An important practical consequence of steerability is that the response of each orientation can be synthesized from the atomic responses</p><formula xml:id="formula_3">f * ψ q ; that is, (f * ρ θ Ψ) (x) = Q q=1 κ q (θ) (f * ψ q ) (x)</formula><p>. A basis of a steerable function space which is particularly easy to handle is given by circular harmonics; see e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. They are defined by a sinusoidal angular part multiplied with a radial function τ : where (r, φ) denote polar coordinates of x = (x 1 , x 2 ) and k ∈ Z is the angular frequency. By construction, ψ k can be rotated by multiplication with a complex exponential,</p><formula xml:id="formula_4">R + → R, i.e. ψ k (r, φ) = τ (r) e ikφ ,<label>(3)</label></formula><formula xml:id="formula_5">ρ θ ψ k (x) = e −ikθ ψ k (x).<label>(4)</label></formula><p>In our network, we utilize a system of circular harmonics ψ jk with j = 1, . . . , J, and k = 0, . . . , K j where the additional index j controls the radial part of ψ jk = τ j (r) e ikφ . <ref type="figure" target="#fig_2">Figure 2</ref> shows the real and imaginary parts of the atoms used in the experiments where we chose Gaussian radial parts τ j (r) = exp(−(r − µ j ) 2 /2σ 2 ) with µ j = j. The maximum angular frequencies K j are limited to the point where aliasing effects occur. We found this system to be convenient for learning as the filters are approximately orthogonal and radially localized. The learned filters are then defined as linear combinations of the elementary filters, that is,</p><formula xml:id="formula_6">Ψ(x) = J j=1 Kj k=0 w jk ψ jk (x),<label>(5)</label></formula><p>with weights w jk ∈ C. The complex phase of the weights allows rotating the atomic filters with respect to each other. Such a composed filter can subsequently be steered as a whole by phase manipulation of the atoms via</p><formula xml:id="formula_7">ρ θΨ (x) = J j=1 Kj k=0 w jk e −ikθ ψ jk (x).<label>(6)</label></formula><p>We select a single orientation by taking their real part</p><formula xml:id="formula_8">Ψ(x) = ReΨ(x)<label>(7)</label></formula><p>and let ρ θ Ψ = Re ρ θΨ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Equivariant network architecture</head><p>The basic building blocks of the proposed SFCNN are three equivariant layer types which we introduce in this section.</p><p>Input layer: The first layer l = 1 of our network ingests an image with C channels I c : R 2 → R, c = 1, . . . C and convolves these withĈ rotated filters ρ θ Ψ</p><p>cc , where Ψ <ref type="bibr" target="#b0">(1)</ref> cc : R 2 → R,ĉ = 1, . . . ,Ĉ, are filter channels of the form <ref type="bibr" target="#b6">(7)</ref>. This results in pre-nonlinearity features</p><formula xml:id="formula_10">y (1) c (x, θ) = C c=1 I c * ρ θ Ψ (1) cc (x) (8) = C c=1 I c * Re J j=1 Kj k=0 wĉ cjk e −ikθ ψ jk (x) = Re C c=1 J j=1 Kj k=0 wĉ cjk e −ikθ (I c * ψ jk ) (x),</formula><p>where the filters are rotated by in total Λ equidistant orientations θ ∈ Θ = {0, . . . , 2π Λ−1 Λ }. In this setting the rotational weight sharing is reflected by the phase manipulation of the weights wĉ cjk which themselves are independent of the angle θ. A higher resolution in orientations can be achieved by simply expanding the tensor containing the phase-factors.</p><p>As usual, after the convolution step a bias β (1) c is added and a nonlinearity σ is applied, so that we end up with the first layer's feature map given by</p><formula xml:id="formula_11">ζ (1) c (x, θ) = σ y (1) c (x, θ) + β (1) c .</formula><p>Note that the resulting representation ζ <ref type="bibr" target="#b0">(1)</ref> c depends on a spatial location x and an orientation angle θ, i.e. on the transformation group applied to the filters.</p><p>Group-convolutional layers: To process the resulting feature maps further we utilize group convolutions which naturally generalize spatial convolutions from translations to more general transformation groups. Given a feature map ζ : G → R and a filter Ψ : G → R living on a group G, their group convolution is defined by (ζ Ψ)(g) = G ζ(h)Ψ(h −1 g) dλ(h), where we use the symbol to distinguish group convolutions from the spatial convolution operator * , and λ denotes a Haar measure. The resulting feature map is again a function on the group. In analogy to spatial convolutions, group convolutions are equivariant under the group operation, i.e.</p><formula xml:id="formula_12">(ϕ h (ζ) Ψ) (g) = ϕ h (ζ Ψ) (g), ∀h, g ∈ G, where ϕ h is given by ϕ h ζ(g) = ζ(h −1 g).</formula><p>For a deeper discussion of group convolutions in neural networks we refer to <ref type="bibr" target="#b1">[2]</ref>.</p><p>The feature maps calculated by the input layer are functions on the semidirect product group R 2 Θ ≤ SE(2). Keeping the parameterization by (x, θ), the group convolutions with summation over input channels can be explicitly instantiated as</p><formula xml:id="formula_13">y (l) c (x, θ) = C c=1 ζ (l−1) c Ψ (l) cc (x, θ) (9) = C c=1 φ∈Θ R 2 ζ (l−1) c (u, φ)Ψ (l) cc (u, φ) −1 (x, θ) du = C c=1 φ∈Θ ζ (l−1) c (·, φ) * ρ φ Ψ (l) cc (·, θ − φ) (x) = C c=1 φ∈Θ ζ (l−1) c (·, φ) * R φ Ψ (l) cc (·, θ) (x).</formula><p>Here the multiplication with the inverse group element,</p><formula xml:id="formula_14">(u, φ) −1 (x, θ) = (ρ −φ (x − u), θ − φ)</formula><p>, was evaluated by switching to a representation of the group. We further introduced the action R φ defined by</p><formula xml:id="formula_15">R φ Ψ(x, θ) := ρ φ Ψ(x, θ − φ)</formula><p>which transforms functions on the group by rotating them spatially and shifting their orientation components cyclically. The above equation reveals that the group convolution can be decomposed into a spatial convolution, rotation and linear combination. In analogy to the first layer we make use of the steerable filters which on the group are defined by Ψ</p><formula xml:id="formula_16">(l) cc (x, θ) = Re J j=1</formula><p>Kj k=0 wĉ cjkθ ψ jk (x). Note that the additional orientation dimension is reflected by an additional index of the weight tensor. Inserting the steerable filters in <ref type="bibr" target="#b8">(9)</ref> we obtain the pre-nonlinearity feature maps of the group-convolutional layers</p><formula xml:id="formula_17">y (l) c (x, θ) (10) = C c=1 φ∈Θ   ζ (l−1) c (·, φ) * Re j,k wĉ cjk,θ−φ e −ikφ ψ jk   (x) = Re C c=1 φ∈Θ j,k wĉ cjk,θ−φ e −ikφ ζ (l−1) c (·, φ) * ψ jk (x).</formula><p>As before, a bias β (l) c is added and the activation function σ is applied, ζ</p><formula xml:id="formula_18">(l) c (x, θ) = σ(y (l) c (x, θ) + β (l) c ).</formula><p>By the linearity of the steerability and the convolution, one can implement the layers either by a direct convolution with linearly combined filters, or by linearly combining the responses of the atomic filters. We implemented both approaches and found that in typical operation regimes the first option is faster since the kernels to be linearly combined have a smaller spatial extent than the atomic responses of the second option.</p><p>Output layer: After the last group-convolutional layer we extract the information of interest for the specific task. For rotation-invariant classification we pool globally over both the orientation dimension and the remaining spatial resolution. A pooling over orientations is also done for rotationequivariant segmentation where spatial dimensions remain and the output rotates according to the rotation of the network's input. If the orientation itself is of interest it could be kept as extra feature.</p><p>Equivariance: Each individual layer L (·) of the network is equivariant under joint translations and rotations in the group R 2 Θ : Rotating the input image leads to a transformation L in (ρ φ I) = R φ L in (I) of the first layer's feature maps. The subsequent group-convolutional layers then transform like L gconv (R φ ζ) = R φ L gconv (ζ). When using orientation pooling in the output layer the resulting feature Orientation components are abbreviated as subscript, i.e. Ψ λ = Ψ(·, θ λ ). Filters in the same color share their weights as they are connected by rotations. The weight sharing of the filters on the group is prescribed by the group convolution <ref type="bibr" target="#b8">(9)</ref>. After the last group-convolutional layer we pool over orientations to obtain predictions which are invariant under rotations of local patches in the field of view. Bottom: Visualization of the layerwise rotation-equivariance. Applying a rotation ρ φ to the input image results in a joint spatial rotation operation and cyclic shift over orientation indices R φ of the feature maps ζ (l) . This transformation behavior can be understood intuitively when paying attention to the relative orientation of each layer's input and filters. maps are rotated:</p><formula xml:id="formula_19">L out (R φ ζ) = ρ φ L out (ζ).</formula><p>Overall, this implies the equivariance of a whole network,</p><formula xml:id="formula_20">L out • L d gconv • L in (ρ φ I) = ρ φ L out • L d gconv • L in (I) ,</formula><p>where d is the number of group-convolutional layers. The layers' equivariance is formally proven in appendix A. The top part of <ref type="figure" target="#fig_3">Figure 3</ref> visualizes the building blocks of a typical SFCNN for rotation-equivariant segmentation. An overview over the transformation behavior of the feature maps under rotation of the input is given in the bottom part. The spatial rotation and cyclic shift over orientation channels R φ of the feature maps on the group can be understood intuitively when paying attention to the relative orientation of each layer's input and filters.</p><p>Compared to a conventional CNN which independently learns filters in Λ orientations in a rotation-invariant recognition task, a corresponding SFCNN consumes Λ times less parameters to extract the same representation.</p><p>SFCNN incur a small computational overhead for building the filter kernels from the circular harmonics basis which we found to be negligible. The computational cost of SFCNNs is therefore equivalent to that of a conventional CNN when the effective number of channels coincide, i.e. when I CNN = ΛI SFCNN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generalizing He's weight initialization scheme</head><p>An important practical aspect of training deep networks is an appropriate initialization of their weights. When the weights' variances are chosen too high or low, the signals propagating through the network are amplified or suppressed exponentially with depth. Glorot and Bengio <ref type="bibr" target="#b2">[3]</ref> and He et al. <ref type="bibr" target="#b3">[4]</ref> investigated this issue and came up with initialization schemes which are accepted as a standard for random weight initialization. In contrast to <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref> our filters are not parameterized in a pixel basis but as a linear combination of a system of atomic filters with weights serving as expansion coefficients. To be specific, we consider filters Ψĉ cx = Q q=1 wĉ cq ψ qx which are built from Q, not necessarily steerable, real valued atomic filters which map C input channels toĈ output channels. This assumption is more general than that of the aforementioned works since they only consider the pixel basis ψ Dirac qx = δ q,x , i.e. atomic filters which are zero everywhere but at one pixel.</p><p>Most of the further assumptions are identical to those in <ref type="bibr" target="#b3">[4]</ref>: We assume the activations and gradients to be i.i.d. and to be independent from the weights. Further, the weights themselves are initialized to be mutually independent and have zero mean. An important difference is that we do not restrict the weights to be identically distributed because of the inherent asymmetry of the different atomic filters. All biases are initialized to be zero and the nonlinearities are chosen to be ReLUs. These assumptions lead to the initialization conditions</p><formula xml:id="formula_21">Var [w q ] = 2 CQ ψ q 2 2 or Var [w q ] = 2 CQ ψ q 2 2</formula><p>for the forward or backward pass, respectively. A detailed derivation is given in appendix B. As discussed in <ref type="bibr" target="#b3">[4]</ref>, the difference between both initializations cancels out for intermediate layers. Note that our results include those of He et al. <ref type="bibr" target="#b3">[4]</ref>, that is,</p><formula xml:id="formula_22">Var [w q ] = 2 nin or Var [w q ] = 2 nout , for ψ Dirac qx = δ q,x with ψ Dirac qx 2 2 = 1.</formula><p>We further want to point out that the learned filters are combined of products w q ψ q which implies that the factors ψ q 2 2 counterbalance different energies of the basis filters. A convenient way to initialize the network is hence to normalize all filters to unit norm and subsequently initialize the weights uniformly by Var [w q ] = 2 CQ or Var [w q ] = 2 CQ . In our group-convolutional layers the filters additionally comprise orientation channels. From the perspective of weight initialization these have the same effect as conventional channels, therefore we propose to normalize their weights variance with an additional factor of Λ. We emphasize that using normalization layers like batch normalization does not obviate the need for a proper weight initialization. This is because such layers scale activations as a whole while our initialization conditions indicates that the relative scale of the summands contributing to each activation needs to be adapted. Further details, in particular on initializing weights of complex-valued filters, are given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Prior and related work</head><p>A priori knowledge about transformation-invariance of images can be exploited in manifold ways. A commonly utilized technique is data augmentation, see e.g. <ref type="bibr" target="#b9">[10]</ref>. The basic idea is to enrich the training set by transformed samples. Augmenting datasets allows to train larger models and is easily applicable without modifying the network architectures. When the augmenting transformations form a group G the additional images I ⊆ I lie on the orbit G.I. In contrast to equivariant models the hypothesis space is not restricted to the quotient space I/G under the utilized symmetry group but the equivariance needs to be learned explicitly by the network. This demands for a high learning capacity which makes the network prone to overfitting.</p><p>Recent work focuses on incorporating equivariance to various transformations directly into the network's architecture. Invariance to specific transformations can be achieved by applying them to the input and subsequently pooling their responses <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In <ref type="bibr" target="#b13">[14]</ref> the regions in symmetry space to pool over are learned to become invariant only to nuisance deformations. Another approach is to resample the input and apply standard convolutions. Henriques and Vedaldi <ref type="bibr" target="#b14">[15]</ref> achieve equivariance w.r.t. Abelian symmetry groups by fixing a sampling grid according to the symmetry while in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> the network itself estimates the grid. In <ref type="bibr" target="#b17">[18]</ref> transformations are dealt with by convolving with filters which are steered by a subnetwork.</p><p>In particular, there has recently been a considerable interest in rotation-equivariant CNNs. The work <ref type="bibr" target="#b18">[19]</ref> introduces four operations which are easily included into existing networks and enrich both the batch-and feature dimension with transformed versions of their content. In <ref type="bibr" target="#b1">[2]</ref>, the feature maps resulting from transformed filters are treated as functions of the corresponding symmetry-group which allows to use group-convolutional layers. As their computational cost is coupled to the size of the group, Cohen and Welling <ref type="bibr" target="#b19">[20]</ref> propose to alternatively use steerable representations as composition of elementary feature types. Besides translations and rotations, the aforementioned works also incorporate reflections, i.e. they operate on the dihedral group. Their current limitation is the restriction to rotations by the angle π 2 , thus to four orientations. In <ref type="bibr" target="#b20">[21]</ref>, several rotated versions of the same image are sent through a conventional CNN. The resulting features are subsequently pooled over the orientation dimension. The approach can be easily extended to other transformations. On the downside, the equivariance is only w.r.t. global transformations. Marcos et al. <ref type="bibr" target="#b21">[22]</ref> perform convolutions with rotated versions of a each filter in a shallow network followed by a global pooling over orientations. These ideas were extended to networks which additionally propagate the orientation of the maximum response <ref type="bibr" target="#b22">[23]</ref>. In both approaches the filter rotation is based on bicubic interpolation, allowing for fine resolutions with respect to the orientation but causing interpolation artifacts. Worrall et al. <ref type="bibr" target="#b23">[24]</ref> achieve continuous resolution in orientations by working with complex valued steerable filters and feature maps. However, this requires the angular frequencies of the feature maps to be kept disentangled. Rotation-equivariant feature extraction can also be achieved by using group-convolutional scattering transforms <ref type="bibr" target="#b24">[25]</ref>. A fundamental difference to our work is that the filter banks are fixed rather than learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>We evaluate the proposed SFCNNs on two datasets exhibiting rotational symmetries. On the rotated MNIST dataset we first investigate specific network properties like the accuracy's dependence on the number of sampled orientations and the generalization of learned patterns over orientations. With the insights gained in these experiments we benchmark the model and the proposed initialization scheme. To evaluate the segmentation capabilities of SFCNNs on real world data we run a further experiment on the ISBI 2012 EM segmentation challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Rotated MNIST</head><p>In our first experiments we investigate the equivariance properties of the proposed network architecture on the rotated MNIST dataset (mnist-rot) which is the standard For our initial experiments we utilize the classification-SFCNN given in <ref type="table" target="#tab_3">Table 2</ref> in the appendix as baseline. It consists of one steerable input layer which maps the input images to the group, five following group-convolutional layers and three fully connected layers. After every two steerable filter layers we perform a spatial 2 × 2 max-pooling. The orientation dimension and the remaining spatial dimensions are pooled out globally after the last convolutional layer. Details on the further training setup are given in appendix C.</p><p>Sampled orientations: The number of sampled orientations Λ is a parameter specific to our network, so we first explore its influence on the test accuracy. We are further interested in the network's sample complexity, i.e. the dependence on the size of the training set. The accuracies resulting when varying these parameters are reported in <ref type="figure" target="#fig_4">Figure 4</ref> (left). As expected, the test error and its standard deviation decrease with the size of the training data set. We observe that the accuracy improves significantly when increasing the number of orientations until it saturates at around 12 to 16 angles. Up to this point, the gain of adding more sampled orientations is considerable. For example, in almost all cases, increasing the angular resolution from 2 to 4 sampled orientations provides a higher gain in accuracy than sticking with 2 orientations and doubling the number of training samples. We want to emphasize that the possibility of SFCNNs to go beyond the four sampled orientations of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref> leads to a significant gain in accuracy. Note that the case Λ = 1 corresponds, up to the different filter parameterization, to conventional CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotational generalization:</head><p>In order to test how well the networks generalize learned patterns over orientations we conduct an experiment where we train them on unrotated digits and record their accuracy over the orientation of rotated digits. Specifically, we take the the first 12000 samples of the conventional MNIST dataset to train a SFCNN with Λ = 16 as well as a conventional CNN of comparable size using either no augmentation, augmentation by rotations which are multiples of either π 4 or π 2 or augmentation by rotations which are densely sampled from [0, 2π). As test set we take the remaining 58000 samples and record the test errors' dependence on the orientation of this dataset. To obtain a fair comparison between the networks we experiment with conventional CNNs with the same number of parameters or the same number of channels like the SFCNN. Since both show the same behavior we only report the accuracies of the network with the same number of channels which performs slightly better. The results are plotted in <ref type="figure" target="#fig_4">Figure 4</ref> (right). One can see that, lacking rotational equivariance, the conventional CNN does not generalize well over orientations. When using rotational augmentation the error reduces considerably on average, it grows, however, for small angles in a neighborhood of zero. This is the case because the network needs to learn to detect the augmented samples additionally which demands an increased learning capacity. The SFCNN on the other hand generalizes quite well over orientations even without augmentation. In continuous space we would expect the test error curve to be 2π Λ -periodic because of the rotational equivariance. The deviations from this behavior can be attributed to the sampling effects of using digitized images. As to be expected for Λ = 16 orientations, the accuracy is not influenced by augmentation with π 2 -rotations since the additional samples lie on the group orbit on which the network is invariant. In contrast to conventional CNNs, SFCNNs do not show an increased error for small angles in a neighborhood of zero when using augmentation. This indicates that the cost of learning rotated versions of each digit is negligible thanks to the approximate rotation equivariance. An augmentation by rotations which are multiples of π 4 or by continuous rotations give  <ref type="bibr" target="#b29">[30]</ref> 10.4 ± 0.27 very similar results. Both seem to act as a regularization preventing the filters to overfit on the pixel grid. We conclude that SFCNNs outperform the rotational generalization of CNNs for all levels of augmentation.</p><p>Benchmarking: Based on the insights from the above experiments we fix the number of sampled orientations to Λ = 16 and tune the network further to the slightly larger architecture given in <ref type="table" target="#tab_5">Table 3</ref> in the appendix. The results are reported in <ref type="table" target="#tab_1">Table 1</ref>. Using the SFCNN with He's weight initialization and no data augmentation, we obtain a test error of 0.957% which already exceeds the previous state-ofthe-art. The proposed initialization scheme, adapted to filter coefficients, significantly improves the test error to 0.880%. When additionally augmenting the dataset with continuous rotations during training time the error decreases further to 0.714%. To summarize, our approach reduces the best previously published error by a factor of 29%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ISBI 2012 2D EM segmentation challenge</head><p>In a second experiment we evaluate the performance of our model on the ISBI 2012 electron microscopy segmentation challenge <ref type="bibr" target="#b4">[5]</ref>. The goal of the challenge is to predict the locations of the cell boundaries in the Drosophila ventral nerve cord from EM images which is a key step for investigating the connectome of the brain. The dataset consists of 30 train and test slices of size 512 × 512 px with a binary segmentation ground truth provided for the training set. <ref type="figure">Figure 5</ref> shows an exemplary raw EM image with the corresponding ground truth segmentation mask and our network's prediction. An important property of the dataset is that the images have no preferred orientation which makes it suitable for evaluating rotation-equivariant networks.</p><p>We build on an established pipeline introduced in <ref type="bibr" target="#b26">[27]</ref> where a crucial step is the boundary prediction via a conventional CNN. In the present experiment, we replaced their network by a SFCNN with a U-net design <ref type="bibr" target="#b30">[31]</ref>. The network architecture is visualized in <ref type="figure">Figure 6</ref> in the appendix. As loss function we chose a pixel wise binary cross entropy loss. The dataset was augmented by random elastic deformations, flips and rotations by multiples of π 2 during train time. In the experiment on rotational generalization we found that augmenting samples by transformations in a subgroup under which the network is equivariant does not have any effect. We therefore sampled Λ = 17 orientations which is mutually prime with the 4 augmented orientations. This way the augmented images do not fall into a subgroup w.r.t. which the network is invariant.</p><p>Segmentation predictions are evaluated by the challenge hosters and ranked w.r.t. the foreground-restricted Rand score V Rand and the information score V Info ; for an explanation of these metrics see <ref type="bibr" target="#b4">[5]</ref>. The current leaderboard in <ref type="figure">Figure 5</ref> (right) shows that our approach yields top-tier results. In particular, it improves upon the results of <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have developed a rotation-equivariant CNN whose filters are learned such that they are steerable. Layerwise equivariance is obtained by using group convolutions. He's weight initialization scheme is extended to general filter bases which empirically leads to an increased accuracy. Our network allows sampling an arbitrary number of filter orientations which improves the performance until a saturation is reached. We confirmed experimentally that SFCNNs generalize learned patterns over orientations and therefore achieve a lower sampling complexity than CNNs in rotation-equivariant recognition tasks. The proposed SFCNNs achieve state-of-the-art results on rotated MNIST and the ISBI 2012 2D EM segmentation challenge.</p><p>In this section we prove the equivariance of the individual layers of Steerable Filter CNNs under rotations by the sampled orientations in Θ, assuming signals on a continuous domain R 2 . Translational equivariance follows directly from either the utilization of spatial convolutions or from the independence of the operation on the spatial position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Input layer</head><p>The first layer maps an image I : R 2 → R to a feature map ζ (1) : R 2 Θ → R by first convolving it with multiple rotated versions ρ θ Ψ of a filter Ψ : R 2 → R and subsequently adding a bias β and applying a nonlinearity σ. Both steps are equivariant under rotations of the image by angles α ∈ Θ. This means that ρ α I(x) is mapped to</p><formula xml:id="formula_23">R α ζ (1) (x, θ) = ρ α ζ (1) (x, θ − α)</formula><p>where R α is the group action on functions on the group. To see that the first step performs an equivariant mapping, simply insert a rotated image,</p><formula xml:id="formula_24">(ρ α I * ρ θ Ψ) (x) = R 2 I(ρ −α u) Ψ(ρ −θ (x − u)) du ,</formula><p>and substituteũ := ρ −α u. Since the transformation is orthogonal we have det ∂ũ ∂u = 1 and hence:</p><formula xml:id="formula_25">(ρ α I * ρ θ Ψ) (x) = R 2 I(ũ) Ψ(ρ −(θ−α) (ρ −α x −ũ)) dũ = ρ α (I * ρ θ−α Ψ) (x) = ρ α y (1) (x, θ − α) = R α y (1) (x, θ) .</formula><p>The mutual transformation behavior is visualized in the following commutative diagram:</p><formula xml:id="formula_26">I(x) ρ α I(x) y (1) (x, θ) ρ α y (1) (x, θ − α) ρ α * ρ θ Ψ R α * ρ θ Ψ</formula><p>Adding a bias β to each feature map channel and applying a nonlinearity σ does not interfere with translational-or rotational equivariance since both operations do neither depend on the spatial position nor orientation channel:</p><formula xml:id="formula_27">y (l) (x, θ) ρ α y (l) (x, θ − α) ζ (l) (x, θ) ρ α ζ (l) (x, θ − α) R α σ( · + β) R α σ( · + β)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Group-convolutional layers</head><p>Given feature maps ζ (l) (x, θ), the group-convolutional layers perform an equivariant mapping of R α ζ (l) (x, θ) to R α ζ (l+1) (x, θ) under the group action R. The step of adding the bias and applying the activation function is equivariant by the same argument as in the first layer. What is left to show is the equivariance R α ζ (l) Ψ (x, θ) = R α ζ (l) Ψ (x, θ) = R α y (l) (x, θ) of the group convolution. Inserting a transformed feature map and writing the group convolution out explicitly yields:</p><formula xml:id="formula_28">R α ζ (l) Ψ (x, θ) = R 2 φ∈Θ ζ (l) (ρ −α u, φ − α) Ψ(ρ −φ (x − u), θ − φ) du .</formula><p>Again, we substituteũ := ρ −α u with det ∂ũ ∂u = 1. Furthermore, we letφ := φ − α under which the sum is invariant thanks to the cyclic structure of the subgroup Θ, and we obtain</p><formula xml:id="formula_29">R α ζ (l) Ψ (x, θ) = R 2 φ ∈Θ ζ (l) (ũ,φ) Ψ(ρ −φ (ρ −α x −ũ), (θ − α) −φ) dũ = ζ (l) Ψ (ρ −α x, θ − α) =ρ α y (l+1) (x, θ − α) =R α y (l+1) (x, θ) .</formula><p>This proves the equivariance of the intermediate layers.</p><p>Again, the relations are illustrated in a commutative diagram:</p><formula xml:id="formula_30">ζ (l) (x, θ) ρ α ζ (l) (x, θ − α) y (l+1) (x, θ) ρ α y (l+1) (x, θ − α) R α Ψ R α Ψ A.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Orientation max-pooling layer</head><p>For rotation-invariant segmentation or classification we max-pool over orientations after the last groupconvolutional layer. The pooling step is itself equivariant and results in a rotated version of its output:</p><formula xml:id="formula_31">max θ R α ζ (l) (x, θ) = max θ ρ α ζ (l) (x, θ − α) = ρ α max θ ζ (l) (x, θ − α) = ρ α max θ ζ (l) (x, θ) .</formula><p>The rotation operator commutes with the maximum over orientation channels because it acts on spatial coordinates only. We again visualize the transformation behavior by a commutative diagram:</p><formula xml:id="formula_32">ζ (l) (x, θ) ρ α ζ (l) (x, θ − α) max θ ζ (l) (x, θ) ρ α max θ ζ (l) (x, θ) R α max θ ρ α max θ</formula><p>In the case of classification the remaining spatial structure is pooled out such that the output is invariant under transformations of the input.</p><p>Instead of the maximum pooling which we applied in our experiments, one could also utilize average pooling layers. The equivariance of average pooling can be derived in analogy to the derivation for maximum pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation of the generalized He weight initialization scheme</head><p>In this section we give the derivation of the generalized weight initialization scheme whose results are stated in the main paper. For completeness we recall the assumptions going into the following calculations. We consider the activation of a single neuron in layer l,</p><formula xml:id="formula_33">ζ (l) cx = max(0, y (l) cx ),<label>(11)</label></formula><p>where rectified linear units were chosen as nonlinearities.</p><p>The pre-nonlinearity activations are given by the convolution with filters Ψ and summing over the input channels:</p><formula xml:id="formula_34">y (l) cx = c ζ (l−1) c * Ψ (l) cc x + β (l) c = c x ζ (l−1) c,x−x Ψ (l) ccx + β (l) c .<label>(12)</label></formula><p>For convenience we shifted the addition of the bias to the pre-nonlinearity activations. The filters are defined by</p><formula xml:id="formula_35">Ψ (l) ccx = Q q=1 w (l) ccq ψ qx ,</formula><p>that is, they are built from Q real valued atomic filters ψ q . We keep the discussion general by not restricting the atomic filters to be steerable. In analogy to Glorot and Bengio <ref type="bibr" target="#b2">[3]</ref> and He et al. <ref type="bibr" target="#b3">[4]</ref> we assume the activations and gradients to be i.i.d. and to be independent from the weights. We let the weights themselves be mutually independent and have zero mean but do not restrict them to be identically distributed because of the inherent asymmetry coming from the different atomic filters. Furthermore we initialize all biases to be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Backpropagation</head><p>In order to prevent vanishing or exploding gradients of the loss E due to inappropriate initialization we demand their variance Var ∂E ∂ζ (l) to be constant across all layers. It follows from <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_1">(12)</ref> that the gradient with respect to the activation ζ (l) c0x0 of a particular neuron in layer l is given by</p><formula xml:id="formula_36">∂E ∂ζ (l) c0x0 = ĉ,x ∂E ∂y (l+1) cx ∂y (l+1) cx ∂ζ (l) c0x0 (13) = ĉ,x ∂E ∂ζ (l+1) cx I y (l+1) cx &gt;0 q w (l+1) cc0q ψ q,x−x0 ,</formula><p>where the indicator function I stems from the derivative of the rectified linear unit. Like He et al. <ref type="bibr" target="#b3">[4]</ref> we assume the factors occurring in <ref type="bibr" target="#b12">(13)</ref> to be statistically independent. Observing that E w (l) and therefore also E ∂E ∂ζ (l) vanish, and without loss of generality setting x 0 = 0 this leads to</p><formula xml:id="formula_37">Var ∂E ∂ζ (l) c0x0 = E   ∂E ∂ζ (l) c0x0 2   = ĉ,ĉ x,x q,q E ∂E ∂ζ (l+1) cx ∂E ∂ζ (l+1) c x E I y (l+1) cx &gt;0 I y (l+1) c x &gt;0 · E w (l+1) cc0q w (l+1) c c0q ψ q,x ψ q ,x = ĉ x q E   ∂E ∂ζ (l+1) cx 2   E I y (l+1) cx &gt;0 · E w (l+1) cc0q 2 ψ 2 q,x = ĉ x q 1 2 Var ∂E ∂ζ (l+1) cx Var w (l+1) cc0q ψ 2 q,x .</formula><p>The factor 1 2 in the last line originates from the symmetric distribution of y (l) in conjunction with the indicator function. Using the fact that the weights' variances are initialized to only depend on q and the assumption of identically distributed gradients, both can be pulled out of the sums:</p><formula xml:id="formula_38">Var ∂E ∂ζ (l) = Var ∂E ∂ζ (l+1) Ĉ 2 q Var w (l+1) q ψ q 2 2 .</formula><p>It seems reasonable to assign the contribution to the overall variance equally to the Q summands. Demanding the gradients' variances to be constant over layers then leads to the initialization condition</p><formula xml:id="formula_39">Var [w q ] = 2 CQ ψ q 2 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Forward pass</head><p>The calculation for the forward pass is similar to the case of backpropagation but considers the variance Var y (l) of pre-nonlinearity activations instead of gradients. As an exact calculation depends on the expectation value E ζ (l−1) , which is not known, we approximate the result by exploiting the central limit theorem. To this end, we note that the pre-nonlinearity activations (12) are summed up from C q | supp ψ q | independent terms of finite variance which is a relatively large number in typical networks. This allows to approximate the variance by the asymptotic result implied by the central limit theorem:</p><formula xml:id="formula_40">Var y (l) cx = Var c x q ζ (l−1) c,x−x w (l) ccq ψ qx (CLT) ≈ c x q Var ζ (l−1) c,x−x w (l) ccq ψ qx = c x q E ζ (l−1) c,x−x 2 E w (l) ccq 2 ψ 2 qx .</formula><p>In the last step we made use of the independence of the weights from the previous layer's feature maps and where δ denotes the delta distribution and Θ is the Heaviside step function. As before, we drop all indices which the random variables are independent from to compute the sums. This leads to</p><formula xml:id="formula_41">Var y (l) ≈ Var y (l−1) C 2 q Var w (l) q ψ q 2 2 ,</formula><p>which in turn suggests a weight initialization according to</p><formula xml:id="formula_42">Var [w q ] = 2 CQ ψ q 2 2</formula><p>to ensure that the activations' variances are not amplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Normalization of complex atomic filters</head><p>The results derived above suggest to initialize the weights of each layer uniformly by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details on the experimental setup</head><p>Here we give further details on the network architectures and the training setup of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Rotated MNIST</head><p>For our initial experiments on the dependence on sampled orientations and the networks' rotational generalization capabilities we utilize the architecture given in <ref type="table" target="#tab_3">Table 2</ref> as baseline. Based on the results of these experiment we fix the number of sampled orientations to Λ = 16 and tune the network architecture further. We achieve the best benchmark results using the slightly larger network given in <ref type="table" target="#tab_5">Table 3</ref>. In particular, we found that increasing the size of the filter masks improved the results. Both architectures consist of one steerable input layer which maps the input images to the group, five following group convolutional layers and three fully connected layers. After every two steerable filter layers we perform a spatial 2 × 2 max-pooling. The orientation dimension and the remaining spatial dimensions are pooled out globally after the last convolutional layer. We normalize the activations by adding batch normalization layers <ref type="bibr" target="#b31">[32]</ref> after each convolutional and fully connected layer. The batch normalization on the group does not interfere with the equivariance when the responses are normalized by averaging over both spatial and orientation dimensions.</p><p>The number of feature channels stated in the tables refers to the number of learned filtersĈ of the corresponding layer. As these filters are themselves applied with respect to Λ orientations we end up withĈΛ responses; e.g. 24 · 16 = 384 effective responses in the first layer of the smaller network. Note that the extraction of this comparatively large number of responses without overfitting is possible because the rotational weight sharing leads to an increased parame-  ter utilization (in the sense of Cohen and Welling <ref type="bibr" target="#b19">[20]</ref>) by a factor Λ. All networks are trained for 40 epochs using the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with standard parameters. The initial learning rate is set to 0.015 and is decayed exponentially with a rate of 0.8 per epoch starting from epoch 15. We regularize the weights with an elastic net penalty with hyperparameters λ L1 = λ L2 which are set to 10 −7 and 10 −8 for the convolutional and fully connected layers respectively. Dropout <ref type="bibr" target="#b33">[34]</ref> is used only in the fully connected layers with a dropping probability of p = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. ISBI 2012 EM segmentation challenge</head><p>The network architecture used to segment the membranes from raw EM images of neural tissue for the ISBI EM segmentation challenge is visualized in <ref type="figure">Figure 6</ref>. Inspired by the U-Net <ref type="bibr" target="#b30">[31]</ref> it is build as a symmetric encoderdecoder network with additional skip-connections between stages of the same resolution. This allows to extract semantic information from a large field of view while at the same time preserving precise spatial localization. Further, we adopt two modifications from <ref type="bibr" target="#b25">[26]</ref>: we do not concatenate the skipped feature maps but add it to the decoder features upsampled from the previous stage, and we use intermediate residual blocks (here of depth 1). On the highest resolution level we learnĈ = 12 filters, applied in Λ = 17 orientations which corresponds toĈΛ = 204 effective channels. The number of filters is doubled when going to the second and third level and is afterwards kept constant since we did not observe further gains in performance when adding more channels. All group-convolutional layers utilize kernels of size 7 × 7 pixels while the input layer applies 11 × 11 pixel kernels.</p><p>As input, we feed the network cropped regions of 256 × <ref type="figure">Figure 6</ref>: Network architecture used to predict the membrane probability map for the ISBI 2012 EM segmentation challenge. The topology is inspired by the U-Net <ref type="bibr" target="#b30">[31]</ref> and FusionNet <ref type="bibr" target="#b25">[26]</ref> but uses the proposed steerable group-convolution layers with Λ = 17 orientations. To mitigate boundary artifacts we feed reflect-padded images into the network.</p><p>256 pixels which are padded to 320 × 320 pixels by reflecting a region of 32 pixels around the borders to alleviate boundary artifacts. The padded regions are augmented by random elastic deformations, reflections and rotations by multiples of π 2 . After the decoder we max-pool over orientations to obtain locally invariant features and crop out 256 × 256 pixels centrally. Two subsequent 1 × 1 convolution layers map these features pixel-wise to the desired probability map.</p><p>The network is optimized by minimizing the spatially averaged binary cross-entropy loss between predictions and the ground truth segmentation masks using the ADAM optimizer. As on the rotated MNIST dataset we regularize the convolutional weights with an elastic net penalty with hyperparameters λ L1 = λ L2 set to 10 −7 and 10 −8 for the steerable and 1 × 1 convolution layers respectively. Here we chose a dropout probability of p = 0.4 both in the steerable as well as in the 1 × 1 convolution layers. The learning rate is decayed exponentially by a factor of 0.85 per epoch starting from an initial rate of 5 · 10 −2 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>gives an overview over the key concepts utilized in Steerable Filter CNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Generalizing</head><label></label><figDesc>He's weight initialization (→ Sec. 3.3) e x a c t r o t a t io n b y p h a s e m a n ip u la t io n o f s h a r e d w e ig h t s Learning steerable filters (→ Sec. 3.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the circular harmonics ψ jk (r, φ) = τ j (r) e ikφ sampled on a 9 × 9 grid. Each row shows a different radial part j, the angular frequencies are arranged in the columns. For larger scales there are higher frequency filters not shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Top: Basic structure of a typical SFCNN for rotation-equivariant segmentation. For clarity, we display only a single group-convolutional layer and a single feature channel and omit pooling and normalization layers. Rotated Greek letters represent the spatial orientations of the filters and the feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Left: Test error versus number of sampled filter orientations for different training subsets from mnist-rot. Shaded regions highlight the standard deviations over several runs. The accuracy improves significantly with increasing angular resolution until it saturates at around 12 to 16 orientations. Right: Rotational generalization capabilities of a conventional CNN and a SFCNN with Λ = 16 using different data augmentation strategies. In this experiment the training set consists of unrotated MNIST digits while the test set for each angle contains the remaining digits, rotated to the corresponding angle. benchmark for rotation-equivariant models. The dataset contains the handwritten digits of the classical MNIST dataset, rotated to random orientations in [0, 2π). It is split in 12000 training and 50000 test images; model selection is done by training on 10000 images and validating on the 2000 remaining samples in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>E[w] = 0. The symmetric distribution of weights leads to a symmetric distribution of pre-nonlinearity activations which in conjunction with ReLU nonlinearities implies E[ζ 2 ] = 1 2 Var[y]. To see this, note that the symmetry of the distribution of pre-nonlinearity activation leads on the one hand to Var[y] = E[y 2 ] + Θ(ζ)p y (ζ) dζ = R +ζ 2 p y (ζ) dζ ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Experimental results on the ISBI 2012 challenge. The shown patches are cropped from slice 30 of the training data set which we used for validation. Left: Raw EM image. Mid-left: Binary membrane ground truth segmentation. Mid-right: Probability map predicted by the proposed network. Right: Top 6 of more than 100 entries of the leaderboard, accessed on November 13, 2017. Higher values mean better accuracy.</figDesc><table><row><cell></cell><cell>Method</cell><cell>V Rand</cell><cell>V Info</cell></row><row><cell></cell><cell>IAL MC/LMC</cell><cell cols="2">0.98792 0.99183</cell></row><row><cell></cell><cell>CASIA MIRA</cell><cell>0.98788</cell><cell>0.99072</cell></row><row><cell></cell><cell>Ours</cell><cell>0.98680</cell><cell>0.99144</cell></row><row><cell></cell><cell>Quan et al. [26]</cell><cell>0.98365</cell><cell>0.99130</cell></row><row><cell></cell><cell>Beier et al. [27]</cell><cell>0.98224</cell><cell>0.98845</cell></row><row><cell></cell><cell cols="2">Drozdzal et al. [28] 0.98058</cell><cell>0.98816</cell></row><row><cell>Figure 5: Method</cell><cell>Test Error (%)</cell><cell></cell></row><row><cell>Ours -CoeffInit, train time augmentation</cell><cell>0.714 ± 0.022</cell><cell></cell></row><row><cell>Ours -CoeffInit</cell><cell>0.880 ± 0.029</cell><cell></cell></row><row><cell>Ours -HeInit</cell><cell>0.957 ± 0.025</cell><cell></cell></row><row><cell>Marcos et al. [23] -test time augmentation</cell><cell>1.01</cell><cell></cell></row><row><cell>Marcos et al. [23]</cell><cell>1.09</cell><cell></cell></row><row><cell>Laptev et al. [21]</cell><cell>1.2</cell><cell></cell></row><row><cell>Worrall et al. [24]</cell><cell>1.69</cell><cell></cell></row><row><cell>Cohen and Welling [2] -G-CNN</cell><cell>2.28 ± 0.0004</cell><cell></cell></row><row><cell>Schmidt and Roth [29]</cell><cell>4.0</cell><cell></cell></row><row><cell>Sohn and Lee [11]</cell><cell>4.2</cell><cell></cell></row><row><cell>Cohen and Welling [2] -conventional CNN</cell><cell>5.03 ± 0.0020</cell><cell></cell></row><row><cell>Larochelle et al.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Test errors on the rotated MNIST dataset. We distinguish He initialization (HeInit) from the proposed initialization scheme (CoeffInit).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Architecture of the SFCNN used in the initial experiments on the resolution of sampled orientations and the rotational generalization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Architecture of the SFCNN used with Λ = 16 sampled orientations in the final benchmarking experiments on rotated MNIST.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice one often uses strided pooling layers which make the prediction more robust to local deformations but reduce the equivariance to a subgroup determined by the stride.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank T. Beier, C. Pape, N. Rahaman and I. Arganda-Carreras for their technical support and U. Köthe and T. Cohen for valuable discussions. This work was partially supported by the German Research Foundation (DFG grant STO1126/2-1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AIS-TATS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crowdsourcing the creation of image segmentation algorithms for connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroanatomy</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Canonical decomposition of steerable functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hel-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="95" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="906" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optical pattern recognition using circular harmonic expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arsenault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="4016" to="4019" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Circular harmonic phase filters for efficient rotation-invariant pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2895" to="2899" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1311" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Locally scale-invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5104</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative template learning in groupconvolutional networks for invariant speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voinea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep symmetry networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Warped convolutions: Efficient invariance to spatial transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1461" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inverse compositional spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00598</idno>
		<title level="m">Dynamic steerable blocks in deep residual networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting cyclic symmetry in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.09346</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04642</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fusionnet: A deep fully residual convolutional neural network for image segmentation in connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Hilderbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multicut brings automated neurite segmentation closer to human performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Prange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kreshuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="102" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2050" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
