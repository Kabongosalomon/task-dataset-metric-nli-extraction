<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep learning have heightened interest among researchers in the field of visual speech recognition (VSR). Currently, most existing methods equate VSR with automatic lip reading, which attempts to recognise speech by analysing lip motion. However, human experience and psychological studies suggest that we do not always fix our gaze at each other's lips during a face-to-face conversation, but rather scan the whole face repetitively. This inspires us to revisit a fundamental yet somehow overlooked problem: can VSR models benefit from reading extraoral facial regions, i.e. beyond the lips? In this paper, we perform a comprehensive study to evaluate the effects of different facial regions with state-of-the-art VSR models, including the mouth, the whole face, the upper face, and even the cheeks. Experiments are conducted on both word-level and sentence-level benchmarks with different characteristics. We find that despite the complex variations of the data, incorporating information from extraoral facial regions, even the upper face, consistently benefits VSR performance. Furthermore, we introduce a simple yet effective method based on Cutout to learn more discriminative features for face-based VSR, hoping to maximise the utility of information encoded in different facial regions. Our experiments show obvious improvements over existing state-of-the-art methods that use only the lip region as inputs, a result we believe would probably provide the VSR community with some new and exciting insights.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual speech recognition (VSR) is the task of recognising speech by analysing video sequences of people speaking. A robust VSR system has a variety of useful applications, such as silent speech interfaces <ref type="bibr" target="#b28">[29]</ref>, audio-visual speech recognition (AVSR) in noisy environments <ref type="bibr" target="#b0">[1]</ref>, face liveness detection, and so on. Its performance has progressed significantly over the past few years, thanks to several successful deep learning architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). It also benefits from the emergence of large-scale, in-the-wild audiovisual datasets, from which deep neural networks can automatically learn strong representations that outperform previous hand-crafted features.</p><p>Traditionally, the term "visual speech recognition" is used almost interchangeably with "lip reading" within the VSR community, since it is usually believed that lip shapes and lip motion most likely contain almost all the information correlated with speech. The information in other facial regions is considered by default weak, and not helpful for VSR This work was done by Yuanhang Zhang during his internship at Institute of Computing Technology, Chinese Academy of Sciences. <ref type="figure">Fig. 1</ref>. Common practices for RoI selection in VSR. To date, there is no clear consensus on a best practice, resulting in very different RoIs in different works (see Sec. II-A). Top row: examples of frames from talking face videos. Bottom row: some examples of cropped RoIs in prior work, from left to right: <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b35">[35]</ref>.</p><p>in practical use, due to the diversity of the speaker's pose and other variations in the facial region that are unrelated to speech production. Accordingly, as part of the dataset creation pipeline, almost all researchers crop regions-of-interest (RoIs) around the mouth after obtaining face bounding boxes and landmarks. By observing only the cropped RoI, the model is expected to focus on fine-grained discrimination of "clean" motion signals within the most relevant lip region, and not be distracted by other parts of the face, whose utilities are less obvious. Some common practices for RoI selection in previous work are depicted in <ref type="figure">Fig. 1</ref>.</p><p>However, this convention of explicit mouth RoI cropping inevitably brings about many questions. Firstly, lip motion is not the only visual signal we can rely on to decode speech. Research on the advantage of performing speechreading with the whole face has a long history <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In particular, movements of articulatory muscles such as the orbicularis oris (which is near the lips) lead to skin movements, often reflected by the cheeks being pushed and pulled, as well as changes in the visibility of the nasolabial folds. The now widely adopted term "speechreading", used in place of "lip reading" implies precisely the contribution of extraoral facial regions, such as the tongue, teeth, and cheeks, to the speech perception task. Evidence from psychology studies, and our experience in human communication also suggest that we in fact do not focus on the speaker's lip all the time throughout a conversation, even in very noisy environments. Instead, we scan different regions of the person's face periodically <ref type="bibr" target="#b31">[31]</ref>.</p><p>Secondly, if we do use a mouth RoI, there are many factors that need to be considered: how much of the face should the RoI cover? Will increased spatial context improve performance by supplying more information, or hinder performance by distracting the network with information unrelated to speech? Should we apply additional corrections to handle pose variation? Answers to these questions are not evident or straight, and there has been no consensus or a universal guideline for choosing RoIs until now. In fact, RoIs are often chosen based on intuition and the researchers' experience. The choices can be very different for different datasets and different methods, making transfer learning and cross-dataset evaluation difficult. Meanwhile, this specialised RoI selection step separates VSR from other face-related tasks, hindering further joint analysis. Using mouth inputs alone makes VSR an isolated problem, while including other facial regions opens up the possibility of various research in affective analysis and visual speech understanding. For example, joint modeling of non-verbal behaviour and verbal behaviour has been shown to be beneficial for learning adaptive word representations <ref type="bibr" target="#b32">[32]</ref>.</p><p>Finally, data encountered in real-world applications may not have the same luxury that some academic datasets enjoy, where data is biased towards frontal or near-frontal views, and high-resolution mouth crops are easily obtainable. Models trained on such well-behaved, cropped data may not perform as well when presented with the various challenges in practice. This severely limits the potential usage scenarios of VSR systems.</p><p>Motivated by these observations, we conduct a comparative study on input RoI selection, to (a) quantitatively estimate the contribution of different facial regions to the VSR task, and (b) determine whether state-of-the-art VSR networks, when presented with complex in-the-wild data, still benefit from additional clues within extraoral regions. Previous studies have explored different RoIs in the lower face <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and demonstrated the importance of suitable RoI coverage and selection. However, these attempts are limited to the lower face, and relatively small datasets with narrow vocabularies. For this study, we approach the problem with state-of-the-art, deep VSR models trained on large-scale "in-the-wild" VSR datasets, which depict many real-world variations, such as pose, lighting, scale, background clutter, makeup, expression, different speaking manners, etc. This is a much fairer reflection of real-world scenarios. Besides, we propose Cutout as an effective approach to encourage the model to utilise all facial regions. This allows researchers to sidestep the ambiguous choice of selecting appropriate RoIs, enhances the visual features, and increases the model's robustness to mild occlusion within the mouth or other facial regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual Speech Recognition</head><p>Visual speech recognition (VSR), commonly referred to as automatic lip reading, is a classical problem in computer vision and has received increased interest over recent years. The combination of deep learning approaches and large-scale audiovisual datasets has been highly successful, achieving remarkable word recognition rates and even surpassing human performance. The deep network approach has become increasingly common and mainstream <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b35">[35]</ref>. These methods have retraced the success of the CNN-LSTM-DNN (CLDNN) paradigm <ref type="bibr" target="#b21">[22]</ref> in the automated speech recognition (ASR) community. However, though many works have reported state-of-the-art results on multiple challenging datasets, few have investigated the influence of input RoI selection, which can be a nuisance due to variations in mouth shapes, face geometry, and pose, as well as the relatively unknown effect of spatial context and extraoral regions. Unlike face recognition, where face frontalisation is a recognised need for pose invariance, and has been thoroughly investigated as an important part of the pipeline, VSR researchers tend to specify RoIs based on their own experience, and in a dataset-specific manner.</p><p>In some controlled-environment datasets, e.g. OuluVS <ref type="bibr" target="#b37">[37]</ref> and GRID <ref type="bibr" target="#b10">[11]</ref>, the subjects remain relatively still while speaking. Lombard GRID <ref type="bibr" target="#b1">[2]</ref> uses a head-mounted camera which the speakers face directly at for the frontal view, essentially fully removing head motion. The Lip Reading in the Wild dataset <ref type="bibr" target="#b8">[9]</ref> is of short duration with small or no face scale variation within clips, and loose registration is enforced by aligning nose centers. A fixed, mouth-centered (and sometimes affine-transformed) rectangular RoI is preferable on these datasets, because the faces are usually stable, frontal or near-frontal, and of uniform sizes. In contrast, OuluVS2 <ref type="bibr" target="#b2">[3]</ref> is a multi-view dataset, and RoIs are processed separately for each camera viewpoint. Chung and Zisserman <ref type="bibr" target="#b9">[10]</ref> are the first to investigate large-scale deep lip reading in profile, and use an extended bounding box covering the whole face to account for significant pose variations. Yang et al. <ref type="bibr" target="#b35">[35]</ref> determine the RoI size based on the distance between the nose and the mouth center and the width of the speaker's lips, which also effectively handles pose variations.</p><p>There have been some previous attempts to address the RoI selection problem explicitly. The most relevant work is <ref type="bibr" target="#b16">[17]</ref>, which perform experiments with rectangular lower face regions with different spatial context and resolution in a connected digits recognition task. <ref type="bibr" target="#b23">[24]</ref> experiments with optical flow features in non-rectangular RoIs after removing head motion, and obtain results better than rectangular RoIs. However, these work are limited by the amount of data used and model capacity, and only investigate the utility of the lower face. We adopt face inputs, and mimic real-world scenarios by experimenting on large-scale, in-the-wild VSR benchmarks that are a magnitude larger than those used in the above two papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human Perception Studies</head><p>Our intuition that we do not fix our gaze at the speaker's lip region when communicating with others is supported by a number of psychology studies on human gaze behaviour during visual and audio-visual speech perception <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b17">[18]</ref>. It has been reported that human gaze patterns usually involve repetitive transitioning between the eyes and the mouth, even at high noise levels and when audio is absent <ref type="bibr" target="#b30">[30]</ref>. Interestingly, <ref type="bibr" target="#b17">[18]</ref> suggests that in a visual-only scenario, speechreading accuracy was related to the difficulty of the presented sentences and individual proficiency, but not to the proportion of gaze time at a specific part of the face. The roles of the upper face, which is less intuitive for VSR, have also been studied <ref type="bibr" target="#b12">[13]</ref>. Studies in audiovisual speech perception show that head and eyebrow motion can help discriminate prosodic contrasts <ref type="bibr" target="#b11">[12]</ref>, which may be helpful for the VSR task. Moreover, in tonal languages like Mandarin Chinese, movements in the neck, head, and mouth might be related to the lexical tones of syllables <ref type="bibr" target="#b6">[7]</ref>. In the context of deep-learning based VSR, recently <ref type="bibr" target="#b38">[38]</ref> has shown that using video information from the cropped lip region together with pinyin (Chinese syllables) information yields 0.85% reduction in tone prediction error, supporting this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPLORING THE INFLUENCE OF ROIS ON VSR</head><p>In this section, we first introduce the deep VSR architectures used in this work, and four manually selected RoIs (some of which include extraoral regions) to be experimented on. Next, we introduce Cutout as a simple strategy to enhance face-based VSR. Finally, we introduce a few visualisation methods we use to diagnose the resulting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Architecture</head><p>3D-ResNet18. The 3D-ResNet18 backbone which holds the current state-of-the-art on the LRW dataset <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b25">[26]</ref> is used for all word-level experiments. It consists of a spatiotemporal convolution layer and a 18-layer 2D residual network which gradually reduces spatial dimensionality, and yields a 512-dimensional average-pooled vector for each frame. We use a 2-layer bidirectional Gated Recurrent Unit (Bi-GRU) with 1024 hidden units as the recurrent backend, and do not experiment with further regularisation such as Dropout and recurrent batch normalisation <ref type="bibr" target="#b25">[26]</ref>, as it deviates from the main objective of this paper.</p><p>LipNet. For sentence-level VSR, we use the LipNet architecture, which achieves state-of-the-art performance on the GRID corpus <ref type="bibr" target="#b3">[4]</ref>. With only three spatiotemporal convolutional layers in the frontend, this lightweight model should be less prone to overfitting on such a small-scale dataset. We also use temporal augmentation and Dropout as recommended by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Description of Manually Selected RoIs</head><p>For fixed-size mouth crops, there are two dominant approaches: one uses fixed bounding box coordinates, and the other uses mouth-centered crops. To make this study comprehensive and self-complete, we experiment with both choices (although eventually we found no difference; see <ref type="table" target="#tab_0">Table I</ref>). For face based models, we first crop and align the faces using detected eye and nose landmarks. This is because these parts undergo less non-rigid deformation, and this allows us to capture more significant motion in the lower face including the cheeks, the mouth and the jaw. At a low computational cost, this allows for more stable face tracks, and helps the network capture information in different spatial patches more consistently. However, it should be noted that face motion consists of rigid head movement (yaw, pitch, and roll) and non-rigid movement (e.g. frowning, raising</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original frame</head><p>Landmark detection Aligned face <ref type="figure">Fig. 2</ref>. Illustration of the sub-face RoIs defined in this paper. We train baselines on the whole face (blue), the upper face (purple), the cheeks (orange), and the mouth (red). eyebrows, and skin movement). Aligning the face retains yaw and pitch, but totally removes roll rotation. To this end, we also experiment with whole face and upper face cropped directly by fixed coordinates on LRW, since the faces are of uniform sizes and loosely aligned. The input sizes we choose are those commonly adopted in prior work, 112×112 and 100 × 100. We do not elaborate on the effects of spatial downsampling, since lower RoI resolution does not always yield poorer performance, as long as it remains above an acceptable level (e.g. 50 × 50 pixels) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[35]</ref>. Besides, in any case datasets collected in-the-wild already contain inherent scale and image quality variations.</p><p>To crop the cheeks without revealing too much information in other regions, we crop a rectangular tile from the aligned face, which has no in-plane rotation. The vertical center of the RoI is set as the mean y coordinate of the 18th, 27th, 57th, and 59th landmark after transformation.</p><p>Finally, we do not experiment with the jaw and the lower neck. Although these parts are directly related to the vibrations of the vocal tract, these regions are not always visible, and such an RoI will be difficult to define.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Enhancing Face Based VSR with Cutout</head><p>Our motivation of using face as input is that it covers all other three sub-RoIs, which has the possibility of providing a strong "holistic" representation. If the network is able to remain robust against the speech-irrelevant variations that human faces present, including pose, lighting, makeup, etc., we can benefit from additional information in other facial regions all at once. However, a network has only limited capacity. On a fixed budget, the goals of achieving invariance to nuisance factors and keeping stronger discrimination capabilities are inherently complimentary, and the recognition performance may suffer if either is not sufficiently realised.</p><p>Indeed, our experiments will later show that the face models have already achieved performance comparable to lip based models. However, when we inspect the convolutional responses of vanilla face-based models (see <ref type="figure" target="#fig_2">Fig. 5</ref>) we find that the network tends to focus on the lip region and does not sufficiently utilise other parts of the face. Inspired by the observation that the vanilla face-based models able to achieve un-expected good performance, we try to enhance further the performance by asking the model to learn more discriminative features by utilising signals spread across the whole face. An intuitive idea is to create a strong patchbased ensemble, which has been shown feasible for facial expression recognition <ref type="bibr" target="#b18">[19]</ref>. However, for VSR it would be computationally expensive and impractical for deployment, since we are dealing with spatiotemporal data. Moreover, the redundancy between patches will burden optimisation.</p><p>Therefore, we propose to apply Cutout <ref type="bibr" target="#b13">[14]</ref>, a regularisation technique which has been popular with image CNNs. It augments the dataset with partially occluded versions of the samples in the dataset, and has already been successfully applied to image classification and object detection <ref type="bibr" target="#b39">[39]</ref>. Our motivation is that this "adversarial erasing" process should help the model pay more attention to less significant motion signals related to speech in extraoral regions.</p><p>During training, a patch within the face region is randomly zeroed. Note that Cutout is applied to identical spatial positions across the video, since we expect the same facial region to be in roughly the same position after decent alignment. Over the course of the training process the model will encounter many masked versions of the same video, and eventually learn discriminative features for all parts of the face, which the model should be able to combine to its advantage during inference. Cutout fits seamlessly into the training process, and can be performed efficiently as a data augmentation step at no additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interpreting Model Behaviour</head><p>To explore what our models are paying attention to throughout the video, we apply three visualisation techniques that can provide insights as to what the network focuses on for the task of visual speech recognition -similar to the use of gaze trackers in human psychology experiments.</p><p>Feature maps. Feature maps are filter responses to input images and outputs from previous layers, and can provide insight into intermediate representations learned by the model. Looking at responses from increasingly deeper layers can give us a sense of how the model combines low-level features into higher-level concepts that are useful for recognition.</p><p>Saliency maps. We use guided backpropagation saliency visualisation <ref type="bibr" target="#b24">[25]</ref>, which has been previously utilised to interpret lip reading <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref> and image classification models. Specifically, let x ∈ R T ×H×W ×C be an input video, V be the alphabet or vocabulary, and φ be the non-linear function that underlies the deep recognition network. For word-level classification, the network outputs a score for the i-th class in the vocabulary,</p><formula xml:id="formula_0">p(i | x) = φ (x) i , where 1 ≤ i ≤ |V |.</formula><p>We compute its gradient with respect to the input x using guided backpropagation. Likewise, for sentence-level VSR, we compute the likelihood of the greedily decoded output y ∈ (V ∪ { }) * , which is ∏ t p(y t | x), and derive it against the input video sequence x to obtain the saliency maps. Eventually we obtain a tensor the same size as the input, which depicts spatial regions the model bases its prediction on for different timesteps.</p><p>Spatiotemporal masking. This approach, adapted from the patch masking method for visualising the receptive fields of 2D ConvNets <ref type="bibr" target="#b36">[36]</ref> has been used to visualise important space-time video patches in audio-visual speech enhancement models <ref type="bibr" target="#b15">[16]</ref>. In our experiments, we mask each frame at the identical position for spatially aligned faces using a small 7 × 7 patch in a sliding window fashion, and measure how overall accuracy is affected by computing performance drop, e.g. ∆ accuracy . This process results in a heatmap depicting the contribution of different facial regions to recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>We train and evaluate on three VSR benchmarks, which cover tonal and atonal languages as well as in-the-wild and scripted speech: the Lip Reading in the Wild (LRW) dataset, the recent LRW-1000 dataset, and the GRID audiovisual corpus. In this section, we first briefly introduce the three datasets we use, and present some implementation details. Next, we compare recognition performance using the four manually selected RoIs in Sec. III-B. We highlight the benefits of incorporating extraoral regions, in particular by using aligned entire faces. Finally, we present results of our best performing model which combine Cutout with face inputs, and make a few useful remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>LRW. LRW [9] is a challenging "in-the-wild" English word-level lip reading dataset derived from BBC news collections, with 500 classes and over 500, 000 instances, of which 25, 000 are reserved for testing.</p><p>LRW-1000. LRW-1000 <ref type="bibr" target="#b35">[35]</ref> is a 1000-class, large-scale, naturally distributed Mandarin Chinese word-level lip reading dataset, also derived from TV broadcasts. With over 700, 000 word instances, it is even more challenging, with significant pose, scale, background clutter, word length, and inter-speaker variations. Note that the whole face was not provided with LRW-1000 in the initial release. We will release face tracks for LRW-1000 1 along with annotations that have undergone another round of manual cleaning as well as corresponding baselines in due course.</p><p>GRID. The GRID audiovisual corpus <ref type="bibr" target="#b10">[11]</ref>, released in 2006, is a popular benchmark for sentence-level VSR. It consists of video recordings from 34 speakers 2 , yielding 33, 000 utterances. All sentences follow a fixed grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Data preprocessing. We detect faces and facial landmarks with the open-source SeetaFace2 toolkit <ref type="bibr" target="#b22">[23]</ref>, and align faces with similarity transformations using upper face landmarks <ref type="bibr" target="#b7">[8]</ref>, which are smoothed using a temporal Gaussian kernel of width 3. For LRW and LRW-1000, the faces are resized to 122 × 122 and randomly cropped to 112 × 112 during training. For GRID, the faces are resized to 100×100.</p><p>Training details. All models are implemented with Py-Torch and trained on 2 to 3 NVIDIA Titan X GPUs, each with 12GB memory. We use the Adam optimizer with default hyperparameters. For word-level VSR, we use three-stage training described in <ref type="bibr" target="#b26">[27]</ref>. Weight decay is set to 0.0001, and   <ref type="bibr" target="#b35">[35]</ref>. For sentence-level VSR, we use a fixed learning rate of 0.0001 and use no weight decay. We also apply random horizontal flipping, but no random cropping is used because of relatively accurate and stable landmarking results. Data partitioning, and evaluation metrics. We use the train / validation / test partitioning provided with LRW and LRW-1000. For GRID, we use 255 random sentences from each speaker for evaluation, and the remainder for training, the same as previous state-of-the-art <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b34">[34]</ref>. The evaluation metrics for word-level and sentence-level tasks are classification accuracy and Word Error Rate (WER), respectively, where WER is defined as WER = # of substitutions, deletions, and insertions length of ground truth transcript .</p><p>Hence lower WERs are preferred. Note that the two metrics are equivalent if one views word recognition as a one-word sentence recognition task, in the sense that Acc = 1 − WER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Manually Selected RoIs</head><p>Baseline results on the manually defined RoIs are shown in <ref type="table" target="#tab_0">Table I, Table II</ref>, and III. We analyze the results step by step for the rest of this subsection.</p><p>Effectiveness of including extraoral regions. Experiment results clearly show that upper face and cheeks carry useful information, since recognition rates are far above chance. Counterintuitively, the upper face achieves nearly half the accuracy of face and mouth models. To ensure that the model has learned useful discriminative features instead of some unknown inherent bias within the dataset, we conduct an additional experiment, transferring from LRW to LRW-1000 while keeping the front-end fixed. Intuitively, if the front-end has learned spurious clues that are irrelevant to the task itself, it should behave poorly on a dataset it has not been trained on. However, despite significant differences between the two datasets in terms of quality and language, classification accuracy is still far above chance after we fixed the front-end, with only 2.26% absolute performance loss. Seeing how the upper face and cheeks convey useful information for VSR, by feeding the model with the entire face, we would expect it to benefit from the additional spatial context, which is indeed the case. Using the entire face instead of only the mouth region yields 1.6% WER reduction on GRID, 3.07% improvement on LRW-1000, and 0.16% improvement on LRW (when directly cropped faces are used). The slight performance regression on LRW with aligned faces will be discussed next.</p><p>Making a case for face alignment. For LRW, there are two sets of face-based experiments, one with face alignment, and the other using faces directly cropped by fixed coordinates. We observe a small performance degradation on LRW when we align faces to our canonical template (0.2% to 0.3% drop relative to mouth and 7/8 original resolution direct crops). Since recognition performance using the upper face actually benefits from alignment, it can be argued that the performance drop is most likely due to slightly lower mouth resolution (about 70 × 70), and not removal of roll during the alignment process. This is not desirable, but acceptable, and there may be room for improvement if we adopt higher resolution inputs and improve landmarking quality. Therefore, we consider aligning the faces into stable face tracks to be beneficial, and use aligned faces for the remaining experiments. By any means, this is also necessary for structured facial region erasing with Cutout.</p><p>Failure modes. As an illustrative example, we further compare confusions made by the model under each crop setting in <ref type="table" target="#tab_0">Table IV and Table V</ref> for the two English datasets. Overall, short words with less context and words that invoke weak extraoral motions perform worst. We observe that the words best predicted by the upper face are long words, such as "Westminster" and "Temperatures", and cheeks are good at making predictions for words that invoke significant extraoral motion. The face based model, which achieves comparable but slightly inferior performance compared to the mouth based model, fails to discriminate words with only subtle differences, such as "benefits" and "benefit".  Recognising such words correctly requires analysis of tongue movement, which is hindered by the lowered overall resolution. However, on the GRID corpus, the face-based model seems to have identified idiosyncratic speaking styles, allowing it to make correct predictions for even short words and letters that are not easy to distinguish, and eventually obtain results better than cropped mouths. Superiority of using entire faces for cross-dataset transfer. A byproduct of using the entire face rather than specifying sub-RoIs is that the visual front-end can be transferred across datasets easily with no explicit sub-RoI cropping after face detection and registration, which is already very routine in face recognition. Empirically, we found that transferring across face crops is much better than transferring across mouth crops from LRW to LRW-1000. When both fine-tuned for 9 epochs, mouth-based models suffer from source and target domain discrepancies, resulting in fluctuating validation loss and accuracy (best value 2.8482/38.40%), whereas the face-based model transferred stably, and converged to a better result (best value 2.7202/40.35%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on Enhancing Face Inputs with Cutout</head><p>Results of combining Cutout with aligned faces can also be found in previous tables. This strategy is extremely powerful, enabling state-of-the-art performance on all three benchmarks. In particular, Cutout significantly reduces overfitting on LRW to the point where the model starts to underfit, as can be seen from the training curves in <ref type="figure" target="#fig_1">Fig. 3</ref>. This is radically different from models trained on faces or mouths without Cutout, where the network eventually reaches near 100% accuracy on the training data, and proves that this harsh regularisation policy is indeed useful for VSR. Below we elaborate on a few interesting observations.</p><p>Effect of the Cutout patch size. The size of the masked out region is an important hyperparameter for Cutout. We experiment with four different sizes: 70 × 70, 56 × 56, 42 ×   <ref type="figure">Fig. 4</ref>). This is probably because it is approximately the average size of the mouth, and the possibility of the entire mouth being blocked allows for more efficient utilisation of extraoral regions. Since we adopt the same canonical template, we use 1/2-size masks for all datasets (i.e. 50 × 50 for GRID).</p><p>Visualising key facial regions. <ref type="figure" target="#fig_2">Fig. 5</ref> provide some saliency visualisations which show that models with Cutout can learn more discriminative features. <ref type="figure" target="#fig_2">Fig. 5(a)</ref> is generated by extracting feature maps from the third ResNet layer, and performing max-pooling across channels, and <ref type="figure" target="#fig_2">Fig. 5</ref>(b)(c) (GRID and LRW-1000) by computing back-prop saliency. The derived maps are colored by intensity and overlaid onto the original frames. Each face thumbnail corresponds to a time step, and the two rows can be compared side-by-side to see the effects of introducing Cutout, especially regions highlighted with a dotted red box. For example, for LRW the convolutional responses are no longer confined to the lips, and for GRID there is stronger saliency in the cheeks, the eyebrows, and other facial muscles. The third row shows that after transferring from LRW to LRW-1000, saliency in extraoral regions persist, which means that the learned facial features generalize well and are relatively robust.</p><p>We also identify regions that are important to the network's predictions by spatiotemporal masking on the entire test set of LRW, and results are depicted in <ref type="figure" target="#fig_3">Fig. 6</ref>. It can be seen that for both models, the area that leads to the most significant performance degradation when masked is still the lip region, which agrees with common intuition. In addition, the model trained with Cutout is also affected by occlusion in extraoral regions such as the cheeks and the upper face, showing that the model has learned strong visual features that encode these weak signals to complement lip motion. More importantly, while the plain model suffers up to 40% performance drop even when only a 7 × 7 patch is  Important facial regions determined by spatiotemporal masking. Left: without Cutout. Middle: an aligned face for reference. Right: with Cutout. Regions that result in more accuracy drop when occluded are colored brighter, and more crucial for model prediction.</p><p>occluded, the drop remains below 2% for the model trained with Cutout. This observation again strongly supports the usefulness of extraoral regions.</p><p>Comparison with attention-based RoI selection. The attention mechanism, inspired by human vision, has been widely used for implicit RoI selection and fine-grained image recognition. Here we compare Cutout with a Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b33">[33]</ref> augmented baseline on LRW, where we plug CBAM into ResNet blocks. Here, we use 5 × 5 instead of 7 × 7 kernels for the spatial attention modules to accommodate to the smaller feature maps. Although CBAM is a powerful spatial-channel attention mechanism which achieved remarkable performance on ImageNet classification and robust remote heart rate estimation <ref type="bibr" target="#b19">[20]</ref>, results show that the attention-augmented model is only marginally better than the baseline on LRW. We believe this is because the subtle movements in extraoral regions are too weak to be captured with the attention mechanism, and the model is still biased towards lip motion.</p><p>Performance across pose. We are also interested in how well the Cutout-augmented model can handle pose variations. We turn to LRW-1000, where the data is divided into different difficulty levels according to yaw rotation. From Table VI, we can see that the model trained with Cutout outperforms the no-augmentation face-based baseline by about 2% on both the easy (yaw ≥ 20 • ) and the medium (≥ 40 • ) subset, but degrades slightly on the hard subset (≥ 60 • ). We believe this is mainly because effective areas are smaller in the hard setting, where there are more frames in profile view. The effective regions are more likely to be erased or occluded when there is significant yaw rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have investigated a previously overlooked problem in VSR: the use of extraoral information. We demonstrate that extraoral regions in the face, such as the upper face and the cheeks, can also be included to boost performance. We show that using simple Cutout augmentation with aligned face inputs can yield stronger features, and vastly improve recognition performance by forcing the model to learn the less obvious extraoral cues from data. Beyond VSR, our findings also have clear implications for other speech-related vision tasks, such as realistic talking face generation, face spoofing detection and audiovisual speech enhancement. Next steps include extending the method to sentence-level VSR where more contextual clues are available, increasing input resolution, and eliminating the need for explicit face alignment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>OF PREDICTIONS ON GRID UNDER EACH CROP SETTING. PREDICTION ERRORS ARE HIGHLIGHTED IN RED. GT: GROUND TRUTH; UF: UPPER FACE; C: CHEEKS; M: MOUTH; F: (ALIGNED) FACE. GT lay white in u four now UF lay white in u four now C lay white at o four now M lay white at o four now F lay white in u four now GT lay white in q five again UF set white at t five again C lay white at q five again M lay white at q five again F lay white in q five again</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Accuracy curves on LRW as training progresses (only the first 75k steps of the temporal convolution backend stage is shown</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Visualisations of model behaviour on LRW, LRW-1000, and GRID with Cutout applied. In particular, note how models trained with Cutout preserve fine-grained features, and yields clearly visible saliency around the cheeks. (a) ResNet layer 3 feature maps for "hundred"; (b) Saliency maps for "bin blue"; (c) Saliency maps for "er ling yi qi" (twenty-seventeen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Important facial regions determined by spatiotemporal masking. Left: without Cutout. Middle: an aligned face for reference. Right: with Cutout. Regions that result in more accuracy drop when occluded are colored brighter, and more crucial for model prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>ON THE LRW DATASET WITH DIFFERENT ROI CHOICES. THE SECOND GROUP USES DIRECTLY CROPPED (UPPER) FACES WHILE THE THIRD APPLIES FACE ALIGNMENT.</figDesc><table><row><cell>Region</cell><cell cols="2">Resolution Accuracy</cell><cell>Description</cell></row><row><cell>Mouth</cell><cell>88 × 88</cell><cell>83.30%</cell><cell>Fixed bounding box [21], [26]</cell></row><row><cell>Mouth</cell><cell>88 × 88</cell><cell>83.30%</cell><cell>Mouth-centered [9]</cell></row><row><cell>Face</cell><cell>112 × 112</cell><cell>83.46%</cell><cell>Nose-centered, 7/8 original size</cell></row><row><cell>Upper face</cell><cell>112 × 112</cell><cell>42.28%</cell><cell>Resized upper half from above</cell></row><row><cell>Face</cell><cell>112 × 112</cell><cell>83.10%</cell><cell>Aligned with eye &amp; nose landmarks [8]</cell></row><row><cell>Cheeks</cell><cell>112 × 112</cell><cell>62.49%</cell><cell>Resized 40 × 112 crop from above</cell></row><row><cell>Upper face</cell><cell>112 × 112</cell><cell>48.33%</cell><cell>Resized upper half</cell></row><row><cell>Face (CBAM)</cell><cell>112 × 112</cell><cell>83.14%</cell><cell></cell></row><row><cell>Face (Cutout)</cell><cell>112 × 112</cell><cell>85.02%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EVALUATION</head><label>II</label><figDesc>ON LRW-1000 WITH DIFFERENT ROI CHOICES.</figDesc><table><row><cell>Region</cell><cell cols="2">Resolution Accuracy</cell><cell>Description</cell></row><row><cell>Mouth</cell><cell>88 × 88</cell><cell>38.64%</cell><cell>Mouth-centered, no roll (as in [35])</cell></row><row><cell>Face</cell><cell>112 × 112</cell><cell>41.71%</cell><cell>Aligned with eye &amp; nose landmarks</cell></row><row><cell>Upper face</cell><cell>112 × 112</cell><cell>15.84%</cell><cell>Resized upper half</cell></row><row><cell>Cheeks</cell><cell>112 × 112</cell><cell>32.50%</cell><cell>Resized 40 × 112 crop from above</cell></row><row><cell>Face (Cutout)</cell><cell>112 × 112</cell><cell>45.24%</cell><cell></cell></row><row><cell>Upper face</cell><cell>112 × 112</cell><cell>13.58%</cell><cell>Front-end loaded from LRW and fixed</cell></row><row><cell cols="4">learning rate is initialised to 0.0003 for stage I / II and 0.001</cell></row><row><cell cols="4">for stage III, decreasing on log scale from the 5th or the 10th</cell></row><row><cell cols="4">epoch onwards, depending on whether Cutout is applied. We</cell></row><row><cell cols="4">apply identical spatial augmentation to every frame in the</cell></row><row><cell cols="4">form of random cropping and random horizontal flipping</cell></row><row><cell cols="4">during training, and take a central crop for testing. Also,</cell></row></table><note>for LRW-1000 training, we initialise front-end weights from the corresponding best model on LRW, following</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EVALUATION</head><label>III</label><figDesc>ON THE GRID CORPUS WITH DIFFERENT ROI CHOICES. A 5-GRAM, CHARACTER-LEVEL LANGUAGE MODEL IS USED DURING BEAM SEARCH. "CER" STANDS FOR CHARACTER ERROR RATE. THE LOWER THE ERROR RATES, THE BETTER.</figDesc><table><row><cell>Region</cell><cell>Resolution</cell><cell>WER</cell><cell>CER</cell><cell>Description</cell></row><row><cell>Mouth</cell><cell>100 × 50</cell><cell cols="2">4.8%[4] 1.9%</cell><cell>Affine warped</cell></row><row><cell>Mouth</cell><cell>100 × 50</cell><cell>4.7%</cell><cell>1.9%</cell><cell>Above (reproduced)</cell></row><row><cell>Face</cell><cell>100 × 100</cell><cell>3.1%</cell><cell cols="2">1.3% Aligned with eye &amp; nose landmarks</cell></row><row><cell>Upper face</cell><cell>100 × 50</cell><cell>14.4%</cell><cell>7.4%</cell><cell>Upper half of above</cell></row><row><cell>Cheeks</cell><cell>100 × 50</cell><cell>6.8%</cell><cell>3.1%</cell><cell>Resized 36 × 100 crop from above</cell></row><row><cell>Face (Cutout)</cell><cell>100 × 100</cell><cell>2.9%</cell><cell>1.2%</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV TOP</head><label>IV</label><figDesc>PREDICTED WORDS, WORST PREDICTED WORDS AND PAIRS EXHIBITING HIGHEST CONFUSION IN LRW UNDER EACH CROP SETTING.</figDesc><table><row><cell></cell><cell cols="2">Accuracy / Network</cell><cell></cell><cell></cell><cell cols="2">Accuracy / Network</cell><cell></cell></row><row><cell>Mouth</cell><cell>Face</cell><cell>Cheeks</cell><cell>Upper Face</cell><cell>Mouth</cell><cell>Face</cell><cell>Cheeks</cell><cell>Upper Face</cell></row><row><cell>AGREEMENT (1)</cell><cell>ACCUSED (1)</cell><cell>AFTERNOON (0.98)</cell><cell>WESTMINSTER (0.96)</cell><cell>ASKED (0.58)</cell><cell>WORLD (0.6)</cell><cell>REALLY (0.32)</cell><cell>GREAT (0.18)</cell></row><row><cell cols="2">ALLEGATIONS (1) AGREEMENT (1)</cell><cell>WEEKEND (0.98)</cell><cell>TEMPERATURES (0.92)</cell><cell>BRITAIN (0.58)</cell><cell>ANSWER (0.58)</cell><cell>COULD (0.3)</cell><cell>OTHER (0.18)</cell></row><row><cell>BEFORE (1)</cell><cell>BEFORE (1)</cell><cell>WELFARE (0.98)</cell><cell>AFTERNOON (0.88)</cell><cell cols="2">MATTER (0.58) BECAUSE (0.58)</cell><cell>GETTING (0.3)</cell><cell>UNTIL (0.18)</cell></row><row><cell>PERHAPS (1)</cell><cell>CAMPAIGN (1)</cell><cell>WESTMINSTER (0.98)</cell><cell>SUNSHINE (0.88)</cell><cell>SPEND (0.58)</cell><cell>COURT (0.58)</cell><cell>MAKES (0.3)</cell><cell>WHICH (0.18)</cell></row><row><cell>PRIME (1)</cell><cell>FOLLOWING (1)</cell><cell>INFORMATION (0.96)</cell><cell>DESCRIBED (0.86)</cell><cell>TAKEN (0.58)</cell><cell>PERSON (0.58)</cell><cell>MATTER (0.3)</cell><cell>BRING (0.16)</cell></row><row><cell></cell><cell></cell><cell cols="3">Target / Estimated (# Errors) / Network</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mouth</cell><cell></cell><cell>Face</cell><cell cols="2">Cheeks</cell><cell></cell><cell>Upper Face</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Ablation results on LRW with different Cutout patch sizes. We achieve best validation accuracy with patches half the size of the input.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell></row><row><cell cols="5">Compared to vanilla mouth and face based models which easily achieve</cell></row><row><cell cols="5">nearly 100% accuracy on the training set, Cutout (denoted by red curves)</cell></row><row><cell cols="3">significantly reduces overfitting.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>86.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>82.5 83.0 83.5 84.0 84.5 85.0 85.5</cell><cell>W /4</cell><cell>3W /8</cell><cell>W /2 Baseline (test) 5W /8 Val Test Baseline (val)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cutout patch size</cell></row><row><cell cols="5">Fig. 4. 42, and 28×28, which are 5/8, 1/2, 3/8, and 1/4 the scale of</cell></row><row><cell cols="5">the whole face, respectively. Experiments on LRW show that</cell></row><row><cell cols="5">among those a 56 × 56 patch is most effective (see</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>W.R.T POSE ON LRW-1000</figDesc><table><row><cell>Methods</cell><cell>Easy</cell><cell>Medium</cell><cell>Hard</cell><cell>All</cell></row><row><cell cols="2">Mouth (ResNet34) [35] 24.89%</cell><cell>20.76%</cell><cell>15.9%</cell><cell>38.19%</cell></row><row><cell>Mouth</cell><cell>25.95%</cell><cell>21.55%</cell><cell cols="2">18.36% 38.64%</cell></row><row><cell>Face</cell><cell>28.87%</cell><cell>28.45%</cell><cell>27.21%</cell><cell>41.71%</cell></row><row><cell>Face (Cutout)</cell><cell>31.74%</cell><cell>30.04%</cell><cell>26.89%</cell><cell>45.24%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For now, we train and evaluate our models with the original word alignment and landmark annotations in<ref type="bibr" target="#b35">[35]</ref> for fair comparison. Note that the original paper used ResNet-34, while this work uses ResNet-18 which is currently more popular due to fewer parameters and better performance.<ref type="bibr" target="#b1">2</ref> One speaker's data is unavailable due to technical reasons.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>We would like to thank Chenhao Wang and Mingshuang Luo's extensive help with data processing. This work is supported in part by the National Key R&amp;D Program of China (No. 2017YFA0700804), and Natural Science Foundation of China (No. 61702486, 61876171).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A corpus of audio-visual lombard speech with frontal and profile views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maddock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OuluVS2: A multiview audiovisual database for non-rigid mouth motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">LipNet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>abs/1611.01599</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resolution limits on visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-27" />
			<biblScope unit="page" from="1371" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Which components of the face do humans and machines best speechread?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guiard-Marigny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adjoudani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speechreading by humans and machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="315" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Seeing pitch: Visual information for lexical tones of mandarin-chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2356" to="2366" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-04" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2016 -13th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part II</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lip reading in profile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prosody off the top of the head: Prosodic contrasts can be discriminated by head motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Audio-visual speech perception off the top of the head</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="31" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The impact of reduced video quality on visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dungan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karaali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-07" />
			<biblScope unit="page" from="2560" to="2564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno>112:1-112:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring ROI size in deep learning based lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koumparoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Auditory-Visual Speech Processing</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word identification and eye fixation locations in visual and visual-plus-auditory presentations of spoken sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Mcconkie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="536" to="552" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Patch-gated CNN for occlusionaware facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="2209" to="2214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust remote heart rate estimation from face utilizing spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition, FG 2019</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-15" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>South Brisbane, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seetatech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seetaface2</surname></persName>
		</author>
		<idno>2019. 4</idno>
		<ptr target="https://github.com/seetafaceengine/SeetaFace2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optical flow based lip reading using non rectangular ROI and head motion reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saitoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-04" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of audiovisual word recognition using residual networks and LSTMs. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining residual networks with LSTMs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-20" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visual contribution to speech intelligibility in noise. The journal of the acoustical society of america</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Sumby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pollack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="212" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lip-Interact: Improving mobile device interaction with silent speech commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 31st</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-14" />
			<biblScope unit="page" from="581" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Eye movement of perceivers during audiovisualspeech perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vatikiotis-Bateson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-M</forename><surname>Eigsti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Munhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="940" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Do the eyes really have it? dynamic allocation of attention when viewing moving faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Võ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3" to="3" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="7216" to="7223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LCANet: End-to-end lipreading with cascaded attention-ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cassimatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition, FG 2018</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition, FG 2019</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lipreading with local spatiotemporal descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A cascade sequence-to-sequence model for chinese mandarin lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMAsia &apos;19: ACM Multimedia Asia</title>
		<editor>C. Xu, M. S. Kankanhalli, K. Aizawa, S. Jiang, R. Zimmermann, and W. Cheng</editor>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
