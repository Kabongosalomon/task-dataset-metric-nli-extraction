<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling Matters in Deep Embedding Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
							<email>cywu@cs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J Smola</forename><surname>Amazon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">R. Manmatha A9/Amazon</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling Matters in Deep Embedding Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Models that transform images into rich, semantic representations lie at the heart of modern computer vision, with applications ranging from zero-shot learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41]</ref> and visual search <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>, to face recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> or fine-grained retrieval <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Deep networks trained to respect pairwise relationships have emerged as the most successful embedding models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>The core idea of deep embedding learning is simple: pull similar images closer in embedding space and push dissimilar images apart. For example, the contrastive loss <ref type="bibr" target="#b10">[11]</ref> forces all positives images to be close, while all negatives should be separated by a certain fixed distance. However, using the same fixed distance for all images can be quite restrictive, discouraging any distortions in the embedding space. This motivated the triplet loss, which only requires negative images to be farther away than any positive images on a per-example basis <ref type="bibr" target="#b24">[25]</ref>. This triplet loss is currently * Part of this work performed while interning at Amazon. among the best-performing losses on standard embedding tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref>. Unlike pairwise losses, the triplet loss does not just change the loss function in isolation, it changes the way positive and negative example are selected. This provides us with two knobs to turn: the loss and the sampling strategy. See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration.</p><p>In this paper, we show that sample selection in embedding learning plays an equal or more important role than the loss. For example, different sampling strategies lead to drastically different solutions for the same loss function. At the same time many different loss functions perform similarly under a good sampling strategy: A contrastive loss works almost as well as the triplet loss, if the two use the same sampling strategy. In this paper, we analyze existing sampling strategies, and show why they work and why not. We then propose a new sampling strategy, where samples are drawn uniformly according to their relative distance from one another. This corrects the bias induced by the geometry of embedding space, while at the same time ensuring any data point has a chance of being sampled. Our proposed sampling leads to a lower variance of gradients, and thus stabilizes training, resulting in a qualitatively better embedding irrespective of the loss function.</p><p>Loss functions obviously also matter. We propose a simple margin-based loss as an extension to the contrastive loss. It only encourages all positive samples to be within a distance of each other rather than being as close as possible. It relaxes the loss, making it more robust. In addition, by using isotonic regression, our margin based loss focuses on the relative orders instead of absolute distances.</p><p>Our margin based loss and distance weighted sampling achieve state-of-the-art image retrieval and clustering performance on the Stanford Online Products, CARS196, and the CUB200-2011 datasets. It also outperforms previous state-of-the-art results on the LFW face verification dataset <ref type="bibr" target="#b15">[16]</ref> using standard publicly available training data. Both our loss function and sampling strategy are easy to implement and efficient to train. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The idea of using neural networks to extract features that respect certain relationships dates back to the 90s. Siamese Networks <ref type="bibr" target="#b3">[4]</ref> find an embedding space such that similar examples have similar embeddings and vice versa. Such networks are trained end-to-end, sharing weights between all mappings. Siamese Networks were first applied to signature verification, and later extended to face verification and dimensionality reduction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, given the limited compute power at the time and their non-convex nature, these approaches initially did not enjoy much attention. Convex approaches were much more popular <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>. For example, the triplet loss <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref> is one of the most prominent methods that emerged from convex optimization.</p><p>Given sufficient data and computational power both schools of thought were combined into a Siamese architecture using triplet losses. This leads to near human performance in face verification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. Motivated by the triplet loss, some enforce constraints on even more examples. For example, PDDM <ref type="bibr" target="#b14">[15]</ref> and Histogram Loss <ref type="bibr" target="#b33">[34]</ref> use quadruplets. Beyond that, the n-pair loss <ref type="bibr" target="#b27">[28]</ref> and Lifted Structure <ref type="bibr" target="#b21">[22]</ref> defines constraints on all images in a batch.</p><p>This plethora of loss functions is quite reminiscent of the ranking problem in information retrieval. There a combination of individual, pair-wise <ref type="bibr" target="#b13">[14]</ref>, and list-wise approaches <ref type="bibr" target="#b34">[35]</ref> are used to maximize relevance. Of note is isotonic regression which disentangles the pairwise comparisons for greater computational efficiency. See <ref type="bibr" target="#b20">[21]</ref> for an overview.</p><p>Some papers explore modeling of other properties. Structural Clustering <ref type="bibr" target="#b28">[29]</ref> optimizes for clustering quality. PDDM <ref type="bibr" target="#b14">[15]</ref> proposes a new module to model local feature structure. HDC <ref type="bibr" target="#b40">[41]</ref> trains an ensemble to model examples of different "hard levels". In contrast, here we show that a simple pairwise loss is sufficient if paired with the right sampling strategy.</p><p>Example selection techniques are relatively less studied. For the contrastive loss it is common to select from all posi-ble pairs at random <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>, and sometimes with hard negative mining <ref type="bibr" target="#b26">[27]</ref>. For the triplet loss, semi-hard negative mining, first used in FaceNet <ref type="bibr" target="#b24">[25]</ref>, is widely adopted <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Sampling has been studied for stochastic optimization <ref type="bibr" target="#b42">[43]</ref> with the goal of accelerating convergence to the same global loss function. In contrast, in embedding learning the sampling actually changes the overall loss function considered. In this paper we show how sampling affects the real-world performance of deep embedding learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Let f (x i ) be an embedding of a datapoint x i ∈ R N , where f : R N → R D is a differentiable deep network with parameters Θ. Often f (x i ) is normalized to have unit length for training stability <ref type="bibr" target="#b24">[25]</ref>. Our goal is to learn an embedding that keeps similar data points close, while pushing dissimilar datapoints apart. Formally we define the distance between two datapoints as</p><formula xml:id="formula_0">D ij := f (x i ) − f (x j ) , where</formula><p>· denotes the Euclidean norm. For any positive pair of datapoints y ij = 1 this distance should be small, and for negative pair y ij = 0 it should be large.</p><p>The contrastive loss directly optimizes this distance by encouraging all positive distances to approach 0, while keeping negative distances above a certain threshold:</p><formula xml:id="formula_1">contrast (i, j) := y ij D 2 ij + (1 − y ij ) [α − D ij ] 2 +</formula><p>. One drawback of the contrastive loss is that we have to select a constant margin α for all pairs of negative samples. This implies that visually diverse classes are embedded in the same small space as visually similar ones. The embedding space does not allow for distortions.</p><p>In contrast the triplet loss merely tries to keep all positives closer to any negatives for each example: triplet (a, p, n) := D 2 ap − D 2 an + α + . This formulation allows the embedding space to be arbitrarily distorted and does not impose a constant margin α.</p><p>From the risk minimization perspective, one might aim at optimizing the aggregate loss over all O(n 2 ) pairs or O(n 3 ) triples respectively. That is</p><formula xml:id="formula_2">R (·) := t∈{all pairs/triplets} (·) (t).</formula><p>This is computationally infeasible. Moreover, once the network converges, most samples contribute in a minor way as very few of the negative margins are violated.</p><p>This lead to the emergence of many heuristics to accelerate convergence. For the contrastive loss, hard negative mining usually offers faster convergence. For the triplet loss, it is less obvious, as hard negative mining often leads to collapsed models, i.e. all images have the same embedding. FaceNet <ref type="bibr" target="#b24">[25]</ref> thus proposed to use a somewhat mysterious semi-hard negative mining: given an anchor a and a positive example p, obtain a negative instance n via n ap := argmin n:D(a,n)&gt;D(a,p)</p><p>D an , within a batch. This yields a violating example that is fairly hard but not too hard. Batch construction also matters. In order to obtain more informative triplets, FaceNet uses a batch size of 1800 and ensures that each identity has roughly 40 images in a batch <ref type="bibr" target="#b24">[25]</ref>. Even how to best select triplets within a batch is unclear. Parkhi et al. <ref type="bibr" target="#b22">[23]</ref> use online selection, so that only one triplet is sampled for every (a, p) pair. OpenFace <ref type="bibr" target="#b1">[2]</ref> employs offline triplet selection, so that a batch has 1 /3 of images as anchors, positives, and negatives respectively. In short, sampling matters. It implicitly defines a rather heuristic objective function by weighting samples. Such an approach makes it hard to reproduce and extend the insights to different datasets, different optimization frameworks or different architectures. In the next section, we analyze some of these techniques, and explain why they offer better results. We then propose a new sampling strategy that outperforms current state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distance Weighted Margin-Based Loss</head><p>To understand what happens when sampling negative uniformly, recall that our embeddings are typically constrained to the n-dimensional unit sphere S n−1 for large n ≥ 128. Consider the situation where the points are uniformly distributed on the sphere. In this case, the distribution of pairwise distances follows</p><formula xml:id="formula_3">q (d) ∝ d n−2 1 − 1 4 d 2 n−3 2 .</formula><p>See <ref type="bibr" target="#b0">[1]</ref> for a derivation. <ref type="figure" target="#fig_1">Figure 2</ref> shows concentration of measure occurring. In fact, in high dimensional space, q(d) approaches N ( √ 2, 1 2n ). In other words, if negative examples are scattered uniformly, and we sample them randomly, we are likely to obtain examples that are √ 2-away. For thresholds less than √ 2, this induces no loss, and thus no progress for learning. Learned embeddings follow a very similar distribution, and thus the same reasoning applies. See supplementary material for details.</p><p>Sampling negative examples that are too hard causes a different issue. Consider a negative pair t := (a, n) or a triplet t := (a, p, n). The gradient with respect to the nega-</p><formula xml:id="formula_4">tive example f (x n ) is in the form of ∂ f (xn) (·) = h an h an w(t)</formula><p>for some function w(·) and h an := f (x a ) − f (x n ). Note that the first term han han determines the direction of the gradient. A problem arises when h an is small, and our estimates of embedding are noisy. Given enough noise z introduced by the training algorithm, direction han+z han+z is dominated by noise. <ref type="figure" target="#fig_4">Figure 3a</ref> shows the nuclear norm of the covariance matrix for the direction of gradient with z ∼ N 0, σ 2 I . We can see that when negative examples are too close/hard, the gradient has high variance and it has low signal to noise ratio. At the same time random samples are often too far apart to yield a good signal.</p><p>Distance weighted sampling. We thus propose a new sampling distribution that corrects the bias while controlling the variance. Specifically, we sample uniformly according to distance, i.e. sampling with weights q(d) −1 . This gives us examples which are spread out instead of being clustered around a small region. To avoid noisy samples, we clip the weighted sampling. Formally, given an anchor example a, distance weighted sampling samples negative pair (a, n ) with Pr (n = n|a) ∝ min λ, q −1 (D an ) .   collapsed model. Random sampling yields only easy examples that induce no loss. Semi-hard negative mining finds a narrow set in between. While it might converge quickly at the beginning, at some point no examples are left within the band, and the network will stop making progress. FaceNet reports a consistent finding: the decrease of loss slows down drastically after some point, and their final system took 80 days to train <ref type="bibr" target="#b24">[25]</ref>. Distance weighted sampling offers a wide range of examples, and thus steadily produce informative examples while controlling the variance. In Section 5, we will see that distance weighted sampling brings performance improvements in almost all loss functions tested. Of course sampling only solves half of the problem, but it puts us in a position to analyze various loss functions. <ref type="figure" target="#fig_5">Figure 4a</ref> and <ref type="figure" target="#fig_5">Figure 4b</ref> depict the contrastive loss and the triplet loss. There are two key differences, which in general explain why the triplet loss outperforms contrastive loss: The triplet loss does not assume a predefined threshold to separate similar and dissimilar images. Instead, it enjoys the flexibility to distort the space to tolerate outliers, and to adapt to different levels of intra-class variance for different classes. Second, the triplet loss only requires positive examples to be closer than negative examples, while the contrastive loss spends efforts on gathering all positive examples as close together as possible. The latter is not necessary. After all, maintaining correct relative relationship is sufficient for most applications, including image retrieval, clustering, and verification.</p><p>On the other hand, in <ref type="figure" target="#fig_5">Figure 4b</ref> we also observe the concave shape of the loss function for negative examples in the triplet loss. In particular, note that for hard negatives (with small D an ), the gradient with respective to negative example is approaching zero. It is not hard to see why hard negative mining results in a collapsed model in this case: it gives large attracting gradients from hard positive pairs, but small repelling gradients from hard negative pairs, so all points are eventually gathered to the same point. To make the loss stable for examples from all distances, one simple remedy is to use 2 instead of 2 2 , i.e. triplet, 2 := (D ap − D an + α) + . <ref type="figure" target="#fig_5">Figure 4c</ref> presents the loss function. Now its gradients with respect to any embedding f (x) will always have length one. See e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref> for more discussions about the benefits of using gradients of a fixed length. This simple fix together with distance weighted sampling already outperforms the traditional 2 2 triplet loss, as shown in Section 5.</p><p>Margin based loss. These observations motivate our design of a loss function which enjoys the flexibility of the triplet loss, has a shape suitable for examples from all distances, while offering the computational efficiency of a contrastive loss. The basic idea can be traced back to the insight that in ordinal regression only the relative order of scores matters <ref type="bibr" target="#b16">[17]</ref>. That is, we only need to know the crossover between both sets. Isotonic regression exploits this by estimating such a threshold separately and then penalizes scores relative to the threshold. We use the same trick, now applied to pairwise distances rather than score functions. The adaptive margin based loss is defined as</p><formula xml:id="formula_5">margin (i, j) := (α + y ij (D ij − β)) + .</formula><p>Here β is a variable that determines the boundary between positive and negative pairs, α controls the margin of separation, and y ij ∈ {−1, 1}. large margin loss on the shifted distance D ij − β. This loss is very similar to a support vector classifier (SVC) <ref type="bibr" target="#b7">[8]</ref>.</p><p>To enjoy the flexibility as a triplet loss, we need a more flexible boundary parameter β which depends on classspecific β (class) and example-specific β (img) terms.</p><formula xml:id="formula_6">β(i) := β (0) + β (class) c(i) + β (img) i</formula><p>In particular, the example-specific offset β s. Instead, we would like to jointly learn these parameters. Fortunately, the gradient of β can be easily calculated as</p><formula xml:id="formula_7">∂ β margin (i, j) = −y ij 1 {α &gt; y ij (β − D ij )}</formula><p>It is clear that larger values of β are more desirable, since they amount to a better use of the embedding space. Hence, to regularize β, we incorporate a hyperparameter ν, and it leads to the optimization problem</p><formula xml:id="formula_8">minimize (i,j) margin (i, j) + ν β (0) + β (class) c(i) + β (img) i</formula><p>Here ν adjusts the difference between the number of points that violate the margin on the left and on the right. This can be seen by observing that their gradients need to cancel out at an optimal β. Note that the use of ν here is very similar to the ν-trick in ν-SVM <ref type="bibr" target="#b23">[24]</ref>.</p><p>Relationship to isotonic regression. Optimizing the margin based loss can be viewed as solving a ranking problem for distances. Technically it shares similarity with learningto-rank problems in information retrieval <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref>. To see this first note at optimal β, the empirical risk can be written as</p><formula xml:id="formula_9">R margin := min β (i,j) (α + y ij (D ij − β)) + .</formula><p>One can show that R margin = (i,j) ξ * ij , where ξ * s are the solution to</p><formula xml:id="formula_10">minimize (i,j)∈X pos ξ ij + (k,l)∈X neg ξ kl subject to D kl + ξ kl − D ij + ξ ij ≥ 2α, (i, j) ∈ X pos , (k, l) ∈ X neg ξ ij , ξ kl ≥ 0,</formula><p>where X pos := {(i, j) : y ij = 1}, and X neg := {(i, j) : y ij = −1}. This is an isotonic regression defined on absolute error. We see that the margin based loss is the amount of "minimum-effort" updates to maintain relative orders. It focuses on the relative relationships, i.e. focusing on the separation of positive-pair distances and the negativepair distances. This is in contrast to traditional loss functions such as the contrastive loss, where losses are defined relative to a predefined threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our method on image retrieval, clustering and verification. For image retrieval and clustering, we use the Stanford Online Products <ref type="bibr" target="#b21">[22]</ref>, CARS196 <ref type="bibr" target="#b18">[19]</ref>, and the CUB200-2011 <ref type="bibr" target="#b36">[37]</ref> datasets, following the experimental setup of Song et al. <ref type="bibr" target="#b21">[22]</ref>. The Stanford Online Product dataset contains 120,053 images of 22,634 categories. The first 11,318 categories are used for training, and the remaining are used for testing. The CARS196 dataset contains 16,185 car images of 196 models. We use the first 98 models for training, and the remaining for testing. The CUB200-2011 dataset contains 11,788 bird images of 200 species. The first 100 species are used for training, the remainder for testing.</p><p>We evaluate the quality of image retrieval based on the standard Recall@k metric, following Song et al. <ref type="bibr" target="#b21">[22]</ref>. We use NMI score, I(Ω,C) / √ H(Ω)H(C), to evaluate the quality of clustering alignments C = {c 1 , . . . , c n }, given a groundtruth clustering Ω = {ω 1 , . . . , ω n }. Here I(·, ·) and H(·) denotes mutual information and entropy respectively. We use K-means algorithm for clustering.</p><p>For verification, we train our model on the largest publicly available face dataset, CASIA-WebFace <ref type="bibr" target="#b39">[40]</ref>, and evaluate on the standard LFW <ref type="bibr" target="#b15">[16]</ref> dataset. The VGG face dataset <ref type="bibr" target="#b22">[23]</ref> is bigger, but many of its links have expired. The CASIA-WebFace dataset contains 494,414 images of 10,575 people. The LFW dataset consists of 13,233 images of 5,749 people. Its verification benchmark contains 6,000 verification pairs, split into 10 subsets. We select the verification threshold for one split based on the remaining nine splits.</p><p>Unless stated otherwise, we use an embedding size of 128 and an input image size of 224 × 224 in all experiments. All models are trained using Adam <ref type="bibr" target="#b17">[18]</ref> with a batch size of 200 for face verification, 80 for Stanford Online Products, and 128 for other experiments. The network architecture follows ResNet-50 (pre-activation) <ref type="bibr" target="#b12">[13]</ref>. To accelerate training, we use a simplified version of ResNet-50 in the face verification experiments. Specifically, we use only 64, 96, 192, 384, 768 filters in the 5 stages respectively, instead of the originally proposed 64, 256, 512, 1024, 2048 filters. We did not observe any obvious performance degradations due to the change. Horizontal mirroring and random crops from 256 × 256 are used for data augmentation. During testing we use a single center crop. Face images are aligned by MTCNN <ref type="bibr" target="#b41">[42]</ref>. When alignment fails, we use a center crop. Following FaceNet <ref type="bibr" target="#b24">[25]</ref>, we use α = 0.2, and for the margin based loss we initialize β (0) = 1.2 and β (class) = β (img) = 0.</p><p>Note that some previous papers use the provided bounding boxes while others do not. To fairly compare with previous methods, we evaluate our methods on both the original images and the ones cropped by bounding boxes. For the CARS196 dataset we scale the cropped images to 256×256. For CUB200, we scale and pad the images such that their longer side is 256 pixels, keeping the aspect ratio fixed.</p><p>Our batch construction follows FaceNet <ref type="bibr" target="#b24">[25]</ref>. We use m = 5 positive images per class in a batch. All positive pairs within a batch are sampled. For each example in a positive pair, we sample one negative pair. This ensures that the number of positive and negative pairs are balanced, and every example belongs to the same number of positive pairs and the same number of negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation study</head><p>We start by understanding the effect of the loss function, the adaptive margin and the specific functional choice. We focus on Stanford Online Products, as it is the largest among the three image retrieval datasets. Note that image retrieval favors triplet losses over contrastive losses, since only relative relationships matter. Here all models are trained from scratch. Since different methods converge at different rates, we train all methods for 100 epochs, and report the performance at their best epoch rather than at the end of training. We compare random sampling and semi-hard negative mining to our distance weighted sampling. For semi-hard sampling, there is no natural choice of a distance lower bound for pairwise loss functions. In this experiment we use a lower bound of 0.5 to simulate the positive distance in triplet loss. We consider the contrastive loss, the triplet loss and our margin based loss. By random sampling, we refer to uniform sampling from all positive and negative pairs. Since such a definition is not applicable for triplet losses, we test only the contrastive and margin based losses.</p><p>Results are presented in <ref type="table">Table 1</ref>. We see that given the same loss function, different sampling distributions lead to very different performance. In particular, while the contrastive loss yields considerably worse results than triplet loss with random sampling, its performance significantly improves when using a sampling procedure similar to triplet loss. This evidence disproves a common misunderstanding of contrastive loss vs. triplet loss: the strength of triplet loss comes not just from the loss function itself, but more importantly from the accompanying sampling methods. In addition, distance weighted sampling consistently offers a performance boost for almost all loss functions. The only exception is the contrastive loss. We found it to be very sensitive to its hyperparameters. While we found good hyperparameters for random and semi-hard sampling, we were not able to find a well-performing hyperparameter for the distance weighted sampling yet. On the other hand, margin based loss automatically learns a suitable offset β and trains well. Notably, the margin based loss outperforms other loss functions by a large margin irrespective of sampling strategies. These observations hold with multiple batch sizes, as shown in <ref type="table" target="#tab_2">Table 2</ref>. We also try pre-training our model using ILSVRC 2012-CLS <ref type="bibr" target="#b8">[9]</ref> dataset, as is commonly done in   prior work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. Pre-training offers a 10% boost in recall.</p><p>In the following sections we focus on pre-trained models for fair comparison. Next, we qualitatively evaluate these methods. <ref type="figure" target="#fig_7">Figure 5</ref> presents the retrieval results on randomly picked query images. We can see that triplet loss generally offers reasonable results, but makes mistakes in some cases. On the other hand, our method gives much more accurate results.</p><p>To evaluate the gains obtained by learning a flexible boundary β, we compare models using a fixed β to models using learned βs. The results are summarized in <ref type="table" target="#tab_4">Table 3</ref>. We see that the use of more flexibly class-specific β (class) indeed offers advantages over various values of fixed β (0) . We also test using example-specific β (img) , but the experiments are inconclusive. We conjecture that learning example-specific β (img) might have introduced too many parameters and caused over-fitting.</p><p>Convergence speed. We further analyze the effects of sampling on the convergence speed. We compare margin based loss using distance weighted sampling with the two most commonly used deep embedding approaches: triplet   loss with semi-hard sampling and contrastive loss with random sampling. The learning curves are shown in <ref type="figure" target="#fig_8">Figure 6</ref>. We see that triplet loss trained with semi-hard negative mining converges slower as it ignores too many examples. Contrastive loss with random sampling converges even slower. Distance weighted sampling, which uses more informative and stable examples, converges faster and more accurately.</p><p>Time complexity of sampling The computational cost of sampling is negligible. On a Tesla P100 GPU, forward and backward pass take about 0.55 second per batch (size 120). Sampling takes only 0.00031 second with semi-hard sampling and 0.0043 second with distance weighted sampling, even with our single-thread CPU implementation. Both strategies take O (nm(n − m)), where n is the batch size, and m is the number of images per class in a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Results</head><p>We now compare our approach to other state-of-the-art methods. Image retrieval and clustering results are summarized in <ref type="table" target="#tab_6">Table 4</ref>, 5 and 6. We can see that our model achieves the best performance in all three datasets. In particular, margin based loss outperforms extensions of triplet loss, such as LiftedStruct <ref type="bibr" target="#b21">[22]</ref>, StructClustering <ref type="bibr" target="#b28">[29]</ref>, N-pairs <ref type="bibr" target="#b27">[28]</ref>, and PDDM <ref type="bibr" target="#b14">[15]</ref>. It also outperforms histogram loss <ref type="bibr" target="#b33">[34]</ref>, which requires computing similarity histograms. Also note that k    <ref type="table">Table 6</ref>: Recall@k and NMI on CUB200-2011 <ref type="bibr" target="#b36">[37]</ref>.</p><p>our model uses only one 128-dimensional embedding for each image. This is much more concise and simpler than HDC <ref type="bibr" target="#b40">[41]</ref>, which uses 3 embedding vectors for each image. <ref type="table" target="#tab_9">Table 7</ref> presents results for face verification. Our model achieves the best accuracy among all models trained on CASIA-WebFace. Also note that here our method outperforms models using a wide range of training procedures. MFM <ref type="bibr" target="#b37">[38]</ref> use a softmax classification loss. CASIA <ref type="bibr" target="#b39">[40]</ref> use a combination of softmax loss and contrastive loss. N-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We demonstrated that sampling matters as much or more than loss functions in deep embedding learning. This should not come as a surprise, since the implicitly defined loss function is (quite obviously) a sample weighted object.</p><p>Our new distance weighted sampling yields a performance improvement for multiple loss functions. In addition, we analyze and provide a simple margin-based loss that relaxes unnecessary constraints from traditional contrastive loss and enjoys the flexibility of the triplet loss. We show that distance weighted sampling and the margin based loss significantly outperform all other loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Empirical pairwise-distance distributions</head><p>To better understand the effects of distance weighted sampling during training, we analyze our learned embeddings. Specifically, we compute empirical pairwise distance distributions for negative pairs based on the embeddings of testing images. <ref type="figure">Figure 7</ref> presents the results on Stanford Online Product dataset. We see that after the first epoch, the distribution already forms a bell shape, and in later epochs, it gradually concentrates. This justifies our motivation of using distance weighted sampling so that examples from all distances have a chance to be sampled.  <ref type="figure">Figure 7</ref>: Empirical pairwise-distance distributions for negative pairs. They roughly follow a bell-shaped curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Stability analysis</head><p>Here we measure the stability of different loss functions when using different batch construction. Specifically, we change the number of images m per class in a batch and see how it impacts the solutions. For this purpose, we experiment with face verification and use the optimal verification boundary on the validation set as a summary of the solution. The results are summarized in <ref type="figure" target="#fig_9">Figure 8</ref>. We see that the triplet loss converges to different solutions when using different batch constructions. In addition, we observe large fluctuations in the early stage, indicating unstable training. On the other hand, the margin based loss is robust, it always converges to the roughly the same geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Ablation study for batch size</head><p>We analyze the sensitivity of our approach with respect to batch sizes. <ref type="table" target="#tab_12">Table 8</ref> presents the results. We see that distance weighted sampling consistently outperforms other sampling strategies, and margin based loss consistently outperforms triplet loss. Loss, batch size @1 @10 @100 @1000  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of deep embedding learning: The first stage samples images and forms a batch. A deep network then transforms the images into embeddings. Finally, a loss function measures the quality of our embedding. Note that both the sampling and the loss function influence the overall training objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Density of datapoints on the D-dimensional unit sphere. Note the concentration of measure as the dimensionality increases -most points are almost equidistant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3b compares the</head><label></label><figDesc>simulated examples drawn from different strategies along with their variance of gradients. Hard negative mining always offers examples in the highvariance region. This leads to noisy gradients that cannot effectively push two examples apart, and consequently a Variance of gradient at different noise levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Semi-hard negative mining Distance weighted sampling (b) Sample distribution for different strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) shows the nuclear norm of a noisy gradient estimate for various levels of noise. High variance means the gradient is close to random, while low variance implies a deterministic gradient estimate. Lower is better. Note that higher noise levels have a lower variance at distance 0. This is due to the spherical projection imposed by the normalization. (b) shows the empirical distribution of samples drawn for different strategies. Distance weighted sampling selects a wide range of samples, while all other approaches are biased towards certain distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4dvisualizes this new loss function. We can see that it relaxes the constraint on positive examples from contrastive loss. It effectively imposes a α (a) Contrastive loss<ref type="bibr" target="#b10">[11]</ref> Dan − α Dap + α Loss vs. pairwise distance. The solid blue lines show the loss function for positive pairs, the dotted green for negative pairs. Our loss finds an optimal boundary β between positive and negative pairs, and α ensures that they are separated by a large margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>role as the threshold in a triple loss. It is infeasible to manually select all the β (class) c s and β (img) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Retrieval results for randomly chosen query images in Stanford Online Products. Our loss retrieves more relevant images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Validation accuracy curve, trained on CASIA-WebFace and evaluated on LFW. The margin based loss with distance weighted sampling converges quickly and stably, outperforming other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Optimal validation threshold for the LFW dataset. Triplet loss with different sampling strategies converges to different solutions. In addition, it has large fluctuations in the early stage, indicating unstable training. Margin based loss always converges stably to the same solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Recall@1 evaluated on Stanford Online Products</cell></row><row><cell cols="3">for various batch sizes (40, 80, 120). Distance weighted</cell></row><row><cell cols="3">sampling consistently outperforms other sampling strate-</cell></row><row><cell cols="3">gies irrespective of the batch size. See supplementary ma-</cell></row><row><cell cols="2">terial for Recall@10, 100, and 1000.</cell><cell></cell></row><row><cell>Query</cell><cell>Triplet (R@1=49.7)</cell><cell>Margin (R@1=61.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Recall@k on Stanford Online Products for margin based loss with fixed and learned β. Results at 8K iterations are reported. The values of learned β</figDesc><table><row><cell>(class) c</cell><cell>range from 0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="7">: Recall@k and NMI on Stanford Online Prod-</cell></row><row><cell>ucts [22].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell cols="2">16 NMI</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Original Images</cell><cell></cell></row><row><cell cols="5">Triplet Semi-hard [25, 29] 51.5 63.8 73.5 82.4</cell><cell>-</cell><cell>53.4</cell></row><row><cell>LiftedStruct [22, 29]</cell><cell cols="4">53.0 65.7 76.0 84.3</cell><cell>-</cell><cell>56.9</cell></row><row><cell>StructClustering [29]</cell><cell cols="4">58.1 70.6 80.3 87.8</cell><cell>-</cell><cell>59.0</cell></row><row><cell>N-pairs [28]</cell><cell cols="4">71.1 79.7 86.5 91.6</cell><cell>-</cell><cell>64.0</cell></row><row><cell>HDC [41]</cell><cell cols="5">73.7 83.2 89.5 93.8 96.7</cell><cell>-</cell></row><row><cell>Margin</cell><cell cols="6">79.6 86.5 91.9 95.1 97.3 69.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cropped Images</cell><cell></cell></row><row><cell>PDDM Triple [15]</cell><cell cols="5">46.4 58.2 70.3 80.1 88.6</cell><cell>-</cell></row><row><cell>PDDM Quadruplet [15]</cell><cell cols="5">57.4 68.6 80.1 89.4 92.3</cell><cell>-</cell></row><row><cell>HDC [41]</cell><cell cols="5">83.8 89.8 93.6 96.2 97.8</cell><cell>-</cell></row><row><cell>Margin</cell><cell cols="6">86.9 92.7 95.6 97.6 98.7 77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Recall@k and NMI on CARS196<ref type="bibr" target="#b18">[19]</ref>.</figDesc><table><row><cell>k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell cols="2">16 NMI</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Original Images</cell><cell></cell></row><row><cell>Histogram [34]</cell><cell cols="5">52.8 64.4 74.7 83.9 90.4</cell><cell>-</cell></row><row><cell cols="6">Binomial Deviance [34] 50.3 61.9 72.6 82.4 88.8</cell><cell>-</cell></row><row><cell>Triplet [25, 29]</cell><cell cols="4">42.6 55.0 66.4 77.2</cell><cell>-</cell><cell>55.4</cell></row><row><cell>LiftedStruct [22, 29]</cell><cell cols="4">43.6 56.6 68.6 79.6</cell><cell>-</cell><cell>56.5</cell></row><row><cell>Clustering [29]</cell><cell cols="4">48.2 61.4 71.8 81.9</cell><cell>-</cell><cell>59.2</cell></row><row><cell>N-pairs [28]</cell><cell cols="4">51.0 63.3 74.3 83.2</cell><cell>-</cell><cell>60.4</cell></row><row><cell>HDC [41]</cell><cell cols="5">53.6 65.7 77.0 85.6 91.5</cell><cell>-</cell></row><row><cell>Margin</cell><cell cols="6">63.6 74.4 83.1 90.0 94.2 69.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cropped Images</cell><cell></cell></row><row><cell>PDDM Triplet [15]</cell><cell cols="5">50.9 62.1 73.2 82.5 91.1</cell><cell>-</cell></row><row><cell cols="6">PDDM Quadruplet [15] 58.3 69.2 79.0 88.4 93.1</cell><cell>-</cell></row><row><cell>HDC [41]</cell><cell cols="5">60.7 72.4 81.9 89.2 93.7</cell><cell>-</cell></row><row><cell>Margin</cell><cell cols="6">63.9 75.3 84.4 90.6 94.8 69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Face verification accuracy on LFW. We directly compare to results trained on CASIA-WebFace, shown in the lower part of the table. Methods shown in the upper part use either more or proprietary data, and are listed purely for reference. pair<ref type="bibr" target="#b27">[28]</ref> use a more costly loss function that is defined on all pairs in a batch. We also list a few other state-of-theart results which are not comparable purely for reference. DeepID2<ref type="bibr" target="#b29">[30]</ref> and DeepID3<ref type="bibr" target="#b30">[31]</ref> use 25 networks on 25 face regions based on positions of facial landmarks. When trained using only one network, their performance degrades significantly. Other models such as FaceNet<ref type="bibr" target="#b24">[25]</ref> and Deep-Face<ref type="bibr" target="#b32">[33]</ref> are trained on huge private datasets.Overall, our model achieves the best results on all datasets among all compared methods. Notably, our method uses the simplest loss function among all -a simple variant of contrastive loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Recall@k evaluated on Stanford Online Products.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Manzil Zaheer for helpful discussions. This work was supported in part by Berkeley Deep-Drive, and an equipment grant from Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The sphere game in n dimensions</title>
		<idno>2017-02-22. 3</idno>
		<ptr target="http://faculty.madisoncollege.edu/alehnen/sphere/hypers.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Openface: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPRAI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Independent component analysis, a new concept? Signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Support-vector networks. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond convexity: Stochastic quasi-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>UMass Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on 3D Representation and Recognition, at ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The power of normalization: Faster evasion of saddle points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">IntervalRank: Isotonic regression with listwise and pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">New support vector algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learnable structured clustering framework for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01213</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">Deepid3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a metric embedding for face recognition using the multibatch method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tadmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">CoFiRank: Collaborative filtering for ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<ptr target="https://github.com/markusweimer/cofirank" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS- TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A lightened cnn for deep face representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05720</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stochastic optimization with importance sampling for regularized loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Query-level learning to rank using isotonic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Allerton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast training of triplet-based deep binary embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
