<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Template Adaptation for Face Verification and Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Crosswhite</surname></persName>
							<email>nate.crosswhite@stresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Systems &amp; Technology Research (STR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Byrne</surname></persName>
							<email>jeffrey.byrne@stresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Systems &amp; Technology Research (STR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Stauffer</surname></persName>
							<email>stauffer@vsandr.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visionary Systems and Research (VSR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Template Adaptation for Face Verification and Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild dataset [1] for imagery and the YouTubeFaces dataset [2] for videos. In contrast, the newly released IJB-A face recognition dataset [3] unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for template-based face verification and identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition performance using deep learning has seen dramatic improvements in recent years. Convolutional networks trained with large datasets of millions of images of thousands of subjects have shown remarkable capability of learning facial representations that are invariant to age, pose, illumination and expression (A-PIE) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. These representations have shown strong performance for recognition of imagery and video in-the-wild in unconstrained datasets, with recent approaches demonstrating capabilities that exceed human performance on the well known Labeled Faces in the Wild dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>The problem of face recognition may be described in terms of face verification and face identification. Face verification involves computing a one-to-one similarity between a probe image and a reference image, to determine if two image observations are of the same subject. In contrast, face identification involves computing a one-to-many similarity between a probe media and a gallery of known subjects in order to determine a probe identity. Face verification is important for access control or re-identification tasks, and face identification is important for watch-list surveillance or forensic search tasks.</p><p>Face recognition performance evaluations have traditionally focused on the problem of face verification. Over the past fifteen years, face datasets have steadily increased in size in terms of number of subjects and images, as well as complexity in terms controlled vs. uncontrolled collection and amount of A-PIE variability <ref type="bibr" target="#b9">[10]</ref>. The Labeled Faces in the Wild dataset <ref type="bibr" target="#b0">[1]</ref> contains 13233 images of 1680 subjects, and compares specific pairs of images of subjects to characterize 1:1 verification performance. Similarly, the YouTubeFaces dataset <ref type="bibr" target="#b1">[2]</ref> contains 3425 videos of 1595 subjects, and compares pairs of videos of subjects for verification. These datasets have set the established standard for face recognition research, with steadily increasing performance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>. Recently, protocols for face identification have been introduced for LFW <ref type="bibr" target="#b11">[12]</ref> to address the performance evaluation for identification on a common dataset. However, the imagery in LFW was constructed with a well known near-frontal selection bias, which means evaluations are not predictive of performance for large in-the-wild pose variation. In fact, recent studies have shown that while algorithm performance for near frontal recognition is equal to or better than humans, performance of automated systems at the extremes of illumination and pose are still well behind human performance <ref type="bibr" target="#b12">[13]</ref>.</p><p>The IJB-A dataset <ref type="bibr" target="#b2">[3]</ref> was created to provide the newest and most challenging dataset for both verification and iden-tification. This dataset includes both imagery and video of subjects manually annotated with facial bounding boxes to avoid the near frontal bias, along with protocols for evaluation of both verification and identification. Furthermore, this dataset performs evaluations over templates <ref type="bibr" target="#b13">[14]</ref> as the smallest unit of representation, instead of image-to-image or video-to-video. A template is a set of all media (images and/or videos) of a subject that are to be combined into a single representation suitable for matching. Template based representations are important for many face recognition tasks, which take advantage of an historical record of observations to further improve performance. For example, a template provides a useful abstraction to capture the mugshot history of a criminal for forensic search in law enforcement, or lifetime enrollment images for visa or driver's licenses in civil identity credentialing for improved access control. Biometric templates have been studied for face recognition, where performance on older algorithms have increased given an historical set of images <ref type="bibr" target="#b13">[14]</ref>. The IJB-A dataset is the only public dataset that enables a controlled evaluation of template-based verification and identification at the extremes of pose, illumination and expression.</p><p>In this paper, we study the problem of template adaptation. Template adaptation is an example of transfer learning, where the target domain is defined by the set of media of a subject in a template. In general, transfer learning includes a source domain for feature encoding of subjects trained offline, and a specific target domain with limited available observations of new subjects. In the case of template adaptation, the source domain may be a deep convolutional network trained offline to predict subject identity, and the target domain is the set of media in templates of never before seen subjects. In this paper, we study perhaps the simplest form of template adaptation based on deep convolutional networks and one-vs-rest linear SVMs. We combine deep CNN features trained offline to predict subject identity, with a simple linear SVM classifier trained at test time using all media in a template as positive features to classify each new subject.</p><p>Extensive evaluation of template adaptation on the IJB-A dataset has generated surprising results. First, template adaptation outperforms all top performing techniques in the literature: convolutional networks combined with triplet loss similarity <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>, joint Bayesian metric learning <ref type="bibr" target="#b15">[16]</ref>, pose specialized networks <ref type="bibr" target="#b16">[17]</ref>, 2D alignment <ref type="bibr" target="#b3">[4]</ref>, 3D frontalization <ref type="bibr" target="#b4">[5]</ref> and novel convolutional network architectures <ref type="bibr" target="#b17">[18]</ref>. Second, template adaptation when combined with these other techniques results in nearly equivalent performance. Third, we show a clear tradeoff between the size of a template (e.g. the number of unique media in the template) and performance, which leads to the conclusion that if the average largest template size is big enough, then a simple template adaptation strategy is the best choice for both verification and identification on template based datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The top performing approaches for face verification on Labeled Faces in the Wild <ref type="bibr" target="#b0">[1]</ref> and YouTubeFaces <ref type="bibr" target="#b1">[2]</ref> are all based on convolutional networks. VGG-Face is the application of the VGG-16 convolutional network architecture <ref type="bibr" target="#b18">[19]</ref> trained on a newly curated dataset of 2.6M images of 2622 subjects. This representation includes triplet loss embedding and 2D alignment for normalization to provide state of the art performance. FaceNet <ref type="bibr" target="#b5">[6]</ref> applied the inception CNN architecture <ref type="bibr" target="#b19">[20]</ref> to the problem of face verification. This approach included metric learning to train a triplet loss embedding to learn a 128 dimensional embedding optimized for verification and clustering. This network was trained using a private dataset of over 200M subjects. Deep-Face <ref type="bibr" target="#b4">[5]</ref>[7] uses a deep network coupled with 3D alignment, to normalize facial pose by warping facial landmarks to a canonical position prior to encoding. DeepID2+ <ref type="bibr" target="#b8">[9]</ref> and DeepID3 <ref type="bibr" target="#b7">[8]</ref> extended the inception architecture to include joint Bayesian metric learning <ref type="bibr" target="#b20">[21]</ref> and multi-task learning for both identification and verification.</p><p>These top performing convolutional network architectures have interesting common properties. First, they all exhibit deep convolutional network structure, often with parallel specialized sub-networks. However, Parkhi et. al <ref type="bibr" target="#b3">[4]</ref> showed that a the VGG-16 very deep architecture <ref type="bibr" target="#b18">[19]</ref>, when trained with a broad and deep dataset containing one thousand examples of 2622 subjects, outperformed networks with specialized networks <ref type="bibr" target="#b5">[6]</ref> and ensembles <ref type="bibr" target="#b7">[8]</ref> on YouTubeFaces. Second, many top performing approaches use some form of pose normalization such as 2D/3D alignment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref> to warp the facial landmarks into a canonical frontal pose. Finally, many approaches use metric learning in the form of triplet loss similarity or joint Bayesian metric learning for the final loss to learn an optimal embedding for verification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16</ref>]. An recent independent study reached a similar conclusion that multiple networks combined in an ensemble and metric learning are crucial for strong performance on LFW <ref type="bibr" target="#b21">[22]</ref>.</p><p>Recent evaluations on IJB-A <ref type="bibr" target="#b2">[3]</ref> are also based on convolutional networks and mirror the top performing approaches on LFW and YouTubeFaces. Recent approaches include deep networks using triplet loss similarity <ref type="bibr" target="#b14">[15]</ref>[23] and joint Bayesian metric learning <ref type="bibr" target="#b15">[16]</ref>, and five pose specialized sub-networks with 3D pose rendering <ref type="bibr" target="#b16">[17]</ref>. Face-BCNN <ref type="bibr" target="#b17">[18]</ref> applies the bilinear CNN architecture to face identification, publishing the earliest results on IJB-A.</p><p>Transfer learning has been well studied in the literature, and we refer to a comprehensive survey on the topic <ref type="bibr" target="#b23">[24]</ref>. Transfer learning and domain adaptation for convolutional networks is typically performed by pretraining the network on a labeled source domain, replacing the final loss layer for <ref type="figure">Figure 1</ref>. Template Adaptation Overview. (left) Without template adaptation, the probe template is about equally similar as shown by the dotted lines to the mated and non-mated templates. (middle) With probe adaptation, a max-margin classifier separates the probe template features from a large negative feature set, which increases the mated template similarity and decreases the non-mated. (right) With gallery adaptation, a max-margin classifier separates each gallery template features from all other gallery templates, which results in desired decrease in similarity between the probe and non-mated template. a new task, then fine-tuning the network on this new objective using data from the target domain <ref type="bibr" target="#b24">[25]</ref>. Prior work has shown that freezing the network, then replacing a softmax loss layer with a linear SVM loss can result in improved performance for classification tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. These approaches can be further improved by jointly training the SVM loss and the CNN parameters, so that the lower level features are fine-tuned with respect to the SVM objective <ref type="bibr" target="#b25">[26]</ref>. However, such retraining requires a large target domain training set to fine tune all parameters in the deep network. In this paper, we focus on updating the linear SVM only, as this classifier has a regularization structure that has been shown to perform well for unbalanced training sets with few positive examples (e.g. from media in a template) and many negative examples <ref type="bibr" target="#b27">[28]</ref> <ref type="bibr" target="#b28">[29]</ref>.</p><p>Finally, we note that the approach of defining a similarity function for face verification using linear SVMs trained on a large negative set was originally proposed as one-shot similarity (OSS) <ref type="bibr" target="#b29">[30]</ref> <ref type="bibr" target="#b30">[31]</ref>. We study the more general form of this original idea, by considering templates of images and videos of varying size, alternative fusion strategies, and the impact of gallery negative sets for identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Template Adaptation</head><p>Template adaptation is a form of transfer learning, combining deep convolutional network features trained on a source domain of many labeled faces, with template specific linear SVMs trained on a target domain using the media in a template. Template adaptation can be further decomposed into probe adaptation for face verification and gallery adaptation for face identification. In this section, we describe these approaches.</p><p>First, we provide preliminary definitions. A media observation x is either a color image of a subject, or a set of m video frames of a subject. An image encoding z = f (x) is a mapping f (x) ∈ R d from an image x to an encoding z with dimensionality d (e.g. fea-tures from a deep CNN). An average encodingz =</p><p>1 m x f (x) is the average of image/frame encodings in a media observation, such as the encodings for all frames in a video. A template X is a set of encoded media ob-</p><formula xml:id="formula_0">servations X = {f (x 1 ), f (x 2 ), . . . , f (x k )} of one sub- ject.</formula><p>The size of a template |X| is defined as the number of unique media used for encoding. Finally, a gallery G = {(X 1 , y 1 ), (X 2 , y 2 ), . . . , (X m , y m )} is a set of tuples of templates X and associated subject identification label y. <ref type="figure">Figure 1</ref> shows an overview of this concept. Each colored shape corresponds to a feature encoding of image or a video feature for the media in a template, such as generated from a convolutional network trained offline. The gray squares correspond to encodings of a large set of media of unique subjects that are very likely to be disjoint from any template. The centroid of the colored shapes corresponds to the average encoding for this template. Probe adaptation is the problem of max-margin classification of the positive features from a template to the large negative feature set. The similarity between the blue probe template and the mated (genuine subject) green template is the margin (dotted lines) of the green feature encodings to the decision surface. Observe that this margin is positive, whereas the margin for the red classifier is negative, so that the blue/green similarity is much larger than blue/red as desired. Gallery adaptation is the problem of max-margin classification where the negative feature set for the gallery templates are defined by the other gallery templates. Observe that adding the magenta subject causes the decision surface for the red and green classifiers to shift, improving the margin score for the probe.</p><p>More formally, probe adaptation is the training of a similarity function s(P, Q) for a probe template P and reference template Q. Train a linear SVM for P , using unit normalized average encodings of media in P as positive features and a large feature set as negatives. The large negative set contains one feature encoding for many subject identities, so this set is very likely to be disjoint with the probe template. Similarly, train a linear SVM for Q, using the unit normalized average encodings for media in Q as positive features and a large feature set as negatives. Finally, let P (q) be notation for evaluating the SVM functional margin (e.g. w T x) trained on P , and evaluated using the unit normalized average media encoding q in template Q. The final similarity score for probe adaptation is the fusion of the two classifier margins using a linear combination s(P, Q) = 1 2 P (q)+ 1 2 Q(p). For implementation details, see section 4.1.</p><p>Gallery adaptation is the training of a similarity function s(P, G) from a probe template P to gallery G. A gallery contains templates G = {X 1 , X 2 , . . . , X m }, and gallery adaptation trains a linear SVM for all pairs s(P, X i ) following the approach for probe adaptation. Gallery adaptation differs from probe adaptation in that the large negative set for a template X i is all unit normalized media encodings from all other templates in G not including X i . In other words, the other non-mated subjects in the gallery are used to construct negative features for X i , whereas the large negative set is used for P . The final similarity score for gallery adaptation is the fusion of the probe classifier and the gallery classifier for each X ∈ G using the linear combination s(P, X) = 1 2 P (x) + 1 2 X(p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The proposed approach in section 3 introduces a number of research questions to study.</p><p>How does this compare to the state of the art? In section 4.2, we compare the template adaptation approach to all published results and show that the proposed approach exceeds the state of the art by a wide margin. Furthermore, in section 4.3 we perform an analysis of alternatives to combine the state of the art techniques with template adaptation and show that when combined, these alternative approaches all result in nearly the same performance.</p><p>How should the negative set be formed? Template adaptation requires training linear SVMs, which require a labeled set of positive and negative feature encodings. In section 4.4, we perform a study to evaluate different strategies of constructing this negative set including using a holdout set, external negative set and combinations. Results show that the gallery based negative set is best for gallery adaptation, and a holdout set derived from the same dataset as the templates is best for verification.</p><p>How large do the templates need to be? In section 4.5, we study the effect of template size, or total number of media in a template, on verification performance to identify the minimum template size necessary, to help guide future template based dataset construction. We show that a minimum of three unique media per template results in diminishing returns for template adaptation. How should template classifier scores be fused? In section 4.6, we study the effect of different strategies for combination of two classifiers, based on winner take all and weighted combinations of on template size. We conclude that an average combination is best with winner take all a close second. What are the error modes of the template adaptation? In section 4.7, we visualize the best and worst templates pairs in IJB-A for verification (identification errors are shown in the supplementary material), and we show that template size (e.g. number of media in a template) has the largest effect on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental System</head><p>We use the VGG-Face deep convolutional neural network <ref type="bibr" target="#b3">[4]</ref>, using the penultimate layer output as the feature encoding f . For computing the average encoding across frames of video, we use face tracks which compute the mean encoding of all frames in a video followed by unit normalization. This approach was shown to be effective for Fisher vector encoding <ref type="bibr" target="#b32">[33]</ref> and deep CNN encoding <ref type="bibr" target="#b3">[4]</ref>.</p><p>Media encoding is preprocessed according to the following pipeline. For each media, we crop each face using the ground truth or detected facial bounding box dilated by a factor of 1.1. Then, we anisotropically rescale this face crop to 224x224x3, such that the aspect ratio is not preserved. This is the assumed input size for the CNN. Next, we encode this face crop for each image or frame in the template using the VGG-face network, and compute average video encodings for each video. Next, we unit normalize each media feature, and train the weights and bias for a linear SVM for each template. We use the LIBLINEAR library with L2regularized L2-loss primal SVM with class weighted hinge loss objective <ref type="bibr" target="#b33">[34]</ref>.</p><formula xml:id="formula_1">min w 1 2 w T w + C p Np i=1 max[0, 1 − y i w T x i ] 2 + C n Nn j=1 max[0, 1 − y j w T x j ] 2<label>(1)</label></formula><p>The loss in (1) includes terms for both positive and negative features, such that C p is the regularization constant for N p positive observations (y i = +1) and C n for negative observations (y i = −1). This formulation of the loss enables data rebalancing for cases where N p &lt;&lt; N n . The positive features in N p are the average media encodings in the template. The negative features are derived from a large negative feature set in N n (either from a large negative set for probe adaptation, or other non-mated templates for gallery adaptation). The parameters C p = C Np+Nn 2Np <ref type="figure">Figure 2</ref>. IJB-A Evaluation. (top) 1:1 DET for verification, 1:N DET for identification and CMC for identification shown for template adaptation and VGG-face <ref type="bibr" target="#b3">[4]</ref>. (bottom) Performance at operating points as compared to published results sorted by rank-1 recall (true positive identification rate or TPIR) for VGG-face <ref type="bibr" target="#b3">[4]</ref>, Bilinear-CNN <ref type="bibr" target="#b17">[18]</ref>, Joint Bayesian <ref type="bibr" target="#b14">[15]</ref>, Triplet Similarity <ref type="bibr" target="#b15">[16]</ref>, Face-Search <ref type="bibr" target="#b31">[32]</ref> and Deep Multipose <ref type="bibr" target="#b16">[17]</ref>. Results show that Template Adaptation sets a new state-of-the-art by a wide margin. and C n = C Np+Nn 2Nn</p><p>adjust the regularization constants to be proportional to the inverse class frequency. The parameter C = 10 in the SVM, trading-off regularizer and loss, was determined using an held-off validation subset of the data. Finally, the learned weights w include a bias term by augmenting x with a constant dimension of one.</p><p>At test time, we evaluate the linear SVMs as described in section 3. We compute the average media encodings for each media in a template, then compute the mean of the media encodings, then unit normalize forming a template encoding. This constructs a single feature for each template. Given two templates P and Q, let the notation P (q) be the evaluation of the functional SVM margin (e.g. P (x) = w T x) for the trained linear SVM for P , given the template encoding q for Q. Finally, the similarity s(P, Q) = 1 2 P (q) + 1 2 Q(p) is a weighted combination of the functional margins for the SVM for P evaluated on template encoding q and Q evaluated on p.</p><p>For baseline comparison, we use the VGG-face network with the output of the 4096d features from the penultimate fully connected layer layer. Media encodings are constructed by averaging features across a video <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4]</ref>, and template encodings are constructed by averaging media encodings over a template, then unit normalizing. Template similarity is equivalent to negative L2 distance over unit normalized template encodings. We also compare results with 2D alignment, triplet similarity embedding and joint Bayesian triplet similarity embedding. For the triplet loss and joint Bayesian metric learning, we use hyperparameter settings such that minibatch = 1800, 1M semi-hard <ref type="bibr" target="#b5">[6]</ref> negative triplets per minibatch, dropconnect regularization <ref type="bibr" target="#b34">[35]</ref>, 3 epochs of Parallel SGD <ref type="bibr" target="#b35">[36]</ref>, fixed learning rate ν = 0.25. For 2D alignment, we use ground truth facial bounding boxes and facial landmark regression <ref type="bibr" target="#b36">[37]</ref>, followed by a robust least squares similarity transform estimation to a reference box to best center the nose.</p><p>For all research studies in sections 4.3 -4.7, we report 1:1 verification ROC curve for all probe and gallery template pairs in IJB-A split 1 and CMC for identification on IJB-A split 1 (see section 4.2 for definitions). This is equivalent to IARPA Janus Challenge Set 2 (CS2) evaluation protocol, which is also reported in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">IJB-A Evaluation</head><p>In this section, we describe the results for evaluation of the experimental system on the IJB-A verification and identification protocols <ref type="bibr" target="#b2">[3]</ref>. IJB-A contains 5712 images and 2085 videos of 500 subjects, for an average of 11.4 images and 4.2 videos per subject. This dataset was manually curated using Mechanical Turk from media-in-the-wild to annotate the facial bounding box and eyes and nose facial landmarks, and this manual annotation avoids the Viola-Jones near-frontal bias. Furthermore, this dataset was curated to control for ethnicity, country of origin and pose biases.</p><p>Metrics for 1:1 verification are evaluated using a decision error tradeoff (DET) curve. The 1:1 DET curve is equivalent to a receiver operating characteristics (ROC) curve, where the true accept rate is one minus the false negative match rate. This evaluation plots the false negative match rate vs. the false match rate as a function of similarity threshold for a given set of pairs of templates for verification.</p><p>Metrics for 1:N identification are the Decision Error Tradeoff (DET) curve and the Cumulative Match Characteristic (CMC) curve. The 1:N DET curve plots the false negative identification rate vs. the false positive identification rate as a function of similarity threshold for a search of L=20 candidate identities in a gallery. The 1:N CMC curve is an information retrieval metric that captures the recall of a specific probe identify within the top-K most similar candidates when searching the gallery. This DET curve is appropriate for limiting the workload for an analyst by allowing for a similarity threshold to be applied to reject false matches even if in the top-K. For detailed description of these metrics, refer to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Performance evaluation for IJB-A requires evaluation of ten random splits of the dataset into training and testing (gallery and probe) sets. The evaluation protocol for 1:1 verification considers specific pairs of mated (genuine) and non-mated (imposter) subjects. The non-mated pairs were chosen to control for gender and skin tone to make the verification problem more challenging. Performance is reported for operating points on each of the curves: 1:1 DET reports false negative match rate at a false match rate of 1e-2, 1:N DET report true positive identification rate (e.g. 1-false negative identification rate) at false positive identification rate of 1e-2, and CMC report true positive identification rate (recall or correct retrieval rate) at rank-one and rank-ten. The 10 splits are used to compute standard deviations for each of these operating points, to characterize statistical significance of the results. <ref type="figure">Figure 2</ref> shows the overall evaluation results on IJB-A. This evaluation compares the baseline approach of VGG-Face only <ref type="bibr" target="#b3">[4]</ref> with the proposed approach of VGG-Face encoding with probe and gallery template adaptation. These results show that identification performance is slightly improved for rank 1 and rank 10 retrieval, however there are large performance improvements for the 1:N DET for identification and the 1:1 DET for verification. The table in <ref type="figure">figure 2</ref> shows performance at specific operating points for verification and identification, and compares to published results in the literature for joint Bayesian metric learning <ref type="bibr" target="#b15">[16]</ref>, triplet similarity embedding <ref type="bibr" target="#b14">[15]</ref>, multi-pose learning <ref type="bibr" target="#b16">[17]</ref>, bilinear CNNs <ref type="bibr" target="#b17">[18]</ref> and very deep CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>. These results show that the proposed template adaptation, while conceptually simple, exhibits state-of-the-art performance by a wide margin on this dataset. <ref type="figure" target="#fig_1">Figure 4</ref> shows an analysis of alternatives study. The state of the art approaches on LFW and YouTubeFaces often augment a very deep CNN encoding with metric learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref> for improved verification scores or 2D alignment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> to better align facial bounding boxes. In this study, we implement triplet loss similarity embedding, joint Bayesian similarity embedding and 2D alignment, and use these alternative feature encodings as input to template adaptation. In this study, we seek to answer whether these alternative strategies will provide improved performance over using CNN encoding only or CNN encoding with template adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of Alternatives</head><p>We report 1:1 DET for all probe and gallery template pairs in IJB-A split 1 and CMC for identification on IJB-A split 1. This study shows that template adaptation on the CNN output provides nearly the same result as template adaptation with metric learning or 2D alignment based features. This implies that the additional training and computational requirements for these approaches are not necessary for template based datasets. Furthermore, this study shows that 2D alignment does not provide much benefit on IJB-A, in contrast with reported performance on near frontal datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. One hypothesis is that this is due to the fact that this dataset has many profile faces for which facial landmark alignment is inaccurate or fails altogether. <ref type="figure" target="#fig_0">Figure 3</ref> shows a negative set study. We study the effect of different combinations of negative feature sets on overall verification performance. Recall that probe and gallery template adaptation require the use of a large negative set for training each linear SVM. This study compares using combinations of features drawn from the non-mated subjects in the gallery (neg) and features drawn from an independent subject disjoint training set (trn). This training set is drawn from the same dataset distribution as the gallery, but is subject disjoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Negative Set Study</head><p>The results in <ref type="figure" target="#fig_0">figure 3</ref> show that using the gallery set as negative feature set provides the best performance for gallery adaptation. Using the disjoint training set for probe adaptation is the best for verification. This is the final strategy used for evaluation in <ref type="figure">figure 2</ref>. This conclusion is somewhat surprising that the probe adaptation was worse when constructing a negative set combining neg+trn, as a larger negative set typically results in better generalization performance for related approaches such as exemplar-SVM <ref type="bibr" target="#b27">[28]</ref>. However, a larger negative set would dilute the effect of the discriminating between gallery subjects, which is the primary goal of the evaluation, so a focused negative set would be appropriate.</p><p>Next, we experimented with the CASIA Web-Face dataset <ref type="bibr" target="#b37">[38]</ref>. The best negative set for probe adaptation is a set drawn from the same distribution as the templates. However, in many operatational conditions, this dataset will not be available. To study these effects, we constructed a dataset by sampling 70K images from CASIA balanced over classes, and pre-encoding these images for template adaptation training. <ref type="figure" target="#fig_0">Figure 3</ref> (bottom) shows that this results in slightly reduced verification performance. One hypothesis is that this imagery exhibits an unmodeled dataset bias for IJB-A faces, or that CASIA is image only, while IJB-A is imagery and videos. <ref type="figure" target="#fig_2">Figure 5</ref> shows an analysis of performance as a function of template size. For this study, we consider pairs of templates (P, Q) and compute the maximum template size as max(|P |, |Q|). Next, we consider max template sizes in the range (1, 2), <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8)</ref>, <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b15">16)</ref>, <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32)</ref>and <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64)</ref>, and compute a verification ROC curve for only those template pairs with sizes within the range. For each, we report a single point on the ROC curve at a false alarm rate of 1e-2 or 1e-3. Results from section 4.2 show that the largest benefit for template adaptation is on verification performance, so we analyze the effect of the template sizes on this metric. <ref type="figure" target="#fig_2">Figure 5</ref> (left) shows mean similarity score for templates of mated subjects only within a given template size range. This shows that as the template size increases the mated similarity score also increases. This is perhaps not surprising, as the more observations of media that are available in a template, the better the subject representation and the better the similarity score. The largest uncertainty as shown by the error bars is when the maximum template size is one, which is also not too surprising. Interestingly, the variance on the similarity scores does not decrease as template sizes increase, rather they stay largely the same even as the mean similarity increases. <ref type="figure" target="#fig_2">Figure 5</ref> (right) shows the effect of template size on ver- (top) Template adaptation compared with CNN encoding with metric learning using triplet similarity embedding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> or Joint Bayesian embedding <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>. (bottom) Template adaptation compared with CNN encoding and 2D alignment <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. In both cases, template adaptation outperforms all methods, and when combined with metric learning or 2D alignment, generates nearly equivalent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Template Size Study</head><p>ification performance. For each point on this curve, we split the dataset into templates that contained sizes within the range shown. Then, we computed a ROC curve and report the true match rate at a false alarm rate of 1e-3 and 1e-2, an operating point on the verification ROC curve. This result shows that the rate of increase in performance is largest for few media, and performance saturates at about 3 media per template. Furthermore, as the number of media per template increases, the verification score at 1e-2 increases by about 19% from one media per template to sixty four. This also shows that the largest benefit for template adaptation is when there are at least three media per template. <ref type="figure">Figure 6</ref> shows a study for comparing three alternatives for fusion of classifiers. Recall from section 4.1 that a final similarity score is computed as a linear combination of the classifiers trained for templates P and Q. In this section, we study different strategies for setting this weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Fusion Study</head><p>In general, the template classifier fusion from section 3 is a linear combination of SVM functional margins, s(P, Q) = αP (q) + (1 − α)Q(p). We explore strategies based on winner take all (α ∈ {0, 1}), template weighted fusion (α = |P |/(|P | + |Q|) and an experiment using the SVM geometric margin (e.g. P (x) = w T x/|w|), as suggested in <ref type="bibr" target="#b28">[29]</ref>. The default strategy is average fusion such that α = 0.5. Results show that the strategy of computing a weighted average with α = 0.5 of probe and gallery templates is the best strategy. We also performed a hyperparameter search over α, which confirmed this selection.</p><p>Finally, we also note that we ran experiments computing average media encodings, computing the margins for each encoding, then averaging the margins. This strategy performed consistently worse than computing average feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Error Analysis</head><p>Finally, we visualized identification and verification errors in different performance domains, in order to gain insight into template-based facial recognition. This analysis provides a better understanding of the error modes to better inform future template-based facial recognition. More detailed figures and additional discussion, including identification analysis, are available in supplemental material. <ref type="figure" target="#fig_3">Figure 7</ref> shows four columns of verification probe and gallery pairs for: the best scoring mated pairs; worst scoring mated pairs; best scoring non-mated pairs; and worst scoring non-mated pairs. After computing the similarity for all pairs of probe and gallery templates, we sort the resulting list. Each row represents a probe and gallery template pair. The templates contain from one to dozens of media. Up to eight individual media are shown with the last space showing a mosaic of the remaining media in the template. Between the templates are the IJB-A Template IDs for probe and gallery as well as the best mated and best non-mated scores. <ref type="figure" target="#fig_3">Figure 7</ref> (far left) shows the highest mated similarities. In the thirty highest scoring correct matches, we immediately note that every gallery template contains dozens of media. The probe templates either contain dozens of media or one media that matches well. <ref type="figure" target="#fig_3">Figure 7</ref> (center left) shows the lowest mated template pairs, representing failed identification. The thirty lowest mated similarities result from single-media probe templates that are low contrast, low resolution, extremely non-frontal, or not oriented upwards. <ref type="figure" target="#fig_3">Figure 7 (center right)</ref> showing the worst non-mated pairs highlights very understandable errors involving single-media probe templates representing impostors in challenging orientations. <ref type="figure" target="#fig_3">Figure 7 (far right)</ref> showing the best non-mated similarities shows the most certain nonmates, again often involving large templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have introduced template adaptation, a simple and surprisingly effective strategy for face verification and identification that achieves state of the art performance on the IJB-A dataset. Furthermore, we showed that this strategy can be applied to existing networks to improve performance. Futhermore, our evaluation provides compelling evidence that there are many face recognition tasks that can benefit from a historical record of media to aid in matching, and that this is an important problem to further evaluate with new template-based face datasets.</p><p>Our analysis shows that performance is highly dependent on the number of media available in a template. This strategy results in performance that results in 19% decrease in verification scores when a template contains a single media, such as comparing image to image or video to video, as in LFW or YouTubeFaces style evaluations. However, when probe or gallery templates are rich and at least one template contains greater than three media, performance quickly saturates and dominates the state of the art.</p><p>Finally, it remains to be seen if the conclusions hold for other datasets. The IJB-A dataset is currently the only public dataset with a template based evaluation protocol, and it may be that our performance claims are due to dataset bias, even though the composition of this dataset was engineered to avoid systemic bias <ref type="bibr" target="#b2">[3]</ref>. Finally, the gallery size for this <ref type="bibr">Figure 6</ref>. Classifier fusion study. We compare strategies for linear weighted fusion of classifiers, and results show that an average fusion used by the default template adaptation is best.</p><p>dataset is limited to 500 subjects, and it remains to be seen if the performance claims scale as the number of subjects increase. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Negative Set Analysis. We compare the effect of different negative sets for template adaptation. (top) The best choice is using the other non-mated gallery templates to define the negative set. (bottom) Experiments with a large unrelated negative set based on CASIA WebFaces results in slightly lowered performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Analysis of Alternatives. We show verification ROC curves (left) and identification CMC curves (right) for IJB-A split-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Template size analysis. (left) Similarity score increases as a function of maximum number of media, where the standard deviation is largest when template size is one. (right) True match rate as a function of maximum number of unique images or videos in a template pair, which shows that verification performance levels off at a maximum of three unique media per template. encodings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Verification error analysis. (far left, blue) The best mated verification template pairs, (center left, blue) The worst mated verification template pairs, (center right, green) The worst non-mated verification template pairs (far right, green) The best non-mated verification template pairs.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) under contract number 2014-14071600010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49.</idno>
	</analytic>
	<monogr>
		<title level="m">University of Massachusetts</title>
		<meeting><address><addrLine>Amherst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus benchmark A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR. (2015) 1, 2, 5</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>In: BMVC. (2015) 1, 2, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>1, 2, 6, 8</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. (2015) 1, 2, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DeepID3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Labeled Faces in the Wild: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Surpassing human-level face verification performance on LFW with GaussianFace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2144" to="2157" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human and algorithm performance on the pasc face recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BTAS</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face recognition vendor test (frvt): Performance of face identification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST Interagency Report 8009</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Triplet similarity embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03418</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unconstrained face verification using deep CNN features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Face recognition using deep multi-pose representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lekust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Oneto-many face recognition with bilinear CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<editor>WACV.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV workshop on ChaLearn Looking at People</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An end-to-end system for unconstrained face verification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV workshop on ChaLearn Looking at People</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on transfer learning. Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR Workshop on DeepVision</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning with linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML Workshop on Representational Learning</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale learning with svm and convolutional for generic object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Three viewpoints toward exemplar svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The one-shot similarity kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective unconstrained face recognition by combining multiple descriptors and learned background statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A compact and discriminative face track descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularization of neural network using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
