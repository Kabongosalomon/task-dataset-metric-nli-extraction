<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Anchor Learning for Arbitrary-Oriented Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
							<email>miaolingjuan@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zhang</surname></persName>
							<email>zhanghw.hongwei@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Li</surname></persName>
							<email>lilinhao@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Anchor Learning for Arbitrary-Oriented Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Arbitrary-oriented objects widely appear in natural scenes, aerial photographs, remote sensing images, etc., and thus arbitrary-oriented object detection has received considerable attention. Many current rotation detectors use plenty of anchors with different orientations to achieve spatial alignment with ground truth boxes. Intersection-over-Union (IoU) is then applied to sample the positive and negative candidates for training. However, we observe that the selected positive anchors cannot always ensure accurate detections after regression, while some negative samples can achieve accurate localization. It indicates that the quality assessment of anchors through IoU is not appropriate, and this further leads to inconsistency between classification confidence and localization accuracy. In this paper, we propose a dynamic anchor learning (DAL) method, which utilizes the newly defined matching degree to comprehensively evaluate the localization potential of the anchors and carries out a more efficient label assignment process. In this way, the detector can dynamically select high-quality anchors to achieve accurate object detection, and the divergence between classification and regression will be alleviated. With the newly introduced DAL, we can achieve superior detection performance for arbitrary-oriented objects with only a few horizontal preset anchors. Experimental results on three remote sensing datasets HRSC2016, DOTA, UCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method achieves substantial improvement compared with the baseline model. Besides, our approach is also universal for object detection using horizontal bound box. The code and models are available at https://github.com/ming71/DAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Object detection is one of the most fundamental and challenging problem in computer vision. In recent years, with the development of deep convolutional neural networks (CNN) , tremendous successes have been achieved on object detection <ref type="bibr" target="#b32">(Ren et al. 2015;</ref><ref type="bibr" target="#b4">Dai et al. 2016;</ref><ref type="bibr" target="#b31">Redmon et al. 2016;</ref><ref type="bibr" target="#b24">Liu et al. 2016)</ref>. Most detection frameworks utilize preset horizontal anchors to achieve spatial alignment with groundtruth(GT) box. Positive and negative samples are then selected through a specific strategy during training phase, which is called label assignment. * Corresponding author (a) (b) <ref type="figure">Figure 1</ref>: Predefined anchor (red) and its regression box (green). (a) shows that anchors with a high input IoU cannot guarantee perfect detection. (b) reveals that the anchor that is poorly spatially aligned with the GT box still has the potential to localize object accurately.</p><p>Since objects in the real scene tend to appear in diverse orientations, the issue of oriented object detection has gradually received considerable attention. There are many approaches have achieved oriented object detection by introducing the extra orientation prediction and preset rotated anchors . These detectors often follow the same label assignment strategy as general object detection frameworks. For simplicity, we call the IoU between GT box and anchors as input IoU, and the IoU between GT box and regression box as output IoU. The selected positives tend to obtain higher output IoU compared with negatives, because their better spatial alignment with GT provides sufficient semantic knowledge, which is conducive to accurate classification and regression.</p><p>However, we observe that the localization performance of the assigned samples is not consistent with the assumption mentioned above. As shown in <ref type="figure">Figure 1</ref>, the division of positive and negative anchors seems not always related to the detection performance. Furthermore, we have counted the distribution of the anchor localization performance for all candidates to explore whether this phenomenon is universal. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(a), a considerable percentage (26%) of positive anchors are poorly aligned with GT after regression, revealing that the positive anchors cannot ensure accurate localization. Besides, more than half of the candidates that achieve high-quality predictions are regressed from negatives (see <ref type="figure" target="#fig_0">Figure 2</ref> (a) Only 74% of the positive sample anchors can localize GT well after regression (with output IoU higher than 0.5), which illustrates that many false positive samples are introduced. (b) Only 42% of the high-quality detections (output IoU is higher than 0.5) come from matched anchors, which implies that quite a lot of negative anchors (58% in this example) have the potential to achieve accurate localization. (c) Current label assignment leads to a positive correlation between the classification confidence and the input IoU. (d) High-performance detection results exhibit a weak correlation between the localization ability and classification confidence, which is not conducive to selecting accurate detection results through classification score during inference.</p><p>erable number of negatives with high localization potential have not been effectively used. In summary, we conclude that the localization performance does not entirely depend on the spatial alignment between the anchors and GT. Besides, inconsistent localization performance before and after anchor regression further lead to inconsistency between classification and localization, which has been discussed in previous work <ref type="bibr" target="#b10">(Jiang et al. 2018;</ref><ref type="bibr" target="#b15">Kong et al. 2019;</ref><ref type="bibr" target="#b9">He et al. 2019)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c), anchor matching strategy based on input IoU induces a positive correlation between the classification confidence and input IoU. However, as discussed above, the input IoU is not entirely equivalent to the localization performance. Therefore, we cannot distinguish the localization performance of the detection results based on the classification score. The results in <ref type="figure" target="#fig_0">Figure 2</ref>(d) also confirm this viewpoint: a large number of regression boxes with high output IoU are misjudged as background.</p><p>To solve the problems, we propose a Dynamic Anchor Learning(DAL) method for better label assignment and further improve the detection performance. Firstly, a simple yet effective standard named matching degree is designed to assess the localization potential of anchors, which compre-hensively considers the prior information of spatial alignment, the localization ability and the regression uncertainty. After that, we adopt matching degree for training sample selection, which helps to eliminate false-positive samples and dynamically mine potential high-quality candidates, as well as suppress the disturbance caused by regression uncertainty. Next, we propose a matching-sensitive loss function to further alleviate the inconsistency between classification and regression, making the classifier more discriminative for proposals with high localization performance, and ultimately achieving high-quality detection.</p><p>Extensive experiments on public datasets, including remote sensing datasets HRSC2016, DOTA, UCAS-AOD, and scene text dataset ICDAR 2015, show that our method can achieve stable and substantial improvements for arbitraryoriented object detections. Integrated with our approach, even the vanilla one-stage detector can be competitive with state-of-the-art methods on several datasets. In addition, experiments on ICDAR 2013 and NWPU VHR-10 prove that our approach is also universal for object detection using horizontal box. The proposed DAL approach is general and can be easily integrated into existing object detection pipeline without increasing the computational cost of inference.</p><p>Our contributions are summarized as follows:</p><p>• We observe that the label assignment based on IoU between anchor and GT box leads to suboptimal localization ability assessment, and further brings inconsistent classification and regression performance.</p><p>• The matching degree is introduced to measure the localization potential of anchors. A novel label assignment method based on this metric is proposed to achieve highquality detection.</p><p>• The matching-sensitive loss is proposed to alleviate the problem of the weak correlation between classification and regression, and improves the discrimination ability of high-quality proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arbitrary-Oriented Object Detection</head><p>The current mainstream detectors can be divided into two categories: two-stage detector <ref type="bibr" target="#b32">(Ren et al. 2015;</ref><ref type="bibr" target="#b4">Dai et al. 2016</ref>) and one-stage detector <ref type="bibr" target="#b31">(Redmon et al. 2016;</ref><ref type="bibr" target="#b24">Liu et al. 2016)</ref>. Existing rotation detectors are mostly built on detectors using horizontal bounding box representation. To localize rotated objects, the preset rotated anchor and additional angle prediction are adopted in the literature <ref type="bibr" target="#b25">(Liu, Ma, and Chen 2018;</ref><ref type="bibr" target="#b28">Ma et al. 2018;</ref><ref type="bibr" target="#b27">Liu et al. 2017)</ref>. Nevertheless, due to the variation of the orientation, these detectors are obliged to preset plenty of rotated anchors to make them spatially aligned with GT box. There are also some methods that detect oriented objects only using horizontal anchors. For example, RoI Transformer <ref type="bibr" target="#b5">(Ding et al. 2019</ref>) uses horizontal anchors but learns the rotated RoI through spatial transformation, reducing the number of predefined anchors. R 3 Det <ref type="bibr" target="#b40">(Yang et al. 2019a)</ref> adopts cascade regression and refined box re-encoding module to achieve state-of-the-art performance with horizontal anchors. Although the above approaches have achieved good performance, they cannot make a correct judgment on the quality of anchors, and thus cause improper label assignment which brings adverse impact during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Assignment</head><p>Most anchor-based detectors densely preset anchors at each position of feature maps. The massive preset anchors lead to serious imbalance problem, especially for the arbitraryoriented objects with additional angle setting. The most common solution is to control the ratio of candidates through a specific sampling strategy <ref type="bibr" target="#b34">(Shrivastava, Gupta, and Girshick 2016;</ref><ref type="bibr" target="#b30">Pang et al. 2019)</ref>. Besides, Focal Loss <ref type="bibr" target="#b23">(Lin et al. 2017b</ref>) lowers the weight of easy examples to avoid its overwhelming contribution to loss. The work <ref type="bibr" target="#b17">(Li, Liu, and Wang 2019)</ref> further considers extremely hard samples as outliers, and use gradient harmonizing mechanism to conquer the imbalance problems. We demonstrate that the existence of outliers is universal, and our method can prevent such noise samples from being assigned incorrectly. Some work have observed problems caused by using input IoU as a standard for label assignment. Dynamic R-CNN <ref type="bibr" target="#b45">(Zhang et al. 2020a</ref>) and ATSS <ref type="bibr" target="#b46">(Zhang et al. 2020b</ref>) automatically adjust the IoU threshold to select high-quality positive samples, but they fail to consider whether the IoU itself is credible. The work ) points out that binary labels assigned to anchors are noisy, and construct cleanliness score for each anchor to supervise training process. However, it only considers the noise of positive samples, ignoring the potentially powerful localization capabilities of massive negatives. HAMBox ) reveals that unmatched anchors can also achieve accurate prediction, and attempts to utilize these samples. Nevertheless, its compensated anchors mined according to output IoU is not reliable; moreover, it does not consider the degradation of matched positives. FreeAnchor ) formulates object-anchor matching as a maximum likelihood estimation procedure to select the most representative anchors, but its formulation is relatively complicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method Rotation Detector Built on RetinaNet</head><p>The real-time inference is essential for arbitrary-oriented object detection in many scenarios. Hence we use the one-stage detector RetinaNet <ref type="bibr" target="#b23">(Lin et al. 2017b</ref>) as the baseline model. It utilizes ResNet-50 as backbone, in which the architecture similar to FPN <ref type="bibr" target="#b22">(Lin et al. 2017a</ref>) is adopted to construct a multi-scale feature pyramid. Predefined horizontal anchors are set on the features of each level P 3 , P 4 , P 5 , P 6 , P 7 . Note that rotation anchor is not used here, because it is inefficient and unnecessary, and we will further prove this point in the next sections. Since the extra angle parameter is introduced, the oriented box is represented in the format of (x, y, w, h, θ). For bounding box regression, we have: where x, y, w, h, θ denote center coordinates, width, height and angle, respectively. x and x a are for the predicted box and anchor, respectively (likewise for y, w, h, θ). Given the ground-truth box offsets t * = t * x , t * y , t * w , t * h , t * θ , the multitask loss is defined as follows:</p><formula xml:id="formula_0">L = L cls (p, p * ) + L reg (t, t * )<label>(2)</label></formula><p>in which the value p and the vector t denote predicted classification score and predicted box offsets, respectively. Variable p * represents the class label for anchors (p * = 1 for positive samples and p * = 0 for negative samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Anchor Selection</head><p>Some researches <ref type="bibr" target="#b35">Song, Liu, and Wang 2020)</ref> have reported that the discriminative features required to localize objects are not evenly distributed on GT, especially for objects with a wide variety of orientations and aspect ratios. Therefore, the label assignment strategy based on spatial alignment, i.e., input IoU, leads to the incapability to capture the critical feature demanded for object detection. An intuitive approach is to use the feedback of the regression results, that is, the output IoU to represent feature alignment ability and dynamiclly guide the training process. Several attempts <ref type="bibr" target="#b10">(Jiang et al. 2018;</ref><ref type="bibr" target="#b18">Li et al. 2020</ref>) have been made in this respect. In particular, we tentatively select the training samples based on output IoU and use it as soft-label for classification. However, we found that the model is hard to converge because of the following two issues:</p><p>• Anchors with high input IoU but low output IoU are not always negative samples, which may be caused by not sufficient training. • The unmatched low-quality anchors that accidentally achieve accurate localization performance tend to be misjudged as positive samples.</p><p>The above analysis shows that regression uncertainty interferes with the credibility of the output IoU for feature alignment. Regression uncertainty has been widely discussed in many previous work <ref type="bibr" target="#b7">(Feng, Rosenbaum, and Dietmayer 2018;</ref><ref type="bibr" target="#b2">Choi et al. 2019;</ref><ref type="bibr" target="#b14">Kendall and Gal 2017;</ref><ref type="bibr" target="#b3">Choi et al. 2018)</ref>, which represents the instability and irrelevance in the regression process. We discovered in the experiment that it also misleads the label assignment. Specifically, highquality samples cannot be effectively utilized, and the selected false-positive samples would cause the unstable training. Unfortunately, neither the input IoU nor the output IoU used for label assignment can avoid the interference caused by the regression uncertainty.</p><p>Based on the above observations, we introduce the concept of matching degree (MD), which utilizes the prior information of spatial matching, feature alignment ability and regression uncertainty of the anchor to measure the localization capacity, which is defined as follows:</p><formula xml:id="formula_1">md = α · sa + (1 − α) · f a − u γ<label>(3)</label></formula><p>where sa denotes a priori of spatial alignment, whose value is equivalent to input IoU. f a represents the feature alignment capability calculated by IoU between GT box and regression box. α and γ are hyperparameters used to weight the influence of different items. u is a penalty term, which denotes the regression uncertainty during training. It is obtained via the IoU variation before and after regression:</p><formula xml:id="formula_2">u = |sa − f a|<label>(4)</label></formula><p>The suppression of interference during regression is vital for high-quality anchor sampling and stable training. Variation of IoU before and after regression represents the probability of incorrect anchor assessment. Note that our construction of the penalty term for regression uncertainty is very simple, and since detection performance is not sensitive to the form of u , the naive, intuitive yet effective form is employed.</p><p>With the newly defined matching degree, we conduct dynamic anchor selection for superior label assignment. In the training phase, we first calculate matching degree between the GT box and anchors, and then the anchors with matching degree higher than a certain threshold (set to 0.6 in our experiments) are selected as positives, while the rest are negatives. After that, for GT that do not match any anchor, the anchor with the highest matching degree will be compensated as positive candidate. To achieve more stable training, we gradually adjust the impact of the input IoU during training. The specific adjustment schedule is as follows:</p><formula xml:id="formula_3">α(t) = 1, t &lt; 0.1 5(α 0 − 1) ·t+1.5−0.5·α 0 , 0.1 ≤ t &lt; 0.3 α 0 , t ≥ 0.3 (5) where t = iters</formula><p>M ax Iteration , M ax Iteration is the total number of iterations, and α 0 is the final weighting factor that appears in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching-Sensitive Loss</head><p>To further enhance the correlation between classification and regression to achieve high-quality arbitrary-oriented detection, we integrate the matching degree into the training process and propose the matching-sensitive loss function (MSL). The classification loss is defined as:</p><formula xml:id="formula_4">L cls = 1 N i∈ψ F L (p i , p * i )+ 1 N p j∈ψp w j ·F L p j , p * j<label>(6)</label></formula><p>where ψ and ψ p respectively represent all anchors and the positive samples selected by the matching degree threshold. N and N p denote the total number of all anchors and positive anchors, respectively. F L(·) is focal loss defined in RetinaNet <ref type="bibr" target="#b23">(Lin et al. 2017b)</ref>. w j indicates the matching compensation factor, which is used to distinguish positive samples of different localization potential. For each groundtruth g, we first calculate its matching degree with all anchors as md. Then positive candidates can be selected according to a certain threshold, matching degree of positives is represented as md pos , where md pos ⊆ md. Supposing that the maximal matching degree for g is md max , the compensation value is denoted as ∆md, we have:</p><formula xml:id="formula_5">∆md = 1 − md max .<label>(7)</label></formula><p>After that, ∆md is added to the matching degree of all positives to form the matching compensation factor: w = md pos + ∆md.</p><p>With the well-designed matching compensation factor, the detector treats positive samples of different localization capability distinctively. In particular, more attention will be paid to candidates with high localization potential for classifier. Therefore, high-quality predictions can be taken through classification score, which helps to alleviate the inconsistency between classification and regression.</p><p>Since matching degree measures the localization ability of anchors, and thus it can be further used to promote highquality localization. We formulate the matching-sensitive regression loss as follows:</p><formula xml:id="formula_7">L reg = 1 N p j∈ψp w j · L smooth L 1 t j , t * j<label>(9)</label></formula><p>where L smooth L 1 denotes the smooth-L 1 loss for regression. Matching compensation factor w is embedded into regression loss to avoid the loss contribution of high-quality positives being submerged in the dominant loss of samples with poor spatial alignment with GT boxes. It can be seen from <ref type="figure" target="#fig_1">Figure 3</ref>(a) that the correlation between the classification score and the localization ability of the regression box is not strong enough, which causes the prediction results selected by the classification confidence to be sometimes unreliable.</p><p>After training with a matching-sensitive loss, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b), a higher classification score accurately characterizes the better localization performance represented by the output IoU, which verifies the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We conduct experiments on the remote sensing datasets HRSC2016, DOTA, UCAS-AOD and a scene text dataset ICDAR 2015.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For the experiments, we build the baseline on RetinaNet as described above. For HRSC2016, DOTA, and UCAS-AOD, only three horizontal anchors are set with aspect ratios of {1/2, 1, 2}. For ICDAR, only five horizontal anchors are set with aspect ratios of {1/5, 1/2, 1, 2, 5}. All images are resized to 800×800. We use random flip, rotation, and HSV colour space transformation for data augmentation. The optimizer used for training is Adam. The initial learning rate is set to 1e-4 and is divided by 10 at each decay step. The total iterations of HRSC2016, DOTA, UCAS-AOD, and ICDAR 2015 are 20k, 30k, 15k, and 40k, respectively. We train the models on RTX 2080Ti with batch size set to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Evaluation of different components We conduct a component-wise experiment on HRSC2016 to verify the contribution of the proposed method. The experimental results are shown in <ref type="table">Table 1</ref>. For variant with output IoU, α is set to 0.8 for stable training, even so, detection performance still drops from 80.8% to 78.9%. It indicates that the output IoU is not always credible for label assignment. With the suppression of regression uncertainty, the prior space alignment and posterior feature alignment can work together effectively on label assignment, and thus performance is dramaticly improved by 4.8% higher than the baseline. Furthermore, the model using the matching sensitivity loss function achieves mAP of 88.6%, and the proportion of highprecision detections is significantly increased. For example, AP 75 is 9.9% higher than the variants with uncertainty supression, which indicates that the matching degree guided loss effectively distinguishes anchors with differential localization capability, and pay more attention to high matching degree anchors to improve high-quality detection results.</p><p>Hyper-parameters To find suitable hyperparameter settings, and explore the relationship between parameters, we conduct parameter sensitivity experiments, and the results are shown in <ref type="table" target="#tab_1">Table 2</ref>. In the presence of uncertainty suppression terms, as the α is reduced appropriately, the influence of feature alignment increases, and the mAP increases. It indicates that the feature alignment represented by the output IoU is beneficial to select anchors with high localization capability. However, when α is extremely large, the performance decreases sharply. The possible reason is that most potential high quality samples are suppressed by uncertainty penalty item when the output IoU can hardly provide feedback information. In this case, weakening the uncertainty suppression ability, that is, increasing γ helps to alleviate this problem and make anchor selection more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Results</head><p>Comparison with other sampling methods The experimental results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Baseline model conducts label assignment based on input IoU. ATSS <ref type="bibr" target="#b46">(Zhang et al. 2020b)</ref> has achieved great success in object detection using horizontal box. When applied to rotation object detection, there is still a substantial improvement, which is 5.3% higher than the baseline model. As for HAMBox , since the samples mined according to the output IoU are likely to be low-quality samples, too many mining samples may cause the network to fail to diverge, we only compensate one anchor for GT that do not match enough anchors. It outperforms 4.6% than baseline. The proposed DAL method significantly increases 7.8% compared with baseline model. Compared with the popular method ATSS, our approach considers the localization performance of the regression box, so the selected samples have more powerful localization capability, and the effect is 2.5% higher than it, which confirms the effectiveness of our method.</p><p>Results on DOTA We compare the proposed approach with other state-of-the-art methods. As shown in <ref type="table" target="#tab_4">Table 4</ref>, we achieve the mAP of 71.44%, which outperforms the baseline model by 3%. Integrated with DAL, even the vanilla Reti-naNet can compete with many advanced methods. Besides, we also embed our approach to other models to verify its universality. S 2 A-Net ) is an advanced rotation detector that achieves the state-of-the-art performance on DOTA dataset. It can be seen that our method further Baseline <ref type="bibr" target="#b41">(Yang et al. 2018)</ref> HAMBox      <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Results on HRSC2016 HRSC2016 contains lots of rotated ships with large aspect ratios and arbitrary orientation. Our method achieves the state-of-the-art performances on HRSC2016, as depicted in  the backbone and the input image is resized to 800×800, our method has reached the highest mAP of 89.77%. Even if we use a lighter backbone ResNet50 and a smaller input scale of 416×416, we still achieve the mAP of 88.6%, which is comparable to many current advanced methods. It is worth mentioning that our method uses only three horizontal anchors in each position, but outperforms the frameworks with a large number of anchors. This shows that it is critical to effectively utilize the predefined anchors and select high-quality samples, and there is no need to preset a large number of rotated anchors. Besides, our model is a one-stage detector, and the feature map used is P 3 − P 7 . Compared with the P 2 − P 6 for two-stage detectors, the total amount of positions that need to set anchor is less, so the inference speed is faster. With input image resized to 416×416, our model reaches 34 FPS on RTX 2080 Ti GPU. <ref type="table" target="#tab_7">Table 6</ref> show that our model achieves a further improvement of 2.3% compared with baseline. Specifically, the detection performance of small vehicles is significantly improved, indicating   that our method is also robust to small objects. Note that the DAL method dramaticly improves AP 75 , which reveals that the matching degree based loss function helps to pay more attention to high-quality samples and effectively distinguish them to achieve high-quality object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on UCAS-AOD Experimental results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on ICDAR 2015</head><p>To verify the generalization of our method in different scenarios, we also conduct experiments on the scene text detection dataset. The results are shown in <ref type="table" target="#tab_9">Table 7</ref>. Our baseline model only achieved an F-measure of 77.5% after careful parameters selection and long-term training. The proposed DAL method improves the detection performance by 4%, achieves an F-measure of 81.5%. With multi-scale training and testing, it reaches 82.4%, which was equivalent to the performance of many well-designed text detectors. However, there are a large number of long texts in ICDAR 2015 dataset, which are often mistakenly detected as several short texts. Designed for general rotation detection, DAL does not specifically consider this situation, therefore, the naive model still cannot outperform current state-of-the-art approaches for scene text detection, such as DB <ref type="bibr" target="#b20">(Liao et al. 2020</ref>  Experiments on Object Detection with HBB When localize objects with horizontal bounding box(HBB), label assignment still suffers from the uneven discriminative feature. Although this situation is not as severe as the rotated objects, it still lays hidden dangers of unstable training. Therefore, our method is also effective in general object detection using HBB. The experimental results on ICDAR 2013 <ref type="bibr" target="#b13">(Karatzas et al. 2013)</ref>, NWPU VHR-10 <ref type="bibr" target="#b1">(Cheng, Zhou, and Han 2016)</ref> and VOC2007 <ref type="bibr" target="#b6">(Everingham et al. 2010</ref>) are shown in the <ref type="table" target="#tab_11">Table 8</ref>. It can be seen that DAL achieves substantial improvements for object detection with HBB, which proves the universality of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a dynamic anchor learning strategy to achieve high-performance arbitrary-oriented object detection. Specifically, matching degree is constructed to comprehensively considers the spatial alignment, feature alignment ability, and regression uncertainty for label assignment. Then dynamic anchor selection and matchingsensitive loss are integrated into the training pipeline to improves the high-precision detection performance and alleviate the gap between classification and regression tasks. Extensive experiments on several datasets have confirmed the effectiveness and universality of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(b)) which implies that a consid-arXiv:2012.04150v2 [cs.CV] 15 Dec 2020 Analysis of the classification and regression capabilities of anchors that use input IoU for label assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The correlation between the output IoU and classification score with and without MSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of detection results on DOTA with our method .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Analysis of different hyperparameters on</cell></row><row><cell>HRSC2016 dataset.</cell></row><row><cell>and 444 images, respectively. DOTA (Xia et al. 2018) is the</cell></row><row><cell>largest public dataset for object detection in remote sensing</cell></row><row><cell>imagery with oriented bounding box annotations. It contains</cell></row><row><cell>2806 aerial images with 188,282 annotated instances, there</cell></row><row><cell>are 15 categories in total. Note that images in DOTA are too</cell></row><row><cell>large, we crop images into 800×800 patches with the stride</cell></row><row><cell>set to 200. UCAS-AOD (Zhu et al. 2015) is an aerial air-</cell></row><row><cell>craft and car detection dataset, which contains 1510 images.</cell></row><row><cell>We randomly divide it into training set, validation set and</cell></row><row><cell>test set as 5:2:3. The ICDAR 2015 dataset is used for Inci-</cell></row><row><cell>dental Scene Text Challenge 4 of the ICDAR Robust Text</cell></row><row><cell>Detection Challenge (Karatzas et al. 2015). It contains 1500</cell></row><row><cell>images, including 1000 training images and 500 test images.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with other label assignment strategies on HRSC2016. .11 55.03 71.00 78.30 81.90 88.46 90.89 84.97 87.46 64.41 65.65 76.86 72.09 64.35 76.95</figDesc><table><row><cell>Methods</cell><cell cols="2">Backbone PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC mAP</cell></row><row><cell>FR-O(Xia et al. 2018)</cell><cell>R-101</cell><cell cols="15">79.09 69.12 17.17 63.49 34.20 37.16 36.20 89.19 69.60 58.96 49.40 52.52 46.69 44.80 46.30 52.93</cell></row><row><cell>R-DFPN(Yang et al. 2018)</cell><cell>R-101</cell><cell cols="15">80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94</cell></row><row><cell>R 2 CNN(Jiang et al. 2017)</cell><cell>R-101</cell><cell cols="15">80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67</cell></row><row><cell>RRPN(Ma et al. 2018)</cell><cell>R-101</cell><cell cols="15">88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01</cell></row><row><cell>ICN(Azimi et al. 2018)</cell><cell>R-101</cell><cell cols="15">81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 68.16</cell></row><row><cell>RoI Trans.(Ding et al. 2019)</cell><cell>R-101</cell><cell cols="15">88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56</cell></row><row><cell>CAD-Net(Zhang, Lu, and Zhang 2019)</cell><cell>R-101</cell><cell cols="15">87.80 82.40 49.40 73.50 71.10 63.50 76.70 90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20 69.90</cell></row><row><cell>DRN(Pan et al. 2020)</cell><cell>H-104</cell><cell cols="15">88.91 80.22 43.52 63.35 73.48 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50 70.70</cell></row><row><cell>O 2 -DNet(Wei et al. 2019)</cell><cell>H-104</cell><cell cols="15">89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04</cell></row><row><cell>SCRDet(Yang et al. 2019b)</cell><cell>R-101</cell><cell cols="15">89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61</cell></row><row><cell>R 3 Det(Yang et al. 2019a)</cell><cell>R-152</cell><cell cols="15">89.49 81.17 50.53 66.10 70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17 73.74</cell></row><row><cell>CSL(Yang and Yan 2020)</cell><cell>R-152</cell><cell cols="15">90.25 85.53 54.64 75.31 70.44 73.51 77.62 90.84 86.15 86.69 69.60 68.04 73.83 71.10 68.93 76.17</cell></row><row><cell>Baseline</cell><cell>R-50</cell><cell cols="15">88.67 77.62 41.81 58.17 74.58 71.64 79.11 90.29 82.13 74.32 54.75 60.60 62.57 69.67 60.64 68.43</cell></row><row><cell>Baseline+DAL</cell><cell>R-50</cell><cell cols="15">88.68 76.55 45.08 66.80 67.00 76.76 79.74 90.84 79.54 78.45 57.71 62.27 69.05 73.14 60.11 71.44</cell></row><row><cell>Baseline+DAL</cell><cell>R-101</cell><cell cols="15">88.61 79.69 46.27 70.37 65.89 76.10 78.53 90.84 79.98 78.41 58.71 62.02 69.23 71.32 60.65 71.78</cell></row><row><cell>S 2 A-Net(Han et al. 2020)</cell><cell>R-50</cell><cell cols="15">89.11 82.84 48.37 71.11 78.11 78.39 87.25 90.83 84.90 85.64 60.36 62.60 65.26 69.13 57.94 74.12</cell></row><row><cell>S 2 A-Net+DAL</cell><cell>R-50</cell><cell cols="2">89.69 83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Performance evaluation of OBB task on DOTA dataset. R-101 denotes ResNet-101(likewise for R-50), and H-104</cell></row><row><cell>stands for Hourglass-104.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Backbone</cell><cell>Size</cell><cell>NA</cell><cell>mAP</cell></row><row><cell>Two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R 2 CNN(Jiang et al. 2017)</cell><cell>ResNet101</cell><cell>800×800</cell><cell>21</cell><cell>73.07</cell></row><row><cell>RC1&amp;RC2(LB et al. 2017)</cell><cell>VGG16</cell><cell>-</cell><cell>-</cell><cell>75.70</cell></row><row><cell>RRPN(Ma et al. 2018)</cell><cell>ResNet101</cell><cell>800×800</cell><cell>54</cell><cell>79.08</cell></row><row><cell>R 2 PN(Zhang et al. 2018)</cell><cell>VGG16</cell><cell>-</cell><cell>24</cell><cell>79.60</cell></row><row><cell>RoI Trans. (Ding et al. 2019)</cell><cell>ResNet101</cell><cell>512×800</cell><cell>5</cell><cell>86.20</cell></row><row><cell>Gliding Vertex(Xu et al. 2020)</cell><cell>ResNet101</cell><cell>512×800</cell><cell>5</cell><cell>88.20</cell></row><row><cell>Single-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RRD(Liao et al. 2018)</cell><cell>VGG16</cell><cell>384×384</cell><cell>13</cell><cell>84.30</cell></row><row><cell>R 3 Det(Yang et al. 2019a)</cell><cell>ResNet101</cell><cell>800×800</cell><cell>21</cell><cell>89.26</cell></row><row><cell>R-RetinaNet(Lin et al. 2017b)</cell><cell>ResNet101</cell><cell>800×800</cell><cell>121</cell><cell>89.18</cell></row><row><cell>Baseline</cell><cell>ResNet50</cell><cell>416×416</cell><cell>3</cell><cell>80.81</cell></row><row><cell>Baseline+DAL</cell><cell>ResNet50</cell><cell>416×416</cell><cell>3</cell><cell>88.60</cell></row><row><cell>Baseline+DAL</cell><cell>ResNet101</cell><cell>416×416</cell><cell>3</cell><cell>88.95</cell></row><row><cell>Baseline+DAL</cell><cell>ResNet101</cell><cell>800×800</cell><cell>3</cell><cell>89.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparisons with state-of-the-art detectors on HRSC2016. NA denotes the number of preset anchor at each location of feature map. improves the performance by 2.83%, reaches the mAP of 76.95%, achieving the best results among all compared models. Some detection results on DOTA are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Using ResNet-101 as</figDesc><table><row><cell>Methods</cell><cell>car</cell><cell>airplane</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>FR-O(Xia et al. 2018)</cell><cell>86.87</cell><cell>89.86</cell><cell>88.36</cell><cell>47.08</cell></row><row><cell>RoI Transformer (Ding et al. 2019)</cell><cell>87.99</cell><cell>89.90</cell><cell>88.95</cell><cell>50.54</cell></row><row><cell>Baseline</cell><cell>84.64</cell><cell>90.51</cell><cell>87.57</cell><cell>39.15</cell></row><row><cell>Baseline+DAL</cell><cell>89.25</cell><cell>90.49</cell><cell>89.87</cell><cell>74.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Detection results on UCAS-AOD dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of different methods on the ICDAR 2015. P, R, F indicate recall, precision and F-measure respectively. * means multi-scale training and testing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Dataset</cell><cell>Backbone</cell><cell>mAP/F</cell></row><row><cell>ICDAR 2013</cell><cell>RetinaNet RetinaNet+DAL</cell><cell>77.2 81.3</cell></row><row><cell>NWPU VHR-10</cell><cell>RetinaNet RetinaNet+DAL</cell><cell>86.4 88.3</cell></row><row><cell>VOC 2007</cell><cell>RetinaNet RetinaNet+DAL</cell><cell>74.9 76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Performance evaluation of HBB task on ICDAR 2013 and NWPU VHR-10.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t x = (x − x a ) /w a , t y = (y − y a ) /h a t w = log (w/w a ) , t h = log (h/h a ) t θ = tan (θ − θ a )(1)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Körner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning rotationinvariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertaintyaware learning from demonstration using mixture density networks with sampling-free variance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6915" to="6922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning RoI transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3266" to="3273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Align Deep Features for Oriented Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09397</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<title level="m">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>ICDAR 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Consistent optimization for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06563</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10588" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Textboxes++: A singleshot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-Time Scene Text Detection with Differentiable Binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented ship detection framework in optical remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="941" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HAMBox: Delving Into Mining High-Quality Anchors on Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13043" to="13051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotated region based CNN for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic Refinement Network for Oriented and Densely Packed Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11207" to="11216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2550" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11563" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10694</idno>
		<title level="m">Oriented objects as pairs of middle lines</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<title level="m">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Arbitrary-Oriented Object Detection with Circular Smooth Label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05597</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cad-net: A contextaware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06002</idno>
		<title level="m">Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
