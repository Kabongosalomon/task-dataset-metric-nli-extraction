<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
							<email>cswhjiang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> MoCo [11]  <p>is effective for unsupervised image representation learning. In this paper, we propose VideoMoCo for unsupervised video representation learning. Given a video sequence as an input sample, we improve the temporal feature representations of MoCo from two perspectives. First, we introduce a generator to drop out several frames from this sample temporally. The discriminator is then learned to encode similar feature representations regardless of frame removals. By adaptively dropping out different frames during training iterations of adversarial learning, we augment this input sample to train a temporally robust encoder. Second, we use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning. This degradation is reflected via temporal decay to attend the input sample to recent keys in the queue. As a result, we adapt MoCo to learn video representations without empirically designing pretext tasks. By empowering the temporal robustness of the encoder and modeling the temporal decay of the keys, our VideoMoCo improves MoCo temporally based on contrastive learning. Experiments on benchmark datasets including UCF101 and HMDB51 show that VideoMoCo stands as a state-of-the-art video representation learning method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised (i.e., self-supervised) feature representation learning receives tremendous investigations along with the development of convolutional neural networks (CNNs). This learning scheme does not require cumbersome man-* T. Pan and Y. Song contribute equally. Y. Song is the corresponding author. The code is available at https://github.com/tinapanpt/VideoMoCo.  <ref type="figure">Figure 1</ref>. VideoMoCo improves MoCo <ref type="bibr" target="#b10">[11]</ref> temporally from two perspectives. First, by taking a video sequence as a training sample, we introduce adversarial learning to augment this sample temporally. Second, we use a temporal decay (i.e., t i ) to attenuate the contributions from older keys in the queue. To this end, the encoder is learned via temporal augmentation within each sample and temporally contrastive learning across different samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Loss</head><p>ual label collections and produces deep features representing general visual contents. These features can be further adapted to suit downstream visual recognition scenarios including image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b52">53]</ref>, object detection <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b19">20]</ref>, visual tracking <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, and semantic segmentation <ref type="bibr" target="#b32">[33]</ref>. Among various unsupervised representation learning studies, contrastive learning <ref type="bibr" target="#b8">[9]</ref> is developed extensively. By treating one sample as positive and the remaining ones as negative (i.e., instance discrimination), contrastive learning improves feature discrimination by considering the massive sample storage <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref> and data augmentation <ref type="bibr" target="#b1">[2]</ref>. To effectively handle large-scale samples, MoCo <ref type="bibr" target="#b10">[11]</ref> builds an on-the-fly dictionary with a queue and a moving-averaged encoder. The learned feature representations have significantly improved a series of downstream recognition performances to approach those by using supervised feature representations. The evolution of contrastive learning heavily focuses on feature representations from static images while leaving the temporal video representations less touched. One of the reasons is that large-scale video data is difficult to store in memory. Using a limited number of video samples leads to inadequate contrastive learning performance. On the other hand, attempts on unsupervised video representation learning focus on proposing pretext tasks related to a sub-property of video content. Examples include sequence sorting <ref type="bibr" target="#b22">[23]</ref>, optical flow estimation <ref type="bibr" target="#b4">[5]</ref>, video playback rate perception <ref type="bibr" target="#b51">[52]</ref>, pace prediction <ref type="bibr" target="#b42">[43]</ref>, and temporal transformation recognition <ref type="bibr" target="#b13">[14]</ref>. Different from these pretext designs, we aim to learn a task-agnostic feature representation for videos. With effective data storage and CNN update <ref type="bibr" target="#b10">[11]</ref> at hand, we rethink unsupervised video representation learning from the perspective of contrastive learning, where the features are learned naturally to discriminate different video sequences without introducing empirically designed pretext tasks.</p><p>In this work, we propose VideoMoCo that improves MoCo for unsupervised video representation learning. VideoMoCo follows the usage of queue structure and a moving-averaged encoder of MoCo, which computes a contrastive loss (i.e., InfoNCE <ref type="bibr" target="#b30">[31]</ref>) efficiently among largescale video samples. Given a training sample with fixedlength video frames, we introduce adversarial learning to improve the temporal robustness of the encoder. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we use a generator (G) to adaptively drop out several frames. The sample with remaining frames, together with the original sample with full frames, are sent to the discriminator / encoder (D) for differentiating. Their difference is then utilized reversely to train G. As a result, G removes temporally important frames based on the current state of D. And D is learned to produce similar feature representations regardless of frame removal. During different training iterations, the frames removed by G are different. This sample is then augmented adversarially to train a temporally robust D. After adversarial learning only D is kept to extract temporally robust features.</p><p>The adversarial learning drops out several frames of an input video sample. We treat its remaining frames as a query sample and perform contrastive learning with keys in the memory queue. However, we notice that the momentum encoder updates after keys enqueue. The feature representation of the keys is not up-to-date when we compute the contrastive loss. To mitigate this effect, we model the degradation of these keys by proposing a temporal decay. If a key stays longer in the queue, its contribution is less via this decay. To this end, we attend the query sample to recent keys during the contrastive loss computation. The encoder is thus learned more effectively without being heavily interfered by the 'ancient' keys. The temporally adversarial learning and the temporal decay improve the temporal feature representation of MoCo. The experiments on several action recognition benchmarks verify that our proposed VideoMoCo performs favorably against state-of-theart video representation approaches.</p><p>We summarize our main contributions as follows:</p><p>• We propose temporally adversarial learning to improve the feature representation of the encoder. • We propose a temporal decay to reduce the effect from historical keys in the memory queues during contrastive learning. • Experiments on the standard benchmarks show that our VideoMoCo extends MoCo to a state-of-the-art video representation learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we perform a literature review on selfsupervised video representation learning, contrastive learning, and generative adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Representation Learning</head><p>Investigations on video representation learning focus on exploiting temporal coherence among consecutive video frames. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46]</ref>, cycle consistency is explored for feature learning. The future frame prediction is proposed in <ref type="bibr" target="#b37">[38]</ref> to learn coherent features. A set of studies propose empirical and unsupervised feature learning pretext tasks including future motion and appearance prediction <ref type="bibr" target="#b41">[42]</ref>, video pace prediction <ref type="bibr" target="#b42">[43]</ref>, and frame color estimation <ref type="bibr" target="#b40">[41]</ref>. In addition to future frame prediction, video frame sorting <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref> is popular. In <ref type="bibr" target="#b11">[12]</ref>, contrastive loss is applied to learn video representations by comparing frames from multiple viewpoints. A consistent frame representation of different sampling rates is proposed in <ref type="bibr" target="#b50">[51]</ref>. Four different temporal transformations (i.e., speed change, random sampling, periodic change, and content warp) of a video are investigated in <ref type="bibr" target="#b13">[14]</ref> to build video representations for action recognition. In addition, multi-modality methods introduce text <ref type="bibr" target="#b38">[39]</ref>, optical flows <ref type="bibr" target="#b4">[5]</ref>, and audios <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b2">3]</ref> for cross-modal supervised learning. Different from existing video representation learning methods, our VideoMoCo utilizes color information and performs instance discrimination without bringing empirical pretext tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>There are wide investigations in contrastive learning for static image recognition. They follow the principle that the feature distances between positive sample pairs are minimized while those between the negative sample pairs are maximized during the training stage. The distance measurement is proposed in <ref type="bibr" target="#b8">[9]</ref>. The instance discrimination is proposed in <ref type="bibr" target="#b47">[48]</ref> for feature learning. This scheme is improved in MoCo <ref type="bibr" target="#b10">[11]</ref> where there is a momentum encoder to build dynamic dictionaries to support contrastive learning.</p><p>In SimCLR <ref type="bibr" target="#b1">[2]</ref>, different combinations of data augmentation methods are evaluated for sample pairs. Only positive samples are introduced in BYOL <ref type="bibr" target="#b6">[7]</ref> during training while achieving superior performance. Its effectiveness is justified by the theoretical analysis <ref type="bibr" target="#b31">[32]</ref>. The objectives utilized in contrastive learning are to maximize the lower bound of mutual information between the input feature and its augmented representation <ref type="bibr" target="#b23">[24]</ref>. Different from existing methods that focus on static image representations, we improve MoCo temporally to gain robust video representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Generative Adversarial Learning</head><p>The adversarial learning is developed in <ref type="bibr" target="#b5">[6]</ref> where CNN is introduced to generate realistic image appearances from random noises. There are two subnetworks in a typical generative adversarial network (GAN). One is the generator and the other is the discriminator. The goal of the generator is to synthesize images that can fool the discriminator, while the discriminator is learned to distinguish between the real images and the synthetic ones from the generator. The generator and the discriminator are trained simultaneously by competing with each other. This unsupervised training mechanism outperforms traditional supervised training scheme to produce realistic image content. There are tremendous investigations on analyzing the GAN training <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8]</ref>. Meanwhile, there are many computer vision applications of GAN including image generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref>, object tracking <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16]</ref>, and semantic segmentation <ref type="bibr" target="#b36">[37]</ref>. Different from major GAN methods for generator learning, we augment training data in an adversarial form to train a temporally robust discriminator. This learning process is specially integrated into contrastive learning. The discriminator is our intended encoder to capture video representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VideoMoCo</head><p>Our VideoMoCo is built upon MoCo for unsupervised video representation learning. We first briefly review MoCo and then present our temporally adversarial learning and temporal decay. Furthermore, we visualize the temporal robustness of the feature by showing the entropy values of the classifier and the network attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MoCo Overview</head><p>Momentum Contrast (MoCo) provides a dictionary lookup for contrastive learning. Given an encoded query q and encoded keys {k 0 , k 1 , k 2 , ...} in a dictionary queue, the contrastive loss of MoCo can be written as:</p><formula xml:id="formula_0">L q = − log exp(q · k + /τ ) K i=0 exp(q · k i /τ )<label>(1)</label></formula><p>where τ is a scalar. The sum is over one positive and K negative samples. This loss tends to classify q as k + via a softmax classification process. The query q is the representation of an input sample via the encoder network, while the keys k i are the representations of the other training samples in the queue. The core of momentum contrast is to dynamically maintain the queue. The samples in the queue are progressively replaced following an FIFO (first in, first out) scheme. After computing the contrastive loss in Eq. (1), the encoder is updated via gradients while the momentum encoder is updated as a moving-average of the encoder weights. We denote the parameters of an encoder as θ q and those of a momentum encoder as θ k . The momentum encoder is updated as:</p><formula xml:id="formula_1">θ k ← mθ k + (1 − m)θ q<label>(2)</label></formula><p>where m ∈ [0, 1) is a momentum coefficient. The momentum encoder is updated slowly based on the encoder change, which ensures stable key representations. VideoMoCo improves MoCo by introducing temporally adversarial learning and temporal decay. Given an input video clip with a fixed number of frames, we send it to a generator and the encoder to produce q. Meanwhile, we reweigh exp(q · k i /τ ) by using t i where t ∈ (0, 1). Then, we follow MoCo to train the encoder and update the momentum encoder accordingly. During inference, we remove G and only use the encoder for feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporally Adversarial Learning</head><p>We propose adversarial learning as a temporal data augmentation strategy to improve feature representations. <ref type="figure" target="#fig_1">Fig. 2</ref> shows an overview. We have an input sample x where there are a fixed number of frames. The generator G takes x as input and produces a temporal mask. The architecture of G follows ConvLSTM <ref type="bibr" target="#b48">[49]</ref>. The output of ConvLSTM predicts the importance of each frame. We drop out 25% of the frames with high importance values by using the temporal mask. We denote the output of G as G(x). Then the query sample x query can be written as:</p><formula xml:id="formula_2">x query = G(x) ⊗ x<label>(3)</label></formula><p>where ⊗ indicates the temporal dropout operation. The feature map size of x query is the same as that of x while the content of the 25% of its frames is removed. We regard the encoder of MoCo as the discriminator. After obtaining x query , we send both x query and x to the discriminator D (i.e., the encoder). The feature representations of D(x query ) and D(x) are expected to become similar. We use L 1 -norm as the loss function to train G, which can be written as:</p><formula xml:id="formula_3">max G L(G) = E x∼Px (|D(x query ) − D(x)| 1 )<label>(4)</label></formula><p>where P x is the data distribution of x. When training D, we use the contrastive loss akin to Eq. (1), which can be written x and x query and compute their similarity loss. We use this loss term reversely to train G. During training iterations, G is learned to continuously attack D by removing different frames of x (i.e., temporal data augmentation), and D is learned to defend this attack by encoding temporally robust features. We use x query to perform contrastive learning to train D as shown in <ref type="figure">Fig. 3</ref>.  <ref type="figure">Figure 3</ref>. Temporally contrastive learning. We follow MoCo to use a memory queue to store keys. Besides, we introduce a temporal decay t i (t ∈ (0, 1)) to model the attenuation of each key in this queue during contrastive learning.</p><p>in the form of adversarial loss as:</p><formula xml:id="formula_4">min D L(D) = − log exp(D(x query ) · k + /τ ) K i=0 exp(D(x query ) · k i /τ )<label>(5)</label></formula><p>where the discriminator / encoder is learned to encode temporally robust feature representations. We take turns to train G and D during each iteration of adversarial learning. After training D, we update the momentum encoder by using Eq. (2). Initially, we train D without using G and only use contrastive learning shown in Eq. (1). When D is learned to approach a semi-stable state, we train D via Eq. (5) by involving G. We empirically observe that utilizing adversarial learning at the initial stage makes D difficult to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Decay</head><p>The adversarial learning illustrated in Sec. 3.2 operates on a mini-batch input following MoCo. When computing the contrastive loss, we notice that MoCo treats the contribution of keys from the queue only based on their representations as shown in Eq. (1). In practice, the momentum encoder evolves after keys entering the queue. The longer the keys in the queue, the more different their representations are compared to those of the current input samples.</p><p>In order to mitigate the discrepancy brought by momentum encoder evolvement, we propose a temporal decay to model key degradations. For each key k i in the queue, we set its corresponding temporal decay as t i where t ∈ (0, 1). Note that this key moves towards the end of the queue during each training iteration. Thus i gradually increases by 1 and t i decreases correspondingly. We can rewrite Eq. (5) by involving the temporal decay as:</p><formula xml:id="formula_5">min D L(D) = − log exp(D(x query ) · k + /τ ) K i=0 t i · exp(D(x query ) · k i /τ )<label>(6)</label></formula><p>where keys contribute to the current sample differently according to their existence time in the queue. In practice, we use Eq. (6) and Eq. (4) alternatively to train G and D. The remaining training procedure follows those of MoCo.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visualization</head><p>VideMoCo improves the temporal feature representation of MoCo by incorporating temporally adversarial learning and temporal decay. In this section, we show some visualizations on how temporal feature representations improve the video classification process. Given an input video sequence, MoCo and VideoMoCo both produce a set of classification scores. We use entropy to measure the classifier's confidence when making the prediction. The entropy computation can be written as follows:</p><formula xml:id="formula_6">H = − N i=1 p(x i ) log p(x i )<label>(7)</label></formula><p>where p(x i ) is the predicted probability of the i-th category. When the entropy value is high, it indicates that the classifier is more uncertain on making the current predictions. We show two input video sequences Billiards and Bal-anceBeam in <ref type="figure" target="#fig_4">Fig. 4</ref>. The complete Billiards video is shown in (a), both MoCo and VideoMoCo predict correctly. When we compute the entropy values of these two methods, we observe that VideoMoCo achieves a lower value than that of MoCo. This indicates that although both VideoMoCo and MoCo accurately predict the input video sequence, VideoMoCo is more confident about the current prediction. On the other hand, we increase the difficulty of (a) by partially occluding several frames as shown in (b). To this end, the entropy value of MoCo increases significantly while the value of VideoMoCo does not. This indicates that even though both MoCo and VideoMoCo can resist temporally occluded video sequences, the feature representation of MoCo has become very fragile while that of VideoMoCo does not degrade significantly.</p><p>Besides the Billiards video sequence, we show the Bal-anceBeam sequence in (c) where these two methods predict correctly. We notice that the entropy values of MoCo and VideoMoCo in (c) are much higher than those in (a). This indicates that compared to the Billiards sequence, the BalanceBeam sequence is more challenging. When we partially occlude several frames in (d), MoCo predicts incorrectly as IceDancing. In contrast, VideoMoCo predicts well. This accurate prediction indicates that the feature representation of VideoMoCo is temporally robust. The entropy values computed based on these two sequences indicate that feature representations in VideoMoCo are more robust than those of MoCo in the temporal domain.</p><p>Besides entropy computation, we compute the attention maps of MoCo and VideoMoCo to visualize network attentions. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the visualization result. Two input frames are in (a). The attention maps from MoCo are in (b), and the attention maps from VideoMoCo are in (c). We note that on the first row, VideoMoCo focuses more on the region where eyebrow pencil tip largely moves. This indicates that the features learned from VideoMoCo attend the network to the temporal motions. In comparison, the attention map of MoCo shows that the network does not pay much atten- tion. In the second row, the attention map from VideoMoCo contains higher attention values at the hand region than that of MoCo. The attention map visualization indicates that VideoMoCo is more effective in concentrating on the video motion areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we introduce implementation details and training configurations of VideoMoCo on the K-400 dataset. To analyze the positive effects brought by the temporally adversarial learning and temporal decay, we conduct an ablation study to show the performance improvement by using each of them. Also, we compare Video-MoCo with state-of-the-art self-supervised video representation methods on two standard action recognition benchmarks UCF101 and HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Datasets</head><p>We illustrate the details of benchmark datasets used for the training and inference stages.</p><p>Kinetics-400 Dataset. Kinetics-400 (K-400 <ref type="bibr" target="#b17">[18]</ref>) is a largescale action recognition dataset. There are 400 human action categories in total and the whole video sequence amount is 306k. This dataset is split as training, validation, and inference parts. We use the video sequences of the training part to train VideoMoCo. The number of training video sequences is about 240k.</p><p>UCF101 Dataset. The UCF101 dataset <ref type="bibr" target="#b35">[36]</ref> is widely used for action recognition. This video sequence is collected from the Internet with predefined 101 action categories. There are over 13k video clips consuming 27 hours. The whole dataset is divided into three training and testing splits. We use the training split 1 to finetune the feature backbone and use the testing split 1 for performance evaluation.</p><p>HMDB51 Dataset. There are 101 video clips in the HMDB51 dataset <ref type="bibr" target="#b14">[15]</ref> with 51 action categories. This dataset is divided into three training and testing splits. We use the training split 1 to finetune the feature backbone and use the testing split 1 for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We illustrate the details of how to integrate the generator G into the pretext contrastive learning process. The discriminator D is the encoder to be utilized for feature extraction. After contrastive learning, we remove G and only use the encoder for downstream finetuning. The downstream finetuning is the same as existing unsupervised video representation learning methods.</p><p>Pretext Contrastive Learning. Our generator is based on ConvLSTM <ref type="bibr" target="#b48">[49]</ref> where there is one LSTM with 256 hidden units for temporal feature extraction. After taking input video sequences, ConvLSTM first uses several convolutional layers to extract features and reshapes these features into vector forms. The vector is then sent to the LSTM to predict the temporal importance of each video frame. We follow prior work to utilize the small variant of the C3D architecture as our encoder. When training on the K-400 dataset, we consecutively sample 32 frames as a training sample. Note that we sample 32 frames densely within one video sequence to create multiple training samples. After obtaining these training samples, we perform a randomly spatial crop for a fixed size of the input (i.e., 32 × 112 × 112 × 3). Meanwhile, we apply horizontal flipping, color jittering, and random decolorization with each frame. During the training process, we send each sample into G and drop out k frames with high importance values. Then, this sample is regarded as the query to train D as illustrated in Eq. 3. We use an SGD solver to iteratively optimize both G and D with an initial learning rate of 0.02. The momentum is set as 0.9 and the batch size is 128. In practice, we first take 100 epochs to train D for initialization, and then train G and D via adversarial learning for the remaining 100 epochs. We find this empirical design effective to train the encoder.</p><p>Downstream finetuning. We use two datasets, UCF101 and HMDB51, to finetune the learned feature backbone. The discriminator is kept after pretext training and we regard it as the encoder. The weights of the fully-connected layer are randomly initialized during the finetuning stage. On each dataset, we train the whole network for 10 epochs and then evaluate their performance on the testing data. The experimental setups and data pre-processing method are the <ref type="table">Table 1</ref>. Ablation analysis on adversarial learning and dropout amount k on UCF101 and HMDB51 datasets. We set k = 8 for the random dropout. Adversarial learning improves the baseline while random dropout decreases. k = 8 is optimal to achieve balanced adversarial learning. same as those of the pretext stage except that the batch size is set as 32 and the learning rate is 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental</head><p>Evaluation. We adopt the standard evaluation protocol <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b25">26]</ref> during testing. To predict each video sequence, we take 10 video clips uniformly from each testing video sequence and average these prediction results. The top-1 accuracy in the action recognition metric is utilized to measure recognition accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We improve MoCo by integrating temporally adversarial learning and temporal decay. In this section, we analyze how these two modules improve the original performance of MoCo. The analysis is based on the configuration that we first train MoCo via pretext and downstream finetuning by using video data. Then, we integrate temporally adversarial learning and temporal decay independently into MoCo during the pretext training. Finally, we combine both of them within MoCo during pretext training. The downstream finetuning is the same for all the configurations. The feature backbone we use is R(2+1)D. UCF101 is used as the training and testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Temporally Adversarial Training</head><p>We validate the effectiveness of temporally adversarial learning by showing the performance improvement upon the baseline. We first implement the baseline by not using the generator G during training. The standard memory queue from MoCo is also adopted. Second, we train the encoder by randomly dropping out several frames and use the remaining frames for adversarial learning. Third, we train the encoder by using the generator to adaptively drop out  <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>) and show the recognition performance accordingly. <ref type="table">Table 1</ref> shows the analysis results. We notice that by using a random dropout scheme, the recognition performance degrades compared to the baseline performance. This is because random dropout does not enable VideoMoCo to learn temporally robust features. By introducing adversarial learning, the recognition performance is improved (i.e., 75.9%/46.2% v.s 77.8%/49.1%) on both datasets. This indicates the effectiveness of our adversarial learning. Meanwhile, we analyze how the dropout amount influences the recognition performance by using different k. The results show that the highest performance is achieved when k = 8. This is because a small k (i.e., k = 4) does not make the sample adversarial enough to train D, while a large k (i.e., k = <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>) breaks the balance between G and D during adversarial learning. Furthermore, the performance of VideoMoCo is further improved by using temporal decay. This indicates that adversarial learning and temporal decay are effective to improve the baseline recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Temporal Decay</head><p>We analyze how temporal decay influences the recognition performance of VideoMoCo.</p><p>We follow MoCo to set the size of the memory queue as 65536, and the momentum value of the encoder update is 0.999. The temporal decay t we choose is [0.999, 0.9999, 0.99999, 0.999999, 0.9999999]. These values reflect how the increase of t influences the recognition performance. <ref type="table" target="#tab_2">Table 2</ref> shows the analysis results where t = 0.99999 achieves the premier performance. When there is no tempo- ral decay (i.e., t = 1), keys in the queue contribute equally during contrastive learning. If t is not relatively large (i.e., t = 0.999), only the latest 8000 keys contribute to the learning process (i.e., t 8000 ≈ 0.0009). To this end, we set t = 0.99999 to ensure that the contributions are from all keys while the contribution from the oldest key halves (i.e., t 65536 ≈ 0.52).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art Approaches</head><p>We compare our approach with several state-of-the-art self-supervised video representation learning methods in both UCF101 and HMDB51 datasets. <ref type="table" target="#tab_3">Table 3</ref> shows the evaluation results where the architectures, input size, number of parameters and pretrained dataset are illustrated as well. The second block in <ref type="table" target="#tab_3">Table 3</ref> indicates that our method performs favorably against existing approaches under the 3D-ResNet18 backbone on both UCF101 and HMDB51 datasets. We notice that DPC uses R3D-34 as a feature backbone whose parameter amount is 3× more than ours. Also, the input size of DPC is larger than ours. Nevertheless, we achieve similar recognition performance on UCF101 and exceeds DPC on HMDB51. When using the same architecture (i.e., 3D-ResNet18) and similar input size, VideoMoCo outperforms DPC by a large margin (i.e., 5.9% gain in UCF101 and 9.1% gain in HMDB51). We also evaluate our method using the R(2+1)D architecture and achieve the premier performance as shown in the third block of <ref type="table" target="#tab_3">Table 3</ref>. Specifically, VideoMoCo improves the existing method <ref type="bibr" target="#b42">[43]</ref> under the same configuration. We note that even though VideoMoCo is trained using K-400 while other methods are trained using UCF101, the performance of VideoMoCo is still premier on both UCF101 and HMDB51 test sets. These evaluations indicate the favorable performance of VideoMoCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>We propose VideoMoCo for self-supervised video representation learning. Different from empirical pretext task investigation, we delve into MoCo and empower its temporal representation by introducing temporally adversarial learning and temporal decay. We treat the encoder as the discriminator and use a generator to perform adversarial learning. The generator augments training samples to robustify discriminators to capture temporally robust feature representations. The training process of the discriminator is from contrastive learning. Meanwhile, we propose temporal decay to model the attenuation of older keys in the queue. These keys ought to contribute less to the current input sample during the learning process. Our adversarial learning improves the temporal robustness of contrastive learning and the learned feature backbone is effective for downstream recognition tasks. The extensive experiments on the standard action recognition datasets UCF101 and HMDB50 demonstrate that our VideoMoCo has sufficiently improved MoCo and performs favorably against state-of-the-art self-supervised video representation learning approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Temporally adversarial learning. The input sample x is a video clip. The generator G containing LSTM dropouts several frames of x to generate x query . The discriminator D (i.e., encoder) extracts features from both</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualizations of CNN predictions between MoCo and VideoMoCo. Given a video sequence, we compute the entropy of the CNN predictions and the classification results. A higher entropy value indicates the CNN classifier is more uncertain about the current predictions. Although both MoCo and VideoMoCo classify video sequences correctly as shown in (a) and (c), the entropy of VideoMoCo is lower than that of MoCo. Meanwhile, if video sequences are partially occluded as shown in (b) and (d), the entropy of VideoMoCo does not increase as much as that of MoCo. The temporally robust feature representations empower VideoMoCo to make correct predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Attention visualization of MoCo and VideoMoCo. We show video frames in (a), the attention maps from MoCo in (b), and the attention maps from VideoMoCo in (c). In the attention visualization maps, pixels marked as red indicate that the network pays more attention to the current region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Analysis on different values of temporal decay t. We experimentally find that t = 0.99999 achieves the best.</figDesc><table><row><cell cols="2">Configuration</cell><cell>Evaluation</cell></row><row><cell>Adv. Decay</cell><cell>Method</cell><cell>UCF101</cell></row><row><cell>×</cell><cell>t=1</cell><cell>77.8</cell></row><row><cell></cell><cell>t = 0.999</cell><cell>73.4</cell></row><row><cell></cell><cell>t = 0.9999</cell><cell>75.2</cell></row><row><cell></cell><cell>t = 0.99999</cell><cell>78.7</cell></row><row><cell></cell><cell>t = 0.999999</cell><cell>78.3</cell></row><row><cell></cell><cell>t = 0.9999999</cell><cell>77.9</cell></row><row><cell></cell><cell>t = 0.99999</cell><cell>78.7</cell></row><row><cell>×</cell><cell>t = 0.99999</cell><cell>76.1</cell></row><row><cell cols="3">several frames. Besides training configurations, we analyze</cell></row><row><cell cols="3">how the amount of dropout frames influences the down-</cell></row><row><cell cols="3">stream recognition performance. We choose to drop out</cell></row><row><cell>k frames (k ∈ [</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-the-art self-supervised learning methods on UCF101 and HMDB51 datasets. Our method Video-MoCo performs favorably against existing methods with a relatively small feature backbone.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell>Input size</cell><cell>Params</cell><cell>Dataset</cell><cell>UCF101</cell><cell>HMDB51</cell></row><row><cell>Shuffle&amp;Learn [29]</cell><cell>Alexnet</cell><cell>256 × 256</cell><cell>62.4M</cell><cell>UCF101</cell><cell>50.2%</cell><cell>18.1%</cell></row><row><cell>Deep RL [1]</cell><cell>CaffeNet</cell><cell>227 × 227</cell><cell>-</cell><cell>UCF101</cell><cell>58.6%</cell><cell>25.0%</cell></row><row><cell>OPN [23]</cell><cell>VGG-M-2048</cell><cell>80 × 80</cell><cell>8.6M</cell><cell>UCF101</cell><cell>59.8%</cell><cell>23.8%</cell></row><row><cell>O3N [4]</cell><cell>AlexNet</cell><cell>227 × 227</cell><cell>62.4M</cell><cell>UCF101</cell><cell>60.3%</cell><cell>32.5%</cell></row><row><cell>Spatio-Temp [42]</cell><cell>C3D</cell><cell>112 × 112</cell><cell>58.3M</cell><cell>UCF101</cell><cell>58.8%</cell><cell>32.6%</cell></row><row><cell>Spatio-Temp [42]</cell><cell>C3D</cell><cell>112 × 112</cell><cell>58.3M</cell><cell>K-400</cell><cell>61.2%</cell><cell>33.4%</cell></row><row><cell>VCOP [50]</cell><cell>C3D</cell><cell>112 × 112</cell><cell>58.3M</cell><cell>UCF101</cell><cell>65.6%</cell><cell>28.4%</cell></row><row><cell>RTT [14]</cell><cell>C3D</cell><cell>112 × 112</cell><cell>58.3M</cell><cell>UCF101</cell><cell>69.9%</cell><cell>39.6%</cell></row><row><cell>DPC [10]</cell><cell>3D-ResNet34</cell><cell>224 × 224</cell><cell>32.6M</cell><cell>K-400</cell><cell>75.7%</cell><cell>35.7%</cell></row><row><cell>RotNet3D [17]</cell><cell>3D-ResNet18</cell><cell>224 × 224</cell><cell>33.6M</cell><cell>K-400</cell><cell>62.9%</cell><cell>33.7%</cell></row><row><cell>ST-puzzle [19]</cell><cell>3D-ResNet18</cell><cell>224 × 224</cell><cell>33.6M</cell><cell>K-400</cell><cell>65.8%</cell><cell>33.7%</cell></row><row><cell>DPC [10]</cell><cell>3D-ResNet18</cell><cell>128 × 128</cell><cell>14.2M</cell><cell>K-400</cell><cell>68.2%</cell><cell>34.5%</cell></row><row><cell>Ours</cell><cell>3D-ResNet18</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>K-400</cell><cell>74.1%</cell><cell>43.6%</cell></row><row><cell>VCP [26]</cell><cell>R(2+1)D</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>UCF101</cell><cell>66.3%</cell><cell>32.2%</cell></row><row><cell>VCOP [50]</cell><cell>R(2+1)D</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>UCF101</cell><cell>72.4%</cell><cell>30.9%</cell></row><row><cell>PRP [52]</cell><cell>R(2+1)D</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>UCF101</cell><cell>72.1%</cell><cell>35.0%</cell></row><row><cell>RTT [14]</cell><cell>R(2+1)D</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>UCF101</cell><cell>81.6%</cell><cell>46.4%</cell></row><row><cell>Pace Prediction [43]</cell><cell>R(2+1)D</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>K-400</cell><cell>77.1%</cell><cell>36.6%</cell></row><row><cell>Ours</cell><cell>R(2+1)D</cell><cell>112 × 112</cell><cell>14.4M</cell><cell>K-400</cell><cell>78.7%</cell><cell>49.2%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal cycleconsistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deligan: Generative adversarial networks for diverse and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video representation learning by recognizing temporal transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Givi</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hmdb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust tracking against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking image inpainting via a mutual encoderdecoder with feature equalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video cloze procedure for self-supervised spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjovsky</forename><surname>Sc Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">fgan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning via neighborhood-relational encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crest: Convolutional residual learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vital: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Center for Research in Computer Vision</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised deep representation learning for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking image deraining via rain streaks and vapors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Video representation learning with visual tempo consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15489</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Yew-Soon Ong, and Chen Change Loy. Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
