<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextCaps : Handwritten Character Recognition with Very Small Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinoj</forename><surname>Jayasundara</surname></persName>
							<email>vinojjayasundara@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirunima</forename><surname>Jayasekara</surname></persName>
							<email>nhirunima@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga</forename><surname>Rodrigo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Sandaru Jayasekara</orgName>
								<orgName type="institution">University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Moratuwa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Suranga Seneviratne</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Moratuwa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TextCaps : Handwritten Character Recognition with Very Small Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many localized languages struggle to reap the benefits of recent advancements in character recognition systems due to the lack of substantial amount of labeled training data. This is due to the difficulty in generating large amounts of labeled data for such languages and inability of deep learning techniques to properly learn from small number of training samples. We solve this problem by introducing a technique of generating new training samples from the existing samples, with realistic augmentations which reflect actual variations that are present in human hand writing, by adding random controlled noise to their corresponding instantiation parameters. Our results with a mere 200 training samples per class surpass existing character recognition results in the EMNIST-letter dataset while achieving the existing results in the three datasets: EMNIST-balanced, EMNIST-digits, and MNIST. We also develop a strategy to effectively use a combination of loss functions to improve reconstructions. Our system is useful in character recognition for localized languages that lack much labeled training data and even in other related more general contexts such as object recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Handwritten character recognition is a nearly solved problem for many of the mainstream languages thanks to the recent advancements in deep learning models <ref type="bibr" target="#b0">[1]</ref>. Nonetheless, for many other languages, handwritten digit recognition remains a challenging problem due to the lack of sufficiently large labeled datasets that are essential to train deep learning models <ref type="bibr" target="#b1">[2]</ref>. While conventional models such as linear classifiers, K-nearest neighbors, non-linear classifiers, and Support Vector Machines (SVM) <ref type="bibr" target="#b2">[3]</ref> can be used for this task, they are not able to achieve the near human level performances provided by deep learning models. Convolutional Neural Networks (CNN) have achieved stateof-the-art results due to their ability to encode deep features and spatial understanding. Although CNNs are good at understanding low level and high level features in images, by doing so, they lose valuable information at pooling layers. CNNs require large number of training samples (usually in the scale of thousands or tens of thousands per class) to train and classify images successfully. As a result, there is a strong interest in training CNNs with a lesser number of training samples.</p><p>In this paper, we propose a technique which tackles this problem of the labeled dataset being small in size, with the aid of Capsule Networks (CapsNets) <ref type="bibr" target="#b3">[4]</ref>. We exploit their ability to augment data just by manipulating the instantiation parameters <ref type="bibr" target="#b4">[5]</ref>. CapsNets learn the properties of an image-in this case a character-in addition to its existence. This makes them useful in learning to recognize characters with a less amount of labeled data. Our architecture is based on the CapsNet architecture proposed by Sabour et al. <ref type="bibr" target="#b3">[4]</ref>, which comprises a capsule network and a fully connected decoder network. We replace the decoder network with a deconvolutional network while doing minor alterations to the capsule network. By adding a controlled amount of noise to the instantiation parameters that represent the properties of an entity, we transform the entity to characterize actual variations that happen in reality. This results in a novel data generation technique, much more realistic than augmenting data with affine transformations. As the reconstruction accuracy is also important in many contexts, we present an empirically appropriate strategy of combining loss functions which significantly improves the reconstruction. Our system achieves results that are on-par with the state-of-the-art with just 200 data points per class, while achieving even better results with larger volumes of training data.</p><p>The key contributions of this paper are as follows:</p><p>• We outperform the state-of-the-art results in EMNISTletters, EMNIST-balanced and EMNIST-digits character datasets, by training our system on all the training samples available. • We evaluate the proposed architecture on a noncharacter dataset, Fashion-MNIST, to ensure the flexibility and robustness. We achieve very good results with 200 training samples and achieve the state-of-theart with the full dataset. • We propose a novel technique for training capsule networks with small number of training samples, as small as 200 per class, and keeping the same set of test samples, while achieving the state-of-the-art performance.</p><p>Our method require only 10% of data necessary for a state-of-the-art system, to produce similar results. • We propose and evaluate several variations to the decoder network and analyze its performance with different loss functions to provide a strategy to select a suitable combination of loss functions. Rest of the paper is organized as follows: in Section 2 we discuss the related works, and in Section 3 we explain our methodology. Subsequently, we discuss our results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>MNIST <ref type="bibr" target="#b5">[6]</ref> is the widely used benchmark for the handwritten digit recognition task. Multiple works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> have used CNN models on MNIST dataset and have achieved results in excess of 99% accuracy. Apart from digit recognition, several attempts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have been made in handwritten character recognition with EMNIST datasets <ref type="bibr" target="#b11">[12]</ref>. A bidirectional neural network is introduced in <ref type="bibr" target="#b13">[14]</ref> which is capable of performing both image classification and image reconstruction by adding a style memory to the output layer of the network.</p><p>The idea of a capsule was introduced in 2011 by <ref type="bibr" target="#b4">[5]</ref>, as a transforming autoencoder. With a three layered CapsNets architecture, and by training the network using dynamic routing, authors of <ref type="bibr" target="#b3">[4]</ref> have achieved 0.25% error rate on the MNIST dataset. This architecture consists of a primary capsule, which was built by stacking convolutional layers, and a fully connected capsule, which uses routing by agreement between higher level capsules and lower level capsules. We draw intuition for this paper from the concept of instantiation parameters proposed in <ref type="bibr" target="#b4">[5]</ref>, while emphasizing that our work is significantly novel and different from <ref type="bibr" target="#b4">[5]</ref>.</p><p>We identified two main solutions in the literature to the low data issue, namely one-shot learning and new data generation. An example of the former is the Siamese networks as proposed in <ref type="bibr" target="#b14">[15]</ref>. A one-shot learning deep model was proposed by Bertinetto et al. <ref type="bibr" target="#b15">[16]</ref>, where they used a second network to predict the parameters of the first network. Since one-shot learning solutions are mostly application-specific, we turn to a new data generation approach.</p><p>Existing literature offers several successful data generation techniques. Although GANs <ref type="bibr" target="#b16">[17]</ref> can be used to generate data, a basic form of a GAN network will not be sufficient to augment the dataset for training, since it can not generate labelled data, unless a separate GAN is trained per class. Another potential choice which has image generation capabilities, Variational Autoencoders (VAEs) <ref type="bibr" target="#b17">[18]</ref> have similar problems. VAEs represent all the images as 1D vectors, whereas capsule networks have dedicated dimensions for each class. As a result, when VAE's 1D vectors are perturbed, there is a high probability that those changes affect multiple classes. Data augmentation techniques such as jittering and flipping (not suitable for characters) offer limited amount of simple augmentation. Thus, they can not offer complex and subtle variations that are more closer to the human variations. A comprehensive comparison of our results with these techniques are provided in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This sections outlines our approach. Prior to experimenting with reduced datasets, in Section 3.1, we attempt to surpass the state-of-the-art results for several hand written digit databases including EMNIST balanced, EMNIST letters and EMNIST digits, with the use of all the training samples provided. Subsequently, we attempt to achieve the state-of-the-art performance using a limited number of training samples, as low as 200 training samples per class, as opposed to, for example, 2400 data points per class in EMNIST balanced and 4800 data points per class in EM-NIST letters.</p><p>In order to address the drawbacks faced when training the classifier with low number of training samples, we propose a novel technique for increasing the number of training samples in Section 3.2. We perform a comprehensive analysis of the effect of the loss function in reconstruction, in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Character Recognition with Capsule Networks</head><p>For the character recognition task, we propose an architecture comprising of a capsule network and a decoder network, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In the capsule network, the first three layers are convolutional layers with 64 3×3 kernels with stride 1, 128 3×3 kernels with stride 1 and 256 3×3 kernels with stride 2 respectively. The fourth layer is a primary capsule layer with 32 channels of 8-dimensional capsules, with each primary capsule containing 8 convolutional units with a 9×9 kernel and a stride of 2. The fifth layer, termed as the character capsule layer, is a fully connected capsule layer with a 16dimensional capsule per class, resulting in M capsules for a dataset with M number of classes. We use dynamic routing  between the primary capsule layer and the character capsule layer, as proposed by <ref type="bibr" target="#b3">[4]</ref>, with 3 routing iterations. The input to the capsule network is a set of J, 28 × 28 images and the output is a J × M × 16 dimensional tensor C, containing the corresponding instantiation parameters, where each C j , j ∈ [J] is the instantiation parameter matrix of the j th training sample.</p><p>Prior to passing C as the input to the decoder network, the corresponding instantiation parameters should be masked with zeros for all the classes except the true class. Hence, the masked tensor C is still a J × M × 16 dimensional matrix, yet containing only the instantiation parameters corresponding to the true class as the non-zero values.</p><p>The decoder network comprises one fully connected layer, followed by five deconvolutional layers <ref type="bibr" target="#b18">[19]</ref> with parameters as shown by <ref type="figure" target="#fig_1">Fig. 2</ref>. The input to the decoder is the masked matrix C, and the output of the decoder is the set of reconstructed 28 × 28 images. Except for the final deconvolution layer, which has sigmoid activation, the fully connected layer and the other deconvolution layers have ReLU activation.</p><p>First, we train the proposed model with the full training sets and evaluate its performance. Subsequently, in an effort to address the issue of lack of high number of training samples in character recognition and similar tasks as elaborated in section 1, we attempt to achieve the state-of-the-art performance using 200 training samples per class, using the same network.</p><p>By examining the results of the above section with low number of training samples, as elaborated in Section 4.1, we observed that even though the capsule network performance achieved the state-of-the-art, the decoder network fails to achieve acceptable reconstruction. The most obvious solution to enhancing the performance of the decoder network is to increase the number of training samples, by generating new training samples from the samples available in the original (reduced) training set. Therefore, we propose a novel method of generating new training samples by augmenting original training samples with the aid of the concept of instantiation parameters in the CapsNets, as described by the following Section 3.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Technique for Image Data Generation Using Perturbation of Instantiation Parameters</head><p>From the concept of instantiation parameters in capsule networks, we can represent any character using a 16 dimensional vector <ref type="bibr" target="#b3">[4]</ref>. With a pre-trained decoder network, we can successfully reconstruct the original image, by using only this instantiation parameter vector. The intuition behind our proposed perturbation algorithm is that by adding controlled random noise to the values of the instantiation vector, we can create new images, which are significantly different from the original images, effectively increasing the size of the training dataset. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the variations of an image, when one particular instantiation parameter is changed thusly. Similarly, each of the instantiation parameter is responsible for a certain property of the image, individually or collectively. Hence, we propose a novel technique of generating a new dataset, from a dataset with limited amount of training samples, as illustrated by <ref type="figure" target="#fig_3">Fig. 4</ref> and Algorithm 1.</p><p>First, as illustrated by <ref type="figure" target="#fig_3">Fig. 4(a)</ref>, we train the network   From <ref type="figure" target="#fig_4">Fig. 5</ref>, we clearly observe that training with such low number of training samples result in poor reconstruction performance. The subtle variations in the input characters are absent from the reconstructions, in addition to being blurred. Hence, we cannot directly apply the concept of perturbation and create new training samples from a such poorly trained model. First, we attempt to eliminate the blurriness in the decoder network output, by proposing the following technique, illustrated by <ref type="figure" target="#fig_3">Fig. 4(c)</ref>. For each reconstructed image in I D,recon , we perform unsharp masking <ref type="bibr" target="#b19">[20]</ref> with radius = 1, threshold = 1 and unsharp strength = 10 times, which sharpens the reconstructed images. Then we combine the new sharpened image set I D,recon,sharped with the initial I D,recon set, in order to create a new target set for the decoder M 1,dec . Subsequently, we re-train the decoder for 10 epochs with this new target set, in order to obtain an improved decoder M 1,dec which provides sharper reconstructions than M 1,dec .</p><p>After training, there can be training samples which are not properly learned, and hence are wrongly reconstructed. If these wrongly reconstructed samples are considered for perturbing and creating new samples, it may result in missclassified samples in the newly generated training dataset. Therefore, prior to applying perturbation and creating new training samples, it is necessary to remove such training samples, after the model is trained.</p><p>Subsequently, we perform new data generation by perturbation, as illustrated by <ref type="figure" target="#fig_3">Fig. 4(d)</ref>. For non-zero instantiation parameters in C D,original , we add random controlled noise (Algorithm 1). Here, we perturb only one instantiation parameter at a time to generate new samples to avoid distortions. Hence, a method of selection of which instantiation parameter to perturb is required. We observed that, for a given class, there exists a relationship between the variance of an instantiation parameter and the actual physical variations in the generated images. Higher the variance, we observe rapid variations and vice versa. Hence, for each instantiation parameter k ∈ [0, 15] in each class m ∈ M , we calculate the variance, σ m,k , across all the training samples that belongs to m, and sort in the descending order. We have 16 choices for the value of a, with a = 0 representing the instantiation parameter with the highest variance and a = 15 representing that with lowest variance. For this study, we generate two datasets with a = 0 and a = 1.</p><p>For a given a, we calculate the noise value to add for each instantiation parameter considered. The adjusted value of the instantiation parameter should not exceeded the max-imum value that it can take for a given class. Adding noise to instantiation parameters without any restrictions will lead to various distortions in the reconstructed images, as illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref> below. The H has visually changed classes to A, and a is visually unrecognizable anymore. Such distortions are detrimental for a data generation technique. Therefore, we propose a carefully designed control mechanism to avoid such distortions, without manual elimination by visual inspection. Hence, for each instantiation parameter k of the class m, we calculate the maximum noise that can be added, τ m,k . We further constrain the noise by τ k , which is the average maximum noise that can be added for the instantiation parameter k across all the classes, even though τ m,k allows for higher values. This is to prevent sudden high variations occurring after perturbations. Hence, the noise value added for any k is capped at τ k . Subsequently, we obtain the new reconstructed images, I D,perturbed , which are significantly different from the original training images, I D,original , by passing the perturbed instantiation parameter tensor to the decoder M 1,dec . Finally, we have two new sets of training samples generated with a = 0 and a = 1. We combine these two sets and obtain 50 random samples per class (user's discretion), to formulate the new dataset D perturbed . We combine D perturbed and D original , which will effectively increase the number of training samples, solving our target problem. Subsequently, as illustrated by <ref type="figure" target="#fig_3">Fig. 4(e)</ref>, we train a new model, M 2 with the new D original + D perturbed dataset, retrain the decoder with the proposed re-training technique and obtain the final model for character classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Various Reconstruction Loss Functions</head><p>We investigate the effect on reconstruction based on the loss function used for reconstruction in a capsule network, in order to identify a well-suited reconstruction loss function for the TextCaps model. First, we study the variations on reconstruction with different loss functions for different number of training samples on the EMNIST-Balanced dataset, and then we extend our analysis to various combinations of loss functions. In this analysis, both spatial and structural similarity measures are used as reconstruction loss functions.</p><p>Since different loss functions we use produce outputs in different scales, it is not possible compare the losses directly. Therefore, we use Peak Signal-to-Noise Ratio (PSNR) given by <ref type="bibr" target="#b0">(1)</ref>, as an independent (of reconstruction loss functions) measure, to determine the quality of reconstructed images.</p><p>P SN R = 10 log 10 M AX 2 i M SE</p><p>where M AX i is the maximum possible pixel value of the image (1 in our case) and M SE is the Mean Squared Error between the test and reconstructed images.</p><p>Let x(p) be the intensity of the p th reconstructed pixel and y(p) be the intensity of the p th true input pixel and N be the total number of pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">MSE</head><p>Following Sabour et al. <ref type="bibr" target="#b3">[4]</ref>, we use M SE, as the loss function for reconstruction, defined by,</p><formula xml:id="formula_1">M SE = 1 N N i=1 (x(p) − y(p)) 2 (2) 3.3.2 L 1 Norm</formula><p>To remove artifacts introduced by M SE, we consider L 1 norm as a loss function which is defined by,</p><formula xml:id="formula_2">L 1 = 1 N N i=1 |x(p) − y(p)| (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">SSIM</head><p>L 1 and M SE do not capture the spatial relationship between pixels. We use SSIM proposed in <ref type="bibr" target="#b20">[21]</ref> to capture spatial relationship between the input image and reconstructed image. SSIM for x, y and the loss function for SSIM , structural dissimilarity (DSSIM ), are defined by,</p><formula xml:id="formula_3">SSIM (x, y) = (2µ x µ y + C 1 ) (µ 2 x + µ 2 y + C 1 )</formula><p>.</p><formula xml:id="formula_4">(2σ xy + C 2 ) (σ 2 x + σ 2 y + C 2 ) (4) DSSIM = 1 N n i=1 1 − SSIM (p)<label>(5)</label></formula><p>where µ x , µ y and σ 2 x , σ 2 y are the means and variances and σ 2 xy is the covariance of reconstructed and true input pixel intensities. C 1 = (K 1 L) 2 and C 2 = (K 2 L) 2 where L is the dynamic range of the pixel values (typically, 2 m − 1, where m is the number of bits per pixel) and K 1 , K 2 are small constants (K 1 = 0.01 and K 2 = 0.03).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Binary Cross-Entropy (BCE)</head><p>BCE is often used as a measure to identify the difference between two distributions, which is defined by,</p><formula xml:id="formula_5">BCE = − 1 N n i=1</formula><p>[y(p) log(x(p))+(1−y(p)) log(1−x(p))]</p><formula xml:id="formula_6">(6) 3.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.5 Combinations of Loss Functions</head><p>To combine two loss functions, rather than linearly combining two loss equations mathematically, we propose a method which combines two reconstructed images together. We slightly modify the CapsNet decoder network using two decoders, one for each loss function and generate two separate reconstruction outputs. Then we compare the absolute values of the difference of each pixel value between the two reconstructed outputs and the test images independently, and assign the pixel value which is closer to the test image to the final reconstructed output. Different loss function combinations we use here for two decoders are</p><formula xml:id="formula_7">L 1 &amp; DSSIM , L 1 &amp; BCE, M SE &amp; DSSIM , M SE &amp; BCE and BCE &amp; DSSIM .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>For each dataset in <ref type="table" target="#tab_1">Table 1</ref>  <ref type="table">Table 2</ref> compares our results to the state-of-the-art. We include the results that we obtained with the full training sets, as well as using only 200 training samples per class. In both instances, we have used the full testing sets shown in <ref type="table" target="#tab_1">Table 1</ref>, to report the average accuracies. We use a combination of marginal loss and the reconstruction loss for training as proposed in <ref type="bibr" target="#b3">[4]</ref>, and further, the training procedure followed for every experiment in this paper is similar to <ref type="bibr" target="#b3">[4]</ref>. For each dataset, we use ensembling to improve our model accuracy, and to avoid over fitting. We use cyclic learning rates for each 30 epochs, giving us 3 ensemble models with 90 epochs <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Handwritten Character Classification</head><p>First, we describe the results we obtained with full training sets and compare with the state-of-the-art. On EMNIST- <ref type="table">Table 2</ref>. Comparison of TextCaps with state-of-the-art results, the mean and the standard deviation from 3 trials are shown</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMNIST-Letters Implementation</head><p>With full train set With 200 samp/class Cohen et al. <ref type="bibr" target="#b11">[12]</ref> 85.15% -Wiyatnoet al. <ref type="bibr" target="#b13">[14]</ref> 91.27% -TextCaps 95.36 ± 0.30% 92.79 ± 0.30%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMNIST-Balanced Implementation</head><p>With full train set With 200 samp/class Cohen et al. <ref type="bibr" target="#b11">[12]</ref> 78.02% -Dufourq et al. <ref type="bibr" target="#b12">[13]</ref> 88.3% -TextCaps 90.46 ± 0.22% 87.82 ± 0.25%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMNIST-Digits Implementation</head><p>With full train set With 200 samp/class Cohen et al. <ref type="bibr" target="#b11">[12]</ref> 95.90% -Dufourq et al. <ref type="bibr" target="#b12">[13]</ref> 99.3% -TextCaps 99.79 ± 0.11% 98.96 ± 0.22%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>With full train set With 200 samp/class Sabour et al. <ref type="bibr" target="#b3">[4]</ref> 99.75% -Cireşan et al. <ref type="bibr" target="#b7">[8]</ref> 99.77% -Wan et al. <ref type="bibr" target="#b23">[24]</ref> 99.79% -TextCaps 99.71 ± 0.18% 98.68 ± 0.30%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashion MNIST Implementation</head><p>With full train set With 200 samp/class Xiao et al. <ref type="bibr" target="#b21">[22]</ref> 89.7% -Bhatnagar et al. <ref type="bibr" target="#b24">[25]</ref> 92.54% -Zhong et al. <ref type="bibr" target="#b25">[26]</ref> 96  <ref type="bibr" target="#b12">[13]</ref> by 0.49%. For MNIST and Fashion-MNIST, our system produced sub-state-of-the-art accuracy. Yet, our results are on-par. Subsequently, we describe and compare the results we obtained with only 200 training samples per class. On EMNIST-letters, we exceed the state-of-the-art results by 1.52%. However for EMNIST-balanced, EMNIST-digits, MNIST we were able to achieve the state-of-the-art results. Even though our system did not surpass the state-of-the-art performance, we highlight that we were able to achieve a near state-of-the-art performance using only 8-10% of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results of the Proposed Image Data Generation Technique</head><p>In this section, we present the results of the decoder retraining technique and the new image data generation by perturbation technique proposed in Section 3.2. We evaluate the success as well as the limitations of the two techniques by referring to these results. <ref type="figure" target="#fig_7">Fig.7 (a)</ref> illustrates several sample images from the EMNSIT-balanced test set and (b) illustrates the corresponding reconstructed images by M 1,dec , which was trained using 200 training samples per class. <ref type="figure" target="#fig_7">Fig. 7 (c)</ref> illustrates the corresponding reconstructions by M 1,dec , which was obtained by re-training M 1,dec using the proposed technique. It is evident that reconstructions by M 1,dec are much sharper than those by M 1,dec . Therefore, our proposed decoder re-training technique is highly successful in sharpening the character reconstruction. Even though the proposed decoder re-training technique is highly succesful in sharpening the reconstruction, it still does not capture the subtle variations of the input images expected from a succesful reconstruction. Hence, we perform the new image data generation by perturbation technique on the images reconstructed by M 1,dec . <ref type="figure" target="#fig_9">Fig. 8</ref> illustrates our results. <ref type="figure" target="#fig_9">Fig. 8 (a)</ref> and (b) are identical to that of <ref type="figure" target="#fig_7">Fig.  7</ref> (a) and (b), which we include for comparison. <ref type="figure" target="#fig_9">Fig. 8</ref> (c) illustrates the corresponding results obtained by M 2,dec , which was trained afresh using the new dataset generated by the proposed technique. It is evident that reconstructions by M 2,dec now captures the subtle variations that we required -curvature of E, slanting of B, asymmetry of X-, and are much closer to the test image than the reconstruction by M 1,dec . We observed similar improvements in approximately 80% of the test images, rendering the proposed technique significantly successful in improving the decoder performance with small number of training samples.   <ref type="figure" target="#fig_12">Fig. 9</ref> illustrates several instances where the proposed method still failed to capture the required subtle variations. Yet, even in these instances, the reconstructions of M 2,dec demonstrates significant improvement over the reconstructions of M 1,dec .    <ref type="figure" target="#fig_0">Fig. 10</ref> below, il- <ref type="figure" target="#fig_0">Figure 10</ref>. Generated images from CGAN for label 2 lustrates the generated images from a Conditional GAN (CGAN). However, at the generation phase, the new images are generated from random noise and that does not allow to apply specific perturbations, producing less realistic variations in images in comparison to what we do in the proposed method. Similarly, the reconstructions obtained when using our proposed technique are far better than it's alternatives in the low data regime, as shown by <ref type="figure" target="#fig_0">Fig. 11</ref>, which contains the reconstructions obtained after training with the respective data augmentation technique. The alternatives produced little or reduced improvement, whereas our method produced visually significant (≈ 1dB PSNR) improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Shift(S) Rotation(R) S &amp; R Noise TextCaps <ref type="figure" target="#fig_0">Figure 11</ref>. Comparison with other data augmentation techniques</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results of the Reconstruction Loss Functions and Analysis</head><p>Next, we discuss the variations on reconstruction with respect to different loss functions and combinations of loss functions. All the modifications we consider here were tested with varying number of training samples (100, 200, 500 and 1000) per class, from the EMNIST-Balanced dataset. We used the CapsNet model proposed in <ref type="bibr" target="#b3">[4]</ref> with minor alterations for this analysis, where decoder network consists of fully connected layers, since that architecture is well established. <ref type="figure" target="#fig_0">Fig. 12(a)</ref> illustrates the variation of PSNR with the amount of training samples used for different loss functions, which leads to a number of interesting observations. For example, when the number of training samples are small (100 or 200), performance of L 1 is poor compared to M SE, yet for higher number of training samples, L 1 performs better. PSNR values for BCE are the highest regardless of the number of training samples. Hence, we conclude that the most suitable loss function for reconstruction loss in general is BCE. <ref type="figure" target="#fig_0">Fig. 13</ref> illustrates the variations in reconstructed images with the use of different loss functions for 200 training samples.</p><p>We observed that if we linearly combine two loss functions to design a modified loss function, the obtained reconstructions were relatively poor. Nonetheless, since different loss functions capture different properties of handwritten characters, we used two decoders with two loss functions and combined the two reconstructed images together to get a resultant reconstructed image with improved quality. With this modification, it was interestingly observed   <ref type="table">Table 3</ref> demonstrates how the individual PSNR values improve with the two-decoder networks for different combinations of reconstruction loss functions. <ref type="table">Table 3</ref>. PSNR values for individual reconstructions when different combinations of loss functions are used. Here, we use the two-decoder network model with one loss function per each decoder. For each loss function combination, the PSNR value in the first row of PSNR pairs corresponds to the first reconstruction loss function (used in the first decoder) whereas the second row corresponds to the second loss function (used in the second decoder). With two loss functions, the quality of final reconstructions were much better and PSNR values significantly improved, compared to the single-decoder model with a single loss function. <ref type="figure" target="#fig_0">Fig. 12(b)</ref> shows the improvement in PSNR of the final output reconstruction, when two loss functions are combined together by a two-decoder network. <ref type="figure" target="#fig_0">Fig. 14</ref> shows the variations in the reconstructed images with the use of different loss function combinations for 200 training samples. <ref type="figure" target="#fig_0">Fig. 12</ref>(a) illustrates that BCE performs better compared to other loss functions when used in either singledecoder or two-decoder network model. <ref type="figure" target="#fig_0">Fig. 12(b)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function combination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced a technique for increasing the size of a dataset by exploiting the concepts in CapsNets. We demonstrated the performance of this technique on wellknown handwritten character datasets. Our algorithm takes limited amount of training samples, and perturb their corresponding instantiation parameters to create new training samples. In comparison to the conventional data augmentation techniques in the class of affine transformations, our technique generates images with subtle human-like variations to stroke pattern, boldness and other localized transformations. By combining the original dataset and the perturbed dataset as the training set, we achieved state-of-theart results and better reconstructions of the input images at the decoder. To further improve the image reconstruction, we analysed the use of different loss functions and their combinations.</p><p>Our proposed method works well with images of characters. We intend to extend this framework to images on the RGB space, and with higher resolution, such as images from ImageNet and COCO. Further, we intend to apply this framework on regionally localized languages by extracting training images from font files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>TextCap Model: Proposed CapsNet model for character classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>TextCap Decoder: Decoder network for the character reconstruction. Input to this network is obtained by masking the DigitCaps layer of the TextCap classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Variation in characters with the perturbation of instantiation parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The overall methodology for improving the decoder performance proposed in Section 3.1, M 1 , with dataset, D original , containing 200 training samples per class and all testing samples. Without loss of generality, we choose the first 200 training samples in each class in the dataset. Subsequently, we consider the trained capsule network, M 1,caps , and the trained decoder network, M 1,dec , separately. Next, as illustrates byFig. 4(b), we obtain the instantiation parameters, C D,original , corresponding to the training images, I D,original , in D original as the output of the capsule network M 1,caps , which can be masked as C D,original and passed as the input of the decoder network M 1,dec . We can obtain the corresponding reconstructed images I D,recon as the output of M 1,dec .Fig. 5shows several such reconstructed images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Original and reconstructed Image pairs with Model M1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Distortions caused by adding uncontrolled noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>Image data generation using perturbation Input: Instantiation parameters C, a th highest variance, Decoder Network model ( M dec ). Output: Perturbed images I perturbed 1: Calculate class variance σ m,k = var j ( C m,j,k ). 2: Getσ m,k ← sort k (σ m,k ) descending. 3: Getk = k corresponding to k = a. 4: τ m,k ← maxj ( C m,j,k )−minj ( C m,j,k ) 2 5: get τ k ← avg i (τ m,k ) 6: for eachĵ ∈ [j] do 7: if C m,ĵ,k &gt; 0 then 8: C m,ĵ,k ← C m,ĵ,k + min(τ m,k , τk) ĵ,k ← C m,ĵ,k − min(τ m,k , τk) 11: I perturbed ← M dec ( C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Recon by M 1,dec (c) Recon by M 1,dec Results of the decoder re-training technique</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Test image(b) Recon by M 1,dec (c) Recon by M 2,dec</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Results of the model M2, trained with the newly generated dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Recon by M 1,dec</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(c) Recon by M 2,dec</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Instances where the model M2 has not been succesful in capturing the subtle variations of the test image -vertical line of G, bottom part of 3 -GANs, VAEs and data augmentation techniques are alternatives to our proposed technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Change in PSNR for different combinations of loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>Change in PSNR with the number of training samples. that individual reconstructions for respective loss functions also improved, due to the effect of the reconstruction loss component in the training loss. However, the amount of improvement of individual PSNRs depends on the loss function combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Variations in reconstruction with different loss functionsOriginalL 1 &amp; DSSIM L 1 &amp; BCE M SE &amp; DSSIM M SE &amp; BCE BCE &amp; DSSIM Variations in reconstruction with different loss function combinations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>, we train TextCaps on 200 training samples from the training set and test using the whole test set. In order to test the performance of the TextCaps architecture, we also evaluate it by training it on full training sets and testing on full test sets. Five datasets used to evaluate TextCaps</figDesc><table><row><cell>Dataset</cell><cell>Classes</cell><cell>Train</cell><cell>Train</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell>samp/class</cell><cell>size</cell><cell>size</cell></row><row><cell>EMNIST-Balanced[12]</cell><cell>47</cell><cell>2,400</cell><cell>112,800</cell><cell>18,800</cell></row><row><cell>EMNIST-Letters[12]</cell><cell>26</cell><cell>4,800</cell><cell>124,800</cell><cell>20,800</cell></row><row><cell>EMNIST-Digits[12]</cell><cell>10</cell><cell>24,000</cell><cell>240,000</cell><cell>40,000</cell></row><row><cell>MNIST[6]</cell><cell>10</cell><cell>6,000</cell><cell>60,000</cell><cell>10,000</cell></row><row><cell>Fashion MNIST[22]</cell><cell>10</cell><cell>6,000</cell><cell>60,000</cell><cell>10,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>illustrates that the combinations BCE &amp; DSSIM and L 1 &amp; BCE perform significantly better than other loss combinations for the two-decoder model. Even though PSNR values for DSSIM loss were not sufficiently significant, it captures the spatial similarity aspects in reconstruction. Hence, the BCE &amp; DSSIM loss combination provides marginally better reconstructions, compared to L 1 &amp; BCE for fewer number of training samples. However, when the number of training samples increase, L 1 &amp; BCE combination produces much better reconstructions.</figDesc><table><row><cell>Original</cell></row><row><cell>L 1</cell></row><row><cell>M SE</cell></row><row><cell>DSSIM</cell></row><row><cell>BCE</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vinojjayasundara/textcaps</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors thank the Senate Research Committee of the University of Moratuwa for the financial support through the grant SRC/LT/2016/04 and the Faculty of Information Technology for providing computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. In: NIPS</title>
		<meeting><address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition using k nearest-neighbor, radial-basis function, and backpropagation neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="440" to="449" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NIPS, Long Beach, CA</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICANN, Berlin, Heidelberg</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multicolumn deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple method for high-performance digit recognition based on sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1985" to="1989" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">What is the best multi-stage architecture for object recognition? In: ICCV</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">EM-NIST: an extension of MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eden: Evolutionary deep networks for efficient machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dufourq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Bassett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRASA-RobMech</title>
		<imprint>
			<biblScope unit="page" from="110" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Style memory: Making a classifier network generative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiyatno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="523" to="531" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2528" to="2535" />
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image enhancement via adaptive unsharp masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramponi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="505" to="510" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
	<note>2017 IEEE Winter Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification of fashion article images using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kolekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICIIP</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
