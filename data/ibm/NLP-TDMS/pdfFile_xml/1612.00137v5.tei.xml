<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RMPE: Regional Multi-Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-02-04">4 Feb 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tencent</forename><surname>Youtu</surname></persName>
						</author>
						<title level="a" type="main">RMPE: Regional Multi-Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-02-04">4 Feb 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76.7 mAP on the MPII (multi person) dataset [3]. Our model and source codes are made publicly available. † .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a fundamental challenge for computer vision. In practice, recognizing the pose of multiple persons in the wild is a lot more challenging than recognizing the pose of a single person in an image <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b41">44]</ref>. Recent attempts approach this problem by using either a two-step framework <ref type="bibr" target="#b31">[34,</ref><ref type="bibr" target="#b12">15]</ref> or a part-based framework <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b18">21]</ref>. The two-step framework first detects human bounding boxes and then estimates the pose within each box independently. The part-based framework first detects body parts independently and then assembles the detected body parts to form multiple human poses. Both frameworks have their advantages and disadvantages. In the two-step framework, the accuracy of pose estimation highly depends on the quality of the detected bounding boxes. In the part-based framework, the assembled hu- * part of this work was done when Hao-Shu Fang was an student intern in Tencent § corresponding author is Cewu Lu † https://cvsjtu.wordpress.com/rmpe-regional-multi-person-pose-estimation/ man poses are ambiguous when two or more persons are too close together. Also, part-based framework loses the capability to recognize body parts from a global pose view due to the mere utilization of second-order body parts dependence.</p><p>Our approach follows the two-step framework. We aim to detect accurate human poses even when given inaccurate bounding boxes. To illustrate the problems of previous approaches, we applied the state-of-the-art object detector Faster-RCNN <ref type="bibr" target="#b32">[35]</ref> and the SPPE Stacked Hourglass model <ref type="bibr" target="#b25">[28]</ref>. <ref type="figure">Figure 1</ref> and <ref type="figure" target="#fig_0">Figure 2</ref> show two major problems: the localization error problem and the redundant detection problem. In fact, SPPE is rather vulnerable to bounding box errors. Even for the cases when the bounding boxes are considered as correct with IoU &gt; 0.5, the detected human poses can still be wrong. Since SPPE produces a pose for each given bounding box, redundant detections result in redundant poses.</p><p>To address the above problems, a regional multi-person pose estimation (RMPE) framework is proposed. Our framework improves the performance of SPPE-based human pose estimation algorithms. We have designed a new symmetric spatial transformer network (SSTN) which is attached to the SPPE to extract a high-quality single person region from an inaccurate bounding box. A novel parallel SPPE branch is introduced to optimize this network. To address the problem of redundant detection, a parametric pose NMS is introduced. Our parametric pose NMS eliminates redundant poses by using a novel pose distance metric to compare pose similarity. A data-driven approach is applied to optimize the pose distance parameters. Lastly, we propose a novel pose-guided human proposal generator (PGPG) to augment training samples. By learning the output distribution of a human detector for different poses, we can simulate the generation of human bounding boxes, producing a large sample of training data.</p><p>Our RMPE framework is general and is applicable to different human detectors and single person pose estimators. We applied our framework on the MPII (multi-person) dataset [3], where it outperforms the state-of-the-art methods and achieves 76.7 mAP. We have also conducted ablation studies to validate the effectiveness of each pro- <ref type="figure">Figure 1</ref>. Problem of bounding box localization errors. The red boxes are the ground truth bounding boxes, and the yellow boxes are detected bounding boxes with IoU &gt; 0.5. The heatmaps are the outputs of SPPE <ref type="bibr" target="#b25">[28]</ref> corresponding to the two types of boxes. The corresponding body parts are not detected in the heatmaps of the yellow boxes. Note that with IoU &gt; 0.5, the yellow boxes are considered as "correct" detections. However, human poses are not detected even with the "correct" bounding boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Person Pose Estimation</head><p>In single person pose estimation, the pose estimation problem is simplified by only attempting to estimate the pose of a single person, and the person is assumed to dominate the image content. Conventional methods considered pictorial structure models. For example, tree models <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b44">47,</ref><ref type="bibr" target="#b39">42]</ref> and random forest models <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b8">11]</ref> have demonstrated to be very efficient in human pose estimation. Graph based models such as random field models <ref type="bibr" target="#b21">[24]</ref> and dependency graph models <ref type="bibr" target="#b14">[17]</ref> have also been widely investigated in the literature <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b29">32]</ref>.</p><p>More recently, deep learning has become a promising technique in object/face recognition, and human pose estimation is of no exception. Representative works include DeepPose (Toshev et al) <ref type="bibr" target="#b37">[40]</ref>, DNN based models <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b11">14]</ref> and various CNN based models <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b1">4,</ref><ref type="bibr" target="#b41">44]</ref>. Apart from simply estimating a human pose, some studies <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b28">31]</ref> consider human parsing and pose estimation simultaneously. For single person pose estimation, these methods could perform well only when the person has been correctly located. However, this assumption is not always satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi Person Pose Estimation</head><p>Part-based Framework Representative works on partbased framework <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b18">21]</ref> are reviewed. Chen et al. presented an approach to parse largely occluded people by graphical model which models humans as flexible compositions of body parts <ref type="bibr" target="#b6">[9]</ref>. Gkiox et al used k-poselets to jointly detect people and predict locations of human poses <ref type="bibr" target="#b12">[15]</ref>. The final pose localization is predicted by a weighted average of all activated poselets. <ref type="bibr">Pishchulin et al. proposed</ref> DeepCut to first detect all body parts, and then label and assemble these parts via integral linear programming <ref type="bibr" target="#b30">[33]</ref>. A stronger part detector based on ResNet <ref type="bibr" target="#b16">[19]</ref> and a better incremental optimization strategy is proposed by Insafutdinov et al <ref type="bibr" target="#b18">[21]</ref>. While part-based methods have demonstrated good performance, their body-part detectors can be vulnerable since only small local regions are considered.</p><p>Two-step Framework Our work follows the two-step framework <ref type="bibr" target="#b31">[34,</ref><ref type="bibr" target="#b12">15]</ref>. In our work, we use a CNN based SPPE method to estimate poses, while Pishchulin et al. <ref type="bibr" target="#b31">[34]</ref> used conventional pictorial structure models for pose estimation. In particular, Insafutdinov et al <ref type="bibr" target="#b18">[21]</ref> propose a similar two-step pipeline which uses the Faster R-CNN as their human detector and a unary DeeperCut as their pose estimator. Their method can only achieve 51.0 in mAP on MPII dataset, while ours can achieve 76.7 mAP. With the development of object detection and single person pose estimation, the two-step framework can achieve further advances in its performance. Our paper aims to solve the problem of imperfect human detection in the two-step framework in order to maximize the power of SPPE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Regional Multi-person Pose Estimation</head><p>The pipeline of our proposed RMPE is illustrated in <ref type="figure">Figure</ref> 3. The human bounding boxes obtained by the human detector are fed into the "Symmetric STN + SPPE" module, and the pose proposals are generated automatically. The generated pose proposals are refined by parametric Pose NMS to obtain the estimated human poses. During the training, we introduce "Parallel SPPE" in order to avoid local minimums and further leverage the power of SSTN. To augment the existing training samples, a pose-guided proposals generator (PGPG) is designed. In the following sections, we present the three major components of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Symmetric STN and Parallel SPPE</head><p>Human proposals provided by human detectors are not well-suited to SPPE. This is because SPPE is specifically trained on single person images and is very sensitive to localisation errors. It has been shown that small translation or cropping of human proposals can significantly affect performance of SPPE <ref type="bibr" target="#b25">[28]</ref>. Our symmetric STN + parallel SPPE was introduced to enhance SPPE when given imperfect human proposals. The module of our SSTN and parallel SPPE is shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STN and SDTN</head><p>The spatial transformer network <ref type="bibr" target="#b19">[22]</ref>(STN) has demonstrated excellent performance in selecting region of interests automatically. In this paper, we use the STN to extract high quality dominant human proposals. Mathematically, the STN performs a 2D affine transformation which can be expressed as</p><formula xml:id="formula_0">x s i y s i = θ 1 θ 2 θ 3   x t i y t i 1   ,<label>(1)</label></formula><p>where θ 1 , θ 2 and θ 3 are vectors in R 2 . {x s i , y s i } and {x t i , y t i } are the coordinates before and after transformation, respectively. After SPPE, the resulting pose is mapped into the original human proposal image. Naturally, a spatial detransformer network (SDTN) is required to remap the estimated human pose back to the original image coordinate. The SDTN computes the γ for de-transformation and generates grids based on γ:</p><formula xml:id="formula_1">x t i y t i = γ1 γ2 γ3   x s i y s i 1   (2)</formula><p>Since SDTN is an inverse procedure of STN, we can obtain the following:</p><formula xml:id="formula_2">γ1 γ2 = θ1 θ2 −1 (3) γ3 = −1 × γ1 γ2 θ3<label>(4)</label></formula><p>To back propagate through SDTN, ∂J(W,b) ∂θ can be derived as</p><formula xml:id="formula_3">∂J(W, b) ∂ θ1 θ2 = ∂J(W, b) ∂ γ1 γ2 × ∂ γ1 γ2 ∂ θ1 θ2 + ∂J(W, b) ∂γ3 × ∂γ3 ∂ γ1 γ2 × ∂ γ1 γ2 ∂ θ1 θ2<label>(5)</label></formula><p>with respect to θ 1 and θ 2 , and</p><formula xml:id="formula_4">∂J(W, b) ∂θ3 = ∂J(W, b) ∂γ3 × ∂γ3 ∂θ3<label>(6)</label></formula><p>with respect to θ 3 . ∂ γ 1 γ 2 ∂ θ 1 θ 2 and ∂γ3 ∂θ3 can be derived from Eqn. (3) and (4) respectively.</p><p>After extracting high quality dominant human proposal regions, we can utilize off-the-shelf SPPE for accurate pose estimation. In our training, the SSTN is fine-tuned together with our SPPE.</p><p>Parallel SPPE To further help STN extract good humandominant regions, we add a parallel SPPE branch in the training phrase. This branch shares the same STN with the original SPPE, but the spatial de-transformer (SDTN) <ref type="figure">Figure 4</ref>. An illustration of our symmetric STN architecture and our training strategy with parallel SPPE. The STN used was developed by Jaderberg et al. <ref type="bibr" target="#b19">[22]</ref>. Our SDTN takes a parameter θ, generated by the localization net and computes the γ for de-transformation. We follow the grid generator and sampler <ref type="bibr" target="#b19">[22]</ref> to extract a human-dominant region. For our parallel SPPE branch, a center-located pose label is specified. We freeze the weights of all layers of the parallel SPPE to encourage the STN to extract a dominant single person proposal.</p><p>is omitted. The human pose label of this branch is specified to be centered. To be more specific, the output of this SPPE branch is directly compared to labels of centerlocated ground truth poses. We freeze all the layers of this parallel SPPE during the training phase. The weights of this branch are fixed and its purpose is to back-propagate center-located pose errors to the STN module. If the extracted pose of the STN is not center-located, the parallel branch will back-propagate large errors. In this way, we can help the STN focus on the correct area and extract high quality human-dominant regions. In the testing phase, the parallel SPPE is discarded. The effectiveness of our parallel SPPE will be verified in our experiments.</p><p>Discussions The parallel SPPE can be regarded as a regularizer during the training phase. It helps to avoid a poor solution (local minimum) where the STN does not transform the pose to the center of extracted human regions. The likelihood of reaching a local minimum is increased because compensation from the SDTN will make the network generate fewer errors. These errors are necessary to train the STN. With the parallel SPPE, the STN is trained to move the human to the center of the extracted region to facilitate accurate pose estimation by SPPE.</p><p>It may seem intuitive to replace parallel SPPE with a center-located poses regression loss in the output of SPPE (before SDTN). However, this approach will degrade the performance of our system. Although STN can partly transform the input, it is impossible to perfectly place the person at the same location as the label. The difference in coordinate space between the input and label of SPPE will largely impair its ability to learn pose estimation. This will cause the performance of our main branch SPPE to decrease. Thus, to ensure that both STN and SPPE can fully lever-age their own power, a parallel SPPE with frozen weights is indispensable for our framework. The parallel SPPE always produces large errors for non-center poses to push the STN to produce a center-located pose, without affecting the performance of the main branch SPPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parametric Pose NMS</head><p>Human detectors inevitably generate redundant detections, which in turn produce redundant pose estimations. Therefore, pose non-maximum suppression (NMS) is required to eliminate the redundancies. Previous methods <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b6">9]</ref> are either not efficient or not accurate enough. In this paper, we propose a parametric pose NMS method. Similar to the previous subsection, the pose P i , with m joints is denoted as { k 1 i , c 1 i , . . . , k m i , c m i }, where k j i and c j i are the j th location and confidence score of joints respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMS scheme</head><p>We revisit pose NMS as follows: firstly, the most confident pose is selected as reference, and some poses close to it are subject to elimination by applying elimination criterion. This process is repeated on the remaining poses set until redundant poses are eliminated and only unique poses are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elimination Criterion</head><p>We need to define pose similarity in order to eliminate the poses which are too close and too similar to each others. We define a pose distance metric d(P i , P j |Λ) to measure the pose similarity, and a threshold η as elimination criterion, where Λ is a parameter set of function d(·). Our elimination criterion can be written as follows:</p><formula xml:id="formula_5">f ( P i , P j |Λ, η) = 1[d(P i , P j |Λ, λ) ≤ η]<label>(7)</label></formula><p>If d(·) is smaller than η, the output of f (·) should be 1, which indicates that pose P i should be eliminated due to redundancy with reference pose P j .</p><p>Pose Distance Now, we present the distance function d pose (P i , P j ). We assume that the box for P i is B i . Then we define a soft matching function</p><formula xml:id="formula_6">K Sim (P i , P j |σ 1 ) = n tanh c n i σ1 · tanh c n j σ1 , if k n j is within B(k n i ) 0 otherwise<label>(8)</label></formula><p>where B(k n i ) is a box center at k n i , and each dimension of B(k n i ) is 1/10 of the original box B i . The tanh operation filters out poses with low-confidence scores. When two corresponding joints both have high confidence scores, the output will be close to 1. This distance softly counts the number of joints matching between poses.</p><p>The spatial distance between parts is also considered, which can be written as</p><formula xml:id="formula_7">H Sim (P i , P j |σ 2 ) = n exp[− (k n i − k n j ) 2 σ 2 ]<label>(9)</label></formula><p>By combining Eqn <ref type="formula" target="#formula_6">(8)</ref> and <ref type="formula" target="#formula_7">(9)</ref>, the final distance function can be written as d(P i , P j |Λ) = K Sim (P i , P j |σ 1 ) + λH Sim (P i , P j |σ 2 ) (10) where λ is a weight balancing the two distances and Λ = {σ 1 , σ 2 , λ}. Note that the previous pose NMS <ref type="bibr" target="#b6">[9]</ref> set pose distance parameters and thresholds manually. In contrast, our parameters can be determined in a data-driven manner.</p><p>Optimization Given the detected redundant poses, the four parameters in the eliminate criterion f ( P i , P j |Λ, η) are optimized to achieve the maximal mAP for the validation set. Since exhaustive search in a 4D space is intractable, we optimize two parameters at a time by fixing the other two parameters in an iterative manner. Once convergence is achieved, the parameters are fixed and will be used in the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pose-guided Proposals Generator</head><p>Data Augmentation For the two-stage pose estimation, proper data augmentation is necessary to make the SSTN+SPPE module adapt to the 'imperfect' human proposals generated by the human detector. Otherwise, the module may not work properly in the testing phase for the human detector. An intuitive approach is to directly use bounding boxes generated by the human detector during the training phase. However, the human detector can only produce one bounding box for each person. By using the proposals generator, this quantity can be greatly increased. Since we already have the ground truth pose and an object detection bounding box for each person, we can generate a large sample of training proposals with the same distribution as the output of the human detector. With this technique, we are able to further boost the performance of our system.</p><p>Insight We find that the distribution of the relative offset between the detected bounding box and the ground truth bounding box varies across different poses. To be more specific, there exists a distribution P (δB|P ), where δB is the offset between the coordinates of a bounding box generated by human detector and the coordinates of the ground truth bounding box, and P is the ground truth pose of a person. If we can model this distribution, we are able to generate many training samples that are similar to human proposals generated by the human detector.</p><p>Implementation To directly learn the distribution P (δB|P ) is difficult due to the variation of human poses. So instead, we attempt to learn the distribution P (δB|atom(P )), where atom(P ) denotes the atomic pose <ref type="bibr" target="#b43">[46]</ref> of P . We follow the method used by Andriluka et al [3] to learn the atomic poses. To derive the atomic poses from annotations of human poses, we first align all poses so that their torsos have the same length. Then we use the k-means algorithm to cluster our aligned poses, and the computed cluster centers form our atomic poses. Now for each person instance sharing the same atomic pose a, we calculate the offsets between its ground truth bounding box and detected bounding box. The offsets are then normalized by the corresponding side-length of ground truth bounding box in that direction. After these processes, the offsets form a frequency distribution, and we fit our data to a Gaussian mixture distribution. For different atomic poses, we have different Gaussian mixture parameters. We visualize some of the distributions and their corresponding clustered human poses in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>Proposals Generation During the training phase of the SSTN+SPPE, for each annotated pose in the training sample we first look up the corresponding atomic pose a. Then we generate additional offsets by dense sampling according to P (δB|a) to produce augmented training proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed method is qualitatively and quantitatively evaluated on two standard multi-person datasets with large occlusion cases: MPII [3] and MSCOCO 2016 Keypoints Challenge dataset[1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation datasets MPII Multi-Person Dataset</head><p>The challenging benchmark MPII Human Pose (multi-person)[3] consists of 3,844 training and 1,758 testing groups with both occluded and overlapped people. Moreover, it contains more than 28,000 training samples for single person pose estimation. We use all the training data in the single person dataset and 90% of the multi-person training set to fine-tune the SPPE, leaving 10% for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO Keypoints Challenge</head><p>We also evaluate our method on the MSCOCO Keypoints Challenge dataset[1]. This dataset requires localization of person keypoints in challenging, uncontrolled conditions. It consists of 105,698 training and around 80,000 testing human instances. The training set contains over 1 million total labeled keypoints. The testing set are divided into four roughly equally sized splits: test-challenge, test-dev, test-standard, and testreserve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details in testing</head><p>In this paper, we use the VGG-based SSD-512 <ref type="bibr" target="#b23">[26]</ref> as our human detector, as it performs object detection effectively and efficiently. In order to guarantee that the entire person region will be extracted, detected human proposals are extended by 30% along both the height and width directions. We use the stacked hourglass model <ref type="bibr" target="#b25">[28]</ref> as the single person pose estimator because of its superior performance. For the STN network, we adopt the ResNet-18 <ref type="bibr" target="#b16">[19]</ref> as our localization network. Considering the memory efficiency, we use a smaller 4-stack hourglass network as the parallel SPPE.</p><p>To show that our framework is general and is applicable to different human detectors and pose estimators, we also do experiments by replacing the human detector with ResNet152 based Faster-RCNN <ref type="bibr" target="#b5">[8]</ref> and replacing the pose estimator with PyraNet <ref type="bibr" target="#b42">[45]</ref>. In this case, we adopt multiscale testing for the human detection and use an input size of 320x256 for the PyraNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Results on MPII dataset. We evaluated our method on full MPII multi-person test set. Quantitative results on the full testing set are given in <ref type="table">Table 1</ref>. Notably, we achieve an average accuracy of 72 mAP on identifying difficult joints such as wrists, elbows, ankles, and knees, which is 3.3 mAP higher than the previous state-of-the-art result. We reach a final accuracy of 70.4 mAP for the wrist and an accuracy of 73 mAP for the knee. By using a stronger human detector and pose estimator, we can further achieve 82.1 mAP, which is 4.6 mAP higher than the previous best result. We present some of our results in <ref type="figure" target="#fig_3">Figure 6</ref>. These results show that our method can accurately predict pose in multi-person images. More results are presented in supplementary materials.</p><p>Results on MSCOCO Keypoints dataset. We fine-tuned the SPPE on the MSCOCO Keypoints training + validating sets and leave 5,000 images for validation. Quantitative results on the test-dev set are given in <ref type="table">Table 2</ref>. Our method achieves the state-of-the-art performance. Note that without specific design for the pose estimation network, our frame work can perform on par with Megvii <ref type="bibr" target="#b7">[10]</ref>, which propose a new pose estimation network. It demonstrates the effectiveness of our proposed framework. And we believe that using the pose network from <ref type="bibr" target="#b7">[10]</ref> can further boost our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We evaluate the effectiveness of the three proposed components, i.e., symmetric STN, pose-guided proposals generator and parametric pose NMS. The ablative studies have been conducted by removing the proposed components from the pipeline or replacing the proposed components with conventional solvers. The straightforward two-step method without the three components and the upper-bound of our framework are tested for comparison. We conducted these experiments on the MPII validation set. In addition, we replace our human detection module to prove the generality of our framework.</p><p>Symmetric STN and Parallel SPPE To validate the importance of symmetric STN and parallel SPPE, two experiments were conducted. In the first experiment, we removed the SSTN, including the parallel SPPE, from our pipeline. In the second experiment, we only removed the parallel SPPE and kept the symmetric STN structure. Both of these results are shown in . "++" denotes using faster-rcnn with softnms <ref type="bibr" target="#b2">[5]</ref> as human detector, PyraNet <ref type="bibr" target="#b42">[45]</ref> with input size 320x256 as pose estimator. gions to minimize the total losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose-guided Proposals Generator</head><p>In <ref type="table" target="#tab_0">Table 3</ref>(b), we demonstrate that our pose-guided proposals generator also plays an important role in our system. In this experiment, we first remove the data augmentation from our training phase. The final mAP drops to 73.0%. Then we compare our data augmentation technique with a simple baseline. The baseline is formed by jittering the locations and aspect ratios of the bounding boxes produced by person detector to generate a large number of additional proposals. We choose those that have IoU&gt;0.5 with ground truth boxes. From our result in <ref type="table" target="#tab_0">Table 3</ref>(b), we can see that our technique is better than the baseline method. Generating training proposals according to the distribution can be regarded as a kind of data re-sampling, which can help the model to better fit human proposals.</p><p>Parametric Pose NMS Since pose NMS is an independent module, we can directly remove it from our final model. The experimental results are shown in <ref type="table" target="#tab_0">Table 3</ref>(c). As we can see, the mAP drops significantly if the parametric pose NMS is removed. This is because the increase in the number of redundant poses will ultimately decrease our precision. We note that the previous pose NMS can also eliminate redundant detection to some extent. The state-of-the- <ref type="figure">Figure 7</ref>. Example failure cases of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>AP AP 50 AP 75 AP M AP L CMU-Pose <ref type="bibr" target="#b4">[7]</ref> 61.8 84.9 67.5 57.1 68.2 G-RMI <ref type="bibr" target="#b27">[30]</ref> 68. art pose NMS algorithms <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b6">9]</ref> are used to replace our parametric pose NMS, with the results given in <ref type="table" target="#tab_0">Table 3</ref>(c). These schemes perform less effectively than ours, since the parameter learning is missing. In terms of efficiency, on our validation set which contains 1300 images, the publicly available implementation of [6] ‡ takes 62.2 seconds to perform pose NMS while using our algorithm takes only 1.8 seconds.</p><p>Upper Bound of Our Framework The upper bound of our framework is tested, where we use the ground truth bounding boxes as human proposals. As shown in <ref type="table" target="#tab_0">Table 3</ref>(e), this setting could yield 84.2% mAP. It verifies that our system is already close to the upper-bound of two-step framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Failure cases</head><p>We present some failure cases in <ref type="figure">Figure 7</ref>. It can be seen that the SPPE can not handle poses which are rarely occurred (e.g. the person performing the 'Human Flag' in the first image). When two persons are highly overlapped, our system get confused and can not separate them apart (e.g. the two persons in the left of the second image). The misses of person detector will also cause the missing detection of human poses (e.g. the person who has laid down in the third image). Finally, erroneous pose may still be detected when an object looks very similar to human which can fool both human detector and SPPE (e.g. the background object in the forth image). ‡ http://www.vision.caltech.edu/ dhall/projects/MergingPoseEstimates/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a novel regional multi-person pose estimation (RMPE) framework is proposed, which significantly outperforms the state-of-the-art methods for multi-person human pose estimation in terms of accuracy and efficiency. It validates the potential of two-step frameworks, i.e., human detector + SPPE, when SPPE is adapted to a human detector. Our RMPE framework consists of three novel components: symmetric STN with parallel SPPE, parametric pose NMS, and pose-guided proposals generator (PGPG). In particular, PGPG is used to greatly argument the training data by learning the conditional distribution of bounding box proposals for a given human pose. The SPPE becomes adept at handling human localization errors due to the utilization of symmetric STN and parallel SPPE. Finally, the parametric pose NMS can be used to reduce redundant detections. In our future work, it would be interesting to explore the possibility of training our framework together with the human detector in an end-to-end manner.  <ref type="table" target="#tab_0">Table 3</ref>. Results of the ablation experiments on our validation set. "w/o X" means without X module in our pipeline. "random jittering*" means generating training proposals by jittering locations and aspect ratios of the detected human bounding boxes. "PoseNMS [x]" reports the result when using the pose NMS algorithm developed in paper [x].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Problem of redundant human detections. The left image shows the detected bounding boxes; the right image shows the estimated human poses. Because each bounding box is operated on independently, multiple poses are detected for a single person. posed component of our framework. Our model and source codes are made publicly available to support reproducible research.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Pipeline of our RMPE framework. Our Symmetric STN consists of STN and SDTN which are attached before and after the SPPE. The STN receives human proposals and the SDTN generates pose proposals. The Parallel SPPE acts as an extra regularizer during the training phase. Finally, the parametric Pose NMS (p-Pose NMS) is carried out to eliminate redundant pose estimations. Unlike traditional training, we train the SSTN+SPPE module with images generated by PGPG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Gaussian distributions of bounding box offsets for several different atomic poses. More results are available in supplementary materials. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Some results of our model's predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(a). We can observe perfor-</cell></row><row><cell>mance degradation when removing parallel SPPE, which</cell></row><row><cell>implies that parallel SPPE with single person image labels</cell></row><row><cell>strongly encourages the STN to extract single person re-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Head Shoulder Elbow Wrist Hip Knee Ankle Total</figDesc><table><row><cell></cell><cell>RMPE, full</cell><cell>90.7</cell><cell>89.7</cell><cell>84.1</cell><cell>75.4 80.4 75.5</cell><cell>67.3</cell><cell>80.8</cell></row><row><cell>a)</cell><cell>w/o SSTN+parallel SPPE w/o parallel SPPE only</cell><cell>89.0 89.9</cell><cell>86.9 88.0</cell><cell>82.8 83.4</cell><cell>73.5 77.1 73.3 74.7 77.8 74.0</cell><cell>65.0 65.8</cell><cell>78.2 79.1</cell></row><row><cell>b)</cell><cell>w/o PGPG random jittering*</cell><cell>82.8 89.3</cell><cell>81.0 87.8</cell><cell>77.5 82.3</cell><cell>68.2 74.6 66.8 70.4 78.4 73.3</cell><cell>60.1 63.8</cell><cell>73.0 77.9</cell></row><row><cell></cell><cell>w/o PoseNMS</cell><cell>85.1</cell><cell>83.6</cell><cell>79.2</cell><cell>69.8 76.4 72.2</cell><cell>63.6</cell><cell>75.7</cell></row><row><cell>c)</cell><cell>PoseNMS [9]</cell><cell>88.9</cell><cell>87.8</cell><cell>83.0</cell><cell>73.8 78.7 74.6</cell><cell>66.3</cell><cell>79.1</cell></row><row><cell></cell><cell>PoseNMS [6]</cell><cell>90.0</cell><cell>88.6</cell><cell>83.7</cell><cell>74.6 79.7 75.1</cell><cell>67.0</cell><cell>79.9</cell></row><row><cell cols="3">d) straight forward two-steps 81.9</cell><cell>80.4</cell><cell>74.1</cell><cell>68.5 69.0 66.1</cell><cell>62.2</cell><cell>71.7</cell></row><row><cell cols="2">e) oracle human detection</cell><cell>94.3</cell><cell>93.4</cell><cell>87.7</cell><cell>80.2 84.3 78.9</cell><cell>70.6</cell><cell>84.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4325</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1605.02914</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Softnmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">4327</biblScope>
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Merging pose estimates across space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4327</biblScope>
			<biblScope unit="page">4329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4327</biblScope>
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An implementation of faster rcnn with study for region sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02138</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4326</biblScope>
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint graph decomposition and node labeling: Problem, algorithms, applications</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context and observation driven latent variable model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computationally efficient regression on a dependency graph for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4323</biblScope>
			<biblScope unit="page">4324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.7302</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human pose estimation with fields of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01779</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attributed grammars for joint estimation of human attributes, part and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4321</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cascaded models for articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional regression forests for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An efficient branch-and-bound algorithm for optimal human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telaprolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops 2016 (ECCVW&apos;16) -Workshop on Crowd Understanding (CUW&apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiple tree models for occlusion and spatial constraints in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>2017. 4326</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<biblScope unit="volume">4327</biblScope>
			<biblScope unit="page">4328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation via parsing a tree structure based human model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
