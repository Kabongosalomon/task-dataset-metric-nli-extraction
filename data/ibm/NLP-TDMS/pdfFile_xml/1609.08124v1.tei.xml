<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Language-Visual Embedding for Movie Understanding with Natural-Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
							<email>atousa.torabi@disneyresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Disney Research Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
							<email>ntandon@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@disneyresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Disney Research Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Language-Visual Embedding for Movie Understanding with Natural-Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Annotation</term>
					<term>Video Retrieval</term>
					<term>learning Joint Visual- Language Embedding</term>
					<term>LSMT</term>
					<term>Soft Attention Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning a joint language-visual embedding has a number of very appealing properties and can result in variety of practical application, including natural language image/video annotation and search. In this work, we study three different joint language-visual neural network model architectures. We evaluate our models on large scale LSMDC16 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> movie dataset for two tasks: 1) Standard Ranking for video annotation and retrieval 2) Our proposed movie multiple-choice test. This test facilitate automatic evaluation of visual-language models for natural language video annotation based on human activities. In addition to original Audio Description (AD) captions, provided as part of LSMDC16, we collected and will make available a) manually generated re-phrasings of those captions obtained using Amazon MTurk b) automatically generated human activity elements in "Predicate + Object" (PO) phrases based on "Knowlywood", an activity knowledge mining model <ref type="bibr" target="#b21">[22]</ref>. Our best model archives Recall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset of 1000 samples. For multiple-choice test, our best model achieve accuracy 58.11% over whole LSMDC16 public test-set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language-based video and image search has been a long standing topic of research among information retrieval, multimedia, and computer vision communities. Several existing on-line platforms (e.g. Youtube) rely on massive human curation efforts, manually assigned tags, click counts and surrounding text to match largely unstructured search phrases in order to retrieve ranked list of relevant videos from a stored library. However, as the amount of unlabeled video content grows, with advent of inexpensive mobile recording devices (e.g. smart phones), the focus is rapidly shifting to automated understand, tagging and search.</p><p>Over the last 1-2 years, there has been an increased interest in jointly modeling images/videos and natural language sentences. Models that jointly learn arXiv:1609.08124v1 [cs.CV] 26 Sep 2016 from videos/images and natural language sentences have broad applicability to visual search, retrieval <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, captioning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26]</ref>, or visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> tasks. There are also abundant evidence <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> that jointly learning from language and visual modalities can be mutually beneficial. These recent trends of multi-modal learning are, at least in part, are enabled by recent advances in deep learning, with high capacity Convolutional Neural Networks (CNNs) driving up performance on image and video recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref> end, and Recurrent Neural networks (RNNs), particularly Long Short-Term Memory (LSTM), making advances in natural language translation and sentence generation. The learning of such high capacity models, with millions and tens of millions of parameters, is made possible by availability of large scale image-(e.g. COCO <ref type="bibr" target="#b12">[13]</ref>, Visual Genome 3 ) and, more recently, video-(LSMDC <ref type="bibr" target="#b17">[18]</ref>, MSR-VTT <ref type="bibr" target="#b28">[29]</ref>) description datasets, where an image or video is associated with one-or-more human generated natural language sentence descriptions.</p><p>While there are a number of recent works that look at joint image-language modeling, the progress on video-language models has been much more limited. The difficulty stems from additional challenges that come from the need to encode the temporal aspects of the video and the sheer volume of the video data required to be processed (e.g. Large Scale Movie Description Challenge (LSMDC) <ref type="bibr" target="#b17">[18]</ref> dataset contains nearly 60-times the number of frames as images in COCO <ref type="bibr" target="#b12">[13]</ref>). In the video-language domain, the few works that exist focus largely on video description generation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>. These models typically use LSTM models to generate sentences given an encoding of the video. Evaluating the video description performance especially on LSMDC that contain audio description (AD) captions and are typically relatively verbose and very precise is not easy and usually have done based on human judgment. In this work we study two tasks: 1) Standard Ranking for video annotation and retrieval 2) Multiple-choice test, which enable us to automatically evaluate joint language-visual model based on precise matrics: recall and accuracy. We provide a baseline results for LSMDC16 video annotation and search and multiple-choice test data, that we built for LSMDC16.</p><p>The rest of the paper is organized as the following, in section 2, we describe exisiting joint visual-language learning related works, in section 3, we explain how we built LSMDC16 multiple-choice test. Section 4 describes our joint visuallanguage models structure and our experiments is presented in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual Caption Generation: There is an increasing interest in jointly learning from images/videos and natural language sentences. In the last 1-2 years, several works have been developed for automatic image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26]</ref> and video <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> captioning. Most of these models use Recurrent Neural Networks (RNN), or LSTMs, in an encoder-decoder architecture (i.e. CNNs or CNN+LSTM encoder, for images or videos respectively, to generate a hidden semantic state and RNN/LSTM decoder that decodes resulting hidden state using a trained language model to produce a final sentence description). Automatic captioning is a very challenging task, both from an algorithmic and evaluation point of view. The latter is particularly challenging with difficulties arising from evaluation of specificity, linguistic correctness and relevance of generated captions. Visual Question Answering: Because of the aforementioned challenges, image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> and, more recently, video <ref type="bibr" target="#b31">[32]</ref> Visual Question Answering (VQA) has became a preferred alternative to caption generation. VQA datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> consist of large number of structured (e.g. multiple choice or fill-in-the-blank) and unstructured (e.g. free form) image-specific questions with corresponding answers. As a result, evaluation in VQA setting tends to be considerably more objective, requiring algorithms to have, at a minimum, certain level of visual understanding to answer the poised questions. Image-Caption Retrieval: Another alternative is image-caption retrieval that has been defined as a standard way to evaluate joint language-visual models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>. The core idea is to rank a set of images according to their relevance to a caption query (a.k.a, image retrieval) or ranking captions according to their relevance to the given image query (a.k.a, caption retrieval). Image-caption retrieval approaches, typically, learn a joint embedding space that minimizes a pair-wise ranking objective function between images and captions. Particularly relevant to our paper, is the work of <ref type="bibr" target="#b23">[24]</ref>, where it is acknowledged that different forms of captions form a visual-semantic hierarchy and the order-preserving constraints are used as objective for learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LSMDC16 Multiple-Choice Test</head><p>In this section we explain our multiple-choice test collection for LSMDC16.</p><p>LSMDC16 multiple-choice test definition: Given a video with 5 possible description choices, the task is assigning correct(ground-truth) caption to the video among 5 possible choices. <ref type="figure" target="#fig_0">Figure 1</ref> illustrate multiple-choice test for a video sample. In order to define 5 possible captions for each video, first we tag each caption in the whole LSMDC16 with one or multiple activity phrase labels (i.e. described in the following paragraph). Second the correct answer is the ground-truth caption and four other disctractor answers are randomly picked from the same subset (i.e. either training, test, or validation captions) with the condition that their activity phrase labels have no word intersection with correct answer activity phrase labels. Activity knowledge mining: The activity elements are extracted from AD captions using "Knowlywood", a model for human activities mining from movie narratives such as AD captions <ref type="bibr" target="#b21">[22]</ref> and it is presented in "Predicate + Object" (PO) format. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates an example of activity phrase labels from a AD caption (for more details about activity mining please see <ref type="bibr" target="#b21">[22]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint visual-language neural network models</head><p>Our overall goal is to learn, given a set of training video-caption pairs</p><formula xml:id="formula_0">D vid = {(V i , S i )}, a ranking function such that the ground-truth pairs (V i , S i ) rank</formula><p>higher than all other pairs (e.g. (V i , S j ) where j = i) by a margin. While this high level goal is easy to specify it requires a number of algorithmic choices and challenges to realize. we implement three different models:</p><formula xml:id="formula_1">M1. Simple Average Glove + Simple Average FC-7 (SA-G + SA- FC7):</formula><p>We use this simple model as the baseline. In this model the sentence is encoded using simple average word vector representations. Also video is encoded using simple average of video frames feature vectors as we will describe in Appendix B.1. Finally the video representation and sentence representation are separately linearly transformed into joint visual-language space. We constrain video and sentence embedding vectors to only positive values by computing their absolute values as suggested in <ref type="bibr" target="#b23">[24]</ref>. The model is trained using a ranking objective function that will be described in Appendix B.2. M2. Language LSTM + Simple Average FC-7 (LSTM + SA-FC7): <ref type="figure" target="#fig_2">Figure 3</ref> (left) illustrates this model. In this model the sentence is encoded using LSTM <ref type="bibr" target="#b6">[7]</ref> and represented in joint semantic space as LSTM last hidden layer output; video is encoded using simple average of video frames feature vectors as we will describe in Appendix B.1. Finally the video representation is linearly transformed into joint semantic space. In this model, we also constrain video and sentence embedding vectors to only positive values by computing their absolute values. The model is trained using a ranking objective function that will be described in Appendix B.2. M3. Language LSTM + Weighted Average FC-7 (LSTM + WA-FC7): <ref type="figure" target="#fig_2">Figure 3</ref> (right) illustrates this model. In this model, similar to above, the sentence is encoded using LSTM and its embedding is absolute values of last hidden state output of LSTM. A soft-attention aligns the output of last hidden state of language LSTM, h N , with all feature vectors of frames in V. Video is then encoded using weighted average of frame feature vectors and the video representation is linearly transformed into joint embedding space obtained by minimizing a ranking loss that we will describe in Appnedix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Datasets: We use two datasets in our experiments: We asked people to re-phrase original captions with 3-10 word sentences using different wording compared to original captions as much as possible.</p><p>As an example, "She walks over to the banquette and sits down." is original caption was re-phrased as "She walked to the feast and took a seat.".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>We implemented our models in Theano-based framework named BLOCKS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. Our data pipeline has been implemented using FUEL framework <ref type="bibr" target="#b14">[15]</ref>. Model configurations: We set LSTMs hidden-state dimension and the dimensions of the joint embedding space to 950 (we also tried 512 but it resulted in lower performance). The dimension of the word embedding is 300. The attention model matching space is also 300 (we also tried 900 which resulted in similar performance), and set the ranking margin α = 0.05 (as proposed in <ref type="bibr" target="#b23">[24]</ref>). Similar to other works, e.g. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, to decrease overfitting issue, we constrain sentence and video embeddings to have a unit (L2) norm. Training: We found it useful to train with both video-sentence (LSMDC) and image-sentence (COCO) datasets; to accommodate this for standard ranking task we treat images as one-frame videos. For both standard ranking and multichoice test training phase is the same. However for multi-choice test we tried both pairwise ranking and annotation ranking as we described in Appendix B.2. In training phase, we randomly sample minibatch of size 200 image/videocaption pairs. The contrastive terms for the minibatch are computed by using 199 contrastive videos for each caption and 199 contrastive captions for each video, similar to <ref type="bibr" target="#b23">[24]</ref>. Our models were trained with Adam optimizer with learning rate 0.001 and we applied clipping gradient with threshold 2.0. We early-stop and save the best model based on monitored loss function of 1000 randomly selected validation samples. For all datasets, including LSMDC16 and COCO, we use original train, valid, and test splits. Testing: For evaluation standard video annotation and retrieval, we use Recall@K and Median Rank (medR) metrics. "r@k" means the percentage of ground-truth captions/videos in the first K retrieved captions/videos and "medR" means the median ranks of ground-truth videos/captions. We report results on COCO test images, LSMDC16 test set, and LSMDC16 test rephrasing captions that we will make public. For multi-choice test evaluation, we compute the accuracy over test-data. In order to do that for each test query, first we compute order similarity (Equation 3 in Appendix B.2) for all 5 answer choices and then video is assigned to description with highest score, then accuracy is computed for all test data as we described in Section 3. <ref type="figure" target="#fig_3">Figure 4</ref> shows results of top 5 ( at each row, from left to right) phrase-based video search. A phrase query have been shown on top of each row. Each row shows the video frame with highest attention weight from the retrieved video computed for corresponding phrase query. In order to compute attention weights for a new phrase query, we forward pass top 5 videos, retrieved by model M1 or M2, and new phrase query through attention network in M3 model, trained on combination of COCO and LSMDC. We observe that our temporal attention network is able to highlight salient video frames in LSMDC video clips by associating a higher attention weights to salient video frames. The phrase query is on top of each row. The value below each image is the attention weight for corresponding query. <ref type="table">Table 1</ref>. Quantitative comparison of models for standard ranking. M1 is SA-G + SA-FC7 model and M2 is LSTM + SA-FC7 (details are in Section 4), and OE stands for order-embeddings model <ref type="bibr" target="#b23">[24]</ref>. The C stands for COCO image dataset, L'16 for LSMDC'16 datasets, and RP for LSMDC'16 rephrases data. The 1000 AD stands for 1000 randomly picked audio description captions from LSMDC16 public test-set; 1000 re-phrase stands for 1000 captions re-phrasings from LSMDC16 public test-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Results for temporal attention network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Annotation <ref type="formula">(</ref>  Even the Standard ranking performance for original AD and rephrased captions is relatively low due to the caption complexity, still with simple phrase queries, we can obtain quiet impressive video retrieval results using trained joint language-visual models for LSMDC16 (see more qualitative results in Appendix A). Multiple-choice test: <ref type="table" target="#tab_3">Table 2</ref> summarizes all our evaluations for M1 and M2 model architectures described in Section 4. The best result are shown in bold. First, M2 (C+L'16+AR) which is trained on combination of COCO and LSMDC has the best performance. Second, training a model using AR (Annotation Ranking) loss has 2% improvement in accuracy compared to pairwise ranking for multiple-choice test (see ranking loss details in Appendix B.2).</p><p>A Phrase-based movie shots search </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model details</head><p>Detail about sentence and video encoding and ranking loss that we used in our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Video and sentence encoding</head><p>Each sentence is a sequence of words. The length of word sequences are variable from one sentence to the next. Each word is initially encoded using GloVe <ref type="bibr" target="#b15">[16]</ref> distributed word vector representation that has been pre-trained on a 6 billion word corpus that includes Wikipedia and Gigaword 5th edition datasets. Each sentence is therefore represented by a matrix:</p><formula xml:id="formula_2">S = [w 1 , ..., w N ] ∈ R N ×dw<label>(1)</label></formula><p>of N word feature vectors, and each vector has d w = 300 dimension in our GloVe representation. In model M 1, fixed length representation of the sentence can then be obtained as simple average of the word representations, 1 N N i=1 w i . The sentence representation is then fed into a linear transformation and the absolute values of transformation output is taken as semantic representation of the full sentence. In model M 2 and M 3, the sentence representation S is fed into an LSTM which results in a sequence of hidden states [h 1 , ..., h N ] ∈ R N ×d h , where d h is the dimension of the LSTM hidden state. The absolute values of the output of the last hidden state h N is taken as semantic representation of the full sentence.</p><p>A video is represented as sequence of frames which are sampled every 10 video frames (images are treated as single frame videos). We extract frame features using pre-trained VGG-19 <ref type="bibr" target="#b19">[20]</ref> convolutional neural network (CNN) in Caffe <ref type="bibr" target="#b8">[9]</ref>. VGG-19 network <ref type="bibr" target="#b19">[20]</ref> is trained for object classification on ImageNet, to classify among 1, 000 object categories. Video sequence length varies from one sequence to the next, which results in a matrix representation:</p><formula xml:id="formula_3">V = [v 1 , ..., v M ] ∈ R M ×dv<label>(2)</label></formula><p>of M video frame feature vectors, each of dimensionality d v . VGG-19, proposed in <ref type="bibr" target="#b19">[20]</ref>, has 16 convolutinal layers and 3 fully connected layers, followed by a softmax output layer. For our experiments we extract second Fully-Connected layer (FC7), therefore d v = 4096. Fixed length representation of the video can then be obtained as simple average of the frame-based representations, 1 M M i=1 v i , a weighted average obtained using soft attention or using an LSTM encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Learning to rank</head><p>Two-level partial order similarity: We use the negative order-violation penalty, proposed in <ref type="bibr" target="#b23">[24]</ref>, for similarity metric, which is defined as</p><formula xml:id="formula_4">S(c, v) = − max (0, c − v) 2 (3)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example of LSMDC16 multiple-choice test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An Example of Activity phrase mining from movie AD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The variations of neural networks models for learning joint visual-language embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Top 5 (from left to right) phrase-based video search results. Each row shows video frmaes with maximum attention weight computed with model M3 (C+L'16 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Top 5 (from left to right) phrase-based video search results with natural language using model M2 (C+L'16 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Multiple-choice test evaluation: For evaluation, we compute test accuracy for public-test captions with 10053 videos. Test accuracy is the percentage of correctly answered questions (i.e. 10053 questions).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of models for multiple-choice test. M1 is SA-G + SA-FC7 model and M2 is LSTM + SA-FC7 (details are in Section 4). The AR stands for annotation ranking (details in Appendix B.2). The C stands for COCO image dataset, L'16 for LSMDC'16 datasets, and RP for LSMDC'16 rephrases data.</figDesc><table><row><cell>Model</cell><cell>multiple-choice (10053 LSMDC16 test)</cell></row><row><cell></cell><cell>accuracy</cell></row><row><cell>M1 (C+L'16)</cell><cell>55.1%</cell></row><row><cell>M2 (L'16)</cell><cell>56.3%</cell></row><row><cell>M2 (C+L'16)</cell><cell>56.6%</cell></row><row><cell>M2 (C+L'16+AR)</cell><cell>58.1%</cell></row><row><cell>5.3 Quantitative Results</cell><cell></cell></row><row><cell cols="2">Standard ranking: Table 1 summarizes all our evaluations for M1 and M2</cell></row><row><cell cols="2">model architectures described in Section 4. The best results are shown in bold.</cell></row><row><cell cols="2">Following are some observations. First, using model M2, we are able to repro-</cell></row><row><cell cols="2">duced COCO image retrieval SOTA results in [24] (see row 1 and 2). Second, M2</cell></row><row><cell cols="2">(C+L'16 ) trained on combination of COCO (C) and LSMDC16 (L'16), has con-</cell></row><row><cell cols="2">sistently a better performance, for both 1000 original captions and re-phrases,</cell></row><row><cell cols="2">compared to the same model only trained on LSMDC16 (L'16) or baseline model</cell></row><row><cell cols="2">M1 (C+L'16 ). Finally model M2 (C+L'16+RP ) trained on combination of</cell></row><row><cell cols="2">COCO (C), LSMDC16 (L'16), and LSDMC16 re-phrases has better medR per-</cell></row><row><cell cols="2">formance compared to M2 (C+L'16 ).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://visualgenome.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>using order violating penatly, where c is either an embedding ( for absolute values of simple average: |W word · 1 M M i=1 v i |, with embedding matrix W word )) or c = |h N | denotes the absolute values of last hidden state of the language LSTM and v is an embedding (for absolute values of simple/weighted average:</p><p>. W word , W video , and h N are netowrk parameters that are learned during training phase.</p><p>The advantage of this similarity distance is that it is asymmetric, compared to cosine similarity which is symmetric, so it can capture the relatedness of captions with very different lengths that describe the same visual content but in different levels of detail (detailed discussion is given in <ref type="bibr" target="#b23">[24]</ref>). Pairwise ranking loss: As baseline, we use the standard loss that has been used in multiple works for image retrieval task in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> and is defined as</p><p>is an encoding for the ground-truth video/sentence pair, c (contrastive captions) are captions that do not belong to v and v (contrastive videos) are video/images that are not captioned by c; α denotes a margin hyperparameter and S is the similarity function in Eq. 3. Annotation ranking loss: The first term in the above pairwise ranking 4, is a hinge loss that promote a pair of video and ground-truth caption is required to score higher than all other pairings of same video with contrastive captions by a margin of α. We name this annotation ranking loss and we show for multi-choice test, training models with this loss has better performance compared to pairwise ranking loss. Annotation ranking loss is defined as </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>CoRR abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Framing image description as a ranking task: Data, models, and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for gen. image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual semantic search: Retrieving videos via complex textual queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2657" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Blocks and fuel: Frameworks for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1506.00619</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Describing and understanding video and the large scale movie description challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/describingmovies" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03705</idno>
		<title level="m">Movie description</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowlywood: Mining activity knowledge from hollywood narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06361v6</idno>
		<title level="m">Order-embeddings of images and language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sequence to sequence -video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Uncovering temporal context for video question and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04670v1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
