<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Cluster for Proposal-Free Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
							<email>yenchang.hsu@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<email>zkira@gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Tech Research Institute Atlanta</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
							<email>jhuang@honda-ri.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Honda Research Institute Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Cluster for Proposal-Free Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposed a novel learning objective to train a deep neural network to perform end-to-end image pixel clustering. We applied the approach to instance segmentation, which is at the intersection of image semantic segmentation and object detection. We utilize the most fundamental property of instance labeling -the pairwise relationship between pixels -as the supervision to formulate the learning objective, then apply it to train a fully convolutional network (FCN) for learning to perform pixel-wise clustering. The resulting clusters can be used as the instance labeling directly. To support labeling of an unlimited number of instance, we further formulate ideas from graph coloring theory into the proposed learning objective. The evaluation on the Cityscapes dataset demonstrates strong performance and therefore proof of the concept. Moreover, our approach won the second place in the lane detection competition of 2017 CVPR Autonomous Driving Challenge, and was the top performer without using external data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I. INTRODUCTION Instance segmentation is a task that combines requirements from both semantic segmentation and object detection. It not only needs the pixel-wise semantic labeling, but also requires instance labeling to differentiate each object at a pixel level. Since the semantic labeling can be directly obtained from an existing semantic segmentation approach, most of the instance segmentation methods focus on dealing with the instance labeling problem. This is usually achieved by assigning a unique identifier to all of the pixels belonging to an object instance.</p><p>Instance labeling becomes a more challenging task when occlusions occur, or when a vastly varying number of objects in a cluttered scene exist. For example, current top performance on the Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref> only reaches 26% accuracy <ref type="bibr" target="#b12">[13]</ref> (without external training data) in terms of average precision, which still leaves much room for improvement.</p><p>Techniques to solve instance segmentation can be roughly grouped into two categories: proposal-based methods and proposal-free methods. In proposal-based methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, a set of object proposals and their classes are first predicted, then foreground-background segmentation in each bounding box is performed. The proposal-free approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref> exclude the step of proposal generation. These approaches usually have two stages. The first is to learn a representation (e.g. a feature vector, an energy level, breakpoints, or object boundaries) at the pixel level, then in the second stage they group the pixels using a clustering algorithm with the learned representation. Additionally, the proposal-free approaches usually only focus on instance labeling and directly leverage the categorical predictions from semantic segmentation for the semantic labeling.</p><p>Our approach belongs to the proposal-free style. We reduce the two-stage paradigm to a single forward pass on a fully convolutional network (FCN) <ref type="bibr" target="#b22">[23]</ref>. We achieve this by designing a novel learning objective, which uses pairwise relationships between pixels as the supervision to guide an FCN to learn pixel-wise clustering. The FCN trained with the proposed objective learns to directly assign a cluster index to each pixel, while each pixel cluster is regarded as an object instance. The clustering is done by the forward propagation of the FCN. It turns out the FCN is capable of learning to do pixel-wise clustering and generalize the learned clustering mechanism to unseen images.</p><p>The number of cluster indices available in the FCN will limit the number of instances that can be separated by our approach. We provide a strategy to deal with the case of an unlimited number of instances. Inspired by graph coloring theory in how it reuses the indices for coloring a graph, we inject the coloring strategy into our learning objective. Therefore the FCN is trained to assign different indices for the neighboring instances, while reusing the index for the objects that are far away from each other. With the coloring result, each individual instance can be naively recovered by connected components extraction.</p><p>We formulate the lane detection problem as an instance labeling problem, and our approach won the second place in the lane detection competition of the 2017 CVPR Autonomous Driving Challenge. The difference of accuracy between ours and the first place is insignificant. Considering that the top performer used a large amount of external data for training while we did not, the advantage of our approach becomes even more significant. We are also able to perform the prediction in real-time (∼ 55 FPS).</p><p>Lane detection is a problem that involves a single category and a limited number of instances; therefore, we extend our evaluation on a multi-category dataset and unlimited instance setting, specifically the Cityscapes dataset. Our approach demonstrates strong performance, achieving 15.1% AP. By comparing to the 9.8% AP of the JGD <ref type="bibr" target="#b18">[19]</ref> which shares similar insights in using graph coloring (also called node  <ref type="figure">Fig. 1</ref>. The overview of our approach. We address the labeling problem by formulating a novel learning objective. It guides the fully convolutional networks to learn to perform instance labeling. labeling), our data-driven learning approach has a significant advantage over their search-based algorithm.</p><p>In summary, we make several contributions. First, we formulate a novel objective to train an FCN to perform instance labeling. Second, we demonstrate how to combine the graph coloring theorem to augment the learning objective. Third, we empirically show a deep FCN is able to learn to do clustering on image pixels in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Proposal-based methods: This type of approach usually follows the detect-then-segment paradigm <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> which first detects a bounding box as the object proposal, then segments out the foreground object in the box region. Some variant approaches, for example <ref type="bibr" target="#b24">[25]</ref>, uses RNNs to generate the proposals instead of using a proposal network. <ref type="bibr" target="#b3">[4]</ref> uses the bounding box as a potential in their CRF formulation. Their performance is affected by the quality of bounding boxes, and prefer a round instance. Therefore their approach is not suitable for a thin and long instance like the lane line of the road. In contrast, proposal-free approaches have no such limitations.</p><p>Proposal-free methods: Although the approaches in this type share the same two-stage scheme of representation learning then clustering, there is a wide spectrum of ways to achieve it. <ref type="bibr" target="#b21">[22]</ref> has a per pixel prediction of breakpoints, then apply a sequential grouping for clustering the pixels. <ref type="bibr" target="#b4">[5]</ref> learns an energy level for each pixel and is followed by watershed clustering. <ref type="bibr" target="#b5">[6]</ref> learns a discriminative feature vector for mean shift clustering. <ref type="bibr" target="#b17">[18]</ref> uses object boundary prediction with a MultiCut algorithm <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref> learn several handpicked features then use heuristic or spectral clustering. <ref type="bibr" target="#b18">[19]</ref> formulate instance labeling as a node labeling problem and find a feasible solution using a search algorithm. <ref type="bibr" target="#b8">[9]</ref> learns position-sensitive score maps, then merge the masks with an assembling module. Our method belongs to this category but is different from above in the way that we do not specify an intermediate representation for learning. We let an FCN <ref type="bibr" target="#b22">[23]</ref> learn to perform instance labeling directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we describe how to formulate the learning objective, and explain how to use a limited amount of indices to label an unlimited amount of instance in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Instance Labeling</head><p>The instance labeling task is defined as follows. We have an RGB image as input, and our task is to predict a mask for each instance. This is done by assigning a unique index (instance ID) to all of the pixels in the mask. The index is an integer i, 1 ≤ i ≤ n, where n is the number of instances in the scene. One crucial property of the assignment is that it is not unique. Specifically, swapping the index between any two masks will still lead to a valid assignment and equivalent segmentation. This is referred to as the quotient space property <ref type="bibr" target="#b16">[17]</ref>. The goal of the task is to learn a function f which can assign an index y i = f (p i ) for a pixel p i , where y i ∈ Z and i is the index of the pixel in an image. The resultant labeling of all pixels in an image, i.e., Y = {y i } ∀i , should fulfill the relationship R. For any two pixels</p><formula xml:id="formula_0">p i , p j , R(p i , p j ) ∈ {0, 1} is defined as, R(p i , p j ) = 1, if p i , p j belong to the same instance. 0, otherwise.</formula><p>(1) Since the labels in the ground-truth are just one instantiation of the labeling based on the underlying relationship R, we propose to directly use R as the supervision for training. Using R as the learning objective is preferable over using the ground-truth labeling. Since the instance ID of any particular instance is assigned in an ad hoc manner, forcing a particular labeling makes the learning task more difficult because the labeling is not consistent from image to image (e.g. a vehicle with similar appearance may be assigned different labels in different images). R is a more precise representation of the actual learning objective. Reconstructing the R from a given labeling is straightforward from equation (1).</p><p>1) The Learning Objective: We use a fully convolutional neural network (FCN) <ref type="bibr" target="#b22">[23]</ref> as f to make pixel-wise prediction. We define the outputs of the FCN as the probability of assigning a pixel to a certain instance index, which is a multinomial distribution. Inspired by <ref type="bibr" target="#b14">[15]</ref>, we intend that if two pixels belong to the same instance, their predicted distributions should be similar and be dissimilar otherwise. The distance between two distributions could be evaluated by the Kullback-Leibler divergence. Given a pair of pixels p i and p j , their corresponding output distributions are denoted as</p><formula xml:id="formula_1">P i = f (p i ) = [t i,1 ..t i,n ] and P j = f (p j ) = [t j,1 ..t j,n ],</formula><p>where n is the number of indices available for labeling. The cost between the pixels that belong to the same instance is Prefer similar distribution <ref type="figure">Fig. 2</ref>. The example outputs of lane detection. The colors represent different instance IDs. The outputs for each pixel is a 6 + 1 dimensional vector, which represents the probability distribution of this pixel being assigned to a certain ID. Our learning objective (eq. <ref type="formula" target="#formula_4">(3)</ref>) guides the f to output a similar distribution for the pixels on the same lane line, and vise versa. During testing time, the pixel will be assigned to an ID with highest probability.</p><p>given by :</p><formula xml:id="formula_2">L(p i , p j ) + = D KL (P i ||P j ) + D KL (P j ||P i ), where D KL (P i ||P j ) = n k=1 t i,k log( t i,k t j,k ).<label>(2)</label></formula><p>The cost L(p i , p j ) + is symmetric w.r.t. p i , p j , in which P i and P j are alternatively assumed to be constant. If p i , p j are from different instances, their output distributions are expected to be different, which can be described by a hinge-loss function:</p><formula xml:id="formula_3">L(p i , p j ) − = L h (D KL (P i ||P j ), σ) + L h (D KL (P j ||P i ), σ), where L h (e, σ) = max(0, σ − e).</formula><p>The margin σ is a hyper-parameter. We use 2, as suggested in <ref type="bibr" target="#b14">[15]</ref>, for all our experiments. We then construct a criterion to evaluate how the outputs of f are compatible with R in the form of a contrastive loss:</p><formula xml:id="formula_4">L(p i , p j ) = R(p i , p j )L(p i , p j ) + + (1 − R(p i , p j ))L(p i , p j ) − .<label>(3)</label></formula><p>An example associated with the idea of equation <ref type="formula" target="#formula_4">(3)</ref> is illustrated in figure 2. We apply equation <ref type="formula" target="#formula_4">(3)</ref> on top of the outputs of a softmax layer in a standard FCN which was originally designed for semantic segmentation. Therefore the loss function is easy to deploy and combine with other pixelwise prediction tasks like semantic segmentation and depth estimation.</p><p>2) Combining the Sampling Strategy: The objective in equation (3) uses pairwise information between pixels. The number of pairs grow quadratically with the number of pixels in an image. Therefore it is not feasible to use all pixels in an image. We adopt a sampling strategy. A fixed total number of pixels (e.g. one thousand) is sampled during the training time. Only the pixels in the ground-truth instance masks are picked (see below for how we handle the background class). All instances in an image receive the same number of samples regardless of their size. The pixels in an instance are randomly sampled with uniform distribution. To create the pairs, all possible pairwise relationships between the sampled pixels are enumerated. Therefore one million pairs (including both orders and self-pairs) per image are generated upon which eq. (3) is applied.</p><p>We treat the background as one instance and handle it differently because of its unbalanced nature. Since the background contains the majority of pixels in an image, the sampled points are very sparse. Using a cityscapes <ref type="bibr" target="#b7">[8]</ref> image (1024x2048) as example, the density of sampled points on background is roughly 0.005%. This leads to an obvious limitation that the boundary between instance and background is hard to learn. In fact the predicted instances tend to stretch significantly into the background region.</p><p>One trivial solution is to increase the number of samples on the background region. We push this notion to the extreme and use all background pixels for training. However, we only consider the unary prediction instead of pairwise relationships. Specifically, we use a binary classification loss for the background, while the background and other instance still share the same output vector which represents the instance index. To achieve that, we reserve the index zero only for the background. Given a n + 1 dimension predicted outputs</p><formula xml:id="formula_5">f (p i ) = P i = [t i,0 ..t i,n ], the summation of non-zero indices [t i,1 ..t i,n ]</formula><p>is the probability of non-background. We formulate the criterion of background classification to be similar to a binary cross entropy loss:</p><formula xml:id="formula_6">L bg = − 1 N N i (I bg i log t i,o + (1 − I bg i ) log( n k=1 t i,k )) (4)</formula><p>N is the total number of pixels in an image. I bg i is the indicator function and it returns 1 if pixel i is background. Note that although the value of n k=1 t i,k is equal to 1 − t i,0 , the resulting derivative is different and our formulation can encourage the outputs of [t i,1 ..t i,n ] when p i is not background. Let T = {(p i , p j )} ∀i,j contain all pairs of sampled pixels, we have the averaged pairwise loss:</p><formula xml:id="formula_7">L pair = 1 |T | (pi,pj )∈T L(p i , p j )<label>(5)</label></formula><p>The full formula for instance segmentation is the direct combination of both:</p><formula xml:id="formula_8">L ins = L pair + L bg<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Addressing an Unlimited Number of Instances</head><p>The f defined in section III-A can only represent n instance IDs. Therefore it limits the maximum number of instances that could be detected. Although the fixed amount of instance IDs is sufficient for applications such as lane detection for autonomous driving, it becomes a limitation for datasets like Cityscapes <ref type="bibr" target="#b7">[8]</ref> for segmenting an arbitrary number of objects. Although we can deal with the problem by increasing the dimension of the output vector, it will introduce two problems.  The first is the distribution of the number of instances in an image usually has a long-tail distribution. Most of the images only contains few instances (ex: 5) and only a small fraction has a large number of objects (ex: 100). In such cases the majority of output nodes will only be trained with a small fraction of training data. Therefore it leads to poor performance. The second is the efficiency consideration. A high dimension output layer will greatly increase the computation time, since the output has a map size equal to the input image. In this section, we describe a generic approach to labeling an unlimited number of instances with a fixed number of IDs.</p><p>Inspired by the graph coloring problem, we reformulate the index assignment task to a graph coloring task. Here we regard the region of instance as a vertex (see <ref type="figure" target="#fig_3">figure 3</ref> (a)-to-(b)). The distance between regions decides whether an edge exists or not. The goal of graph coloring is to assign a color to each vertex so that neighboring vertices have different colors. A graph is called k-colorable if we can find an assignment with k or fewer colors. The minumum k of a graph called its chromatic number. The k could possibly be much smaller than the number of vertices (the number of instances). For example, if we set the distance threshold to 1 pixel, there will only be edges between adjacent instances. This case is also called the map coloring problem. According to the fourcolor theorem <ref type="bibr" target="#b2">[3]</ref>, we only need four colors to make sure any instances has a color different from its neighbors. The colors mentioned here are equivalent to the set of indices we used to label instance pixels.</p><p>A compatible k-colored map means no adjacent instance has the same color. Under this condition, the individual instance region can be extracted by finding the connected components at the pixel level, i.e., by growing a region which share the same ID. Each connected component (instance segment) will be assigned a unique ID for the final outputs. <ref type="figure" target="#fig_3">Figure 3</ref> (a)-to-(d) illustrates the process.</p><p>1) Learning to do graph coloring: In this section we demonstrate a strategy to train a deep neural network to do graph coloring (also called node labeling). We show that by relaxing the setting of graph coloring, we can deploy the constraints of coloring by only slightly changing the sampling strategy described in section III-A2.</p><p>First, we relax the coloring rules from a constraint that must be satisfied to be a soft guideline. The guideline is "Neighboring instances should have different IDs". It is presented in the learning objective and only used in training stage. Second, we set the distance threshold to a value larger than 1 pixel (our experiment uses 256). The threshold only applied to the pairs of the randomly sampled pixels in section III-A2. Compared to the original T that contains all pairs of sampled pixels, the T only contains the pairs (p i , p j ) which have spatial distance (|p i p j |) within threshold :</p><formula xml:id="formula_9">T = {(p i , p j )} ∀i,j,|pipj |≤<label>(7)</label></formula><p>Therefore the averaged pairwise loss (eq. (5)) becomes: <ref type="figure" target="#fig_3">Figure 3</ref>(c) demonstrates an example of the sampling. The yellow dots are the sampled pixels. The black edges means its two nodes should have similar predicted label distribution, while the white edges represent the dissimilar pairs. Any two pixels that have distance larger than are considered to have no edge between them and therefore contribute no loss at all to the learning objective.</p><formula xml:id="formula_10">L pair = 1 |T | (pi,pj )∈T L(p i , p j )<label>(8)</label></formula><p>2) Choosing the number of color: The eq. (5) is a special case of eq. (8) with = ∞. With the infinity threshold, there are edges between all instances, so the k has to be equal to the number of object instance in an image. With the decreasing of the , the chromatic number of the graph is also decreasing. The trend stops at = 1, which becomes a map coloring problem and has chromatic number 4. Note that it is not necessary to consider the case when is smaller than one pixel. In that case all instances are independent, i.e., no edge between any vertices, therefore one color is sufficient to color the graph. However, individual instance pixels can't be extracted with the resulting coloring.</p><p>Since we transform the coloring constraints to a soft learning objective, the choice of n has no hard requirement. Based on the arguments in above paragraph, setting n to any number larger or equal to four could be sufficient, and it is also dependent on the setting of . We determine the two parameters empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bells and Whistles</head><p>We consider two factors in instance segmentation applications, which are limited/unlimited number of instances and single/multiple categories.</p><p>For applications with a limited number of instances, applying the approach in section III-A is sufficient. One example is lane detection for autonomous vehicles, which usually has a bounded number of visible lanes in the camera view. The benefit of applying section III-A standalone is that it is a fully end-to-end solution that can be accomplished in a standard FCN. No post-processing is required. In contrast, when the number of instances is unlimited, the approaches in  For the case of multiple categories, we note that our instance ID assignment approach is category agnostic. Therefore it needs external information to help assigning the class to each instance. For each instance mask, we average the predicted semantic segmentation probability in the masked region and find the dominant category. The intersection between our instance mask and the dominant category mask of semantic segmentation is used as the final instance output. Since we use FCN for the f , it is straightforward to add an output branch to predict the semantic segmentation while sharing most of the layers except the final layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NETWORK ARCHITECTURE</head><p>This section describes the network architecture used for f . <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the diagram. This style of FCN is widely used in pixel-wise prediction and was referred as FPN <ref type="bibr" target="#b20">[21]</ref>. The major benefit of using FPN is its configurable dimension for the pixel-wise feature map. In our implementation, the layers Conv-1 to Conv-5 have the weights initialized by pretrained ResNet <ref type="bibr" target="#b13">[14]</ref>. The Conv-2p to Conv-5p (called Conv-Xp for abbreviation) have kernel size 3x3 and are followed by batch normalization <ref type="bibr" target="#b15">[16]</ref> and ReLU. The Conv-Xp layers have the outputs of channel dimension c, which is configurable. The outputs of Conv-Xp layers are up-sampled and have element-wise summation with the outputs from lower layers. The resulting feature map M has c feature channels and is four times smaller than the input image. Furthermore, since we use element-wise summation to combine the features from different convolution blocks, the Conv-Xp work like learning the residual representation for constructing the M .</p><p>The task-specific layers are added on top of M . For the instance ID assignment task, we use two convolution layers. The first one has 3x3 kernel and c output channels, followed by batch normalization and ReLU. The second one has 1x1 kernel with n+1 dimension outputs, which maps to n instance IDs and one background ID. Other pixel-wise prediction tasks can also be added here to construct a multi-head structure for multi-task learning; for example, semantic segmentation, boundary detection, depth estimation, and object center prediction. Those tasks can reuse the same two-layer structure by only changing the number of final outputs to fit their target number of categories. In our evaluation on Cityscapes dataset, we add semantic segmentation to help assign the category of each object instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATION</head><p>In this section, we evaluate the performance of our proposed method on two vastly different datasets. The first one is a lane detection dataset and the second is the benchmark Cityscapes dataset. Our submission to the lane detection competition won the 2nd place, evaluated for both performance and speed, while on the Cityscapes instance segmentation results our entry is among the top 10 of all entries, and among the top four proposal-free entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lane Detection</head><p>The Tusimple dataset <ref type="bibr" target="#b1">[2]</ref> contains 3626 video clips of highway driving scenes in the training set. One image from each video clip is annotated with ground-truth lane lines. The number of lane lines vary from 3 to 5. The lane lines are labeled in arbitrary order. The task is to predict all individual lane lines for the test set of 2782 images. We can consider the lane lines as a thin and long region on the image. Therefore it becomes a single category multi-instance segmentation task.</p><p>The evaluation metric used by the challenge is a recall with penalty on extra detections. The recall score is calculated as score = number of matched lane points number of ground-truth lane points <ref type="bibr" target="#b8">(9)</ref> The detected lane lines are not densely evaluated per pixel, but rather sampled with horizontal lines spaced every 10 pixels. The sampled points are then compared with sampled points in the ground-truth. If their distance is below 20 pixels, it is considered a matched point. The score for each lane line is computed as above and then averaged to give the final score for an image. Since recall is biased toward methods with many detections, the final score also penalizes extra detections beyond N +2, where N is the number of lines in ground-truth. Submissions must also achieve a minimum speed of 5 FPS on a single GPU to be accepted.</p><p>The key challenge of this competition is to correctly predict the number of lines and their exact position. We formulate it as an instance segmentation problem by drawing the lane lines with 10 pixel-width. In that way we obtain a thin and long mask for each lane lines. Since there are at most 6 lane lines in the dataset, it is a problem with a limited amount of instances. We designed the network output to be a 7-D vector at each pixel location, representing the probability of the pixel belonging to a particular index, including background. The loss function used here is only the equation (6).</p><p>1) Experiment Setting: The network has a backbone of ResNet-18 <ref type="bibr" target="#b13">[14]</ref> with configuration c = 32. To train the network, we split the training images into 80% for training and 20% for validation. We applied standard data augmentation (e.g. horizontal flipping and color jittering) on the training images. We sampled 100 pixels from each line to compute the eq. (6). The stochastic gradient descent is used to optimize </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User ID/Method</head><p>Accuracy FP% FN% Ext. data XingangPan <ref type="bibr" target="#b26">[27]</ref> 96.53% the proposed learning objective with initial learning rate 0.01, which decays per 20 epochs with factor 0.1 until 50 epochs. During testing, the outputs of the network have cluster indices assigned to all pixels, while each cluster index corresponds to a line (see <ref type="figure" target="#fig_5">figure 5</ref>). For benchmarking purpose, the mean x-coordinate of each line at specific hight is calculated to produce the exact submission format.</p><p>2) Results and Discussion: <ref type="table">Table I</ref> shows the top 5 performers among 14 teams of the lane detection challenge. The accuracy is defined in equation <ref type="bibr" target="#b8">(9)</ref>. False-positive and falsenegatives are also listed for reference. Our method won the second position and is the top performer without using labeled external training data. The top performer XinganPan uses the approach that requires a specifically designed layer, e.g. the SCNN <ref type="bibr" target="#b26">[27]</ref>, and needs the lanes labeled in certain order, e.g. from left to right, so it can uses a standard cross entropy loss to classify the lines. In contrast, our approach only uses standard convolution layers and the lanes can be presented in random order. Therefore our method can largely simplify the labeling effort for constructing the training data. The third performer DavyNeven also utilizes an instance segmentation strategy <ref type="bibr" target="#b5">[6]</ref>. Its learning objective learns the embedding of pixel feature vector and therefore needs extra post-processing to cluster the pixels for discovering the lines. It can be non-trivial to make the hyper-parameters of the predefined clustering algorithm perform well. And it is hard to decide the number of road lanes. In contrast, our network performs clustering in an endto-end fashion and can predict the active clusters with very few false positives, therefore it shows a significant advantage over DavyNaven in terms of FP%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cityscapes Instance Segmentation</head><p>The Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref> has high quality instance segmentation annotation for 8 different object classes. It is a common benchmark for comparing instance segmentation performance. Cityscapes is a more challenging dataset than the lane detection dataset in three ways. First, lane lines are relatively well structured while objects in Cityscapes have arbitrary shape, scale, and location. Secondly, the number of objects in Cityscapes is larger and unbounded. Lastly, Cityscapes contains multiple categories. Therefore it is a target to demonstrate the generalizability of our approach.</p><p>1) Experiment Setting: We use the official splits of training, validation, and testing set, which have 2975, 500, and 1525 images, respectively. For evaluation, we also use the official scoring, which calculates the average precision (AP) with various intersection-over-union (IoU) thresholds, i.e., 50% to 95% with step size 5%, between predicted instances and ground truth instances. Additionally, we report the AP at 50% overlap, AP of objects closer than 50m, and AP of objects closer than 100m.</p><p>The network used here has a backbone of pre-trained ResNet-101. The feature dimension c is set to 512. To enlarge the field of view, we add the pyramid pooling module <ref type="bibr" target="#b29">[30]</ref> after Conv-5. Our pyramid pooling has the same four pooling scales as <ref type="bibr" target="#b29">[30]</ref>, but we do up-sampling and element-wise sum </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Network raw output Our final output Input with the 32x feature map in figure 4, instead of projection and concatenation in the original design. For the purpose of obtaining the category of instance and post-processing, we add two extra task-specific modules on top of the feature map M . One is for semantic segmentation and the other is for predicting the object center. The later module has 2-D output which corresponds to the vector pointing to object center from a specific pixel. Its usage is described in the next section.</p><p>For training the network, we sampled 50 pixels from each object. The loss L pair has the form of eq. (8), while the is set to 256 pixels and the n is set to 8, which are tuned with the validation set. We use the cross entropy loss for the semantic segmentation and use the smooth L1 loss for the regression of object center prediction. The weights for the instance ID assignment, semantic segmentation, and object center prediction are 1, 0.1, 0.01, respectively. We use stochastic gradient descent to optimize the three losses jointly with learning rate 0.01, which decays per 30 epochs with factor 0.1. The training proceeds for 90 epochs.</p><p>2) Post-processing: Each instance obtains a category from the prediction of semantic segmentation. The calculation is described in section III-C. Since we apply the graph coloring strategy for the unlimited number of instance, the connected component extraction has to be applied. Therefore the occluded object might be separated into multiple masks after the step. Here we use the predicted object center to reunion those segments. The average predictive object center is first obtained for each segment, then two segments are merged if their average center are within 20 pixels, which is tuned with the validation set. The merge operation not only helps the occlusion case, but also the situation that an object is separated into several segments due to its large size.</p><p>To calculate the AP, it requires a confidence score for each instance. Similar to SGN <ref type="bibr" target="#b21">[22]</ref> and DWT <ref type="bibr" target="#b4">[5]</ref>, we assign confidence value 1 to all our predictions, except for the instances which have size smaller than a threshold (e.g. 1500 pixel). In the later case, its confidence score is its region size (in pixel) divided by the threshold.</p><p>3) Results and Discussion: Our results on the test set is summarized in table II. We ranked forth among the proposalfree approaches. Our method (15.1%) has significant advantage over the JGD (9.8%) <ref type="bibr" target="#b18">[19]</ref> which also leverages the graph labeling concept.</p><p>We analyze the effect of semantic segmentation quality in table III. Since the semantic segmentation is used to decide the category of each pixel, it plays a substantial rule to affect the AP score. Three semantic segmentations are compared, which are the semantic segmentation outputs from our network (row 3), the prediction from PSPnet <ref type="bibr" target="#b29">[30]</ref> (row 4), and the groundtruth (row 5). The results show a clear trend that the AP increased (from 16.0% to 28.4%) as the semantic segmentation enhanced. This is despite the fact that the same set of our network outputs is used in all the evaluations from row 3 to row 8.</p><p>We also evaluate the effect of the confidence score. The oracle ranking is used in table III row 6 to row 8. When it is combined with ground-truth semantic segmentation, the AP of our instance masks could reach 38.4%. It also explains why the qualitative result in <ref type="figure" target="#fig_6">figure 6</ref> are visually appealing but it gets fair AP score in the benchmark. The limitation of using AP to evaluate instance segmentation is also discussed in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Besides the effect of semantic segmentation and confidence ranking, another dominant failure mode is that neighboring segments are assigned with the same ID. An example is the third row in <ref type="figure" target="#fig_6">figure 6</ref> which merges adjacent cars. Another defect is that the network sometimes does over-segmentation, for example, the cars in the last row, but such a problem is usually mitigated by the merging step. We leave possible enhancement for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We proposed a novel objective to train a network to perform a clustering-based instance labeling. By adjusting the sampling method, we are able to inject the graph coloring strategy into the learning objective. The strong performance on two vastly different datasets demonstrates the generalizability and applicability of proposed learning strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The concept of how graph coloring is related to instance ID assignment. For details please see section III-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The network architecture used in this work. section III-A and III-B are both applied. Connected component extraction is then needed as a post-processing to generate the final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The visualization of the lane detection on Tusimple dataset (our validation split). The red lines in top row are our predictions, while the green lines are the ground-truth. The second row shows the raw outputs from our network. The colors represent the assigned IDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Sample outputs of our model on Cityscapes validation set. The colors represent different instance IDs. GT is the ground-truth. Network outputs has eight colors. The rightmost column is the final outputs after connected component extraction and merging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>RESULTS OF THE TOP FIVE PERFORMERS OF THE 2017 CVPR LANE DETECTION CHALLENGE [1]. FP: FALSE POSITIVE. FN: FALSE NEGATIVE. EXT. DATA: USE EXTERNAL LABELED TRAINING DATA.</figDesc><table><row><cell></cell><cell></cell><cell>6.17</cell><cell>1.80</cell><cell>yes</cell></row><row><cell>Ours</cell><cell>96.50%</cell><cell>8.51</cell><cell>2.69</cell><cell>no</cell></row><row><cell>DavyNeven [6]</cell><cell>96.40%</cell><cell>23.65</cell><cell>2.76</cell><cell>N/A</cell></row><row><cell>xxxxcvcxxxx</cell><cell>96.14%</cell><cell>20.33</cell><cell>3.87</cell><cell>N/A</cell></row><row><cell>TF Placeholder</cell><cell>95.96%</cell><cell>6.54</cell><cell>4.23</cell><cell>N/A</cell></row><row><cell></cell><cell cols="2">TABLE I</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the National Science Foundation and National Robotics Initiative (grant # IIS-1426998).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://benchmark.tusimple.ai/#/t/1/leaderboard,2017" />
		<title level="m">Cvpr 2017 workshop on autonomous driving challenge leaderboard</title>
		<imprint>
			<date type="published" when="2017-07-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://benchmark.tusimple.ai/#/" />
		<title level="m">Cvpr 2017 workshop on autonomous driving challenge</title>
		<imprint>
			<date type="published" when="2017-11-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Haken</surname></persName>
		</author>
		<title level="m">The four-color problem. Mathematics today: Twelve informal essays</title>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="153" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The partition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="115" />
			<date type="published" when="1993-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural network-based clustering using pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR workshop</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Object detection free instance segmentation with labeling transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08991</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint graph decomposition &amp; node labeling: Problem, algorithms, applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Proposalfree network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="312" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L X W</forename><surname>Xingang Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional instanceaware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D X J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
