<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-view image synthesis using geometry-guided conditional GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Regmi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Markable.AI</orgName>
								<address>
									<region>Newyork</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-view image synthesis using geometry-guided conditional GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of generating images across two drastically different views, namely ground (street) and aerial (overhead) views. Image synthesis by itself is a very challenging computer vision task and is even more so when generation is conditioned on an image in another view. Due the difference in viewpoints, there is small overlapping field of view and little common content between these two views. Here, we try to preserve the pixel information between the views so that the generated image is a realistic representation of cross view input image. For this, we propose to use homography as a guide to map the images between the views based on the common field of view to preserve the details in the input image. We then use generative adversarial networks to inpaint the missing regions in the transformed image and add realism to it. Our exhaustive evaluation and model comparison demonstrate that utilizing geometry constraints adds fine details to the generated images and can be a better approach for cross view image synthesis than purely pixel based synthesis methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Novel view synthesis is a long-standing problem in computer vision. Earlier works in this area synthesize views of single objects or natural scenes with small variation in viewing angle. For generating views of single objects in a uniform background or scenes, the task regards learning a mapping or transformation across views. With a small camera movement, there is a high degree of overlap in field of views resulting in images with high content overlap. The generative models can learn to copy large parts of the image content from the input to the output and perform the synthesis task satisfactorily. Despite this, view synthesis task is very challenging due to presence of multiple objects in the scene. The network needs to learn the object relations and occlusions in the scene.</p><p>Generating cross-view natural scenes conditioned on images from drastically different views (e.g., generating top-view from street view scene) is very painstaking. This is mainly because there is very little overlap between the corresponding field of views. Thus, simply copying and pasting pixels from one view to another would not be a solution. Rather, it is needed to learn the object classes present in the input view and to understand the correspondences in the target view with appropriate object relations and transformations (i.e., geometric reasoning).</p><p>In this work, we address the problem of synthesizing groundlevel images from overhead imagery and vice versa using conditional Generative Adversarial Networks <ref type="bibr" target="#b20">[Mirza and Osindero (2014)</ref>]. Also, when possible, we guide the generative networks by feeding homography transformed images as inputs to improve the synthesized results. Basically, conditional GANs try to generate new images from conditioning variables as input. The conditioning variables could be other images, text descriptions, class labels, etc.</p><p>Our first approach exploits the success of the first GANbased image-to-image translation network put forward by <ref type="bibr" target="#b10">Isola et al. (2017)</ref> as a general purpose architecture on multiple image translation tasks. This work translates images of objects or scenes which are represented by RGB images, gradient fields, edge maps, aerial images, sketches, etc across these representations. Thus, it essentially operates on different representations of images in a single view. We use this architecture as a starting point (base model) in our task and obtain encouraging results. The limitation of this approach to our problem, however, is that the images to be transformed to are very different as they come from two drastically different views, have small fields of view overlap, and objects in the images might be occluded. As a result, learning to map the pixels between the views is difficult as the corresponding pixels in two views may represent different object classes. To address this challenge, here we propose to use the semantic segmentation maps of target view images to regularize the training process. This helps the network to learn the semantic class of the pixels in the target view and to guide the network to generate the target pixels. By encouraging the networks to generate the segmentation maps in target view, the network learns the semantic classes of each pixel which are important cues in assisting to generate cross-view images that preserve semantic information from the source view to the target view.</p><p>The next approach we take to solve the cross-view image synthesis task is to exploit the geometric relation between the views to guide the synthesis. For this, we first compute the ho-arXiv:1808.05469v2 [cs.CV] 18 Jul 2019 mography transformation matrix between the views and then project the aerial images to street-view perspective. By doing so, we obtain an intermediate image that looks very close to the target view image but not as realistic and with some missing regions. Now, our problem reduces to preserving the scene layout and details while filling in the missing regions and adding realism to the transformed image. For this, we use cGAN architectures described in previous approach. We also use different cGANs that work specifically for inpainting and realism tasks to preserve the pixel information from the homography transformed image in a controlled manner.</p><p>To summarize, we propose the following methods. We start with the simple image-to-image translation network of <ref type="bibr" target="#b10">Isola et al. (2017)</ref> as a baseline (here called Pix2pix). We then propose two new cGAN architectures that generate images as well as segmentation maps in the target view. Augmenting semantic segmentation generation to the architectures helps improve the quality of generated images. The first architecture, called X-Fork, is a slight modification of the baseline, forking at the penultimate block to generate two outputs, target view image and segmentation map. The second architecture, called X-Seq, has a sequence of two baseline networks connected. The target view image generated by the first network is fed to the second to generate its corresponding segmentation map. Once trained, both architectures are able to generate better images than the baseline that generates only the target view images. This implies that learning to generate segmentation map along with the image indeed improves the quality of generated images. We also use homography to transform the aerial image to ground view and feed the transformed image to these networks to further improve the results. Finally, we propose a method to preserve details from the homography transformed image in a controlled setting to generate the street view images. It constitutes two subtasks: a) generating missing regions by inpainting, and b) adding realism by using GAN to preserves details visible in aerial view into the street view image. We call this approach H-Regions. Throughout the paper, H in a method's name indicates that the input is the homography transformed image. <ref type="bibr" target="#b43">Zhai et al. (2017)</ref> explored the relationship between the cross-view images by learning to predict the semantic layout of the ground image from its corresponding aerial image. They used the predicted layout to synthesize ground-level panorama. Prior works relating the aerial and ground imageries have addressed problems such as cross-view co-localization <ref type="bibr" target="#b17">[Lin et al. (2013)</ref>; <ref type="bibr" target="#b39">Vo and Hays (2016)</ref>], ground-to-aerial geo-localization <ref type="bibr" target="#b16">[Lin et al. (2015)</ref>; MH and Lee (2018)] and geo-tagging the cross-view images <ref type="bibr" target="#b40">[Workman et al. (2015)</ref>]. Recently, the images generated by our cross-view image synthesis approach have been successfully used to bridge the domain gap between aerial and street-view images in geo-localization tasks <ref type="bibr" target="#b27">[Regmi and Shah (2019)</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Relating Aerial and Ground-level Images</head><p>Cross-view relations have also been studied between egocentric (first person) and exocentric (surveillance or third-person) domains for different purposes. Human re-identification by matching viewers in top-view and egocentric cameras have been tackled by establishing the correspondences between the views in <ref type="bibr" target="#b0">Ardeshir and Borji (2016)</ref>. <ref type="bibr" target="#b34">Soran et al. (2014)</ref> utilize the information from one egocentric camera and multiple exocentric cameras to solve the action recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning View Transformations</head><p>Existing works on viewpoint transformation have been conducted to synthesize novel views of the same objects <ref type="bibr" target="#b5">[Dosovitskiy et al. (2017)</ref>; <ref type="bibr" target="#b37">Tatarchenko et al. (2016)</ref>; <ref type="bibr" target="#b46">Zhou et al. (2016)</ref>]. <ref type="bibr" target="#b46">Zhou et al. (2016)</ref> proposed models that learn to copy the pixel information from the input view and utilize them to preserve the identity and structure of the objects to generate new views. <ref type="bibr" target="#b37">Tatarchenko et al. (2016)</ref> trained an encode-decoder network to obtain 3D models of cars and chairs which they later used to generate different views of an unseen car or chair. <ref type="bibr" target="#b5">Dosovitskiy et al. (2017)</ref> learned generative models by training on 3D renderings of cars, chairs and tables and synthesized intermediate views and objects by interpolating between views and models. <ref type="bibr" target="#b6">Goodfellow et al. (2014)</ref> are the pioneers of Generative Adversarial Networks that are very successful at generating sharp and unblurred images, much better compared to existing methods such as Restricted Boltzmann Machines <ref type="bibr" target="#b8">[Hinton et al. (2006)</ref>; <ref type="bibr" target="#b32">Smolensky (1986)</ref>] or deep Boltzmann Machines <ref type="bibr" target="#b28">[Salakhutdinov and Hinton (2009)</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">GAN and cGAN</head><p>Conditional GANs synthesize images conditioned on different parameters during both training and testing. Examples include conditioning on labels of MNIST to generate digits by <ref type="bibr" target="#b20">Mirza and Osindero (2014)</ref>, on image representations to translate an image between different representations by <ref type="bibr" target="#b10">Isola et al. (2017)</ref>, and generating panoramic ground-level scenes from aerial images of the same location by <ref type="bibr" target="#b43">Zhai et al. (2017)</ref>. <ref type="bibr" target="#b25">Reed et al. (2016)</ref> synthesize images conditioned on detailed textual descriptions of the objects in the scene, and <ref type="bibr" target="#b44">Zhang et al. (2017)</ref> improved on that by using a two-stage Stacked GAN. <ref type="bibr" target="#b11">Kim et al. (2017)</ref> utilized the GAN networks to learn the relation between images in two different domains such that these learned relations can be transferred between the domains. Similar work by <ref type="bibr" target="#b47">Zhu et al. (2017)</ref> learned mappings between unpaired images using cycle-consistency loss. They assume that a mapping from one domain to the other and back to the first should generate the original image. Both works exploited large unpaired datasets to learn the relation between domains and formulated the mapping task between images in different domains as a generation problem. <ref type="bibr" target="#b47">Zhu et al. (2017)</ref> compare their generation task with previous works on paired datasets by <ref type="bibr" target="#b10">Isola et al. (2017)</ref>. They conclude that the result with paired images is the upper-bound for their unpaired examples. <ref type="bibr" target="#b33">Song et al. (2017)</ref> propose geometry-guided adversarial networks to synthesize identity-preserving facial expressions. The facial geometry is used as a controlled input to guide the network to synthesize facial images with desired expressions. Similar work by <ref type="bibr" target="#b12">Kossaifi et al. (2017)</ref> improves the visual quality of synthesized images by enforcing a mechanism to control the shapes of the objects. They map the generator's output to a mean shape and implicitly enforce the geometry of the objects and also add skip connections to transfer priors to the generated objects. <ref type="bibr" target="#b24">Pathak et al. (2016)</ref> generated missing parts of images using networks trained jointly with adversarial and reconstruction losses and produced sharp and coherent images. <ref type="bibr" target="#b42">Yeh et al. (2017)</ref> tackle the problem of image inpainting by searching for the encoding of the corrupted image that is closest to another image in the latent space and passing it through the generator to reconstruct the image. The closeness is defined based on the weighted context loss of the corrupted image, and a prior loss that penalizes unrealistic images. <ref type="bibr" target="#b41">Yang et al. (2017)</ref> propose a multi-scale patch synthesis approach for high-resolution image inpainting by jointly optimizing on image content and texture constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Cross-Domain Transformations using GANs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Geometry-guided Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Image Inpainting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background on GANs</head><p>Th Generative Adversarial Network (GAN) proposed by <ref type="bibr" target="#b6">Goodfellow et al. (2014)</ref> consists of two adversarial networks: a generator and a discriminator that are trained simultaneously based on the min-max game theory. The generator G is optimized to map a d-dimensional noise vector (usually d=100) to an image (i.e., synthesizing) that is close to the true data distribution. The discriminator D, on the other hand, is optimized to accurately distinguish between the synthesized images coming from the generator and the real images coming from the true data distribution. The objective function of such a network is</p><formula xml:id="formula_0">min G max D L GAN (G, D) = E x ∼ p data (x) [logD(x)]+ E z ∼ p z (z) [log(1 − D(G(z)))],<label>(1)</label></formula><p>where x is real data sampled from data distribution p data and z is a d-dimensional noise vector sampled from a Gaussian distribution p z . Conditional GANs synthesize images looking into some auxiliary variable which may be labels <ref type="bibr" target="#b20">[Mirza and Osindero (2014)</ref>], text embeddings <ref type="bibr" target="#b44">[Zhang et al. (2017)</ref>; <ref type="bibr" target="#b25">Reed et al. (2016)</ref>] or images ; <ref type="bibr" target="#b47">Zhu et al. (2017)</ref>; <ref type="bibr" target="#b11">Kim et al. (2017)</ref>]. In conditional GANs, both the discriminator and the generator networks receive the conditioning variable represented by c in <ref type="figure">Eqn.</ref> (2). The generator uses this additional information during image synthesis while the discriminator makes its decision by looking at the pair of conditioning variable and the image it receives. Real pair input to the discriminator consists of true image from distribution and its corresponding label while the fake pair consists of the synthesized image and the label. For conditional GAN, the objective function is min G</p><formula xml:id="formula_1">max D L cGAN (G, D) = E x,c ∼ p data (x,c) [logD(x, c)] +E x ,c ∼ p data (x ,c) [log(1 − D(x , c))],<label>(2)</label></formula><p>where x = G(z, c) is the generated image. In addition to the GAN loss, previous works [e.g., <ref type="bibr" target="#b10">Isola et al. (2017)</ref>; <ref type="bibr" target="#b47">Zhu et al. (2017)</ref>; <ref type="bibr" target="#b24">Pathak et al. (2016)</ref>] have tried to minimize the L1 or L2 distances between real and generated image pairs. This step aids the generator to synthesize images very similar to the ground truth. Minimizing the L1 distance generates less blurred images than minimizing the L2 distance. That is, using the L1 distance increases image sharpness in generation tasks. Therefore, we use the L1 distance in our method. The expression to minimize the L1 distance is</p><formula xml:id="formula_2">min G L L1 (G) = E x,x ∼ p data (x,x ) [|| x − x || 1 ],<label>(3)</label></formula><p>The objective function for such conditional GAN network is the sum of Eqns. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>.</p><p>Considering the synthesis of the ground level imagery (I g ) from aerial image (I a ), the conditional GAN loss and L1 loss are represented as in Eqns. <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_4">(5)</ref>, respectively. For ground to aerial synthesis, the roles of I a and I g are reversed.</p><formula xml:id="formula_3">min G max D L cGAN (G, D) = E I g ,I a ∼ p data (I g ,I a ) [logD(I g , I a )] +E I a ,I g ∼ p data (I a ,I g ) [log(1 − D(I g , I a ))],<label>(4)</label></formula><formula xml:id="formula_4">min G L L1 (G) = E I g ,I g ∼ p data (I g ,I g ) [|| I g − I g || 1 ],<label>(5)</label></formula><p>where I g = G(I a ). The objective function for an image-to-image translation network is the sum of conditional GAN loss in Eqn.</p><p>(4) and L1 loss in Eqn. <ref type="formula" target="#formula_4">(5)</ref>, as represented in Eqn. <ref type="formula" target="#formula_5">(6)</ref>:</p><formula xml:id="formula_5">L network = λ 1 L cGAN (G, D) + λ 2 L L1 (G),<label>(6)</label></formula><p>where, λ 1 and λ 2 are the balancing factors between the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Framework</head><p>In this section, we discuss the baseline methods and the proposed architectures for the task of cross-view image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>The naive way to approach this task is to consider it as an image to image translation problem. We run the experiments in the following settings as our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Cross-view Image-to-Image Translation (X-Pix2pix)</head><p>For this, the generator is an encoder-decoder network that takes in an image in first view as input and learns to generate the image in the other view as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Cross-view Image-to-Image Translation with Stacked</head><p>Outputs (X-SO) The network takes an image in first view as input and generates a single output of 6 channels, the first 3 channels correspond to the RGB image and the next three channels represent the segmentation map. The L1 loss and adversarial loss are computed over six channels of output and the corresponding ground truth images. to baseline architecture except that G forks to synthesize image and segmentation map in target view, and b) X-Seq: a sequence of two cGANs, G1 synthesizes target view image that is used by G2 for segmentation map synthesis in corresponding view. In both architectures, I a and I g are real images in aerial and ground views, respectively. S g is the ground-truth segmentation map in street-view. I g and S g are synthesized image and segmentation map in ground view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proposed Methods 4.2.1. Crossview Fork (X-Fork)</head><p>Our first architecture, known as the Crossview Fork, is shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. The generator network is forked to synthesize two outputs of 3 channels each, the first output is the RGB image and the second output is the segmentation map both in target view. The fork-generator architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The first six blocks of decoder share the weights. This is because the image and segmentation map contain a lot of shared features. The number of kernels used in each layer (block) of the generator are shown below the blocks.</p><p>The inherent idea behind this architecture is multi-task learning by the generator network. When the generator is enforced to learn the semantic class of the pixels together with the image synthesis, this helps to improve the image synthesis task. The generated segmentation map serves as an auxiliary output. The objective function for this network is shown in Eqn. 7.</p><formula xml:id="formula_6">L X−Fork = λ 1 L cGAN (G, D) + λ 2 L L1 (G(I g , I g )) + λ 2 L L1 (G(S g , S g ))<label>(7)</label></formula><p>Note that the L 1 loss is minimized for images as well as segmentation maps whereas the adversarial loss is optimized for images only. This is because we only care about pixel accuracy for segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Crossview Sequential (X-Seq)</head><p>Our second architecture uses a sequence of two cGAN networks as shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. The first network performs crossview image synthesis and the generated image is fed to the second network as a conditioning input to synthesize its corresponding segmentation map. This two-stage end-to-end learning of image and segmentation map synthesis produces improved image quality compared to the network without second  stage. The joint objective function for X-Seq architecture is shown in Eqn. 8 below.</p><formula xml:id="formula_7">L X−S eq = L network (G 1 , D 1 ) + L network (G 2 , D 2 ) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Cross-view Pix2pix with Homography (H-Pix2pix)</head><p>Another approach that we take here is to feed the homography transformed image as an input to the translation network. Our hypothesis is that the majority of the scene from the first view is transformed into the second perspective using the homography and this should ease the synthesis task. The large missing regions in the transformed images are mostly related to sky and buildings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Cross-view Stacked Outputs with Homography (H-SO)</head><p>The network takes the homography transformed image as input and generates target view image and the segmentation map stacked together as a 6-channel output; first 3 channels for image and the next three channels for its segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">Cross-view Fork with Homography (H-Fork)</head><p>Here, we use the homography transformed image I ah as input to the network architecture proposed in subsection 4.2.1. The hypothesis behind this idea is that the use of transformed images as input should ease the cross-view synthesis task compared to synthesizing by feeding the aerial images I a directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6.">Crossview Sequence with Homography (H-Seq)</head><p>In this setup, we feed the homography transformed image I ah rather than the original input image I a as input to the X-Seq network of subsection 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.7.">Crossview Regions with Homography (H-Regions)</head><p>In this method, we attempt to preserve the structural details visible in the aerial view images and guide the network to transfer those details to the synthesized ground view images. For this, we use the homography transformed image as input to our method and solve the synthesis task in following subtasks:</p><p>Subtask I: The homography transformed image (I ah in <ref type="figure">Figure</ref> 3) has a large portion of missing region (R 1 ) in the image. Our first task is to fill in the missing region in the transformed images. We use an encoder-decoder network that takes I ah as input and generates only the upper half of the image (I g ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ia</head><p>Iah Ig' Ig R1 R2 <ref type="figure">Fig. 3</ref>: An aerial image I a shown in the left is first transformed to street-view perspective using homography (I ah ). The transformed image needs inpainting in upper box region (R 1 ) and further processing in car region (R 2 ). These two regions are generated and the region in between is copied from homography transformed image I ah to obtain I g . We further train I g to smooth the region boundaries and add realism to it. I g is the corresponding ground truth image.</p><p>Subtask II: The street-view images are recorded by a camera mounted on a car's dashboard. Therefore, all the street-view images contain a part of the car's hood around the lower central region (R 2 ) in them which can be seen in image I g in <ref type="figure">Figure  3</ref>. Note that, this scenario is present in SVA dataset and can be avoided in real applications where the dash camera can be mounted such that the hood of the car is not captured in the frames. The homography transformed image I ah also has a car around that region which has been transformed from the aerial view of the car but it does not realistically represent a car in street-view. To address this, we mask a probable car-region and train a small network dedicated to learn mapping of the car region. This helps generate a realistic car region in ground view images (See region R 2 in image I g ).</p><p>Once we have images generated from the first two tasks, we copy them to their respective spatial locations in homography transformed image I ah creating a street-view image I g and preserving the pixels of remaining regions. Copying pixels in this manner helps us preserve the structural information that has been transformed using homography. The problem with this approach is that images do not look realistic at region boundaries. So, we further train another network to add realism to this image.</p><p>Subtask III: GANs are successful at generating images that look very realistic to human eyes. Here, we train a conditional GAN architecture on I g . For this, we first define bands around the region boundaries as shown in <ref type="figure">Figure 3</ref>. We formulate the loss function to preserve (by copying) the pixel information outside the bands to the output image while at the same time adding realism to the whole image. This step helps a lot to improve the visual quality of the synthesized image.</p><p>We now define our loss functions for the subtasks. For subtask one, we use conditional GAN network to inpaint missing regions in I ah by optimizing the network on adversarial and L 1 losses for the missing regions only. For subtask two, we only consider region R 2 by masking out the remaining regions in input and output images and optimizing for adversarial and L 1 losses for car region only. Once we have results from the above two subtasks, we compute I g as shown in Eqn. 9.</p><formula xml:id="formula_8">I g = I Inpaint M 1 + I car M 2 + I ah (M -M 1 -M 2 )<label>(9)</label></formula><p>Here, I Inpaint is the image generated from the inpainting network of subtask one. I car is the car image generated in subtask two. M 1 and M 2 are 3-channel binary masks for regions R 1 and R 2 , M being the 3-channel all ones image. The masks M 1 and M 2 are manually computed looking at the homography transformed image (I ah ) in <ref type="figure">Figure 3</ref>. This was done for a single frame only and worked well for all the images in the dataset. If the hood of the car was not visible in the street-view image, we wouldn't even need region R2 and correspondingly mask M 2 . I g is fed to the realism network to generate the final image in the target view. is the element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>For the experiments in this work, we use three datasets described in the following. Dayton. This cross-view image dataset is provided by <ref type="bibr" target="#b39">Vo and Hays (2016)</ref>. It consists of more than 1M pairs of street-view and overhead view images collected from 11 different US cities. We select 76,048 image pairs from Dayton images and create a train/test split of 55,000/21,048 pairs. We call it Dayton Dataset. The images in the original dataset have resolution of 354×354. We resize them to 256×256. We use this dataset for experiments in both aerial to ground a2g and ground to aerial g2a directions. CVUSA. We recruit this dataset <ref type="bibr" target="#b40">[Workman et al. (2015)</ref>] for direct comparison of our work with <ref type="bibr" target="#b43">Zhai et al. (2017)</ref>. It consists of 35,532/8,884 train/test split of image pairs. Following <ref type="bibr" target="#b43">Zhai et al. (2017)</ref>, the aerial images are center-cropped to 224 × 224 and then resized to 256 × 256. We only generate a single camera-angle image rather than the panorama. To do so, we take the first quarter of the ground level images as well as segmentations from the dataset and resize them to 256 × 256 in our experiments. SVA. The Surround Vehicle Awareness (SVA) dataset <ref type="bibr" target="#b22">[Palazzi et al. (2017)</ref>] is a synthetic dataset collected from Grand Theft Auto V (GTAV) video game. The game camera is toggled between frontal and bird's eye view to simultaneously capture images in the two views at each game time step. We use the train/test split as provided in the dataset. The original dataset has 100 sets of training set images and 50 sets of test set images. The consecutive frames in each set are very similar to each other, so we use every tenth frame to remove redundancy in the dataset. Finally, we have a training set of 46,030 image pairs and a test set of 22,254 image pairs. The images are resized to 256 × 256 for experiments in this work. Sample images from SVA dataset are shown in the leftmost and rightmost columns of <ref type="figure">Figure 7</ref>. We use this dataset for experiments in aerial to ground a2g direction only. Note that, homography related experiments are performed on this dataset only.</p><p>The proposed Fork and Sequence networks learn to generate the target view images and segmentation maps conditioned on source view image or their homography transformed image. Training procedure requires the images as well as their semantic segmentation maps. The CVUSA dataset has annotated segmentation maps for ground view images, but for SVA and Dayton datasets such information is not available. To compensate, we use one of the leading semantic segmentation methods, known as the RefineNet ]. This network is pre-trained on outdoor scenes of the Cityscapes dataset <ref type="bibr" target="#b4">[Cordts et al. (2016)</ref>] and is used to generate the segmentation maps that are utilized as ground truth maps. These semantic maps have pixel labels from 20 classes (e.g., road, sidewalk, building, vegetation, sky, void, etc). <ref type="figure" target="#fig_2">Figure 4</ref> shows image pairs from the Dayton dataset and their segmentation masks overlaid in both views. As can be seen, the segmentation mask (label) generation process is not perfect since it is unable to segment parts of buildings, roads, cars, etcetera in images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We use homography as a preprocessing step to transform the visual features from aerial images to ground perspective. Since the locations of aerial and ground view camera are fixed in the SVA dataset, first we randomly pick a pair of images from the dataset. We then manually select four points in the aerial image and find their corresponding locations in the ground-view image and use them to compute the homography matrix that transforms the aerial image to ground view and vice versa. Surprisingly, this method works well and avoids expensive computations for homography estimation usually done for each pair of images separately by computing SIFT features of the images and then finding the keypoints in the images. We also tried computing the SIFT features and then finding keypoints in two images and transforming images based on those points. This method could not find the corresponding points in two views, most likely because of very large perspective variation between the views.</p><p>We use the conditional GAN architecture very similar to <ref type="bibr" target="#b10">Isola et al. (2017)</ref> as the base architecture. The generator is an encoder-decoder network with blocks of Convolution, Batch Normalization <ref type="bibr" target="#b9">[Ioffe and Szegedy (2015)</ref>] and activation layers. Leaky ReLU with a slope of 0.2 is used as the activation function in the encoder, whereas the decoder has ReLU activation except for its final layer where Tanh is used. The first three blocks of the decoder have a Dropout layer in between Batch normalization and activation layer, with dropout rate of 50%. The discriminator is taken as it is from the <ref type="bibr" target="#b10">Isola et al. (2017)</ref>. For the experiments on the SVA dataset, we removed two blocks of CBL and UBDL from the generator architecture, primarily to save training time. We observed that removal of these blocks did not have much impact on the quality of synthesized images. Also, note that the synthesized semantic maps are 3-channel RGB images which effectively mitigated the class imbalance among the semantic classes. This was primarily done to consider all 20 semantic classes during the training and reduce bias towards dominant classes like houses, trees, road and sky. Had the semantic classes been limited to the dominant ones only, it would regularize the synthesized images to not learn the less prevalent objects in the target view images. Also, we brought in some confidence by the success of the pix2pix in synthesizing 3-channel segmentation maps from RGB images.</p><p>The convolutional kernels used in the networks are 4×4 in size with a stride of 2. The upconvolution in the decoder is Torch <ref type="bibr" target="#b3">[Collobert et al. (2011)</ref>] implementation of S patialFullConvolution, and upsamples the input by 2. The convolutional operation downsamples the images by 2. No pooling operation is used in the networks. The λ 1 and λ 2 used in the objective function for different networks are the balancing factors between the GAN loss and the L1 loss. We fix λ 1 at 1 and λ 2 at 100 for Fork and Seq models. For realism task in the H-Regions method, λ 1 =5 for adversarial loss and λ 2 =2 for pixel-wise loss worked the best. Following the idea to smooth the labels by <ref type="bibr" target="#b35">Szegedy et al. (2016)</ref> and demonstration of its effectiveness by <ref type="bibr" target="#b29">Salimans et al. (2016)</ref>, we use onesided label smoothing to stabilize the training process, replacing 1 by 0.9 for real labels. During the training, we utilized different data augmentation methods such as random jitter and horizontal flipping of images. The network is trained end-toend with weights initialized using a random Gaussian distribution with zero mean and 0.02 standard deviation. Our methods are implemented in Torch <ref type="bibr" target="#b3">[Collobert et al. (2011)</ref>]. Our code and data is available online at: https://github.com/kregmi/crossview-image-synthesis.git.</p><p>We train the networks for 100 epochs on low resolution and 35 epochs on high resolution images of Dayton dataset and for 20 epochs on SVA dataset. Experiments are conducted on CVUSA dataset for comparison with the work of <ref type="bibr" target="#b43">Zhai et al. (2017)</ref>. Following their setup, we train our architectures for 30 epochs using the Adam optimizer and moment parameters β1 = 0.5 and β2 = 0.999. We observed that the training for X-SO method on low resolution of Dayton dataset and CVUSA dataset was very unstable; so the qualitative and quantitative results are not very good for them. For H-Regions, we conduct experiments as follows. For subtask I, we train the network for 20 epochs. For subtask II, we train another network for one epoch only since the network needs to learn the mapping of the car and to preserve its color from source view to the target view. Eventually, we train a final realism network for 5 epochs (subtask III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation and Model Comparison</head><p>We have conducted experiments in a2g (aerial-to-ground) and g2a (ground-to-aerial) directions on the Dayton dataset and a2g direction only on CVUSA and SVA datasets. We consider image resolutions of 64×64 and 256×256 on the Dayton dataset while for experiments on CVUSA and SVA datasets, 256×256 resolution images are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation measures</head><p>It is not straightforward to evaluate the quality of synthesized images <ref type="bibr" target="#b1">[Borji (2018)</ref>]. In fact, evaluation of GAN methods continues to be an open problem <ref type="bibr" target="#b38">[Theis et al. (2016)</ref>]. Here we utilize four quantitative measures and one qualitative measure to evaluate our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Qualitative measure</head><p>For the subjective evaluation of different methods, we run a user study on the images synthesized using these methods. We show an aerial image along with the corresponding images in ground view synthesized using seven different methods to 10 users. We specifically ask each user to select the most realistic image that also contains the most visual details from the aerial view image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Quantitative measures</head><p>Inception Score: A common quantitative measure used in GAN evaluation is the Inception Score proposed by <ref type="bibr" target="#b29">Salimans et al. (2016)</ref>. The core idea behind the inception score is to assess how diverse the generated samples are within a class while being meaningfully representative of the class at the same time. One major criticism regarding the Inception score is that the CNN trained on ImageNet objects may not be adequate for other scene datasets (our case). To address this, here we use the AlexNet model <ref type="bibr" target="#b13">[Krizhevsky et al. (2017)</ref>] trained on Places dataset ] with 365 categories to compute the inception score. The Places dataset has images similar to those in our datasets.</p><p>We observe that the confidence scores predicted by the pretrained model on our datasets are dispersed between classes for many samples and not all the categories are represented by the images. Therefore, we compute inception scores on Top-1 and Top-5 classes, where "Top-k" means that top k predictions for each image are unchanged while the remaining predictions are smoothed by an epsilon equal to (1 -(top-k predictions))/(n-k classes). Accuracy: In addition to inception score, we compute the topk prediction accuracy between real and generated images. We use the same pre-trained Alexnet model to obtain annotations for real images and class predictions for generated images. We compute top-1 and top-5 accuracies. For each setting, accuracies are computed in two ways: 1) considering all images, and 2) considering real images whose top-1 (highest) prediction is greater than 0.5. Below each accuracy heading, the first column considers all images whereas the second column computes accuracies the second way. KL(model data): We compute the KL divergence between the model generated images and the real data distribution for quantitative analysis of our work, similar to some generative works <ref type="bibr" target="#b2">[Che et al. (2016)</ref>; <ref type="bibr" target="#b21">Nguyen et al. (2017)</ref>]. We again use the same pre-trained Alexnet described earlier. The lower KL score implies that the generated samples are closer to the real data distribution. SSIM, PSNR and Sharpness Difference: We employ three measures from the image quality assessment literature to evaluate our models, similar to <ref type="bibr" target="#b18">Mathieu et al. (2016)</ref>; <ref type="bibr" target="#b14">Ledig et al. (2017)</ref>; <ref type="bibr" target="#b30">Shi et al. (2016)</ref>; <ref type="bibr" target="#b23">Park et al. (2017)</ref>. Structural-Similarity (SSIM) measures the similarity between the images based on their luminance, contrast and structural aspects. SSIM value ranges between -1 and +1. Peak Signal-to-Noise Ratio (PSNR) measures the peak signal-to-noise ratio between two images to assess the quality of a transformed (generated) image compared to its original version. Sharpness difference (SD) measures the loss of sharpness during image generation. For each of these score, the higher the the better. Please refer to <ref type="bibr" target="#b26">Regmi and Borji (2018)</ref> for details on how to compute these scores. FID Score: An alternative metric to evaluate the quality of generated images is by computing Frechet Inception Distance <ref type="bibr" target="#b7">[Heusel et al. (2017)</ref>] between the generated samples and the real images. We use the same AlexNet model (as above) pretrained on the Places Dataset to compute the FID score. The lower value of FID score for a method, the better. The FID scores that we obtain in this work are relatively larger than numbers reported in other works mainly because of the variations in the image statistics of the Places Dataset used during the training and our test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Model Comparison</head><p>We conduct the qualitative and quantitative evaluation on 3 datasets. We report homography results on SVA dataset only. This is because the aerial view image for SVA dataset contains high overlap with the field of view of street-view image and thus application of homoraphy to preserve the details from aerial image seemed valid for this dataset compared to the other two.</p><p>We conduct the user studies to compare the images synthesized using different methods as well as illustrate qualitative results in <ref type="figure">Figures 5, 6</ref> and 7 and conduct an in-depth quantitative evaluation on test images of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Qualitative Comparison</head><p>Dayton dataset. For 64×64 resolution experiments, the networks are modified by removing the last two blocks of CBL from discriminator and encoder, and the first two blocks of UBDR from decoder of the generator. We run experiments on all three methods. Qualitative results are depicted in <ref type="figure">Figure  5</ref> (left). The results affirm that the networks have learned to transfer the image representations across the views. Generated ground level images clearly show details about road, trees, sky, clouds, and pedestrian lanes. Trees, grass, road, house roofs are well rendered in the synthesized aerial images.</p><p>For 256×256 resolution synthesis, we conduct experiments on all three architectures and illustrate the qualitative results in <ref type="figure">Figure 5</ref> (right). We observe that the images generated in high resolution contain more details of objects in both views and are less granulated than those in low resolution. Houses, trees, pedestrian lanes, and roads look more natural. CVUSA dataset. Test results on the CVUSA dataset show that images generated by the proposed methods are visually better compared to <ref type="bibr" target="#b43">Zhai et al. (2017)</ref> and <ref type="bibr" target="#b10">Isola et al. (2017)</ref>. Our proposed methods are more successful in transforming the pixel classes and thus generating correct class objects in target view compared to the baselines. SVA dataset. The qualitative results on the SVA dataset for aerial to street-view synthesis is shown in <ref type="figure">Figure 7</ref>. The proposed methods are capable at generating roads, car-hood, markers on road, sky and other details in the images. We observe that the images generated by the proposed H-Regions method contain more details around the central regions. This is due to enforcing the network to preserve those details from the aerial images. User Study. We conducted the perceptual test over 100 test images on 10 subjects to compare the images synthesized by a2g a2g synthesis g2a</p><p>Pix2pix X-SO X-Fork X-Seq Pix2pix X-SO X-Fork X-Seq g2a synthesis a2g a2g synthesis g2a</p><p>Pix2pix X-SO X-Fork X-Seq Pix2pix X-SO X-Fork X-Seq g2a synthesis  different methods on the SVA dataset. The results are presented in <ref type="table" target="#tab_1">Table 1</ref>. The most preferred method is H-Regions, closely contested by H-Seq and X-Seq methods. The results illustrate the following: a) The use of homography transformed input drastically outperforms corresponding experiments with untransformed aerial image as input, and b) Users preferred the images synthesized using H-Regions because of the method's ability to preserve the pixel information onto the target view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Quantitative Comparison</head><p>We report the quantitative results on three datasets next. Dayton dataset. The quantitative scores over different measures are provided in Tables 2, 3, 4, 5 and 6 under the columns Dayton (64 x 64) and Dayton (256 x 256) for low and high resolution synthesis.</p><p>Inception Score: The scores for X-Fork generated images are closest to that of real data distribution for Dayton dataset in low resolution in both directions and also in high resolution in a2g direction. The X-Seq method works best for g2a synthesis in high resolution for Dayton dataset. Inception scores on top-k classes follow a similar pattern as in all classes (except for Top-1 class on low resolution and Top-5 class on high resolution in g2a direction over Dayton dataset).</p><p>Accuracy: Results are shown in <ref type="table" target="#tab_4">Table 3</ref>. We observe that the X-Fork method works better at low resolution whereas X-Seq is better at high resolution synthesis in both directions.</p><p>KL(model data): The scores are provided in <ref type="table" target="#tab_5">Table 4</ref>. As it can be seen, our proposed methods generate much better results than the baselines. X-Fork generates images very similar to real distribution in all experiments except on the high resolution a2g experiment where X-Seq is slightly better than X-Fork.</p><p>FID Score: The FID scores are presented in <ref type="table" target="#tab_6">Table 5</ref>. X-Fork performs the best on lower resolution images while X-Seq works the best on higher resolution images in both directions a2g and g2a.</p><p>SSIM, PSNR, and SD: The scores are reported in <ref type="table" target="#tab_7">Table 6</ref>. The X-Seq model works the best in a2g direction while X-Fork outperforms the rest in the g2a direction. CVUSA dataset. The quantitative evaluation is shown in Tables 2, 3, 4 and 6 under the column CVUSA.</p><p>Inception Score: The X-Seq method works best for CVUSA dataset in terms of inception score.</p><p>Accuracy: Results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Images generated with X-Fork method obtain the best accuracy closely followed by X-Seq method.</p><p>KL(model data): The scores are provided in <ref type="table" target="#tab_5">Table 4</ref>. As it can be seen, our proposed methods generate much better results than the baseline. X-Fork generates images very similar to real distribution and X-Seq very close to it.</p><p>FID Score: X-Seq works the best on the CVUSA dataset closely followed by X-Fork network. See <ref type="table" target="#tab_6">Table 5</ref>. SSIM, PSNR, and SD: Images generated using X-Fork are better than other methods in terms of SSIM, PSNR and SD. We find that X-Fork improves over Zhai et al. by 5.03% in SSIM, 8.93% in PSNR, and 12.35% in SD. SVA dataset. The quantitative evaluation on SVA dataset is illustrated in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>Inception Score: H-Regions generates images that have the inception score closest to that of real data.</p><p>Accuracy: H-Seq method performs best in terms of accuracy. KL(model data): Images synthesized using X-Seq method have the closest distribution to the ground truth distribution among all the methods.</p><p>FID Score: The H-Regions performs the best in terms of FID score in SVA dataset. The H-models perform better than their X-counterparts.  SSIM, PSNR, and SD: X-Seq achieves the highest numbers in terms of SSIM and PSNR whereas H-Regions has the best SD.</p><p>Also, H-Pix2pix already performs very good compared to X-Pix2pix because homography simplified the learning task by transforming the image to the target view. H-methods outperform their X-counterparts for most of the evaluation metrics.</p><p>Because there is no consensus in evaluation of GANs, we had to use several scores. <ref type="bibr" target="#b38">Theis et al. (2016)</ref> show that these scores often do not agree with each other and this was observed in our evaluations as well. Nonetheless, we find that the proposed methods are consistently superior to the baselines in terms of quantitative and qualitative evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Conclusion</head><p>We explored image generation using conditional GANs between two drastically different views. Generating semantic segmentations together with images in the target view helps the networks learn better images compared to the baselines. Using homography to guide the cross-view synthesis allows preserving the overlapping regions between the views. Extensive qualitative and quantitative evaluations testify the effectiveness of our methods. Future research can explore other cues such as edge maps to facilitate synthesis tasks. Also, efforts can be put towards automatically finding regions that we defined manually. The challenging nature of the problem leaves room for further improvements. Potential application of this work can be in bridging the big domain-gap between street-view and aerial imagery for cross-view image geo-localization, multi-view object synthesis and so on.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our proposed network architectures. a) X-Fork: Similar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Generator of X-Fork architecture inFigure 1a. BN means batch-normalization layer. The first six blocks of decoder share weights, forking at the penultimate block. The number of channels in each convolution layer is shown below each blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>RGB image pairs from train set (left), segmentation masks from pre-trained RefineNet] overlaid on them (middle) and segmentation masks generated by X-Fork method overlaid on them (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Example images generated by different methods in low (64 × 64) resolution (left) and high (256 × 256) resolution (right) in a2g and g2a directions on the Dayton dataset. Qualitative results of our methods and baselines on CVUSA dataset in a2g direction. First two columns show true image pairs, next four columns show images generated by<ref type="bibr" target="#b43">Zhai et al. (2017)</ref>, X-Pix2pix], X-SO, X-Fork and X-Seq methods respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="7">: % of user preferences over images synthesized by different</cell></row><row><cell cols="5">methods (over 100 images from the SVA dataset).</cell><cell></cell><cell></cell></row><row><cell cols="7">X-Pix2pix X-Fork X-Seq H-Pix2pix H-Fork H-Seq H-Reg.</cell></row><row><cell>4</cell><cell>7.8</cell><cell>16.8</cell><cell>12.6</cell><cell>14.2</cell><cell>18.2</cell><cell>26.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Example images generated by different methods in a2g direction for SVA dataset.</figDesc><table><row><cell>Aerial Homography X-Pix2pix</cell><cell>X-SO</cell><cell>X-Fork</cell><cell>X-Seq</cell><cell>H-Pix2pix</cell><cell>H-SO</cell><cell>H-Fork</cell><cell>H-Seq H-Regions</cell><cell>GT</cell></row><row><cell>Fig. 7:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Inception Scores of models over Dayton and CVUSA datasets.</figDesc><table><row><cell>Dir.</cell><cell>Methods</cell><cell></cell><cell>Dayton (64 × 64)</cell><cell></cell><cell cols="3">Dayton (256 × 256)</cell><cell></cell><cell>CVUSA</cell><cell></cell></row><row><cell></cell><cell></cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell></cell><cell></cell><cell>classes</cell><cell>class</cell><cell>classes</cell><cell>classes</cell><cell>class</cell><cell>classes</cell><cell>classes</cell><cell>class</cell><cell>classes</cell></row><row><cell></cell><cell>Zhai et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.8434</cell><cell>1.5171</cell><cell>1.8666</cell></row><row><cell></cell><cell>X-Pix2pix</cell><cell>1.8029</cell><cell>1.5014</cell><cell>1.9300</cell><cell>2.8515</cell><cell>1.9342</cell><cell>2.9083</cell><cell>3.2771</cell><cell>2.2219</cell><cell>3.4312</cell></row><row><cell>a2g</cell><cell>X-SO</cell><cell>1.8577</cell><cell>1.4975</cell><cell>2.0301</cell><cell>2.9459</cell><cell>2.0963</cell><cell>2.9980</cell><cell>1.7575</cell><cell>1.4145</cell><cell>1.7791</cell></row><row><cell></cell><cell>X-Fork</cell><cell>1.9600</cell><cell>1.5908</cell><cell>2.0348</cell><cell>3.0720</cell><cell>2.2402</cell><cell>3.0932</cell><cell>3.4432</cell><cell>2.5447</cell><cell>3.5567</cell></row><row><cell></cell><cell>X-Seq</cell><cell>1.8503</cell><cell>1.4850</cell><cell>1.9623</cell><cell>2.7384</cell><cell>2.1304</cell><cell>2.7674</cell><cell>3.8151</cell><cell>2.6738</cell><cell>4.0077</cell></row><row><cell></cell><cell>Real Data</cell><cell>2.2096</cell><cell>1.6961</cell><cell>2.3008</cell><cell>3.7090</cell><cell>2.5590</cell><cell>3.7900</cell><cell>4.9971</cell><cell>3.4122</cell><cell>5.1150</cell></row><row><cell></cell><cell>X-Pix2pix</cell><cell>1.7970</cell><cell>1.3029</cell><cell>1.6101</cell><cell>3.5676</cell><cell>2.0325</cell><cell>2.8141</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-SO</cell><cell>1.4798</cell><cell>1.2163</cell><cell>1.5042</cell><cell>3.3397</cell><cell>2.0232</cell><cell>3.3485</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>g2a</cell><cell>X-Fork</cell><cell>1.8557</cell><cell>1.3162</cell><cell>1.6521</cell><cell>3.1342</cell><cell>1.8656</cell><cell>2.5599</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Seq</cell><cell>1.7854</cell><cell>1.3189</cell><cell>1.6219</cell><cell>3.5849</cell><cell>2.0489</cell><cell>2.8414</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Real Data</cell><cell>2.1408</cell><cell>1.4302</cell><cell>1.8606</cell><cell>3.8979</cell><cell>2.3146</cell><cell>3.1682</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracies: Top-1 and Top-5 on Dayton and CVUSA datasets.</figDesc><table><row><cell>Dir.</cell><cell>Methods</cell><cell></cell><cell cols="2">Dayton (64 × 64)</cell><cell></cell><cell></cell><cell cols="2">Dayton (256 × 256)</cell><cell></cell><cell></cell><cell cols="2">CVUSA</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Accuracy (%)</cell></row><row><cell></cell><cell>Zhai et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.97</cell><cell>14.03</cell><cell>42.09</cell><cell>52.29</cell></row><row><cell></cell><cell>X-Pix2pix</cell><cell>7.90</cell><cell>15.33</cell><cell>27.61</cell><cell>39.07</cell><cell>6.8</cell><cell>9.15</cell><cell>23.55</cell><cell>27.00</cell><cell>7.33</cell><cell>9.25</cell><cell>25.81</cell><cell>32.67</cell></row><row><cell cols="2">a2g X-SO</cell><cell>4.68</cell><cell>7.49</cell><cell>16.72</cell><cell>24.96</cell><cell>27.56</cell><cell>41.15</cell><cell>57.96</cell><cell>73.20</cell><cell>0.29</cell><cell>0.21</cell><cell>6.14</cell><cell>9.08</cell></row><row><cell></cell><cell>X-Fork</cell><cell>16.63</cell><cell>34.73</cell><cell>46.35</cell><cell>70.01</cell><cell>30.00</cell><cell>48.68</cell><cell>61.57</cell><cell>78.84</cell><cell>20.58</cell><cell>31.24</cell><cell>50.51</cell><cell>63.66</cell></row><row><cell></cell><cell>X-Seq</cell><cell>4.83</cell><cell>5.56</cell><cell>19.55</cell><cell>24.96</cell><cell>30.16</cell><cell>49.85</cell><cell>62.59</cell><cell>80.70</cell><cell>15.98</cell><cell>24.14</cell><cell>42.91</cell><cell>54.41</cell></row><row><cell></cell><cell>X-Pix2pix</cell><cell>1.65</cell><cell>2.24</cell><cell>7.49</cell><cell>12.68</cell><cell>10.23</cell><cell>16.02</cell><cell>30.90</cell><cell>40.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">g2a X-SO</cell><cell>0.10</cell><cell>0.00</cell><cell>0.98</cell><cell>0.28</cell><cell>8.52</cell><cell>12.57</cell><cell>27.35</cell><cell>32.76</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Fork</cell><cell>4.00</cell><cell>16.41</cell><cell>15.42</cell><cell>35.82</cell><cell>10.54</cell><cell>15.29</cell><cell>30.76</cell><cell>37.32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Seq</cell><cell>1.55</cell><cell>2.99</cell><cell>6.27</cell><cell>8.96</cell><cell>12.30</cell><cell>19.62</cell><cell>35.95</cell><cell>45.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>KL Divergence between model and data distributions onDayton and CVUSA datasets.</figDesc><table><row><cell>Dir. Method</cell><cell cols="3">Dayton(64×64) Dayton(256×256) CVUSA</cell></row><row><cell>Zhai et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>27.43 ± 1.63</cell></row><row><cell>X-Pix2pix</cell><cell>6.29 ± 0.8</cell><cell>38.26 ± 1.88</cell><cell>59.81 ± 2.12</cell></row><row><cell>a2g X-SO</cell><cell>4.99 ± 0.71</cell><cell>7.20 ± 1.37</cell><cell>414.25 ± 2.37</cell></row><row><cell>X-Fork</cell><cell>3.42 ± 0.72</cell><cell>6.00 ± 1.28</cell><cell>11.71 ± 1.55</cell></row><row><cell>X-Seq</cell><cell>6.22 ± 0.87</cell><cell>5.93 ± 1.32</cell><cell>15.52 ± 1.73</cell></row><row><cell>X-Pix2pix</cell><cell>6.39 ± 0.90</cell><cell>7.88 ± 1.24</cell><cell>-</cell></row><row><cell>X-SO</cell><cell>16.45 ± 0.9</cell><cell>9.15 ± 1.22</cell><cell>-</cell></row><row><cell>g2a X-Fork</cell><cell>4.45 ± 0.84</cell><cell>6.92 ± 1.15</cell><cell>-</cell></row><row><cell>X-Seq</cell><cell>7.20 ± 0.92</cell><cell>7.07 ± 1.19</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Frechet Inception Distance (FID) scores on Dayton and</figDesc><table><row><cell>CVUSA datasets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dir. Method</cell><cell cols="3">Dayton(64×64) Dayton(256×256) CVUSA</cell></row><row><cell>Zhai et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>571.32</cell></row><row><cell>X-Pix2pix</cell><cell>361.10</cell><cell>64.97</cell><cell>644.45</cell></row><row><cell>a2g X-SO</cell><cell>724.42</cell><cell>94.56</cell><cell>891.62</cell></row><row><cell>X-Fork</cell><cell>227.16</cell><cell>71.47</cell><cell>185.42</cell></row><row><cell>X-Seq</cell><cell>676.14</cell><cell>46.34</cell><cell>89.12</cell></row><row><cell>X-Pix2pix X-SO</cell><cell>323.54 412.38</cell><cell>71.35 108.14</cell><cell>--</cell></row><row><cell>g2a X-Fork</cell><cell>239.94</cell><cell>80.03</cell><cell>-</cell></row><row><cell>X-Seq</cell><cell>388.85</cell><cell>69.73</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>SSIM, PSNR and Sharpness Difference between real data and samples generated using different methods on Dayton and CVUSA</figDesc><table><row><cell>datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dir.</cell><cell>Methods</cell><cell></cell><cell cols="2">Dayton (64 × 64)</cell><cell cols="3">Dayton (256 ×256)</cell><cell></cell><cell>CVUSA</cell></row><row><cell></cell><cell></cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell><cell>SD ↓</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SD</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SD</cell></row><row><cell></cell><cell>Zhai et al. (2017)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.4147</cell><cell>17.4886</cell><cell>16.6184</cell></row><row><cell></cell><cell>X-Pix2pix</cell><cell>0.4808</cell><cell>19.4919</cell><cell>16.4489</cell><cell>0.4180</cell><cell>17.6291</cell><cell>19.2821</cell><cell>0.3923</cell><cell>17.6578</cell><cell>18.5239</cell></row><row><cell cols="2">a2g X-SO</cell><cell>0.4960</cell><cell>19.7442</cell><cell>16.6670</cell><cell>0.4772</cell><cell>19.6203</cell><cell>19.2939</cell><cell>0.3451</cell><cell>17.6201</cell><cell>16.9919</cell></row><row><cell></cell><cell>X-Fork</cell><cell>0.4921</cell><cell>19.6273</cell><cell>16.4928</cell><cell>0.4963</cell><cell>19.8928</cell><cell>19.4533</cell><cell>0.4356</cell><cell>19.0509</cell><cell>18.6706</cell></row><row><cell></cell><cell>X-Seq</cell><cell>0.5171</cell><cell>20.1049</cell><cell>16.6836</cell><cell>0.5031</cell><cell>20.2803</cell><cell>19.5258</cell><cell>0.4231</cell><cell>18.8067</cell><cell>18.4378</cell></row><row><cell></cell><cell>X-Pix2pix</cell><cell>0.3675</cell><cell>20.5135</cell><cell>14.7813</cell><cell>0.2693</cell><cell>20.2177</cell><cell>16.9477</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-SO</cell><cell>0.3254</cell><cell>16.5433</cell><cell>16.2329</cell><cell>0.2620</cell><cell>19.9827</cell><cell>16.8748</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">g2a X-Fork</cell><cell>0.3682</cell><cell>20.6933</cell><cell>14.7984</cell><cell>0.2763</cell><cell>20.5978</cell><cell>16.9962</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>X-Seq</cell><cell>0.3663</cell><cell>20.4239</cell><cell>14.7657</cell><cell>0.2725</cell><cell>20.2925</cell><cell>16.9285</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Quantitative evaluation of samples generated using different methods on SVA Dataset in a2g direction.</figDesc><table><row><cell>Methods</cell><cell>X-Pix2pix</cell><cell>X-SO</cell><cell>X-Fork</cell><cell>X-Seq</cell><cell>H-Pix2pix</cell><cell>H-SO</cell><cell>H-Fork</cell><cell>H-Seq</cell><cell>H-Regions</cell></row><row><cell>*Inception Score, all ↑</cell><cell>2.0131</cell><cell>2.4951</cell><cell>2.1888</cell><cell>2.2232</cell><cell>2.1906</cell><cell>2.3202</cell><cell>2.3202</cell><cell>2.2394</cell><cell>2.6328</cell></row><row><cell cols="2">*Inception Score, Top-1 ↑ 1.7221</cell><cell>1.8940</cell><cell>1.9776</cell><cell>1.9842</cell><cell>1.9507</cell><cell>1.9410</cell><cell>1.9525</cell><cell>1.9892</cell><cell>2.0732</cell></row><row><cell cols="2">*Inception Score, Top-5 ↑ 2.2370</cell><cell>2.6634</cell><cell>2.3664</cell><cell>2.4344</cell><cell>2.4069</cell><cell>2.7340</cell><cell>2.3918</cell><cell>2.4385</cell><cell>2.8347</cell></row><row><cell>Accuracy (Top-1, all) ↑</cell><cell>8.5961</cell><cell>7.5146</cell><cell>17.3794</cell><cell>19.5056</cell><cell>18.0706</cell><cell>5.2444</cell><cell>18.0182</cell><cell>20.7391</cell><cell>15.4803</cell></row><row><cell>Accuracy (Top-1, 0.5) ↑</cell><cell>30.3288</cell><cell>30.9507</cell><cell>53.4725</cell><cell>57.1010</cell><cell>54.8068</cell><cell>26.4697</cell><cell>51.0756</cell><cell>57.5378</cell><cell>48.0767</cell></row><row><cell>Accuracy (Top-5, all) ↑</cell><cell>9.0260</cell><cell>10.3905</cell><cell>23.8315</cell><cell>25.8807</cell><cell>23.4400</cell><cell>5.2544</cell><cell>26.6746</cell><cell>28.5517</cell><cell>21.8225</cell></row><row><cell>Accuracy (Top-5, 0.5) ↑</cell><cell>29.9102</cell><cell>38.9822</cell><cell>63.5045</cell><cell>65.3005</cell><cell>62.3072</cell><cell>31.9527</cell><cell>62.8166</cell><cell>67.4649</cell><cell>56.8994</cell></row><row><cell>KL(model || data) ↓</cell><cell>19.5553</cell><cell>12.0906</cell><cell>4.1925</cell><cell>3.7585</cell><cell>4.2894</cell><cell>12.8761</cell><cell>4.7246</cell><cell>4.4260</cell><cell>6.0638</cell></row><row><cell>SSIM ↑</cell><cell>0.3206</cell><cell>0.4552</cell><cell>0.4235</cell><cell>0.4638</cell><cell>0.4327</cell><cell>0.4457</cell><cell>0.424</cell><cell>0.4249</cell><cell>0.4044</cell></row><row><cell>PSNR ↑</cell><cell>17.9944</cell><cell>21.5312</cell><cell>21.24</cell><cell>22.3411</cell><cell>21.686</cell><cell>21.7709</cell><cell>21.6327</cell><cell>21.4770</cell><cell>20.9848</cell></row><row><cell>SD ↓</cell><cell>17.0254</cell><cell>17.5285</cell><cell>16.9371</cell><cell>17.4138</cell><cell>16.9468</cell><cell>17.3876</cell><cell>16.8653</cell><cell>17.5616</cell><cell>17.6858</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ego2top: Matching viewers in egocentric and top-view videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ardeshir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46454-1_16</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-11" />
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03446</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1612.02136</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="692" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
		<idno>doi:10.1162/neco.2006. 18.7.1527</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.2006.18.7.1527" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045167" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to discover crossdomain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D., Teh, Y.W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, International Convention Centre</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00684</idno>
		<title level="m">Gagan: Geometry-aware generative adverserial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<idno>doi:10.1145/3065386</idno>
		<ptr target="http://doi.acm.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.19</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299135</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="5007" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cvm-net: Cross-view matching network for image-based ground-to-aerial geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H M F R</forename><surname>Mh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7258" to="7267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
		<ptr target="http://arxiv.org/abs/1411.1784" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to map vehicles into bird&apos;s eye view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing</title>
		<editor>Stanco, F.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transformationgrounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1703.02921</idno>
		<ptr target="http://arxiv.org/abs/1703.02921" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Balcan, M.F., Weinberger, K.Q.</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-view image synthesis using conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bridging the domain gap for ground-to-aerial image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11045</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">*Inception Score for real (ground truth) data is 3.0347, 2.3886 and 3.3446 for all, top-1 and top-5 setups respectively. using an efficient sub-pixel convolutional neural network</title>
		<idno type="DOI">10.1109/CVPR.2016.207</idno>
		<idno>doi:10.1109/CVPR.2016.207</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.207" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="194" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Geometry guided adversarial facial expression synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition in the presence of one egocentric and multiple static cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="178" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Cvpr</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">308</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Localizing and Orienting Street Views Using Overhead Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_30</idno>
		<idno>doi:10.1007/978-3-319-46448-0_30</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-46448-0_30" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="494" to="509" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Highresolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting groundlevel scene layout from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
