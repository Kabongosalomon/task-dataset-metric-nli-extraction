<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WEAKLY-SUPERVISED IMAGE SEMANTIC SEGMENTATION USING GRAPH CONVOLUTIONAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Yi</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Yang Ming Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-You</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Yang Ming Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Po</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Yang Ming Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Yang Ming Chiao Tung University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WEAKLY-SUPERVISED IMAGE SEMANTIC SEGMENTATION USING GRAPH CONVOLUTIONAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Weakly-supervised image semantic seg- mentation, Graph Convolutional Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses weakly-supervised image semantic segmentation based on image-level class labels. One common approach to this task is to propagate the activation scores of Class Activation Maps (CAMs) using a random-walk mechanism in order to arrive at complete pseudo labels for training a semantic segmentation network in a fully-supervised manner. However, the feed-forward nature of the random walk imposes no regularization on the quality of the resulting complete pseudo labels. To overcome this issue, we propose a Graph Convolutional Network (GCN)-based feature propagation framework. We formulate the generation of complete pseudo labels as a semi-supervised learning task and learn a 2-layer GCN separately for every training image by back-propagating a Laplacian and an entropy regularization loss. Experimental results on the PASCAL VOC 2012 dataset confirm the superiority of our scheme to several state-of-the-art baselines. Our code is available at https: //github.com/Xavier-Pan/WSGCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Image semantic segmentation aims for classifying pixels in an image into their semantic classes. Training a semantic segmentation network often requires costly pixel-wise class labels. To avoid the time-consuming annotation process, weakly-supervised learning is introduced to utilize relatively low-cost weak labels such as bounding boxes, scribbles, and image-level class labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Recent research has been focused on weakly-supervised image semantic segmentation based on image-level class labels. The work in <ref type="bibr" target="#b2">[3]</ref> is representative of the popular twostage framework. It begins with producing pseudo labels, followed by training a semantic segmentation network supervisedly using these labels. The generation of pseudo labels usually proceeds in three steps. The first step predicts crude * Both authors contributed equally to the paper This work is supported by National Center for High-performance Computing, Taiwan. estimates of labels, known as partial pseudo labels, by using Class Activation Maps (CAMs) <ref type="bibr" target="#b7">[8]</ref>. They are partial in that only some pixels will receive their pseudo labels. In the second step, these partial pseudo labels are utilized to train an affinity network, requiring that pixels sharing the same label should have similar features. Lastly, the affinity network is applied to evaluate a Markov transition matrix for propagating the activation scores of CAMs through a random-walk mechanism, with the aim of producing complete pseudo labels for all the pixels. On the basis of this two-stage framework, some works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> improve on initial CAMs while others <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> attempt to learn better affinity matrices.</p><p>This paper addresses the propagation mechanism through learning a graph neural network. To derive complete pseudo labels, all the prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> rely on the randomwalk mechanism to propagate the activation scores of CAMs as a form of label propagation. Due to its feed-forward nature, there is no regularization imposed on the resulting complete pseudo labels, the quality of which depends highly on the Markov matrix. Departing from label propagation, we propose a feature propagation framework based on learning a Graph Convolutional Network (GCN) <ref type="bibr" target="#b8">[9]</ref> for every training image. Our contributions include:</p><p>1. We propagate the high-level semantic features of image pixels on a graph using a 2-layer graph convolutional network, followed by decoding the propagated features into semantic predictions. 2. We cast the problem of obtaining complete pseudo labels from partial pseudo labels as a semi-supervised learning problem. 3. We learn a separate GCN for every training image by back-propagating a Laplacian loss and an entropy loss to ensure the consistency of semantic predictions with image spatial details.</p><p>Experimental results show that our complete pseudo labels have higher accuracy in Mean Intersection over Union (mIoU) than label propagation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The net effect is that the semantic segmentation network trained with our complete pseudo labels outperforms the state-of-the-art baselines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> on the PASCAL VOC 2012 dataset. Overview of the proposed 2-stage weakly-supervised image semantic segmentation framework with GCN-based feature propagation. In our implementation, the Affinity Network is from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>This section surveys prior works that use image-level class labels for weakly-supervised semantic segmentation, with a particular focus on the improvement of CAMs and how pseudo labels are derived from CAMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Class Activation Maps</head><p>The CAMs <ref type="bibr" target="#b7">[8]</ref> derived from image-level class labels often serve as initial seeds for generating partial pseudo labels. Many efforts have been devoted to improving the quality of CAMs. RRM <ref type="bibr" target="#b9">[10]</ref> learns jointly an image classifier and a semantic segmentation network, with the hope of regularizing the feature extraction towards better CAM generation. By the same token, SEAM <ref type="bibr" target="#b4">[5]</ref> imposes an equivariant constraint on learning the feature extractor, requiring that the CAMs of two input images related through an affine transformation follow the same affine relationship. SC-CAM <ref type="bibr" target="#b5">[6]</ref> divides the object category into sub-categories and learns a feature extractor that can identify find-grained parts of an object. GroupWSSS <ref type="bibr" target="#b11">[12]</ref> learns a graph neural network to leverage the semantic relations among images which share class labels partially for better CAM generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generation of Complete Pseudo Labels</head><p>There are approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> that focus on generating complete pseudo labels from CAMs. DSRG <ref type="bibr" target="#b1">[2]</ref> expands the partial pseudo labels by annotating iteratively the unlabeled pixels adjacent to the labeled ones through a semantic segmentation network. The propagation process, however, does not take into account the affinity between pixels. By contrast, PSA <ref type="bibr" target="#b2">[3]</ref> learns an affinity network to guide the propagation of the CAM scores by weighting differently the edges connecting adjacent pixels. The affinity network is trained by minimizing the l 1 norm between the feature vectors of neighboring pixels that share the same semantic class according to their partial pseudo labels. IRNet <ref type="bibr" target="#b3">[4]</ref> extends the idea to incorporate the boundary map in determining the affinity between pixels. Specifically, pixels separated by a strong boundary are considered semantically dissimilar, and vice versa. SEAM <ref type="bibr" target="#b4">[5]</ref> and SC-CAM <ref type="bibr" target="#b5">[6]</ref> adopt the same propagation framework for CAMs as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> to produce complete pseudo labels.</p><p>In common, most of these methods build the label propagation on a random-walk mechanism. The feed-forward nature of the random walk does not provide any guarantee on the quality of the complete pseudo labels and does not consider low-level features during propagation. Our work presents the first attempt at replacing the random walk with a GCN-based feature propagation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>This work introduces a GCN-based feature propagation scheme, with the aim of predicting the semantic label for every pixel to produce complete pseudo labels. Unlike the random walk, which relies solely on the affinity between pixels in the feature domain to propagate the activation scores of CAMs, our scheme learns a GCN by regularizing the feature propagation with not only the aforementioned affinity information but also the color information of the input image. Furthermore, recognizing that the generation of pseudo labels is an off-line process, we train a separate GCN to optimize feature propagation for every training image. We choose GCN instead of the convolutional neural networks because of the irregular affinity relations among feature samples. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the overall architecture of the proposed method. As shown, the process proceeds in two stages: (1) the generation of pseudo labels, and (2) the training of a semantic segmentation network. In the first stage, we follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> to generate partial pseudo labels P ∈ N H/S×W/S for an input image I ∈ R 3×H×W of width W and height H. Note that P is the same size as the CAMs with S denoting the downscaling factor. At the location (x, y), the pseudo label P (x, y) is assigned either a class label c ∈ C ∪ {c bg } or ignored, where C is the set of foreground classes, c bg denotes the background class, and the ignored signals the unlabeled state. Given the partial pseudo labels P , we consider the generation of complete pseudo labelsP ∈ N H×W to be a semi-supervised learning problem on a graph. The output of the first stage then comprises the complete pseudo labels for the image I, which are utilized in the second stage as groundtruth labels for training the semantic segmentation network. The following details the operation of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference of Complete Pseudo Labels on a Graph</head><p>We begin by defining the graph structure. Let G = (V, E) be a graph, where V = {v 1 , ..., v N } is a collection of nodes and E = {A i,j |v i , v j ∈ V} specify edges connecting nodes v i and v j with weights A i,j . In our task, a node v i ∈ R D in V refers collectively to the co-located feature samples at (x i , y i ) in D feature maps, each being of size H/S × W/S with the total number of nodes N = H/S × W/S. The choice of node features is detailed in Section 4.1. The pseudo label of v i is denoted as p i = P (x i , y i ). The edge weight A i,j measures the affinity between v i and v j . Because GCN is amenable to a wide choice of affinity measures, we test two different measures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> in our experiments (Section 4.1).</p><p>To generate complete pseudo labelsP , we perform feature propagation and inference on the graph G through a 2layer GCN. The feed-forward inference proceeds as follows:</p><formula xml:id="formula_0">Q = σ s (Ã(σ r (ÃV W 1 )W 2 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">V = [v 1 , v 2 , ..., v N ] T ∈ R N ×D is a matrix of node feature vectors, each of which is D-dimensional; W 1 ∈ R D×16 , W 2 ∈ R 16×(|C|+1)</formula><p>are the two learnable network parameters; σ r (·), σ s (·) correspond to the ReLU and the softmax activation functions, respectively; andÃ = A + I N is the sum of the affinity matrix A ∈ R N ×N and an identity matrix I N of size N . Note that the number of classes which include background class is denoted as |C| + 1. In the re-</p><formula xml:id="formula_2">sulting matrix Q = [q 1 , q 2 , ..., q N ] T ∈ R N ×(|C|+1) , each row q T i , i = 1, 2, .</formula><p>. . , N signals the probability distribution of semantic classes at pixel (x i , y i ) in the feature domain. These probability distributions are then interpolated spatialwise (using bilinear interpolation) to arrive at a full-resolution semantic prediction map, followed by applying the dCRF <ref type="bibr" target="#b12">[13]</ref> in a channel-wise manner and taking the maximum across channels at every pixel for complete pseudo labelsP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training a GCN for Feature Propagation</head><p>The training of a GCN for every image is formulated as a semi-supervised learning problem and incorporates four loss functions, namely the (1) foreground loss f g , (2) background loss bg , (3) entropy loss ent , and (4) Laplacian loss lp :</p><formula xml:id="formula_3">= f g + bg + β 1 ent + β 2 lp ,<label>(2)</label></formula><p>where β 1 and β 2 are hyper-parameters. The first two are evaluated as the sum of cross entropies over the foreground and background pixels in the feature domain, respectively, where the foreground pixels have their partial pseudo labels P (x, y) ∈ C while the background pixels have P (x, y) = c bg . The rationale behind the separation of the cross entropies into the foreground and background groups is to address the imbalance between these two classes of pixels. In symbols, we have</p><formula xml:id="formula_4">f g = − 1 |V f g | i∈V f g log (q i ) pi ,<label>(3)</label></formula><formula xml:id="formula_5">bg = − 1 |V bg | i∈V bg log (q i ) pi ,<label>(4)</label></formula><p>where q i from the Q in Eq. (1) denotes the predicted class distribution at pixel i; (q i ) pi refers specifically to the predicted probability of the class corresponding to the partial pseudo label p i ; V f g and V bg are the foreground and background pixels according to the partial pseudo labels P . For those pixels in the feature domain with their pseudo labels P (x, y) marked as ignored, i.e., unlabeled pixels, we impose the following entropy loss, requiring that the uncertainty about their class predictions should be minimized. In other words, it encourages the class predictions q i at those unlabeled pixels to approximate one-hot vectors.</p><formula xml:id="formula_6">ent = − 1 |V ig | i∈Vig c∈C (q i ) c log (q i ) c ,<label>(5)</label></formula><p>whereC = C ∪ {c bg } and V ig refers to unlabeled pixels. In addition, motivated by the observation that neighbouring pixels with similar color values usually share the same semantic class, we introduce a Laplacian loss to ensure the consistency of the class predictions with the image contents. This prior knowledge is incorporated into the training of the GCN in the form of the Laplacian loss:</p><formula xml:id="formula_7">lp = 1 2|V| i∈V j∈V Φ i,j q i − q j 2 2 ,<label>(6)</label></formula><p>which aims to minimize the discrepancy (measured in l 2 norm) between the class prediction of pixel i and those of its surrounding pixels j in a neighborhood N i according to the affinity weight Φ i,j that reflects the similarity between pixel i and pixel j in terms of their color values and locations as given by</p><formula xml:id="formula_8">Φ i,j =    exp(− ||Ī i −Ī j || 2 2σ 2 1 − ||f i − f j || 2 2σ 2 2 ) if j ∈ N i 0 otherwise ,<label>(7)</label></formula><p>whereĪ ∈ R 3×H/S×W/S is the bilinearly downscaled version of the input image I;Ī i and f i = (x i , y i ) refer to the color value at pixel (x i , y i ) and its x, y coordinates, respectively; σ 1 = √ 3, σ 2 = 10 are hyper-parameters; and N i is a 5 × 5 window centered at pixel (x i , y i ). Note that Φ, which relies on low-level color and spatial information to regularize the GCN output, is to be distinguished from the affinity matrix A in Eq. (1), which uses high-level semantic information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> to specify the graph structure of GCN for feature propagation as will be detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets and Metrics: Following most of the prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> we evaluate our method on the PAS-CAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b13">[14]</ref>. It includes images with pixel-wise class labels, of which 1,464, 1,449, and 1,456 are used for training, validation, and test, respectively. Following the common training strategy for semantic segmentation, we adopt augmented training images <ref type="bibr" target="#b14">[15]</ref>, which are composed of 10,582 images annotated by pixel-wise class labels. However, we use only image-level class labels for weak supervision during training. To evaluate the semantic segmentation accuracy, we resort to Mean Intersection over Union (mIoU) as the other baselines. Implementation and Training: There are two variants, termed WSGCN-I and WSGCN-P, of our model that use affinity matrices A, node features V and CAMs from IR-Net <ref type="bibr" target="#b3">[4]</ref> and PSA <ref type="bibr" target="#b2">[3]</ref>, respectively. The WSGCN-I constructs the affinity matrix A using the boundary detection network <ref type="bibr" target="#b3">[4]</ref> and takes as node features V the features that sit in the last layer of the boundary detection network and before its 1 × 1 convolutional layer. In contrast, the WSGCN-P follows Affin-ityNet <ref type="bibr" target="#b2">[3]</ref> in specifying the affinity matrix A and uses the semantic features for affinity evaluation as node features V. Both variants apply the same CAMs as their counterparts in IRNet <ref type="bibr" target="#b3">[4]</ref> and PSA <ref type="bibr" target="#b2">[3]</ref>. For every training image, we train both variants for 250 gradient update steps with a dropout rate of 0.3. The β 1 , β 2 in Eq. (2) are set to 10 and 10 −2 , respectively. The learning rate of the optimizer Adaptive Moment Estimation (Adam) is set to 0.01, with a weight decay of 5 × 10 −4 . For a fair comparison, we follow <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> to adopt DeepLabv2 <ref type="bibr" target="#b15">[16]</ref> as our semantic segmentation network in the second stage. The backbone ResNet-101 is pre-trained on ImageNet. Baselines: We compare our approach with several stateof-the-art weakly-supervised image semantic segmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> on training, validation, and test sets. The comparison with PSA <ref type="bibr" target="#b2">[3]</ref> and IRNet <ref type="bibr" target="#b3">[4]</ref> singles out the benefits of our feature propagation framework over label propagation since our GCN adopts the same affinity matrix as theirs for propagating features rather than activation Method mIoU (%) PSA <ref type="bibr" target="#b2">[3]</ref> 59.7 SC-CAM <ref type="bibr" target="#b5">[6]</ref> 63.4 SEAM <ref type="bibr" target="#b4">[5]</ref> 63.6 IRNet <ref type="bibr" target="#b3">[4]</ref> 66.5 SingleStage <ref type="bibr" target="#b6">[7]</ref> 66.9 WSGCN-P 64.0 WSGCN-I 68.0 scores of CAMs. <ref type="table" target="#tab_0">Table 1</ref> presents a quality comparison of complete pseudo labels between our WSGCN-I/P and several state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> in terms of mIoU on the PAS-CAL VOC 2012 training set. Most of these baselines propagate activation scores of CAMs with random walk (i.e., label propagation) to infer complete pseudo labels. We see that our WSGCN-I achieves the highest mIoU (68.0%) among all the competing methods. It shows a 1.5% mIoU improvement over IRNet <ref type="bibr" target="#b3">[4]</ref>. Likewise, WSGCN-P has a 4.3% mIoU gain as compared to PSA <ref type="bibr" target="#b2">[3]</ref>. These results confirm the superiority of our feature propagation framework over the label propagation adopted by these baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quality of Complete Pseudo Labels</head><p>Further insights can be gained by noting that in generating complete pseudo labels for every training image, our schemes rely on training a specific GCN by back-propagation. On the contrary, label propagation is a feed-forward process. During back-propagation, our schemes involve not only highlevel node features V in constructing an affinity matrix A for feature propagation but also low-level color/spatial information (cp. Φ in lp ) in regularizing label predictions, whereas the label propagation used by the baselines depends largely on high-level node features V to compute a Markov transition matrix. A side experiment shows that substituting the affinity matrix A in Eq. (1) for Φ in lp causes the mIoU of complete pseudo labels to decrease from 68% to 66.3%. This is in agreement with the general observation that highlevel semantic features usually contain less information about spatial details. Note also that the performance gap between WSGCN-I and WSGCN-P comes from the use of different affinity matrices A and node features V, emphasizing the influence of these design choices on the quality of complete pseudo labels. <ref type="table" target="#tab_1">Table 2</ref> further analyzes how the single use of various loss functions and their combinations along with dCRF contribute to the quality of complete pseudo labels. By default, the foreground and background losses are enabled. They alone CVPR19 ResNet-50 63.5 64.8 SEAM <ref type="bibr" target="#b4">[5]</ref> CVPR20 ResNet-38 64.5 65.7 SC-CAM † <ref type="bibr" target="#b5">[6]</ref> CVPR20 ResNet-101 66.1 65.9 RRM † <ref type="bibr" target="#b9">[10]</ref> AAAI20 ResNet-101 66.3 66.5 SingleStage † † <ref type="bibr" target="#b6">[7]</ref> CVPR20 ResNet-101 65.7 66.6 MCIS* <ref type="bibr" target="#b10">[11]</ref> ECCV20 ResNet-101 66. (WSGCN-I without any check mark) offer 62.0% mIoU. The entropy loss improves the mIoU further by 1.5%. The Laplacian loss attains even higher gain (2.4%) due to the incorporation of low-level color/spatial regularization. When combined, they show a synergy effect of 4.7%. Recall that the entropy loss encourages the GCN to produce a one-hot-like semantic prediction while the Laplacian loss requires the predictions to be close to each other for adjacent pixels with similar colors. It is interesting to note that dCRF (as a postprocessing step) can further improve on the entropy loss, the Laplacian loss as well as their combination. Although dCRF uses similar low-level information to the Laplacian loss, it functions as a separate, refinement step rather than a substitute for the Laplacian loss. Unlike dCRF, the Laplacian loss is included in the training loop of GCN. <ref type="figure" target="#fig_2">Fig. 2</ref> visualizes how these loss functions improve incrementally the quality of complete pseudo labels. We see that based on the semantic segmentation resulting from the foreground and background losses, the entropy loss tends to grow the foreground regions while the Laplacian loss can help alleviate semantic segmentation errors at object boundaries.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study of Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semantic Segmentation Performance</head><p>Using complete pseudo labels as supervision, we train a semantic segmentation network in the second stage. <ref type="table">Table 3</ref> compares the semantic segmentation accuracy of our 2-stage training framework with GCN-based feature propagation to several state-of-the-art methods. We see that our WSGCN-I outperforms all the baselines which do not use saliency maps on both validation and test datasets, with the ResNet-101 backbone pre-trained on ImageNet. It gains 0.5% (re- spectively, 2%) more mIoU on the test (respectively, validation) dataset, when pre-training the backbone on MS-COCO dataset as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="table" target="#tab_2">Table 4</ref> further compares WSGCN-P and WSGCN-I with their counterparts PSA <ref type="bibr" target="#b2">[3]</ref> and IRNet <ref type="bibr" target="#b3">[4]</ref>, respectively. Note that the main difference between WSGCN-P and WSGCN-I is the choice of the affinity matrix (Section 4.1). For a fair comparison, we choose DRN-D-105 <ref type="bibr" target="#b16">[17]</ref>, which is pretrained on ImageNet and is publicly accessible for reproducibility, as the backbone in DeepLabv2. It is seen that these variants show 0.9% to 2.5% mIoU improvements, confirming the superiority of our feature propagation framework to label propagation. The results also demonstrate that our scheme can work with different affinity matrices. <ref type="table" target="#tab_3">Table 5</ref> additionally compares our WSGCN-I and IR-Net <ref type="bibr" target="#b3">[4]</ref> with the ResNet-101 backbone. As shown, WSGCN-I achieves higher IoU in most categories than IRNet <ref type="bibr" target="#b3">[4]</ref> and has a 2.3% mIoU improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper presents a GCN-based feature propagation framework for weakly-supervised image semantic segmentation. Unlike feed-forward label propagation, our approach optimizes the generation of complete pseudo labels by learning off-line a separate GCN for every training image. The learning process involves feature propagation based on high-level semantic affinity between pixels and label prediction regularized by low-level spatial coherence. On our computation platform with 4 NVIDIA Tesla V100 GPUs and 1 Intel Xeon 3GHz CPU, the runtime needed to infer complete pseudo labels for one sample image is about 2.45s, as compared to 1.43s for label propagation with IRNet <ref type="bibr" target="#b3">[4]</ref>. Experimental results on PASCAL VOC 2012 benchmark validate the superiority of our framework to label propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed 2-stage weakly-supervised image semantic segmentation framework with GCN-based feature propagation. In our implementation, the Affinity Network is from [3, 4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 66.9 GroupWSSS* [12] AAAI21 ResNet-101 68.2 68.5 WSGCN-I (ours) -ResNet-101 66.7 68.8 WSGCN-I † (ours) -ResNet-101 68.7 69.3 *: Using saliency maps as extra supervision signals. †: Pre-training the backbone in DeepLabv2 on MS-COCO. † †: Using DeepLabv3+ as the semantic segmentation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Sample results of pseudo labels. From top to bottom: partial pseudo labels where white regions indicate unlabeled pixels, complete pseudo labels by f g + bg , complete pseudo labels by f g + bg + ent , complete pseudo labels by all four losses (WSGCN-I), complete pseudo labels by IRNet, and the ground-truths. All complete pseudo labels are refined with dCRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quality comparison of complete pseudo labels on PASCAL VOC 2012 training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Effect of various loss functions on the quality of complete pseudo labels evaluated on PASCAL VOC 2012 training set.</figDesc><table><row><cell>Method</cell><cell cols="2">Entropy Laplacian loss loss</cell><cell cols="2">dCRF (post-processing)</cell><cell>mIoU (%)</cell></row><row><cell>WSGCN-I WSGCN-I WSGCN-I WSGCN-I WSGCN-I WSGCN-I WSGCN-I</cell><cell>√ √ √ √</cell><cell>√ √ √ √</cell><cell>√ √ √</cell><cell></cell><cell>62.0 63.5 64.4 66.7 66.0 66.1 68.0</cell></row><row><cell cols="6">Table 3. Comparison of semantic segmentation accuracy</cell></row><row><cell cols="4">evaluated on PASCAL VOC 2012 dataset.</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Publication Backbone</cell><cell cols="2">mIoU (%) val test</cell></row><row><cell>PSA [3]</cell><cell></cell><cell cols="4">CVPR18 ResNet-38 61.7 63.7</cell></row><row><cell>IRNet [4]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison of PSA, IRNet, and our method with DRN-D-105 backbone.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU (%) val test</cell></row><row><cell>PSA [3]</cell><cell cols="2">DRN-D-105 60.9 61.9</cell></row><row><cell cols="3">WSGCN-P (ours) DRN-D-105 63.1 64.4</cell></row><row><cell>IRNet [4]</cell><cell cols="2">DRN-D-105 66.0 66.5</cell></row><row><cell cols="3">WSGCN-I (ours) DRN-D-105 66.9 67.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison of WSGCN-I and IRNet with the ResNet-101 backbone on the PASCAL VOC 2012 test set. Method bkg aero bike bird boat bot. bus car cat chair cow tab. dog horse mbk per. plant sheep sofa train tv mIoU IRNet [4] 90.2 75.8 32.5 76.5 49.6 65.9 82.6 76.2 82.7 31.8 77.5 41.8 79.4 75.0 81.2 75.2 47.9 81.1 51.5 62.5 59.1 66.5 WSGCN-I 91.0 79.8 33.8 78.2 50.9 65.7 86.8 79.3 86.0 27.4 75.2 48.9 83.4 76.2 82.7 74.4 64.3 86.0 51.0 64.2 58.6 68.8</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with interpixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yude</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Group-wise semantic mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
