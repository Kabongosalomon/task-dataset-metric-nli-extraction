<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Deep Video Deblurring Using Temporal Sharpness Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Deep Video Deblurring Using Temporal Sharpness Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Input frame (b) Kim and Lee <ref type="bibr" target="#b11">[12]</ref> (c) STFAN <ref type="bibr" target="#b33">[32]</ref> (d) EDVR <ref type="bibr" target="#b28">[27]</ref> (e) Ours (f) Sharpness prior of (a) <ref type="figure">Figure 1</ref>. Deblurred result on a real challenging video. Our algorithm is motivated by the success of variational model-based methods. It explores sharpness pixels from adjacent frames by a temporal sharpness prior (see (f)) and restores sharp videos by a cascaded inference process. As our analysis shows, enforcing the temporal sharpness prior in a deep convolutional neural network (CNN) and learning the deep CNN by a cascaded inference manner can make the deep CNN more compact and thus generate better-deblurred results than both the CNN-based methods <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b33">32]</ref> and variational model-based method <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a simple and effective deep convolutional neural network (CNN) model for video deblurring. The proposed algorithm mainly consists of optical flow estimation from intermediate latent frames and latent frame restoration steps. It first develops a deep CNN model to estimate optical flow from intermediate latent frames and then restores the latent frames based on the estimated optical flow.</p><p>To better explore the temporal information from videos, we develop a temporal sharpness prior to constrain the deep CNN model to help the latent frame restoration. We develop an effective cascaded training approach and jointly train the proposed CNN model in an end-to-end manner. We show that exploring the domain knowledge of video deblurring is able to make the deep CNN model more compact and efficient. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods on the benchmark datasets as well as real-world videos. The training code and test model are available at https://github.com/csbhr/CDVD-TSP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video deblurring, as a fundamental problem in the vision and graphics communities, aims to estimate latent frames from a blurred sequence. As more videos are taken using hand-held and onboard video capturing devices, this problem has received active research efforts within the last decade. The blur in videos is usually caused by camera shake, object motion, and depth variation. Recovering latent frames is highly ill-posed as only the blurred videos are given.</p><p>To recover the latent frames from a blurred sequence, conventional methods usually make assumptions on motion blur and latent frames <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">29]</ref>. Among these methods, the motion blur is usually modeled as optical flow <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">29]</ref>. The key success of these methods is to jointly estimate the optical flow and latent frames under the constraints by some hand-crafted priors. These algorithms are physically inspired and generate promising results. However, the assumptions on motion blur and latent frames usually lead to complex energy functions which are difficult to solve.</p><p>The deep convolutional neural network (CNN), as one of the most promising approach, has been developed to solve video deblurring. Motivated by the success of deep CNNs in single image deblurring, Su et al. <ref type="bibr" target="#b23">[24]</ref> concatenate consecutive frames and develop a deep CNN based on an encoder-decoder architecture to directly estimate the latent frames. Kim et al. <ref type="bibr" target="#b12">[13]</ref> develop a deep recurrent network to recurrently restore latent frames by the concatenating multiframe features. To better capture the temporal information, Zhang et al. <ref type="bibr" target="#b32">[31]</ref> develop spatial-temporal 3D convolutions to help latent frame restoration. These methods perform well when the motion blur is not significant and displacement among input frames is small. However, they are less effective for the frames containing significant blur and large displacement as they do not consider the alignment among input frames <ref type="bibr" target="#b5">[6]</ref>.</p><p>To remedy this problem, several methods estimate the alignment among consecutive input frames explicitly <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">27]</ref> or implicitly <ref type="bibr" target="#b33">[32]</ref> to restore latent frames using endto-end trainable deep CNNs. For example, the alignment methods <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b18">[19]</ref> have been extended to handle video deblurring by <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b33">[32]</ref>. The methods by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">27]</ref> explicitly adopt the optical flow or deformable convolution to estimate the alignment among consecutive input frames to help video deblurring. These algorithms show that using better alignment strategies is able to improve the performance of video deblurring. Nevertheless, the main success of these algorithms is due to the use of large-capacity models. These models cannot be generalized well on real cases. We note there exist lots of prior knowledge in variational model-based approaches and have been effective in video deblurring. A natural question is that can we use the domain knowledge in variational model-based approaches to make deep CNN models more compact so that they can improve the accuracy of video deblurring?</p><p>To solve this problem, we propose a simple and compact CNN model for video deblurring. Different from the variational model-based methods that warp the consecutive frames to generate blurred frames based on the estimated optical flow, our algorithm warps the adjacent frames into the reference frame so that the consecutive frames align well and thus generating a clearer intermediate latent frame. As the generated intermediate latent frame may contain artifacts and blur effect, we further develop a deep CNN model based on an encoder-decoder architecture to remove artifacts and blur. To better explore the properties of consecutive frames, we develop a temporal sharpness prior to constrain the deep CNN models. However, as our algorithm estimates optical flow from intermediate latent frames as the motion blur information, it requires a feedback loop. To effectively train the proposed algorithm, we develop a cascaded training approach and jointly train the proposed model in an end-to-end manner. Extensive experiments show that the proposed algorithm is able to generate favorable results against state-of-the-art methods as shown in <ref type="figure">Figure 1</ref>.</p><p>The main contributions are summarized as follows:</p><p>• We propose a simple and compact deep CNN model that simultaneously estimates the optical flow and latent frames for video deblurring.</p><p>• To better explore the properties of consecutive frames, we develop a temporal sharpness prior to constrain deep CNN models.</p><p>• We quantitatively and qualitatively evaluate the proposed algorithm on benchmark datasets and real-world videos and show that it performs favorably against state-of-the-art methods in terms of accuracy and model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hand-crafted prior-based methods. Early video or multiframe deblurring methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> usually assume that there exist sharp contents and interpolate them to help the restoration of latent frames. The main success of these methods is due to the use of sharp contents from adjacent frames. However, these methods are less effective for the blur caused by moving objects and usually generate smooth results due to the interpolation.</p><p>To overcome this problem, several algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> formulate the video deblurring by a variational approach. These algorithms first formulate motion blur as optical flow and develop kind of priors to constrain the latent frames and optical flow for video deblurring. Dai and Wu <ref type="bibr" target="#b4">[5]</ref> analyze the relations of motion blur and optical flow and alternatively estimate the transparency map, foreground, and background of latent frames. As this method relies on the accuracy of transparency maps, it is further extended by <ref type="bibr" target="#b10">[11]</ref>, where deblurring process is achieved by alternatively estimating optical flow and latent frames. Kim et al. <ref type="bibr" target="#b11">[12]</ref> approximate the motion blur using bidirectional optical flows based on <ref type="bibr" target="#b10">[11]</ref>. To deal with more complex motion blur, Gong et al. <ref type="bibr" target="#b6">[7]</ref> develop CNNs to estimate optical flow and use the conventional deconvolution algorithm <ref type="bibr" target="#b34">[33]</ref> to restore latent frames. In <ref type="bibr" target="#b30">[29]</ref>, Wulff and Black develop a novel layered model of scenes in motion and restore latent frames layer by layer. These algorithms are based on the physics models, which are able to remove blur and generate decent results. However, the priors imposed on motion blur and latent frames usually lead to complex energy functions which are difficult to solve. Deep learning based-methods. Due to the success of CNNs based on encoder and decoder architectures in image restoration <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>, this kind of network has been widely used in multi-frame <ref type="bibr" target="#b0">[1]</ref> or video deblurring <ref type="bibr" target="#b23">[24]</ref>. Instead of using 2D convolution, Zhang et al. <ref type="bibr" target="#b32">[31]</ref> employ spatialtemporal 3D convolutions to help latent frame restoration. As demonstrated by <ref type="bibr" target="#b5">[6]</ref>, these methods can be improved using optical flow for alignment. To better use spatial and temporal information, Kim et al. <ref type="bibr" target="#b13">[14]</ref> develop an optical flow estimation step for alignment and aggregate information across the neighboring frames to restore latent ones.</p><p>Wieschollek et al. <ref type="bibr" target="#b29">[28]</ref> recurrently use the features from the previous frame in multiple scales based on a recurrent network. In <ref type="bibr" target="#b12">[13]</ref>, Kim et al. develop a spatial-temporal recurrent network with a dynamic temporal blending layer for latent frame restoration. Zhou et al. extend the kernel prediction network <ref type="bibr" target="#b18">[19]</ref> to improve frame alignment. In <ref type="bibr" target="#b28">[27]</ref>, Wang et al., develop pyramid, cascading, and deformable convolution to achieve better alignment performance. The latent frames are restored by a deep CNN model with temporal and spatial attention strategies. By training the networks in an end-to-end manner, these aforementioned methods generate promising deblurred results.</p><p>We note that the main success of these algorithms on video deblurring is due to the use of large-capacity models. Their generalization ability on real applications is limited as shown in <ref type="figure">Figure 1</ref>. Different from these methods, we explore the simple and well-established principles to make the CNN model more compact instead of enlarging network model capacity for video deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation</head><p>To better motivate our work, we first revisit the conventional variational model-based methods.</p><p>For the blur process in videos, the i-th blurred image is usually modeled as:</p><formula xml:id="formula_0">B i = 1 2τ τ t=0 H t i→i+1 (I i ) + H t i→i−1 (I i )dt,<label>(1)</label></formula><p>where I i denotes the i-th clear image; τ denotes the relative exposure time (which also means the camera duty cycle); H t i→i+1 and H t i→i−1 denote the warping functions which warp the frame I i into I i+1 and I i−1 . If we denote the bidirectional optical flow at frame i as u i→i+1 and u i→i−1 , H t i→i+1 (I i ) and H t i→i−1 (I i ) can be represented as</p><formula xml:id="formula_1">I i (x + tu i→i+1 ) and I i (x + tu i→i−1 ).</formula><p>Based on the blur model (1), the deblurring process can be achieved by minimizing:</p><formula xml:id="formula_2">L(u, I) = i ρ I (W(I i ), B i ) + ϕ(I i ) (2) + i j ρ u (I i , I i+j (x + u i→i+j )) + φ(u i→i+j ), where ρ I (W(I i ), B i ) denotes the data term w.r.t. W(I i ) and B i ; W(I i ) denotes the integration term in (1); ρ u (I i (x), I i+j (x + u i→i+j )) denotes the data term w.r.t. I i (x) and I i+j (x + u i→i+j ); ϕ(I i ) and φ(u i→i+j )</formula><p>denote the constraints on latent image I i and optical flow u i→i+j . In the optimization process, most conventional methods (e.g., <ref type="bibr" target="#b11">[12]</ref>) estimate the latent image and optical flow by iteratively minimizing:</p><formula xml:id="formula_3">i ρ I (W(I i ), B i ) + ϕ(I i ),<label>(3)</label></formula><p>and</p><formula xml:id="formula_4">i j ρ u (I i , I i+j (x + u i→i+j )) + φ(u i→i+j ). (4)</formula><p>We note that alternatively minimizing <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula">(4)</ref> is able to remove blur. However, the deblurring performance mainly depends on the choice of constraints w.r.t. latent image I i and optical flow u i→i+j , and it is not trivial to determine proper constraints. In addition, the commonly used constraints usually lead to highly non-convex objective functions which are difficult to solve. We further note that most deep CNN-based methods directly estimate the sharp videos from blurred input and generate promising results. However, they estimate the warping functions from blurred inputs instead of latent frames and do not explore the domain knowledge of video deblurring, which are less effective for the videos with significant blur effect.</p><p>To overcome these problems, we develop an effective algorithm which makes full use of the well-established principles in the variational model-based methods and explores the domain knowledge to make deep CNNs more compact for video deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Algorithm</head><p>The proposed algorithm contains the optical flow estimation module, latent image restoration module, and the temporal sharpness prior. The optical flow estimation module provides motion information for the latent frame restoration, while the latent frame restoration module further facilitates the optical flow estimation so that it makes the estimated flow more accurate. The temporal sharpness prior is able to explore the sharpness pixels from adjacent frames so that it can facilitate better frame restoration. All the modules are jointly trained in a unified framework by an endto-end manner. In the following, we explain the main ideas for each component in details. For simplicity, we use three adjacent frames to illustrate the main ideas of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Optical flow estimation</head><p>The optical flow estimation module is used to estimate optical flow between input adjacent frames, where the estimated optical flow provides the motion information for the image restoration <ref type="bibr" target="#b2">(3)</ref>. As demonstrated in <ref type="bibr" target="#b24">[25]</ref>, the optical flow estimation (4) can be efficiently solved by a deep neural network. We use the PWC-Net <ref type="bibr" target="#b24">[25]</ref> as the optical flow estimation algorithm. Given any two intermediate latent frames I i and I i+1 , we compute optical flow by:</p><formula xml:id="formula_5">u i→i+1 = N f (I i ; I i+1 ),<label>(5)</label></formula><p>where N f denotes the optical flow estimation network which takes two images as the input. For any other two frames, the network N f shares the same network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Latent frame restoration</head><p>With the estimated optical flow, we can use variational model (3) to restore latent frames according to existing methods, e.g., <ref type="bibr" target="#b11">[12]</ref>. However, solving (3) involves large computation of W(I i ) and needs to define the prior on latent frame I i , which makes the restoration more complex. We note that the effect of W(I i ) (i.e., the blur process <ref type="formula" target="#formula_0">(1)</ref>) is to generate a blurred frame so that it is closed to the observed input frame B i as much as possible. The discretization of (1) can be written as <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_6">W(I i ) = 1 1 + 2τ τ d=1 H t i→i+1 (I i ) + H t i→i−1 (I i ) + I i (x) .</formula><p>(6) According to the estimated optical flow u i→i+1 and u i→i−1 , if we set τ to be 1, W(I i ) can be approximated by:</p><formula xml:id="formula_7">W(I i ) = 1 3 (I i (x + u i→i+1 ) + I i (x + u i→i−1 ) + I i (x)) .<label>(7)</label></formula><p>Instead of generating a blurred frame, we want to generate clear one according to the estimated optical flow u i−1→i , and u i+1→i so that I i+1 (x+u i+1→i ) and I i−1 (x+u i−1→i ) can be aligned with I i (x) well. Thus, we can use the following formula to update latent frame I i :</p><formula xml:id="formula_8">I i ← 1 3 (I i+1 (x + u i+1→i ) + I i−1 (x + u i−1→i ) + I i (x)) .<label>(8)</label></formula><p>However, directly using (8) will lead to the results contains significant artifacts due to the misalignment from I i+1 (x + u i+1→i ) and I i−1 (x + u i−1→i ). To avoid this problem and generate high-quality latent frame I i , we use I i+1 (x + u i+1→i ) and I i−1 (x + u i−1→i ) as the guidance frames and develop a deep CNN model to restore latent frame I i by:</p><formula xml:id="formula_9">I i ← N l (C(I i+1 (x + u i+1→i ); I i (x); I i−1 (x + u i−1→i ))),<label>(9)</label></formula><p>where C(·) denotes the concatenation operation and N l denotes the restoration network. Similar to <ref type="bibr" target="#b24">[25]</ref>, we use the bilinear interpolation to compute the warped frames.</p><p>For the deep CNN model N l , we use an encoder-decoder architecture based on <ref type="bibr" target="#b25">[26]</ref>. However, we do not use the ConvLSTM module in N l . Other network architectures are the same as <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Temporal sharpness prior</head><p>As demonstrated in <ref type="bibr" target="#b3">[4]</ref>, the blur in the video is irregular, and thus there exist some pixels that are not blurred. Following the conventional method <ref type="bibr" target="#b3">[4]</ref>, we explore these sharpness pixels to help video deblurring.</p><p>According to the warped frames I i+1 (x + u i+1→i ) and</p><formula xml:id="formula_10">I i−1 (x + u i−1→i ), if the pixel x in I i (x) is a sharp one, the pixel values of I i+1 (x + u i+1→i ) and I i−1 (x + u i−1→i )</formula><p>should be close to that of I i (x). Thus, we define this crite-rion as: <ref type="formula" target="#formula_0">(10)</ref>, if the value of S i (x) is close to 1, the pixel x is likely to be clear. Thus, we can use S i (x) to help the deep neural network to distinguish whether the pixel is clear or not so that it can help the latent frame restoration. To increase the robustness of S i (x), we define D(I i+j (x + u i+j→i ); I i (x)) as</p><formula xml:id="formula_11">S i (x) = exp   − 1 2 j&amp;j =0 D(I i+j (x + u i+j→i ); I i (x))   , (10) where D(I i+j (x + u i+j→i ); I i (x)) is defined as I i+j (x + u i+j→i ) − I i (x) 2 . Based on</formula><formula xml:id="formula_12">D(I i+j (x+u i+j→i ); I i (x)) =<label>(11)</label></formula><p>y∈ω(x)</p><formula xml:id="formula_13">I i+j (y + u i+j→i ) − I i (y) 2 ,</formula><p>where ω(x) denotes an image patch centered at pixel x.</p><p>With the temporal sharpness prior S i (x), we modify the latent frame restoration (9) by</p><formula xml:id="formula_14">I i ← N l (C(C Ii ; S i (x))),<label>(12)</label></formula><p>where</p><formula xml:id="formula_15">C Ii = C(I i+1 (x + u i+1→i ); I i (x); I i−1 (x + u i−1→i )</formula><p>). We will show that using S i (x) is able to help latent frame restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference</head><p>As the proposed algorithm contains intermediate the optical flow estimation, latent frame estimation, and temporal sharpness computation, we train the proposed algorithm in a cascaded manner.</p><p>Let Θ t = {O t , L t } denote the model parameters of optical flow estimation and latent frame restoration networks at stage (iteration) t. We learn the stage-dependent model parameters Θ t from N training video sequences, where each video sequence contains {B n i , I n gt,i } M i=1 training samples. Given 2j + 1 blurred frames, the parameter Θ t is learned by minimizing the cost function:</p><formula xml:id="formula_16">J (Θ t ) = N n=1 M i=1 F Θt (B n i−j ; ...; B n i ; ...; B n i+j ) − I n gt,i 1 ,<label>(13)</label></formula><p>where F Θt denotes the whole network for video deblurring, which takes 2j + 1 blurred frames as the input. That is, the intermediate latent frame at t-stage is I t i = F Θt (B n i−j ; ...; B n i ; ...; B n i+j ). Algorithm 1 summarizes the main steps of the cascaded training approach, where T denotes the number of stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we evaluate the proposed algorithm using publicly available benchmark datasets and compare it to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Proposed cascaded training algorithm.</head><p>Input: Training video sequences {B n i , I n gt,i } M i=1 ; n = 1, ..., N . Initialize I n i ← B n i . for t = 1 → T do for Any three frames I n i−1 , I n i , and I n i+1 do Estimating optical flow u i−1→i , u i+1→i according to <ref type="bibr" target="#b4">(5)</ref>. Computing S i (x) according to <ref type="bibr" target="#b9">(10)</ref>. Latent frame restoration according to <ref type="bibr" target="#b11">(12)</ref>. end for Estimating model parameters Θ t by minimizing <ref type="bibr" target="#b12">(13)</ref>. Updating I n i according to <ref type="bibr" target="#b11">(12)</ref> with the estimated parameter Θ t . end for Output: Model parameters {Θ t } T t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Parameter settings and training data</head><p>For fair comparisons with state-of-the-art methods, we use the video deblurring dataset by Su et al. <ref type="bibr" target="#b23">[24]</ref> for training and evaluation, where 61 videos are used for training and the remaining 10 videos for the test. We use the similar data augmentation method to <ref type="bibr" target="#b33">[32]</ref> to generate training data. The size of each image patch is 256 × 256 pixels. We initialize the latent frame restoration network according to <ref type="bibr" target="#b7">[8]</ref> and train it from scratch. For the PWC-Net, we use the pretrained model <ref type="bibr" target="#b24">[25]</ref> to initialize it. In the training process, we use the ADAM optimizer <ref type="bibr" target="#b14">[15]</ref> with parameters β 1 = 0.9, β 2 = 0.999, and = 10 −8 . The minibatch size is set to be 8. The learning rates for N l and PWC-Net are initialized to be 10 −4 and 10 −6 and decrease to half after every 200 epochs. We empirically set T = 2 as a trade-off between accuracy and speed. At each stage, we use 3 frames to generate one deblurred image. Thus, the proposed algorithm needs 5 frames when T = 2. To better make the network compact, the network at each stage shares the same model parameters. Similar to <ref type="bibr" target="#b31">[30]</ref>, we further use the hard example mining strategy to preserve sharp edges. We implement our algorithm based on the PyTorch. More experimental results are included in the supplemental material. The training code and test model are available at the authors' website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with the state of the art</head><p>To evaluate the performance of the proposed algorithm, we compare it against state-of-the-art algorithms including the variational model-based method <ref type="bibr" target="#b11">[12]</ref> and deep CNNsbased methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b25">26]</ref>. To evaluate the quality of each restored image on synthetic datasets, we use the PSNR and SSIM as the evaluation metrics. <ref type="table">Table 1</ref> shows the quantitative results on the benchmark dataset by Su et al. <ref type="bibr" target="#b23">[24]</ref>, where the proposed algorithm performs favorably against the state-of-the-art methods in terms of PSNR and SSIM. <ref type="figure">Figure 2</ref> shows some deblurred results from the test dataset <ref type="bibr" target="#b23">[24]</ref>. The variational model-based method <ref type="bibr" target="#b11">[12]</ref> does not recover the structures well and generates the results with significant blur residual. The method <ref type="bibr" target="#b25">[26]</ref> develops end-to-end-trainable deep CNN models to deblur dynamic scenes. However, the deblurred images contain significant blur residual as the temporal information is not used. The video deblurring algorithm <ref type="bibr" target="#b23">[24]</ref> directly concatenates consecutive frames as the input of an end-to-end trainable deep CNN model. However, the structures of the deblurred image are not sharp <ref type="figure">(Figure 2(e)</ref>). We note that the EDVR method <ref type="bibr" target="#b28">[27]</ref> develops a pyramid, cascading, and deformable alignment module and uses a PreDeblur module for video deblurring. However, this method is less effective when the PreDeblur does not remove blur from input frames. The results in <ref type="figure">Figure 2</ref>(f) show that the structures of the images by the EDVR method are not recovered well. In contrast, the proposed method recovers finer image details and structures than the state-of-the-art algorithms.</p><p>We further evaluate the proposed method on the GOPRO dataset by Nah et al. <ref type="bibr" target="#b19">[20]</ref> following the protocols of state-ofthe-art methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="table">Table 2</ref> shows that the proposed algorithm generates the deblurred videos with higher PSNR and SSIM values. <ref type="figure">Figure 3</ref> shows some deblurred results from <ref type="bibr" target="#b19">[20]</ref>. We note that state-of-the-art methods do not generate sharp images and remove the non-uniform blur well. In contrast, the proposed algorithm restores much clearer images, where the license numbers are recognizable. Real examples. We further evaluate our algorithm on the real video deblurring dataset by Cho et al. <ref type="bibr" target="#b3">[4]</ref>. <ref type="figure">Figure 4</ref> shows that the state-of-the-art methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b23">24]</ref> do not restore the sharp frames well. Our algorithm generates much clearer frames with better detailed structures. For example, the man and the boundaries of the buildings are much clearer <ref type="figure">(Figure 4(h)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis and Discussions</head><p>We have shown that the proposed algorithm performs favorably against state-of-the-art methods. To better understand the proposed algorithm, we perform further analysis and discuss its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Effectiveness of the cascaded training</head><p>The proposed cascaded training algorithm ensures that the proposed method estimates optical flow from intermediate latent frames and updates the intermediate latent frames iteratively. One may wonder whether the cascaded training algorithm helps video deblurring. To answer this question, we compare the method without using cascaded training algorithm (i.e., w/o CT in <ref type="table" target="#tab_2">Table 3</ref>), where we set stage number T to be 1 in Algorithm 1 for fair comparisons. <ref type="table">Table 1</ref>. Quantitative evaluations on the video deblurring dataset <ref type="bibr" target="#b23">[24]</ref> in terms of PSNR and SSIM. All the comparison results are generated using the publicly available code. All the restored frames instead of randomly selected 30 frames from each test set <ref type="bibr" target="#b23">[24]</ref> are used for evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Kim and Lee <ref type="bibr" target="#b11">[12]</ref> Gong et al. <ref type="bibr">[</ref> (e) Su et al. <ref type="bibr" target="#b23">[24]</ref> (f) EDVR <ref type="bibr" target="#b28">[27]</ref> (g) STFAN <ref type="bibr" target="#b33">[32]</ref> (h) Ours <ref type="figure">Figure 2</ref>. Deblurred results on the test dataset <ref type="bibr" target="#b23">[24]</ref>. The deblurred results in (c)-(g) still contain significant blur effects. The proposed algorithm generates much clearer frames.</p><p>(a) Blurred frame (b) GT (c) Kim and Lee <ref type="bibr" target="#b11">[12]</ref> (d) Tao et al. <ref type="bibr" target="#b25">[26]</ref> (e) Su et al. <ref type="bibr" target="#b23">[24]</ref> (f) EDVR <ref type="bibr" target="#b28">[27]</ref> (g) STFAN <ref type="bibr" target="#b33">[32]</ref> (h) Ours <ref type="figure">Figure 3</ref>. Deblurred results on the test dataset <ref type="bibr" target="#b19">[20]</ref>. The proposed method generates much better deblurred images, where the license numbers are recognizable. <ref type="table" target="#tab_2">Table 3</ref> shows the quantitative evaluations on the benchmark dataset by Su et al. <ref type="bibr" target="#b23">[24]</ref>. We note that the method without using cascaded training algorithm estimates the op-tical flow from blurred inputs using PWC-Net, where this strategy is widely for image alignment in video deblurring <ref type="bibr" target="#b5">[6]</ref>. However, this method does not generate high- <ref type="table">Table 2</ref>. Quantitative evaluations on the video deblurring dataset <ref type="bibr" target="#b19">[20]</ref> in terms of PSNR and SSIM. * denotes the reported results from <ref type="bibr" target="#b20">[21]</ref>. Methods Tao et al. <ref type="bibr" target="#b25">[26]</ref> Su et al. <ref type="bibr" target="#b23">[24]</ref> Wieschollek et al. <ref type="bibr">[</ref>  <ref type="bibr" target="#b21">[22]</ref> (c) Cho et al. <ref type="bibr" target="#b3">[4]</ref> (d) Kim and Lee <ref type="bibr" target="#b11">[12]</ref> (e) Su et al. <ref type="bibr" target="#b23">[24]</ref> (f) EDVR <ref type="bibr" target="#b28">[27]</ref> (g) STFAN <ref type="bibr" target="#b33">[32]</ref> (h) Ours <ref type="figure">Figure 4</ref>. Deblurred results on a real video from <ref type="bibr" target="#b3">[4]</ref>. The proposed algorithm recovers a high-quality image with clearer details. quality deblurred results ( <ref type="figure" target="#fig_0">Figure 5(b)</ref>) as optical flow is related to the latent frames information instead of blurred ones during the exposure time. In contrast, the proposed algorithm generates the results with higher PSNR and SSIM values.</p><p>We further compare the deblurred results generated by different stages in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure" target="#fig_0">Figure 5</ref>. We note that using more stages generates better deblurred images. However, the improved performance is not significant. Thus, we use two stages as a trade-off between accuracy and speed.</p><p>We further note that directly estimating optical flow from blurred inputs will increase ambiguity at frame boundaries for video deblurring. <ref type="figure">Figure 6(d)</ref> demonstrates that the boundaries of the estimated optical flow are blurry, which accordingly affects the important boundaries restoration <ref type="figure">(Figure 6(b)</ref>). In contrast, the optical flow by the proposed method contains sharp boundaries well ( <ref type="figure">Figure 6(f)</ref>), which facilitates the latent frame restoration <ref type="figure">(Figure 6(c)</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Effectiveness of the temporal sharpness prior</head><p>We develop a temporal sharpness prior to better explore the properties of consecutive frames so that it makes the deep CNN models more compact. To demonstrate the effectiveness of this prior, we disable this prior in the proposed method and retrain the algorithm without using the temporal sharpness prior with the same settings for fair comparisons. We evaluate the temporal sharpness prior on 4 videos with significant blur effects from the test dataset <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_3">Table 4</ref> (a) Blurred frame    and <ref type="figure" target="#fig_2">Figure 7</ref> show both quantitative and qualitative evaluations. We note that the temporal sharpness prior is able to distinguish the sharpness pixels and blurred pixels from adjacent frames so that it can help the deep CNN model for better frame restoration. <ref type="figure" target="#fig_2">Figure 7(b)</ref> shows the visualizations of the temporal sharpness prior, where the blurred pixels can be better detected. The comparisons in <ref type="table" target="#tab_3">Table 4</ref> demonstrate that using the temporal sharpness prior is able to improve the accuracy of video deblurring. <ref type="figure" target="#fig_2">Figure 7</ref> further shows that using the temporal sharpness prior is able to generate the frames with clearer structures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Effect of optical flow</head><p>As several algorithms either directly concatenate consecutive frames <ref type="bibr" target="#b23">[24]</ref> or estimate filter kernels <ref type="bibr" target="#b33">[32]</ref> instead of using optical flow for video deblurring, one may wonder whether optical flow helps video deblurring. To answer this question, we remove the optical flow estimation module and compare with the method that directly concatenates consecutive frames as the input of the restoration network N l 1 . <ref type="table" target="#tab_4">Table 5</ref> shows that using optical flow is able to improve the performance of video deblurring.</p><p>In addition, we further evaluate the optical flow estimation module using FlowNet 2.0 <ref type="bibr" target="#b8">[9]</ref>. <ref type="table" target="#tab_4">Table 5</ref> shows that the proposed method is robust to optical flow modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Model size</head><p>As stated in Section 1, we aim to improve the accuracy of video deblurring while do not increase the model capacity using domain knowledge of video deblurring. <ref type="table">Table 6</ref> shows that the proposed algorithm has a relatively smaller model size against state-of-the-art methods. Compared to the baseline models, the proposed model does not increase any model size while generating much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Limitations</head><p>Although the temporal sharpness prior is effective for videos with significant blur, it is less effective when the blur exists in each position of all frames. In such cases, the temporal sharpness prior is less likely to distinguish whether the pixel is clearer or not. <ref type="table">Table 7</ref> shows the deblurred results on 3 videos from the test dataset <ref type="bibr" target="#b23">[24]</ref>, where each position in a frame contains blur effect. We note that using the temporal sharpness prior does not improve the deblurring performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Concluding Remarks</head><p>We have proposed a simple and effective deep CNN model for video deblurring. The proposed CNN explores the simple and well-established principles used in the variational model-based methods and mainly consists of optical flow estimation from intermediate latent frames and latent frame restoration. We have developed a temporal sharpness prior to help the latent image restoration and an effective cascaded training approach to train the proposed CNN model. By training in an end-to-end manner, we have shown that the proposed CNN model is more compact and efficient and performs favorably against state-of-the-art methods on both benchmark datasets and real-world videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplemental Material</head><p>We show the network details in this section. More detailed analysis and experimental results are included in the supplemental material which can be obtained at https: //jspan.github.io/.</p><p>Network details. <ref type="figure">Figure 8</ref> shows the flowchart of the proposed algorithm at one stage. The proposed network shares the same network parameters when handling every three adjacent frames. The network architecture for the latent image restoration is shown in <ref type="figure">Figure 9</ref>. For the optical flow estimation, we use the PWC-Net <ref type="bibr" target="#b24">[25]</ref> to estimate optical flow. All the network modules are jointly trained in an end-to-end manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Effectiveness of the cascaded training algorithm for video deblurring. (b) denotes the deblurred result by the proposed method without using cascaded training. (c)-(e) denote the results from stage 1, 2, and 3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Figure 6 .</head><label>26</label><figDesc>Effect of optical flow on video deblurring. The optical flow by the proposed method contains sharp boundaries well (see (f)), which facilitates the latent frame restoration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Effectiveness of the temporal sharpness prior. (a) Blurred input. (b) Visualizations of the intermediate temporal sharpness prior. (c)-(d) denote the results without and with the temporal sharpness prior, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effectiveness of the cascaded training algorithm for video deblurring, where "CT" is the abbreviation of cascaded training.</figDesc><table><row><cell cols="5">Methods w/o CT Stage 1 Stage 2 Stage 3</cell></row><row><cell>PSNRs</cell><cell>31.33</cell><cell>31.59</cell><cell>32.13</cell><cell>32.20</cell></row><row><cell>SSIMs</cell><cell>0.9125</cell><cell>0.9161</cell><cell>0.9268</cell><cell>0.9272</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Effectiveness of the temporal sharpness prior on video deblurring.</figDesc><table><row><cell cols="3">Methods w/o temporal sharpness prior</cell><cell>Ours</cell></row><row><cell>PSNRs</cell><cell>34.48</cell><cell></cell><cell>34.63</cell></row><row><cell>SSIMs</cell><cell>0.9126</cell><cell></cell><cell>0.9268</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Effect of the optical flow estimation module.</figDesc><table><row><cell cols="4">Methods w/o optical flow w/ FlowNet 2.0 w/ PWC-Net</cell></row><row><cell>PSNRs</cell><cell>31.19</cell><cell>32.06</cell><cell>32.13</cell></row><row><cell>SSIMs</cell><cell>0.9055</cell><cell>0.9254</cell><cell>0.9268</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Comparisons of model sizes against state-of-the-art methods and baselines. "TSP" is the abbreviation of temporal sharpness prior. Evaluations on the blurred videos, where the blur exists in each position of each frame. The temporal sharpness prior is less effective when it fails to identify the clear pixels.</figDesc><table><row><cell>Methods</cell><cell cols="2">Su et al. [24] EDVR [27]</cell><cell>w/o CT</cell><cell>w/o TSP</cell><cell>Ours</cell></row><row><cell>Model size</cell><cell>15.30M</cell><cell>23.60M</cell><cell>16.19M</cell><cell>16.19M</cell><cell>16.19M</cell></row><row><cell cols="4">Methods w/o temporal sharpness prior</cell><cell>Ours</cell><cell></cell></row><row><cell cols="2">PSNRs</cell><cell>31.31</cell><cell></cell><cell>31.33</cell><cell></cell></row><row><cell cols="2">SSIMs</cell><cell>0.9238</cell><cell></cell><cell>0.9239</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The proposed method without using optical flow reduces to the network N l which takes the concatenation of consecutive frames as the input.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Burst image deblurring using permutation invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="748" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A variational framework for simultaneous motion estimation and restoration of motion-blurred video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leah</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Berkels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reblur2deblur: Deblurring videos via self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video deblurring for hand-held cameras using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
		<idno>64:1-64:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion from blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep video deblurring: The devil is in the details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anton van den Hengel, and Qinfeng Shi. From motion blur to motion flow: A deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4058" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatio-temporal transformer network for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating sharp panoramas from motion-blurred videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2424" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Full-frame video stabilization with motion inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weina</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1150" to="1163" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2502" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deblurring images via dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Convolutional layer (5 x 5, stride: 1) Convolutional layer (5 x 5</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">stride: 2) network architectures for the latent frame restoration. Each convolutional and deconvolutional layers are followed by a ReLU unit except the last one that outputs the latent frame. The number of feature channels in intermediate layers are 32</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>Residual Block Deconvolutional layer (3 x 3. respectively</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EDVR: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling blurred video with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Julian</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="236" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep flow-guided video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial spatio-temporal learning for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="291" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
