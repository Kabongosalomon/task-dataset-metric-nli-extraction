<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the full body 3D pose and shape of humans from images has been a challenging goal of computer vision going all the way back to the work of Hogg <ref type="bibr" target="#b14">[15]</ref>. The inherent ambiguity of the problem has forced the researchers to use monocular image sequences for inference <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b2">3]</ref>, employ multiple camera views <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16]</ref>, or even explore alternative sensors, like Kinect <ref type="bibr" target="#b52">[53]</ref> or IMUs <ref type="bibr" target="#b51">[52]</ref>. In these settings, the body shape reconstruction results are remarkable. However, estimating 3D pose and shape from single color images remains the ultimate goal for 3D human analysis.</p><p>Considering the particularly challenging nature of such a problem, the literature remains undeniably sparse. Most approaches rely on iterative optimization, attempting to estimate a full body 3D shape that is consistent with 2D image observations, like silhouettes, edges, shading, or 2D keypoints <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b13">14]</ref>. Despite the significant runtime required to solve the complicated optimization problem, the common failures because of local minima, and the error-prone reliance on ambiguous 2D cues, optimization-based solutions remain the leading paradigm for this problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>. Even the emergence of deep learning has not changed significantly the landscape. ConvNets did not seem as a viable candidate for this problem because they require a huge amount of training data and they are infamous for their low resolution 3D predictions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44]</ref>. The goal of our work is to demonstrate that ConvNets can indeed offer an attractive solution for this problem, by proposing an efficient and effective direct prediction approach, which is competitive and even outperforms iterative optimization methods.</p><p>To make this feasible, a critical design choice for our approach is the incorporation of a parametric statistical body shape model (SMPL <ref type="bibr" target="#b24">[25]</ref>) within our end-to-end framework, presented in <ref type="figure">Figure 1</ref>. The advantage of such a representation is that we can generate high quality 3D meshes in the form of 6890 vertices while estimating only a small number of parameters, i.e., 72 for pose and 10 for shape. This low-dimensional parameterization makes the model friendly for direct network prediction. In fact, this prediction is feasible and accurate by using only 2D keypoints and silhouettes as input. This allows us to relax the limiting assumption that natural images with 3D shape ground truth are available for training. In contrast, we can leverage the available 2D image annotations (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref>) to train for image-to-2D inference, while using instances of the parametric model to train for 2D-to-3D shape inference. Simultaneously, another major advantage of employing this parametric model is that its structure allows us to generate the estimated 3D mesh at training time and optimize directly for the surface, by using a 3D per-vertex loss. This loss has better correlation with the vertex-to-vertex 3D error that is typically used for evaluation and improves training compared to  <ref type="figure">Figure 1</ref>. Schematic representation of our framework. (a) An initial ConvNet, Human2D, predicts 2D heatmaps and masks from a single color image, using 2D pose data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> for training. (b) Two networks estimate the parameters of the statistical model SMPL <ref type="bibr" target="#b24">[25]</ref>, using instances of the parametric model for training. The PosePrior estimates pose parameters (θ) from keypoints, and the ShapePrior estimates shape parameters (β) from silhouettes. (c) The framework can be finetuned end-to-end without requiring images with 3D shape ground truth, by projecting the full body 3D mesh to the image and optimizing for the consistency of the projection with 2D annotations (keypoints and masks). The blue parts (Mesh Generator and Renderer) indicate components without learnable parameters. naive parameter regression. Finally, we propose to employ a differentiable renderer to project the generated 3D mesh back to the 2D image. This enables end-to-end finetuning of the network by optimizing for the consistency of the projection with annotated 2D observations, i.e., 2D keypoints and masks. The complete framework offers a modular direct prediction solution to the problem of 3D human pose and shape estimation from a single color image and outperforms previous approaches on the relevant benchmarks.</p><p>Our main contributions can be summarized as follows:</p><p>• an end-to-end framework for 3D human pose and shape estimation from a single color image.</p><p>• incorporation of a parametric statistical shape model, SMPL, within the end-to-end framework, enabling:</p><p>prediction of the SMPL model parameters from ConvNet-estimated 2D keypoints and masks to avoid training on synthetic image examples. generation of the 3D body mesh at training time and supervision based on the 3D shape consistency. use of a differentiable renderer for 3D mesh projection and refinement of the network with supervision based on the consistency with 2D annotations.</p><p>• superior performance compared to previous approaches for 3D human pose and shape estimation at significantly faster running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>3D human pose estimation: In order to estimate a convincing 3D reconstruction of the human body, it is crucial to get an accurate prediction of the 3D pose of the person. Many recent works follow the end-to-end paradigm <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55]</ref>, using images as input to predict 3D joint locations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28]</ref>, regress 3D heatmaps <ref type="bibr" target="#b30">[31]</ref>, or classify the image in a particular pose class <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. Unfortunately, an important constraint is that most of these ConvNets require images with 3D pose ground truth for training, limiting the available training data sources. Other approaches commit to the 2D pose estimates provided by state-of-the-art ConvNets and focus on the 3D pose reconstruction <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b56">57]</ref>, recover 3D pose exemplars <ref type="bibr" target="#b7">[8]</ref>, or produce multiple 3D pose candidates consistent with the 2D pose <ref type="bibr" target="#b17">[18]</ref>. Notably, Martinez et al. <ref type="bibr" target="#b26">[27]</ref> demonstrate state-of-the-art results using a simple multi-layer perceptron which regresses the 3D joint locations from 2D pose input. Our goal is significantly different from the aforementioned works, since instead of a rough stickman-like figure, we estimate the whole surface geometry of the human body. Human shape estimation: Concurrently with advances in 3D human pose, a different set of works addressed the problem of human shape estimation. In this case, given a single image, most methods attempt to estimate the parameters of a statistical body shape model like SCAPE <ref type="bibr" target="#b4">[5]</ref> or SMPL <ref type="bibr" target="#b24">[25]</ref>. The input is usually silhouettes, while regression forests <ref type="bibr" target="#b8">[9]</ref> and ConvNets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> have been proposed for the prediction. Knowledge of human shape is useful for biometric applications, however we argue that for 3D perception the potential and the challenges are significantly greater when pose and shape are inferred jointly. Joint 3D human pose and shape estimation: Despite individual advances in pose and shape prediction, their joint estimation makes the task significantly harder. This has consistently fostered research in non single image scenarios, for more robust results. Xu et al. <ref type="bibr" target="#b53">[54]</ref> propose a pipeline for full performance capture from monocular video assuming knowledge of the shape mesh for the observed subject. Alldieck et al. <ref type="bibr" target="#b2">[3]</ref> estimate pose and shape jointly from monocular video relying on optical flow cues. Rhodin et al. <ref type="bibr" target="#b35">[36]</ref> and Huang et al. <ref type="bibr" target="#b15">[16]</ref> use images from multiple calibrated cameras and rely on keypoint detections, silhouettes and temporal consistency to recover a reconstruction of the body. An alternative setting is proposed by Weiss et al. <ref type="bibr" target="#b52">[53]</ref> making use of the depth modality of the Kinect sensor to tackle the same problem. In the same spirit of exploring different sensors, von Marcard et al. <ref type="bibr" target="#b51">[52]</ref> use a sparse set of IMUs on the subject to recover pose and shape jointly. 3D human pose and shape from a single color image: In the most challenging case of using only a single color image as input, the work of Sigal et al. <ref type="bibr" target="#b40">[41]</ref> is among the first to estimate high quality 3D shape estimates, by fitting the parametric model SCAPE <ref type="bibr" target="#b4">[5]</ref> to ground truth image silhouettes. Guan et al. <ref type="bibr" target="#b13">[14]</ref> use silhouettes, edges and shading as cues during the fitting process, but still require initialization through a user specified 2D skeleton. A fully automatic approach was proposed very recently by Bogo et al. <ref type="bibr" target="#b6">[7]</ref>. They use 2D keypoint detections from a 2D pose ConvNet <ref type="bibr" target="#b32">[33]</ref> and fit the parametric model SMPL <ref type="bibr" target="#b24">[25]</ref> to these 2D locations. Their 3D pose results are very accurate, but shape remains highly underconstrained. To improve upon this, Lassner et al. <ref type="bibr" target="#b21">[22]</ref> extends the fitting using silhouettes provided by a segmentation ConvNet. The common theme of these works is that they pose an optimization problem and attempt to fit a body model to a set of 2D observations. The drawback though is that solving this iterative optimization problem is very slow, it can easily fail because of local minima, and it relies a lot on error-prone 2D observations. Alternatively, direct prediction approaches estimate 3D pose and shape in a discriminative way, without explicitly optimizing a specific objective during inference. Relevant to this paradigm is the work of Lassner et al. <ref type="bibr" target="#b21">[22]</ref>, where a ConvNet detects 91 landmarks of the human body and then a random forest estimates the 3D body and shape from these detections. However, to train for these landmarks, they still require alignment of body shapes with images. In contrast, we demonstrate that only a much smaller set of annotations are critical for the reconstruction, i.e., 2D joints and masks, which can be provided by human annotators and are abundant for in-the-wild images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref>, while we also incorporate everything within a unified end-to-end framework. Concurrently, Tan et al. <ref type="bibr" target="#b42">[43]</ref> use an encoder-decoder ConvNet, where the decoder is trained to predict the silhouette corresponding to SMPL parameters. We differ to them by identifying that from these parameters we can analytically generate the body mesh and project it to the image in a differentiable way (as in <ref type="bibr" target="#b46">[47]</ref> for face models), avoiding half a million of extra learnable weights. Instead, we focus our computational and learning effort in the image to 3D shape part of the framework. Our work is also related to the concurrent work of Tung et al. <ref type="bibr" target="#b49">[50]</ref>, however our framework can be trained from scratch instead of relying on synthetic image data for pretraining, and we demonstrate state-of-theart results for model-based 3D pose and shape prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Human body shape models</head><p>Statistical body shape models, like SCAPE <ref type="bibr" target="#b4">[5]</ref> or SMPL <ref type="bibr" target="#b24">[25]</ref>, are powerful tools, which provide significant opportunities for an end-to-end framework. One of the important advantages is their low-dimensional parameter space, which is very suitable for direct network prediction. With this parameter representation, we can keep the output prediction space small, compared to voxelized or point cloud representations. Simultaneously, the low dimensional prediction does not sacrifice the quality of the output, since we can still generate high quality 3D meshes from the estimated parameters. Furthermore, from a learning perspective, we bypass the problem of learning the statistics of the human body, and devote the network capacity at the inference of the model parameters from image evidence. In contrast, approaches without the aid of a model put additional burden on the learning side, which often leads to embarrassing prediction errors (e.g., failing to reconstruct limbs under occlusion, missing body details, etc). Moreover, most models offer a convenient disentanglement of pose and shape which is useful to independently focus on the factors that affect each one of the two. Last but certainly not least for end-to-end approaches, the function which generates the 3D mesh from parameter inputs is differentiable, making the models compatible with current end-to-end pipelines.</p><p>In this work, we employ the more recent SMPL model, introduced by Loper et al. <ref type="bibr" target="#b24">[25]</ref>. We provide the essential notation here, and we refer the reader to <ref type="bibr" target="#b24">[25]</ref> for more details. SMPL defines a function M(β, θ; Φ), where β are the shape parameters, θ are the pose parameters and Φ are fixed parameters of the model. The direct output of this function is a body mesh P ∈ R N ×3 with N = 6890 vertices P i ∈ R 3 . The shape of the model uses a linear combination of a low number of principal body shapes which are learned from a large dataset of body scans <ref type="bibr" target="#b37">[38]</ref>. The shape parameters β are the linear coefficients of these base shapes. The pose of the body is defined through a skeleton rig with 23 joints. The pose parameters θ are expressed in the axis angle representation and define the relative rotation between parts of the skeleton. In total, 72 parameters define the pose (3 for each of the 23 joints, plus 3 for the global rotation). Given the rest pose shape retrieved by the shape parameters β, SMPL defines pose-dependent deformations and uses the pose parameters θ to produce the final output mesh. Conveniently, the body joints J are a linear combination of a sparse set of mesh vertices, making joints a direct outcome of the estimated body mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Technical approach</head><p>The conventional ConvNet-based approach for our task would be to acquire a large amount of color images with 3D shape ground truth and train the network with these inputoutput pairs. However, except for small-scale datasets <ref type="bibr" target="#b21">[22]</ref> or synthetically generated image examples <ref type="bibr" target="#b50">[51]</ref> this type of data is typically unavailable. Therefore, to deal with this task, we need to rethink the typical pipeline. Our main goal is to leverage all the resources we have available and use our insights for the problem to build an effective framework. As a first step, from findings of prior work, we identify that 3D pose can be estimated reliably from 2D pose estimates <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>, while the shape can be inferred from silhouette measurements <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. This observation conveniently decomposes the problem in a) estimation of keypoints and masks from color images and, b) prediction of 3D pose and shape from the 2D evidence. The advantage of this practice is that the framework can be trained without requiring images with 3D shape ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Keypoints and silhouette prediction</head><p>The first step of our framework focuses on 2D keypoint and silhouette estimation. This part is motivated by the availability of large-scale benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref> with 2D joints and mask annotations. Considering the volume and the variability of this data, we leverage it to train a ConvNet for 2D pose and silhouette prediction, that is particularly reliable under various imaging conditions and poses.</p><p>In the past, two individual ConvNets have been used to provide 2D keypoints and masks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. In contrast, for a more elegant solution, we train a single ConvNet, which we denote as Human2D, that generates two outputs, one for keypoints and one for silhouettes. Human2D follows the Stacked Hourglass design <ref type="bibr" target="#b29">[30]</ref>, using two hourglasses, which was found to be a good trade-off between accuracy and running time. The keypoint output is in the form of heatmaps <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b31">32]</ref>, where an MSE loss, L hm , between the ground truth and the predicted heatmaps is used for supervision. The silhouette output has two channels (body and background) and is supervised using a pixelwise binary cross entropy loss, L sil . For training, we combine the two losses: L hg = λL hm +L sil , where λ = 100. This ConvNet falls under the multi-task learning paradigm <ref type="bibr" target="#b33">[34]</ref>. Through  <ref type="figure">Figure 2</ref>. We aim to learn the mapping from silhouettes and keypoints to model parameters, so we can synthesize body model instances and project them to the image plane to simulate the network input. We only require a source to sample pose parameters, and a source to sample body shape parameters. Projections from different viewpoints can also be employed for data augmentation.</p><p>sharing, the two tasks might benefit each other, but multitask learning can also pose certain challenges (e.g., appropriate weighting of the losses), as Kokkinos identifies <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D pose and shape prediction</head><p>The second step is significantly more challenging, requiring estimation of the full body 3D pose and shape from 2D keypoints and silhouettes. Silhouettes and/or keypoints have been used extensively for 3D model fitting through iterative optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref>. Here, we demonstrate that this mapping can also be learned from data while it is possible to get a reliable prediction in a single estimation step.</p><p>For this mapping, we train two network components: (a) the PosePrior, which uses 2D keypoint locations as input together with the confidence of the detections (realised by the maximum value of each heatmap) and estimates the pose coefficients θ, and (b) the ShapePrior, which uses the silhouette as input and estimates the shape coefficients β. In general, the silhouette can be helpful for 3D pose inference <ref type="bibr" target="#b5">[6]</ref> and vice versa <ref type="bibr" target="#b6">[7]</ref>. However, empirically we discovered this disentanglement to provide more stable and accurate 3D predictions, while it also leads to a more modular pipeline (e.g. updating only the PosePrior, without retraining the whole network). Regarding the architecture, the PosePrior uses two bilinear units <ref type="bibr" target="#b26">[27]</ref>, where the input is the 2D keypoint locations and the maximum responses from each heatmap, and the output is the 72 SMPL pose parameters θ. The ShapePrior uses a simple architecture with five 3 × 3 convolutional layers, each one followed by maxpooling, and an additional bilinear unit at the end with 10 outputs, corresponding to the SMPL shape parameters β.</p><p>The form of the input (2D keypoints and masks) and the output (shape and pose parameters) allows us to produce large amount of training data by generating instances of the SMPL model with different 3D pose and shape <ref type="figure">(Figure 2)</ref>. In fact, we can leverage MoCap data (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>) to sample 3D poses, and body scans (e.g., <ref type="bibr" target="#b37">[38]</ref>) to sample body shapes. For the input, we only need to project the 3D model to the image plane (possibly from different viewpoints), and compute silhouettes and 2D keypoint locations to generate input-output pairs for training. This data generation is feasible, exactly because we used an intermediate silhouette and keypoints representation. In contrast, attempting to learn a mapping directly from color images would require generation of synthetic image examples <ref type="bibr" target="#b50">[51]</ref>, which typically do not reach the variability of in-the-wild images.</p><p>In the previous paragraphs, we deliberately avoided discussing the supervision of the Priors networks. Past works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43]</ref> have examined supervision schemes using a typical L 2 loss between the predicted and ground truth parameters. One shortcoming of this naive parameter regression approach, is that different parameters might have effects of different scale on the final reconstruction (e.g., the global body rotation is much more crucial than the local rotation of the hand with respect to the wrist). To avoid handselecting or tuning the supervision for each parameter, we aim for a more global solution. Our approach entails the generation of the full body mesh at training time, where we optimize explicitly for the predicted surface by applying a 3D per-vertex loss. Since the function M(β, θ; Φ) is differentiable, we can backpropagate through it and handle this mesh generator as a typical layer of our network, without any learnable parameters. Given the predicted mesh ver-ticesP i and the corresponding groundturth vertices P i , we can supervise the network with a 3D per-vertex loss:</p><formula xml:id="formula_0">L M = N i=1 P i − P i 2 2 ,<label>(1)</label></formula><p>which considers all the vertices equally and has better correlation with the 3D per-vertex error which is usually employed for evaluation. Alternatively, if the focus is mainly on 3D pose, we can also supervise the network considering only the M relevant 3D joints J i , which are trivially exposed by the model as a sparse linear combination of the mesh vertices. In this case, denoting withĴ i the estimated joints, the corresponding loss can be expressed as:</p><formula xml:id="formula_1">L J = M i=1 Ĵ i − J i 2 2 .<label>(2)</label></formula><p>Empirically, we found that the best training strategy is to initially get a reasonable initialization for the network parameters using an L 2 parameter loss, and then activate also the vertex loss L M (or the joints loss L J if the focus is on pose only), to train a better model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Differentiable renderer</head><p>Our previous analysis relaxed the assumption that images with 3D shape ground truth are available for training and relied on geometric 3D data (MoCap and body scans).</p><p>In some cases though, even this type of data might be unavailable. For example, LSP <ref type="bibr" target="#b18">[19]</ref> has gymnastics or parkour poses which are not represented in typical MoCap. Luckily, our generated 3D mesh has potential to leverage these 2D annotations for training purposes.</p><p>To close the loop, our complete approach includes an additional step that projects the 3D mesh to the image and examines consistency with 2D annotations. In concurrent work, a decoder-type network was used to learn the mapping from SMPL parameters to silhouettes <ref type="bibr" target="#b42">[43]</ref>. However, here we identify that this mapping is known and involves the projection of the 3D mesh to the image, which can be expressed in a differentiable way, without the need to train a network with learnable weights. More specifically, for our implementation, we employ an approximately differentiable renderer, OpenDR <ref type="bibr" target="#b25">[26]</ref>, which projects the mesh and the 3D joints to the image space, and enables backpropagation. The projection operation Π gives rise to: (a) the silhouette Π(P ) =Ŝ, which is represented as a 64 × 64 binary image, and (b) the projected 2D joints Π(Ĵ ) =Ŵ ∈ R M ×2 . In this case, the supervision comes from the comparison of these projections with the annotated silhouettes S, and the 2D keypoints W , using L 2 losses:</p><formula xml:id="formula_2">L Π = µ M i Ŵ i − W i 2 2 + Ŝ − S 2 2 ,<label>(3)</label></formula><p>where µ = 10. The goal of this type of supervision is twofold: (a) it can be employed for end-to-end refinement of the network, using only images with 2D keypoints and/or masks for training, and (b) it can be useful to mildly adapt a generic pose or shape prior to a new setting (e.g., new dataset), where only 2D annotations are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical evaluation</head><p>This section focuses on the empirical evaluation of the proposed approach. First, we present the benchmarks that we employed for quantitative and qualitative evaluation. Then, we provide some essential implementation details of the approach. Finally, quantitative and qualitative results are presented on the selected datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>For the empirical evaluation, we employed two recent benchmarks that provide color images with 3D body shape ground truth, the UP-3D dataset <ref type="bibr" target="#b21">[22]</ref> and the SURREAL dataset <ref type="bibr" target="#b50">[51]</ref>. Additionally, we used the Human3.6M <ref type="bibr" target="#b16">[17]</ref> dataset for further evaluation of the 3D pose accuracy. UP-3D: It is a recent dataset that collects color images from 2D human pose benchmarks, like LSP <ref type="bibr" target="#b18">[19]</ref> and MPII <ref type="bibr" target="#b3">[4]</ref> and uses an extended version of SMPLify <ref type="bibr" target="#b6">[7]</ref> to provide 3D human shape candidates. The candidates were evaluated by human annotators to select only the images with good 3D shape fits. It comprises 8515 images, where 7818 are used for training and 1389 for testing. We report results on this test set, while we also consider subsets, based on the original dataset (LSP, MPII, or FashionPose) of the UP-3D images. Finally, we examine a reduced test set of 139 images, selected by Tan et al. <ref type="bibr" target="#b42">[43]</ref> aiming to limit the range for the global rotation. We report results using the mean per-vertex error, between predicted and ground truth shape. SURREAL: It is a recent dataset which provides synthetic image examples with 3D shape ground truth. The dataset draws poses from MoCap <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> and body shapes from body scans <ref type="bibr" target="#b37">[38]</ref> to generate valid SMPL instances for each image. The synthetic images are not very realistic, but the accurate ground truth, makes it a useful benchmark for evaluation. We report results on the Human3.6M part of the dataset, considering all test videos and keeping every fifth frame of each video to avoid excessive redundancy in the data. Results are reported using the mean per-vertex error. Human3.6M: It is a large-scale indoor dataset that contains multiple subjects performing typical actions like "Eating" and "Walking". We follow the protocol of Bogo et al. <ref type="bibr" target="#b6">[7]</ref> using all videos of subjects S9 and S11 from 'cam3' for evaluation. The original videos are downsampled from 50fps to 10fps to remove redundancy as is done in <ref type="bibr" target="#b21">[22]</ref>. Results are reported using the reconstruction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>The Human2D network is trained on MPII <ref type="bibr" target="#b3">[4]</ref>, LSP <ref type="bibr" target="#b18">[19]</ref> and LSP-extended <ref type="bibr" target="#b19">[20]</ref> data, using the silhouettes from Lassner et al. <ref type="bibr" target="#b21">[22]</ref>. We use a batch size of 4, learning rate set to 3e-4, and rmsprop for the optimization. Augmentation for rotation (±30 • ), scale (0.75-1.25) and flipping (leftright) is used. The training lasts for 1.2M iterations.</p><p>For the Priors networks, we train with a batch size of 256, learning rate set to 3e-4, and using rmsprop for the optimization. Initially, the networks are trained for 40k iterations using an L 2 parameter loss, and then for 60k more iterations using also L M (or L J if we focus on pose only) weighted equally with the parameter loss.</p><p>The end-to-end refinement with the reprojection loss lasts for 2k iterations with a batch size of 4, learning rate set to 8e-5, and using rmsprop for the optimization. To improve training robustness, the end-to-end updates are alternated with individual updates of the Human2D and the Priors networks (as described in the previous two paragraphs). This helps the individual components to maintain their original purpose, while we are also leveraging the strength of end-to-end training to integrate them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Component evaluation</head><p>In this section, we evaluate the components of our approach, using the UP-3D dataset. We train two different versions of our system, where for Priors we leverage data Avg error Data source for Priors UP-3D CMU Parameter loss (axis-angle) 514.9 589.9</p><p>Parameter loss (rot matrix) 140.7 152.2 + Per-vertex loss 120.7 142.0 + Reprojection finetuning 117.7 135.5 <ref type="table">Table 1</ref>. Ablative study on UP-3D, comparing the different supervision forms on the same architecture. The numbers are mean pervertex errors (mm). Two versions of the Priors networks are used, trained with data from UP-3D <ref type="bibr" target="#b21">[22]</ref> and CMU <ref type="bibr" target="#b50">[51]</ref> respectively. All networks are trained for the same number of iterations. Our experiment focuses on the type of supervision. Naively training the Priors networks using an L 2 loss for the θ and β parameters <ref type="bibr" target="#b42">[43]</ref>, keeps the prediction error high as can be seen in <ref type="table">Table 1 (line 1)</ref>. Alternatively, we can transform the θ parameters from axis-angle representation to rotation matrix using the Rodrigues' rotation formula <ref type="bibr" target="#b11">[12]</ref>, and apply an L 2 loss on this representation instead (line 2). This leads to more stable training and better performance, as has also been observed by Lassner et al. <ref type="bibr" target="#b21">[22]</ref>. However, generating the body mesh and further training of the network using our proposed per-vertex supervision (line 3) is even more appropriate and elevates our framework to state-of-the-art performance (see Section 5.4). Finally, the additional end-to-end finetuning with 2D annotations and the reprojection error (line 4) offers a mild refinement to the network. In the UP-3D case, the benefit is small, since the Priors have already observed very similar examples with full 3D ground truth, so 2D annotations become redundant. However, when training the Priors with CMU data, the domain shift, from CMU poses to UP-3D poses is significant, so these 2D annotations offers a clear  <ref type="table">Table 2</ref>. Detailed results on UP-3D <ref type="bibr" target="#b21">[22]</ref>. The numbers are mean per vertex errors (mm), except for the 'Reduced' column where only 91 landmarks <ref type="bibr" target="#b21">[22]</ref> contribute to the error. Our approach outperforms the other baselines across the table. performance benefit. This is an interesting empirical result demonstrating that training with reprojection losses can be useful not only for end-to-end refinement, but it can also assist the network with novel information recovered from 2D annotations. Some qualitative results from UP-3D using our best model are presented in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with state-of-the-art</head><p>UP-3D: We compare with two state-of-the-art direct prediction approaches by Lassner et al. <ref type="bibr" target="#b21">[22]</ref> and Tan et al. <ref type="bibr" target="#b42">[43]</ref>. We do not include the SMPLify method <ref type="bibr" target="#b6">[7]</ref> since a version of this algorithm was used to generate the ground truth for this dataset, so we observed that many estimated reconstructions had only minimal differences from the ground truth.</p><p>For <ref type="bibr" target="#b21">[22]</ref> we use the publicly available code to generate predictions. The complete results are presented in <ref type="table">Table 2</ref>. Our approach outperforms the other two baselines by significant margins. It is interesting to note that a version of <ref type="bibr" target="#b42">[43]</ref>, which uses over 100k images (most of them synthetic) with ground truth pose and shape parameters to directly supervise the network (line 'Direct') is outperformed by our approach which does not have access to this data. Finally, in <ref type="figure" target="#fig_2">Figure 3</ref>, we provide a qualitative comparison with our closest competitor, the direct prediction approach of <ref type="bibr" target="#b21">[22]</ref>. SURREAL: We compare with two state-of-the-art approaches, one based on iterative optimization, SMPLify <ref type="bibr" target="#b6">[7]</ref>, and one based on direct prediction <ref type="bibr" target="#b21">[22]</ref>. We use the publicly available code for both approaches to generate predictions. For our approach, we train the PosePrior using CMU data Avg Lassner et al. <ref type="bibr" target="#b21">[22]</ref> (GT shape) 200. <ref type="bibr" target="#b4">5</ref> Bogo et al. <ref type="bibr" target="#b6">[7]</ref> (GT shape) 177.2 Ours (GT shape) 151.5</p><p>Bogo et al. <ref type="bibr" target="#b6">[7]</ref> 202.0 Ours 155.5 <ref type="table">Table 3</ref>. Detailed results on the Human3.6M part of SUR-REAL <ref type="bibr" target="#b50">[51]</ref>. Numbers are mean per vertex errors (mm). "GT shape" indicates that the shape coefficients are known.  <ref type="table" target="#tab_2">Table 4</ref>. Detailed results on Human3.6M <ref type="bibr" target="#b16">[17]</ref>. Numbers are reconstruction errors (mm). The numbers are taken from the respective papers, except for (*), which were obtained from <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg</head><p>which we found to be more general than UP-3D. Also, we train two ShapePriors, for female and male subjects respectively, since the gender is known for this dataset. We emphasize that the testing was conducted on the Human3.6M part of the dataset to avoid any overlap with the training of the different methods (in terms of images or priors). The complete results are presented in <ref type="table">Table 3</ref>. Since Lassner et al. <ref type="bibr" target="#b21">[22]</ref> provide only a non gender-specific model for shape, we also report results considering only the pose estimates, and assuming known shape parameters. Our approach outperforms the other two baselines. For this dataset we observed that because of the challenging color images (low illumination, out-of-context backgrounds, etc), the 2D detections where more noisy than usual, providing some hard failures for the iterative optimization approach <ref type="bibr" target="#b6">[7]</ref>. In contrast, our approach was more resistant to these noisy cases recovering a coherent 3D shape in most cases. Human3.6M: Finally, for Human3.6M we evaluate only the estimated 3D pose, since there is no body shape ground truth available. Our network is the same as before (Priors trained on CMU), although, we use the 3D joints error for supervision (equation 2), since the focus is on pose. Among others, we compare with the SMPLify method <ref type="bibr" target="#b6">[7]</ref> and the direct prediction approach of Lassner et al. <ref type="bibr" target="#b21">[22]</ref>. Similarly to the other approaches we compare with, we do not use any data from this dataset for training. The detailed results are presented in  <ref type="table">Table 5</ref>. Accuracy and f1 scores for foreground-background and six-part segmentation on LSP test set for different versions of SMPLify. Using our direct prediction as an anchor improves vanilla SMPLify, while also achieving a 3x speedup. The numbers for the first and third rows are taken from <ref type="bibr" target="#b21">[22]</ref>. sults on Human3.6M (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>), but they do so only by leveraging the training data of this dataset for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Boosting SMPLify</head><p>In the previous section, we validated that our direct prediction approach can achieve state-of-the-art results with a single prediction step. However, we aspire our method to have greater applicability, by being complementary to iterative optimization solutions. In fact, here we demonstrate that our direct predictions can be a useful initialization and provide a reliable anchor for the SMPLify approach <ref type="bibr" target="#b6">[7]</ref>.</p><p>To keep it simple, we make only minor modifications to the SMPLify optimization. First, we use our predicted pose as an initialization, instead of the typical mean pose. Additionally, we avoid the hierarchical four-step optimization, and we limit the whole procedure in a single step. The reason for the multi-stage optimization is to explore the pose space and get a roughly correct pose estimate. However, using our predicted pose as initialization makes this search unnecessary, so we require only the last step of the previously complex optimization scheme. Finally, we add one more data term to the optimization: E anchor (θ) = i ρ(θ i − θ init i ), to avoid deviations from our predicted, anchor pose. Similarly to <ref type="bibr" target="#b6">[7]</ref>, we use the Geman-McClure penalty function, ρ <ref type="bibr" target="#b12">[13]</ref>, for the optimization. This anchoring, does not typically have effect on the quality of the output, but it can accelerate the convergence. We can also use the shape parameters as anchor, but we observed that pose had greater effect than shape on the optimization.</p><p>For our evaluation, we use the public implementation of SMPLify and we run the original code, as well as our anchored version, on the LSP test set. The anchored version is three times faster on average than vanilla SMPLify. More importantly, this speedup comes also with a quantitative performance benefit. In <ref type="table">Table 5</ref> we present the segmentation accuracy of different SMPLify versions, by projecting the 3D shape estimate on the image. To demonstrate that the performance benefit of our anchored version is non-trivial, we report the results for running SMPLify on the ground truth 2D joints and silhouettes. Improved fits from the anchored version are presented in <ref type="figure" target="#fig_4">Figure 5</ref>. These results validate the additional benefit of our direct prediction approach, since it can also enhance current pipelines that rely on iterative optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Running time</head><p>Our approach requires a single forward pass from the ConvNet to estimate the full body 3D human pose and shape. This translates to only 50ms on a Titan X GPU. In comparison, SMPLify <ref type="bibr" target="#b6">[7]</ref> report roughly 1 minute for the optimization, while the publicly available (unoptimized) code runs on 3 minutes per image on average. When the number of landmarks increases to 91, Lassner et al. <ref type="bibr" target="#b21">[22]</ref> report that the SMPLify optimization can get two times slower. This makes our direct prediction approach more than three orders of magnitude faster than the state-of-theart iterative optimization approaches. Regarding other direct prediction approaches, Lassner et al. <ref type="bibr" target="#b21">[22]</ref> reports runtime of 378ms, but we demonstrate significantly better performance with our end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary</head><p>The goal of this paper was to present a viable ConvNetbased approach to predict 3D human pose and shape from a single color image. A central part of our solution was the incorporation of a body shape model, SMPL, in the endto-end framework. Through this inclusion we enabled: a) prediction of the parameters from 2D keypoints and silhouettes, b) generation of the full body 3D mesh at training time using supervision for the surface with a per-vertex loss, and c) integration of a differentiable renderer for further end-to-end refinement using 2D annotations. Our approach achieved state-of-the-art results on relevant benchmarks, outperforming previous direct prediction and optimizationbased solutions for 3D pose and shape prediction. Finally, considering the efficiency of our approach, we demonstrated its potential to accelerate and improve typical iterative optimization pipelines.</p><p>Project Page: https://www.seas.upenn.edu/˜pavlakos/ projects/humanshape</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Training on real images (b) Training on human shape instances (c) End-to-end finetuning on real images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Successful 3D pose and shape predictions of our approach on challenging examples of UP-3D. either from UP-3D (provided by Lassner et al. [22]), or from CMU MoCap (provided by Varol et al. [51]). The Human2D network remains the same in both cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples from UP-3D where our approach (blue shapes) performs significantly better than the direct prediction method of Lassner et al.<ref type="bibr" target="#b21">[22]</ref> (pink shapes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>LSP examples with improved SMPLify fits (right side of each image) when our direct prediction is used as an initialization and anchor for the iterative optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Our approach again outperforms the other baselines. Some works have reported better results re-88.07 87.71 63.98 SMPLify + our anchor 92.17 88.38 88.24 64.62</figDesc><table><row><cell cols="2">FB Seg.</cell><cell cols="2">Part Seg.</cell></row><row><cell>acc.</cell><cell>f1</cell><cell>acc.</cell><cell>f1</cell></row><row><cell cols="4">SMPLify 91.89 SMPLify on GT 92.17 88.23 88.82 67.03</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We gratefully appreciate support through the following grants: NSF-IIP-1439681 (I/UCRC), ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, DARPA FLA program and NSF/IUCRC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CMU Graphics Lab Motion Capture Database</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optical flow-based 3D human motion estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kassubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SCAPE: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Bȃlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">HS-Nets: Estimating human body shape from silhouettes with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human shape from silhouettes using generative HKS descriptors and cross-modal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shape from selfies: Human body shape estimation using cca regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A compact formula for the derivative of a 3-D rotation in exponential coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="384" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical methods for tomographic image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the International Statistical Institute</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Bȃlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model-based vision: a program to see a walking person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards accurate markerless human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Classner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for human 3D pose consistent with 2D joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UberNet: Training a &apos;universal&apos; convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OpenDR: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2D and 3D human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reconstructing 3D human pose from 2D image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">General automatic human shape and motion capture using volumetric contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Civilian american and european surface anthropometry resource (caesar), final report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Robinette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fleming</surname></persName>
		</author>
		<idno>AFRL-HE-WP-TR-2002-0169</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>US Air Force Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MoCap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to fuse 2D and 3D image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sparse inertial poser: Automatic 3D human pose estimation from sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Home 3D body scans from noisy image and range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02136</idno>
		<title level="m">MonoPerfCap: Human performance capture from monocular video</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Sparse representation for 3D shape estimation: A convex relaxation approach. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
