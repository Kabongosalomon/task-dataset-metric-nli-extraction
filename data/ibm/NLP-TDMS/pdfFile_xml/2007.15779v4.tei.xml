<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lucas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Additional Key Words and Phrases: Biomedical</term>
					<term>NLP</term>
					<term>Domain-specific pretraining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In natural language processing (NLP), pretraining large neural language models on unlabeled text has proven to be a successful strategy for transfer learning. A prime example is Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b15">[16]</ref>, which has become a standard building block for training task-specific NLP models. Existing pretraining work typically focuses on the newswire and Web domains. For example, the original BERT model was trained on Wikipedia 1 and BookCorpus <ref type="bibr" target="#b62">[62]</ref>, and subsequent efforts have focused on crawling additional text from the Web to power even larger-scale pretraining <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b50">50]</ref>.</p><p>In specialized domains like biomedicine, past work has shown that using in-domain text can provide additional gains over general-domain language models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">45]</ref>. However, a prevailing assumption is that out-domain text is still helpful and previous work typically adopts a mixed-domain approach, e.g., by starting domain-specific pretraining from an existing general-domain language model <ref type="figure">(Figure 1</ref> top). In this paper, we question this assumption. We observe that mixed-domain pretraining such as continual pretraining can be viewed as a form of transfer learning in itself, where the source domain is general text, such as newswire and the Web, and the target domain is specialized text such as biomedical papers. Based on the rich literature of multi-task learning and transfer learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b59">59]</ref>, successful transfer learning occurs when the target data is scarce and the source domain is highly relevant to the target one. For domains with abundant unlabeled text such as biomedicine, it is unclear that domain-specific pretraining can benefit by transfer from general domains. In fact, the majority of Domain-Specific Pretraining from Scratch Text Source <ref type="figure">Fig. 1</ref>. Two paradigms for neural language model pretraining. Top: The prevailing mixed-domain paradigm assumes that out-domain text is still helpful and typically initializes domain-specific pretraining with a general-domain language model and inherits its vocabulary. Bottom: Domain-specific pretraining from scratch derives the vocabulary and conducts pretraining using solely in-domain text. In this paper, we show that for domains with abundant text such as biomedicine, domain-specific pretraining from scratch can substantially outperform the conventional mixed-domain approach. general domain text is substantively different from biomedical text, raising the prospect of negative transfer that actually hinders the target performance.</p><p>We thus set out to conduct a rigorous study on domain-specific pretraining and its impact on downstream applications, using biomedicine as a running example. We show that domain-specific pretraining from scratch substantially outperforms continual pretraining of generic language models, thus demonstrating that the prevailing assumption in support of mixed-domain pretraining is not always applicable <ref type="figure">(Figure 1</ref>).</p><p>To facilitate this study, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets, and conduct in-depth comparisons of modeling choices for pretraining and task-specific fine-tuning by their impact on domain-specific applications. Our experiments show that domain-specific pretraining from scratch can provide a solid foundation for biomedical NLP, leading to new state-of-the-art performance across a wide range of tasks. Additionally, we discover that the use of transformer-based models, like BERT, necessitates rethinking several common practices. For example, BIO tags and more complex variants are the standard label representation for named entity recognition (NER). However, we find that simply using IO (in or out of entity mentions) suffices with BERT models, leading to comparable or better performance.</p><p>To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and taskspecific models for the community, and created a leaderboard featuring our comprehensive benchmark at https://aka.ms/BLURB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS 2.1 Language Model Pretraining</head><p>In this section, we provide a brief overview of neural language model pretraining, using BERT <ref type="bibr" target="#b15">[16]</ref> as a running example.</p><p>2.1.1 Vocabulary. We assume that the input consists of text spans, such as sentences separated by special tokens <ref type="bibr">[SEP]</ref>. To address the problem of out-of-vocabulary words, neural language models generate a vocabulary from subword units, using Byte-Pair Encoding (BPE) <ref type="bibr" target="#b51">[51]</ref> or variants such as WordPiece <ref type="bibr" target="#b31">[32]</ref>. Essentially, the BPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in the given corpus. It does this by first shattering all words in the corpus and initializing the vocabulary with characters and delimiters. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus and can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size (e.g., <ref type="bibr" target="#b29">30</ref>,000 in standard BERT models or 50,000 in RoBERTa <ref type="bibr" target="#b38">[39]</ref>). In this paper, we use the WordPiece algorithm which is a BPE variant that uses likelihood based on the unigram language model rather than frequency in choosing which subwords to concatenate. The text corpus and vocabulary may preserve the case (cased) or convert all characters to lower case (uncased).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Model</head><p>Architecture. State-of-the-art neural language models are generally based on transformer architectures <ref type="bibr" target="#b55">[55]</ref>, following the recent success of BERT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. The transformer model introduces a multi-layer, multi-head self-attention mechanism, which has demonstrated superiority in leveraging GPU-based parallel computation and modeling long-range dependencies in texts, compared to recurrent neural networks, such as LSTMs <ref type="bibr" target="#b21">[22]</ref>. The input token sequence is first processed by a lexical encoder, which combines a token embedding, a (token) position embedding and a segment embedding (i.e., which text span the token belongs to) by element-wise summation. This embedding layer is then passed to multiple layers of transformer modules <ref type="bibr" target="#b55">[55]</ref>. In each transformer layer, a contextual representation is generated for each token by summing a non-linear transformation of the representations of all tokens in the prior layer, weighted by the attentions computed using the given token's representation in the prior layer as the query. The final layer outputs contextual representations for all tokens, which combine information from the whole text span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Self-Supervision.</head><p>A key innovation in BERT <ref type="bibr" target="#b15">[16]</ref> is the use of a Masked Language Model (MLM) for self-supervised pretraining. Traditional language models are typically generative models that predict the next token based on the preceding tokens; for example, n-gram models represent the conditional probability of the next token by a multinomial of the preceding n-gram, with various smoothing strategies to handle rare occurrences <ref type="bibr" target="#b43">[43]</ref>. Masked Language Model instead randomly replaces a subset of tokens by a special token (e.g., [MASK]), and asks the language model to predict them. The training objective is the cross-entropy loss between the original tokens and the predicted ones. In BERT and RoBERTa, 15% of the input tokens are chosen, among which a random 80% are replaced by [MASK], 10% are left unchanged and 10% are randomly replaced by a token from the vocabulary. Instead of using a constant masking rate of 15%, a standard approach is to gradually increase it from 5% to 25% with 5% increment for every 20% of training epochs, which makes pretraining more stable <ref type="bibr" target="#b36">[37]</ref>. The original BERT algorithm also uses Next Sentence Prediction (NSP), which determines for a given sentence pair whether one sentence follows the other in the original text. The utility of NSP has been called into question <ref type="bibr" target="#b38">[39]</ref>, but we include it in our pretraining experiments to enable a head-to-head comparison with prior BERT models.</p><p>2.1.4 Advanced Pretraining Techniques. In the original formulation of BERT <ref type="bibr" target="#b15">[16]</ref>, the masked language model (MLM) simply selects random subwords to mask. When a word is only partially masked, it is relatively easy to predict the masked portion given the observed ones. In contrast, whole-word masking (WWM) enforces that the whole word must be masked if one of its subwords is chosen. This has been adopted as the standard approach because it forces the language model to capture more contextual semantic dependencies.</p><p>In this paper, we also explore adversarial pretraining and its impact on downstream applications. Motivated by successes in countering adversarial attacks in computer vision, adversarial pretraining introduces perturbations in the input embedding layer that maximize the adversarial loss, thus forcing the model to not only optimize the standard training objective (MLM), but also minimize adversarial loss <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Biomedical Language Model Pretraining</head><p>In this paper, we will use biomedicine as a running example in our study of domain-specific pretraining. In other words, biomedical text is considered in-domain, while others are regarded as out-domain. Intuitively, using in-domain text in pretraining should help with domain-specific applications. Indeed, prior work has shown that pretraining with PubMed text leads to better performance in biomedical NLP tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">45]</ref>. The main question is whether pretraining should include text from other domains. The prevailing assumption is that pretraining can always benefit from more text, including out-domain text. In fact, none of the prior biomedical-related BERT models have been pretrained using purely biomedical text <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">45]</ref>. Here, we challenge this assumption and show that domain-specific pretraining from scratch can be superior to mixed-domain pretraining for downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.1</head><p>Mixed-Domain Pretraining. The standard approach to pretraining a biomedical BERT model conducts continual pretraining of a general-domain pretrained model, as exemplified by BioBERT <ref type="bibr" target="#b33">[34]</ref>. Specifically, this approach would initialize with the standard BERT model <ref type="bibr" target="#b15">[16]</ref>, pretrained using Wikipedia and BookCorpus. It then continues the pretraining process with MLM and NSP using biomedical text. In the case of BioBERT, continual pretraining is conducted using PubMed abstracts and PubMed Central full text articles. BlueBERT <ref type="bibr" target="#b45">[45]</ref> uses both PubMed text and de-identified clinical notes from MIMIC-III <ref type="bibr" target="#b25">[26]</ref>.</p><p>Note that in the continual pretraining approach, the vocabulary is the same as the original BERT model, in this case the one generated from Wikipedia and BookCorpus. While convenient, this is a major disadvantage for this approach, as the vocabulary is not representative of the target biomedical domain.</p><p>Compared to the other biomedical-related pretraining efforts, SciBERT <ref type="bibr" target="#b7">[8]</ref> is a notable exception as it generates the vocabulary and pretrains from scratch, using biomedicine and computer science as representatives for scientific literature. However, from the perspective of biomedical applications, SciBERT still adopts the mixed-domain pretraining approach, as computer science text is clearly out-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Domain-Specific</head><p>Pretraining from Scratch. The mixed-domain pretraining approach makes sense if the target application domain has little text of its own, and can thereby benefit from pretraining using related domains. However, this is not the case for biomedicine, which has over thirty million abstracts in PubMed, and adds over a million each year. We thus hypothesize that domain-specific pretraining from scratch is a better strategy for biomedical language model pretraining.</p><p>A major advantage of domain-specific pretraining from scratch stems from having an in-domain vocabulary. <ref type="table">Table 1</ref> compares the vocabularies used in various pretraining strategies. BERT models using continual pretraining are stuck with the original vocabulary from the general-domain corpora, which does not contain many common biomedical terms. Even for SciBERT, which generates its vocabulary partially from biomedical text, the deficiency <ref type="table">Table 1</ref>. Comparison of common biomedical terms in vocabularies used by the standard BERT, SciBERT and PubMedBERT (ours). A ✓ indicates the biomedical term appears in the corresponding vocabulary, otherwise the term will be broken into word pieces as separated by hyphen. These word pieces often have no biomedical relevance and may hinder learning in downstream tasks.</p><formula xml:id="formula_0">Biomedical Term Category BERT SciBERT PubMedBERT (Ours) diabetes disease ✓ ✓ ✓ leukemia disease ✓ ✓ ✓ lithium drug ✓ ✓ ✓ insulin drug ✓ ✓ ✓ DNA gene ✓ ✓ ✓ promoter gene ✓ ✓ ✓ hypertension disease hyper-tension ✓ ✓ nephropathy disease ne-ph-rop-athy ✓ ✓ lymphoma disease l-ym-ph-oma ✓ ✓ lidocaine drug lid-oca-ine] ✓ ✓ oropharyngeal organ oro-pha-ryn-ge-al or-opharyngeal ✓ cardiomyocyte cell card-iom-yo-cy-te cardiomy-ocyte ✓ chloramphenicol drug ch-lor-amp-hen-ico-l chlor-amp-hen-icol ✓ RecA gene Rec-A Rec-A ✓ acetyltransferase gene ace-ty-lt-ran-sf-eras-e acetyl-transferase ✓ clonidine drug cl-oni-dine clon-idine ✓ naloxone drug na-lo-xon-e nal-oxo-ne ✓</formula><p>compared to a purely biomedical vocabulary is substantial. As a result, standard BERT models are forced to divert parametrization capacity and training bandwidth to model biomedical terms using fragmented subwords. For example, naloxone, a common medical term, is divided into four pieces ([na, ##lo, ##xon, ##e]) by BERT, and acetyltransferase is shattered into seven pieces ([ace, ##ty, ##lt, ##ran, ##sf, ##eras, ##e]) by BERT. <ref type="bibr" target="#b1">2</ref> Both terms appear in the vocabulary of PubMedBERT. Another advantage of domain-specific pretraining from scratch is that the language model is trained using purely in-domain data. For example, SciBERT pretraining has to balance optimizing for biomedical text and computer science text, the latter of which is unlikely to be beneficial for biomedical applications. Continual pretraining, on the other hand, may potentially recover from out-domain modeling, though not completely. Aside from the vocabulary issue mentioned earlier, neural network training uses non-convex optimization, which means that continual pretraining may not be able to completely undo suboptimal initialization from the general-domain language model.</p><p>In our experiments, we show that domain-specific pretraining with in-domain vocabulary confers clear advantages over mixed-domain pretraining, be it continual pretraining of general-domain language models, or pretraining on mixed-domain text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BLURB: A Comprehensive Benchmark for Biomedical NLP</head><p>The ultimate goal of language model pretraining is to improve performance on a wide range of downstream applications. In general-domain NLP, the creation of comprehensive benchmarks, such as GLUE <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref>, greatly</p><p>BioBERT <ref type="bibr" target="#b33">[34]</ref> SciBERT <ref type="bibr" target="#b7">[8]</ref> BLUE <ref type="bibr" target="#b45">[45]</ref> BLURB BC5-chem <ref type="bibr" target="#b34">[35]</ref> ✓ accelerates advances in language model pretraining by enabling head-to-head comparisons among pretrained language models. In contrast, prior work on biomedical pretraining tends to use different tasks and datasets for downstream evaluation, as shown in <ref type="table" target="#tab_0">Table 2</ref>. This makes it hard to assess the impact of pretrained language models on the downstream tasks we care about. To the best of our knowledge, BLUE <ref type="bibr" target="#b45">[45]</ref> is the first attempt to create an NLP benchmark in the biomedical domain. We aim to improve on its design by addressing some of its limitations. First, BLUE has limited coverage of biomedical applications used in other recent work on biomedical language models, as shown in <ref type="table" target="#tab_0">Table 2</ref>. For example, it does not include any question-answering task. More importantly, BLUE mixes PubMed-based biomedical applications (six datasets such as BC5, ChemProt, and HoC) with MIMIC-based clinical applications (four datasets such as i2b2 and MedNLI). Clinical notes differ substantially from biomedical literature, to the extent that we observe BERT models pretrained on clinical notes perform poorly on biomedical tasks, similar to the standard BERT. Consequently, it is advantageous to create separate benchmarks for these two domains.</p><formula xml:id="formula_1">✓ ✓ ✓ BC5-disease [35] ✓ ✓ ✓ ✓ NCBI-disease [18] ✓ ✓ - ✓ BC2GM [53] ✓ - - ✓ JNLPBA [27] ✓ - - ✓ EBM PICO [44] - ✓ - ✓ ChemProt [31] ✓ ✓ ✓ ✓ DDI [21] ✓ - ✓ ✓ GAD [11] ✓ - - ✓ BIOSSES [54] - - ✓ ✓ HoC [20] - - ✓ ✓ PubMedQA [25] - - - ✓ BioASQ [42] ✓ - - ✓</formula><p>To facilitate investigations of biomedical language model pretraining and help accelerate progress in biomedical NLP, we create a new benchmark, the Biomedical Language Understanding &amp; Reasoning Benchmark (BLURB). We focus on PubMed-based biomedical applications, and leave the exploration of the clinical domain, and other high-value verticals to future work. To make our effort tractable and facilitate head-to-head comparison with prior work, we prioritize the selection of datasets used in recent work on biomedical language models, and will explore the addition of other datasets in future work.</p><p>BLURB is comprised of a comprehensive set of biomedical NLP tasks from publicly available datasets, including named entity recognition (NER), evidence-based medical information extraction (PICO), relation extraction, sentence similarity, document classification, and question answering. See <ref type="table">Table 3</ref> for an overview of the BLURB datasets. For question answering, prior work has considered both classification tasks (e.g., whether a reference text contains the answer to a given question) and more complex tasks such as list and summary <ref type="bibr" target="#b42">[42]</ref>. The latter types often require additional engineering effort that are not relevant to evaluating neural language models. For simplicity, we focus on the classification tasks such as yes/no question-answering in BLURB, and leave the inclusion of more complex question-answering to future work.  <ref type="table">Table 3</ref>. Datasets used in the BLURB biomedical NLP benchmark. We list the numbers of instances in train, dev, and test (e.g., entity mentions in NER and PICO elements in evidence-based medical information extraction).</p><p>To compute a summary score for BLURB, the simplest way is to report the average score among all tasks. However, this may place undue emphasis on simpler tasks such as NER for which there are many existing datasets. Therefore, we group the datasets by their task types, compute the average score for each task type, and report the macro average among the task types. To help accelerate research in biomedical NLP, we release the BLURB benchmark as well as a leaderboard at http://aka.ms/BLURB.</p><p>Below are detailed descriptions for each task and corresponding datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Named Entity Recognition (NER).</head><p>BC5-Chemical &amp; BC5-Disease. The BioCreative V Chemical-Disease Relation corpus <ref type="bibr" target="#b34">[35]</ref> was created for evaluating relation extraction of drug-disease interactions, but is frequently used as a NER corpus for detecting chemical (drug) and disease entities. The dataset consists of 1500 PubMed abstracts broken into three even splits for training, development, and test. We use a pre-processed version of this dataset generated by Crichton et al. <ref type="bibr" target="#b13">[14]</ref>, discard the relation labels, and train NER models for chemical (BC5-Chemical) and disease (BC5-Disease) separately.</p><p>NCBI-Disease. The Natural Center for Biotechnology Information Disease corpus <ref type="bibr" target="#b17">[18]</ref> contains 793 PubMed abstracts with 6892 annotated disease mentions linked to 790 distinct disease entities. We use a pre-processed set of train, development, test splits generated by Crichton et al. <ref type="bibr" target="#b13">[14]</ref>.</p><p>BC2GM. The Biocreative II Gene Mention corpus <ref type="bibr" target="#b53">[53]</ref> consists of sentences from PubMed abstracts with manually labeled gene and alternative gene entities. Following prior work, we focus on the gene entity annotation. In its original form, BC2GM contains 15000 train and 5000 test sentences. We use a pre-processed version of the dataset generated by Crichton et al. <ref type="bibr" target="#b13">[14]</ref>, which carves out 2500 sentences from the training data for development.</p><p>JNLPBA. The Joint Workshop on Natural Language Processing in Biomedicine and its Applications shared task <ref type="bibr" target="#b26">[27]</ref> is a NER corpus on PubMed abstracts. The entity types are chosen for molecular biology applications: protein, DNA, RNA, cell line, and cell type. Some of the entity type distinctions are not very meaningful. For example, a gene mention often refers to both the DNA and gene products such as the RNA and protein. Following prior work that evaluates on this dataset <ref type="bibr" target="#b33">[34]</ref>, we ignore the type distinction and focus on detecting the entity mentions. We use the same train, development, and test splits as in Crichton et al. <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Evidence-Based Medical Information Extraction (PICO).</head><p>EBM PICO. The Evidence-Based Medicine corpus <ref type="bibr" target="#b44">[44]</ref> contains PubMed abstracts on clinical trials, where each abstract is annotated with P, I, and O in PICO: Participants (e.g., diabetic patients), Intervention (e.g., insulin), Comparator (e.g., placebo) and Outcome (e.g., blood glucose levels). Comparator (C) labels are omitted as they are standard in clinical trials: placebo for passive control and standard of care for active control. There are 4300, 500, and 200 abstracts in training, development, and test, respectively. The training and development sets were labeled by Amazon Mechanical Turkers, whereas the test set was labeled by Upwork contributors with prior medical training. EBM PICO provides labels at the word level for each PIO element. For each of the PIO elements in an abstract, we tally the F1 score at the word level, and then compute the final score as the average among PIO elements in the dataset. Occasionally, two PICO elements might overlap with each other (e.g., a participant span might contain within it an intervention span). In EBM-PICO, about 3% of the PIO words are in the overlap. Note that the dataset released along with SciBERT appears to remove the overlapping words from the larger span (e.g., the participant span as mentioned above). We instead use the original dataset <ref type="bibr" target="#b44">[44]</ref> and their scripts for preprocessing and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Relation Extraction.</head><p>ChemProt. The Chemical Protein Interaction corpus <ref type="bibr" target="#b30">[31]</ref> consists of PubMed abstracts annotated with chemicalprotein interactions between chemical and protein entities. There are 23 interactions organized in a hierarchy, with 10 high-level interactions (including NONE). The vast majority of relation instances in ChemProt are within single sentences. Following prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref>, we only consider sentence-level instances. We follow the ChemProt authors' suggestions and focus on classifying five high-level interactions -UPREGULATOR (CPR : 3), DOWNREGULATOR (CPR : 4), AGONIST (CPR : 5), ANTAGONIST (CPR : 6), SUBSTRATE (CPR : 9) -as well as everything else (false). The ChemProt annotation is not exhaustive for all chemical-protein pairs. Following previous work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">45]</ref>, we expand the training and development sets by assigning a false label for all chemical-protein pairs that occur in a training or development sentence, but do not have an explicit label in the ChemProt corpus. Note that prior work uses slightly different label expansion of the test data. To facilitate head-to-head comparison, we will provide instructions for reproducing the test set in BLURB from the original dataset.</p><p>DDI. The Drug-Drug Interaction corpus <ref type="bibr" target="#b20">[21]</ref> was created to facilitate research on pharmaceutical information extraction, with a particular focus on pharmacovigilance. It contains sentence-level annotation of drug-drug interactions on PubMed abstracts. Note that some prior work <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b61">61]</ref> discarded 90 training files that the authors considered not conducive to learning drug-drug interactions. We instead use the original dataset and produce our train/dev/test split of 624/90/191 files.</p><p>GAD. The Genetic Association Database corpus <ref type="bibr" target="#b10">[11]</ref> was created semi-automatically using the Genetic Association Archive. <ref type="bibr" target="#b2">3</ref> Specifically, the archive contains a list of gene-disease associations, with the corresponding sentences in the PubMed abstracts reporting the association studies. Bravo et al. <ref type="bibr" target="#b10">[11]</ref> used a biomedical NER tool to identify gene and disease mentions, and create the positive examples from the annotated sentences in the archive, and negative examples from gene-disease co-occurrences that were not annotated in the archive. We use an existing preprocessed version of GAD and its corresponding train/dev/test split created by Lee et al. <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Sentence Similarity.</head><p>BIOSSES. The Sentence Similarity Estimation System for the Biomedical Domain <ref type="bibr" target="#b54">[54]</ref> contains 100 pairs of PubMed sentences each of which is annotated by five expert-level annotators with an estimated similarity score in the range from 0 (no relation) to 4 (equivalent meanings). It is a regression task, with the average score as the final annotation. We use the same train/dev/test split in Peng et al. <ref type="bibr" target="#b45">[45]</ref> and use Pearson correlation for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Document Classification.</head><p>HoC. The Hallmarks of Cancer corpus was motivated by the pioneering work on cancer hallmarks <ref type="bibr" target="#b19">[20]</ref>. It contains annotation on PubMed abstracts with binary labels each of which signifies the discussion of a specific cancer hallmark. The authors use 37 fine-grained hallmarks which are grouped into ten top-level ones. We focus on predicting the top-level labels. The dataset was released with 1499 PubMed abstracts <ref type="bibr" target="#b5">[6]</ref> and has since been expanded to 1852 abstracts <ref type="bibr" target="#b4">[5]</ref>. Note that Peng et al. <ref type="bibr" target="#b45">[45]</ref> discarded a control subset of 272 abstracts that do not discuss any cancer hallmark (i.e., all binary labels are false). We instead adopt the original dataset and report micro F1 across the ten cancer hallmarks. Though the original dataset provided sentence level annotation, we follow the common practice and evaluate on the abstract level <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b60">60]</ref>. We create the train/dev/test split, as they are not available previously. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Question Answering (QA).</head><p>PubMedQA. The PubMedQA dataset <ref type="bibr" target="#b24">[25]</ref> contains a set of research questions, each with a reference text from a PubMed abstract as well as an annotated label of whether the text contains the answer to the research question (yes/maybe/no). We use the original train/dev/test split with 450, 50, and 500 questions, respectively.</p><p>BioASQ. The BioASQ corpus <ref type="bibr" target="#b42">[42]</ref> contains multiple question answering tasks annotated by biomedical experts, including yes/no, factoid, list, and summary questions. Pertaining to our objective of comparing neural language models, we focus on the the yes/no questions (Task 7b), and leave the inclusion of other tasks to future work. Each question is paired with a reference text containing multiple sentences from a PubMed abstract and a yes/no answer. We use the official train/dev/test split of 670/75/140 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Task-Specific Fine-Tuning</head><p>Pretrained neural language models provide a unifying foundation for learning task-specific models. Given an input token sequence, the language model produces a sequence of vectors in the contextual representation. A task-specific prediction model is then layered on top to generate the final output for a task-specific application. Given task-specific training data, we can learn the task-specific model parameters and refine the BERT model parameters by gradient descent using backpropragation.</p><p>Prior work on biomedical NLP often adopts different task-specific models and fine-tuning methods, which makes it difficult to understand the impact of an underlying pretrained language model on task performance. In this section, we review standard methods and common variants used for each task. In our primary investigation comparing pretraining strategies, we fix the task-specific model architecture using the standard method identifed here, to facilitate a head-to-head comparison among the pretrained neural language models. Subsequently, we start with the same pretrained BERT model, and conduct additional investigation on the impact for the various choices in the task-specific models. For prior biomedical BERT models, our standard task-specific methods generally lead to comparable or better performance when compared to their published results.</p><p>Neural Language Model (e.g., BERT) Transform Input (e.g., replace entities by dummy tokens)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>mutation is a mediator of resistance … … $ mutation is a mediator of $ resistance …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Representation</head><p>Preprocessing Featurizer (e.g. concatenation of entity vectors) Predict <ref type="figure">Fig. 2</ref>. A general architecture for task-specific fine-tuning of neural language models, with a relation-extraction example. Note that the input goes through additional processing such as word-piece tokenization in the neural language model module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">A General</head><p>Architecture for Fine-Tuning Neural Language Models. <ref type="figure">Figure 2</ref> shows a general architecture of fine-tuning neural language models for downstream applications. An input instance is first processed by a TransformInput module which performs task-specific transformations such as appending special instance marker (e.g., [CLS]) or dummifying entity mentions for relation extraction. The transformed input is then tokenized using the neural language model's vocabulary, and fed into the neural language model. Next, the contextual representation at the top layer is processed by a Featurizer module, and then fed into the Predict module to generate the final output for a given task.</p><p>To facilitate a head-to-head comparison, we apply the same fine-tuning procedure for all BERT models and tasks. Specifically, we use cross-entropy loss for classification tasks and mean-square error for regression tasks. We conduct hyperparameter search using the development set based on task-specific metrics. Similar to previous work, we jointly fine-tune the parameters of the task-specific prediction layer as well as the underlying neural language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Task-Specific Problem Formulation and Modeling</head><p>Choices. Many NLP applications can be formulated as a classification or regression task, wherein either individual tokens or sequences are the prediction target. Modeling choices usually vary in two aspects: the instance representation and the prediction layer. <ref type="table" target="#tab_3">Table 4</ref> presents an overview of the problem formulation and modeling choices for tasks we consider and detailed descriptions are provided below. For each task, we highlight the standard modeling choices with an asterisk (*).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NER.</head><p>Given an input text span (usually a sentence), the NER task seeks to recognize mentions of entities of interest. It is typically formulated as a sequential labeling task, where each token is assigned a tag to signify whether it is in an entity mention or not. The modeling choices primarily vary on the tagging scheme and classification method. BIO is the standard tagging scheme that classifies each token as the beginning of an entity (B), inside an entity (I), or outside (O). The NER tasks in BLURB are only concerned about one entity type  (in JNLPBA, all the types are merged into one). In the case when there are multiple entity types, the BI tags would be further divided into fine-grained tags for specific types. Prior work has also considered more complex tagging schemes such as BIOUL, where U stands for the last word of an entity and L stands for a single-word entity. We also consider the simpler IO scheme that only differentiates between in and out of an entity. Classification is done using a simple linear layer or more sophisticated sequential labeling methods such as LSTM or conditional random field (CRF) <ref type="bibr" target="#b32">[33]</ref>.</p><p>• TransformInput: returns the input sequence as is.</p><p>• Featurizer: returns the BERT encoding of a given token.</p><p>• Tagging scheme: BIO*; BIOUL; IO.</p><p>• Classification layer: linear layer*; LSTM; CRF.</p><p>PICO. Conceptually, evidence-based medical information extraction is akin to slot filling, as it tries to identify the PIO elements in an abstract describing a clinical trial. However, it can be formulated as a sequential tagging task like NER, by classifying tokens belonging to each element. A token may belong to more than one element, e.g., participant (P) and intervention (I).</p><p>• TransformInput: returns the input sequence as is.</p><p>• Featurizer: returns the BERT encoding of a given token. Relation Extraction. Existing work on relation extraction tends to focus on binary relations. Given a pair of entity mentions in a text span (typically a sentence), the goal is to determine if the text indicates a relation for the mention pair. There are significant variations in the entity and relation representations. To prevent overfitting by memorizing the entity pairs, the entity tokens are often augmented with start/end markers or replaced by a dummy token. For featurization, the relation instance is either represented by a special [CLS] token, or by concatenating the mention representations. In the latter case, if an entity mention contains multiple tokens, its representation is usually produced by pooling those of individual tokens (max or average). For computational efficiency, we use padding or truncation to set the input length to 128 tokens for GAD and 256 tokens for ChemProt and DDI which contain longer input sequences. Sentence Similarity. The similarity task can be formulated as a regression problem to generate a normalized score for a sentence pair. By default, a special [SEP] token is inserted to separate the two sentences, and a special [CLS] token is prepended to the beginning to represent the pair. The BERT encoding of [CLS] is used to compute the regression score.</p><p>• Question Answering. For the two-way (yes/no) or three-way (yes/maybe/no) question-answering task, the encoding is similar to the sentence similarity task. Namely, a [CLS] token is prepended to the beginning, followed by the question and reference text, with a [SEP] token to separate the two text spans. The [CLS] BERT encoding is then used for the final classification. For computational efficiency, we use padding or truncation to set the input length to 512 tokens.</p><p>•  For biomedical domain-specific pretraining, we generate the vocabulary and conduct pretraining using the latest collection of PubMed 5 abstracts: 14 million abstracts, 3.2 billion words, 21 GB. (The original collection contains over 4 billion words; we filter out any abstracts with less than 128 words to reduce noise.)</p><p>We follow the standard pretraining procedure based on the Tensorflow implementation released by NVIDIA. <ref type="bibr" target="#b5">6</ref> We use Adam <ref type="bibr" target="#b29">[30]</ref> for the optimizer using a standard slanted triangular learning rate schedule with warm-up in 10% of steps and cool-down in 90% of steps. Specifically, the learning rate increases linearly from zero to the peak rate of 6 × 10 −4 in the first 10% of steps, and then decays linearly to zero in the remaining 90% of steps. Training is done for 62,500 steps with batch size of <ref type="bibr" target="#b7">8,</ref><ref type="bibr">192</ref>, which is comparable to the computation used in previous biomedical pretraining. <ref type="bibr" target="#b6">7</ref> The training takes about 5 days on one DGX-2 machine with 16 V100 GPUs. We find that the cased version has similar performance to the uncased version in preliminary experiments; thus, we focus on uncased models in this study. We use whole-word masking (WWM), with a masking rate of 15%. We denote the resulting BERT model PubMedBERT.</p><p>For comparison, we use the public releases of BERT <ref type="bibr" target="#b15">[16]</ref>, RoBERTa <ref type="bibr" target="#b38">[39]</ref>, BioBERT <ref type="bibr" target="#b33">[34]</ref>, SciBERT <ref type="bibr" target="#b7">[8]</ref>, Clinical-BERT <ref type="bibr" target="#b0">[1]</ref>, and BlueBERT <ref type="bibr" target="#b45">[45]</ref>. See <ref type="table" target="#tab_6">Table 5</ref> for an overview. BioBERT and BlueBERT conduct continual pretraining from BERT, whereas ClinicalBERT conducts continual pretraining from BioBERT; thus, they all share the same vocabulary as BERT. BioBERT comes with two versions. We use BioBERT++ (v1.1), which was trained for a longer time and performed better. ClinicalBERT also comes with two versions. We use Bio+Clinical BERT.</p><p>Prior pretraining work has explored two settings: BERT-BASE with 12 transformer layers and 100 million parameters; BERT-LARGE with 24 transformer layers and 300 million parameters. Prior work in biomedical pretraining uses BERT-BASE only. For head-to-head comparison, we also use BERT-BASE in pretraining Pub-MedBERT. BERT-LARGE appears to yield improved performance in some preliminary experiments. We leave an in-depth exploration to future work.</p><p>For task-specific fine-tuning, we use Adam <ref type="bibr" target="#b29">[30]</ref> with the standard slanted triangular learning rate schedule (warm-up in the first 10% of steps and cool-down in the remaining 90% of steps) and a dropout probability of 0.1. Due to random initialization of the task-specific model and drop out, the performance may vary for different random seeds, especially for small datasets like BIOSSES, BioASQ, and PubMedQA. We report the average scores from ten runs for BIOSSES, BioASQ, and PubMedQA, and five runs for the others.</p><p>For all datasets, we use the development set for tuning the hyperparameters with the same range: learning rate (1e-5, 3e-5, 5e-5), batch size <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32)</ref> and epoch number . Ideally, we would conduct separate hyperparameter tuning for each model on each dataset. However, this would incur a prohibitive amount of computation, as we have to enumerate all combinations of models, datasets and hyperparameters, each of which requires averaging over multiple runs with different randomization. In practice, we observe that the development performance is not very sensitive to hyperparameter selection, as long as they are in a ballpark range. Consequently, we focus on hyperparameter tuning using a subset of representative models such as BERT and BioBERT, and use a common set of hyperparameters for each dataset that work well for both out-domain and in-domain language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>In this section, we conduct a thorough evaluation to assess the impact of domain-specific pretraining in biomedical NLP applications. First, we fix the standard task-specific model for each task in BLURB, and conduct a head-tohead comparison of domain-specific pretraining and mixed-domain pretraining. Next, we evaluate the impact of various pretraining options such as vocabulary, whole-word masking (WWM), and adversarial pretraining. Finally, we fix a pretrained BERT model and compare various modeling choices for task-specific fine-tuning.  <ref type="table">Table 6</ref>. Comparison of pretrained language models on the BLURB biomedical NLP benchmark. The standard task-specific models are used in the same fine-tuning process for all BERT models. The BLURB score is the macro average of average test results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification, question answering). See <ref type="table">Table 3</ref> for the evaluation metric used in each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain-Specific Pretraining vs Mixed-Domain Pretraining</head><p>We compare BERT models by applying them to the downstream NLP applications in BLURB. For each task, we conduct the same fine-tuning process using the standard task-specific model as specified in subsection 2.4. <ref type="table">Table 6</ref> shows the results. By conducting domain-specific pretraining from scratch, PubMedBERT consistently outperforms all the other BERT models in most biomedical NLP tasks, often by a significant margin. The gains are most substantial against BERT models trained using out-domain text. Notably, while the pretraining corpus is the largest for RoBERTa, its performance on biomedical NLP tasks is among the worst, similar to the original BERT model. Models using biomedical text in pretraining generally perform better. However, mixing out-domain data in pretraining generally leads to worse performance. In particular, even though clinical notes are more relevant to the biomedical domain than general-domain text, adding them does not confer any advantage, as evident by the results of ClinicalBERT and BlueBERT. Not surprisingly, BioBERT is the closest to PubMedBERT, as it also uses PubMed text for pretraining. However, by conducting domain-specific pretraining from scratch, including using the PubMed vocabulary, PubMedBERT is able to obtain consistent gains over BioBERT in most tasks. A notable exception is PubMedQA, but this dataset is small, and there are relatively high variances among runs with different random seeds.</p><p>Compared to the published results for BioBERT, SciBERT, and BlueBERT in their original papers, our results are generally comparable or better for the tasks they have been evaluated on. The ClinicalBERT paper does not report any results on these biomedical applications <ref type="bibr" target="#b0">[1]</ref>.   To assess the impact of pretraining options on downstream applications, we conduct several ablation studies using PubMedBERT as a running example. <ref type="table">Table 7</ref> shows results assessing the effect of vocabulary and wholeword masking (WWM). Using the original BERT vocabulary derived from Wikipedia &amp; BookCorpus (by continual pretraining from the original BERT), the results are significantly worse than using an in-domain vocabulary from PubMed. Additionally, WWM leads to consistent improvement across the board, regardless of the vocabulary in  <ref type="table">Table 9</ref>. Evaluation of the impact of pretraining corpora and time on the performance on BLURB. In the first two columns, pretraining was first conducted on Wiki &amp; Books, then on PubMed abstracts. All use the same amount of compute (twice as long as original BERT pretraining), except for the third column, which only uses half (same as original BERT pretraining).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study on Pretraining Techniques</head><p>use. A significant advantage in using an in-domain vocabulary is that the input will be shorter in downstream tasks, as shown in <ref type="table">Table 8</ref>, which makes learning easier. <ref type="figure" target="#fig_2">Figure 3</ref> shows examples of how domain-specific pretraining with in-domain vocabulary helps correct errors from mixed-domain pretraining. Furthermore, we found that pretraining on general-domain text provides no benefit even if we use the in-domain vocabulary; see <ref type="table">Table 9</ref>. The first column corresponds to BioBERT, which conducted pretraining first on the general domain and then on PubMed. The second column adopted the same continual pretraining strategy, except that the in-domain vocabulary (from PubMed) was used, which actually led to slight degradation in performance. On the other hand, by conducting pretraining from scratch on PubMed, we attained similar performance even with half of the compute (third column), and attained significant gain with the same amount of compute (fourth column; PubMedBERT). In sum, general-domain pretraining confers no advantage here in domain-specific pretraining.</p><p>In our standard PubMedBERT pretraining, we used PubMed abstracts only. We also tried adding full-text articles from PubMed Central (PMC), <ref type="bibr" target="#b7">8</ref> with the total pretraining text increased substantially to 16.8 billion words (107 GB). Surprisingly, this generally leads to a slight degradation in performance across the board. However, by extending pretraining for 60% longer (100K steps in total), the overall results improve and slightly outperform the standard PubMedBERT using only abstracts. The improvement is somewhat mixed across the tasks, with some gaining and others losing. We hypothesize that the reason for this behavior is two-fold. First, PMC inclusion is influenced by funding policy and differs from general PubMed distribution, and full texts generally contain more noise than abstracts. As most existing biomedical NLP tasks are based on abstracts, full texts may be slightly  <ref type="table">Table 10</ref>. Evaluation of the impact of pretraining text on the performance of PubMedBERT on BLURB. The first result column corresponds to the standard PubMedBERT pretrained using PubMed abstracts ("PubMed"). The second one corresponds to PubMedBERT trained using both PubMed abstracts and PMC full text ("PubMed+PMC"). The last one corresponds to PubMedBERT trained using both PubMed abstracts and PMC full text, for 60% longer ("PubMed+PMC (longer training)").</p><p>out-domain compared to abstracts. Moreover, even if full texts are potentially helpful, their inclusion requires additional pretraining cycles to make use of the extra information. Adversarial pretraining has been shown to be highly effective in boosting performance in general-domain applications <ref type="bibr" target="#b36">[37]</ref>. We thus conducted adversarial pretraining in PubMedBERT and compared its performance with standard pretraining <ref type="table">(Table 11)</ref>. Surprisingly, adversarial pretraining generally leads to a slight degradation in performance, with some exceptions such as sentence similarity (BIOSSES). We hypothesize that the reason may be similar to what we observe in pretraining with full texts. Namely, adversarial training is most useful if the pretraining corpus is more diverse and relatively out-domain compared to the application tasks. We leave a more thorough evaluation of adversarial pretraining to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study on Fine-Tuning Methods</head><p>In the above studies on pretraining methods, we fix the fine-tuning methods to the standard methods described in subsection 2.4. Next, we will study the effect of modeling choices in task-specific fine-tuning, by fixing the underlying pretrained language model to our standard PubMedBERT (WWM, PubMed vocabulary, pretrained using PubMed abstracts). Prior to the current success of pretraining neural language models, standard NLP approaches were often dominated by sequential labeling methods, such as conditional random fields (CRF) and more recently recurrent neural networks such as LSTM. Such methods were particularly popular for named entity recognition (NER) and relation extraction.</p><p>With the advent of BERT models and the self-attention mechanism, the utility of explicit sequential modeling becomes questionable. the entire text span. Therefore, it's conceivable that even a linear layer on top can perform competitively. We find that this is indeed the case for NER and relation extraction, as shown in <ref type="table" target="#tab_0">Table 12</ref>. The use of a bidirectional LSTM (Bi-LSTM) does not lead to any substantial gain compared to linear layer. We also investigate the tagging scheme used in NER. The standard tagging scheme distinguishes words by their positions within an entity. For sequential tagging methods such as CRF and LSTM, distinguishing the position within an entity is potentially advantageous compared to the minimal IO scheme that only distinguishes between inside and outside of entities. But for BERT models, once again, the utility of more complex tagging schemes is diminished. We thus conducted a head-to-head comparison of the tagging schemes using three biomedical NER tasks in BLURB. As we can see in <ref type="table">Table 13</ref>, the difference is minuscule, suggesting that with self-attention, the sequential nature of the tags is less essential in NER modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input text</head><p>Classification The use of neural methods also has subtle, but significant, implications for relation extraction. Previously, relation extraction was generally framed as a classification problem with manually-crafted feature templates. To prevent overfitting and enhance generalization, the feature templates would typically avoid using the entities in question. Neural methods do not need hand-crafted features, but rather use the neural encoding of the given text span, including the entities themselves. This introduces a potential risk that the neural network may simply memorize the entity combination. This problem is particularly pronounced in self-supervision settings, such as distant supervision, because the positive instances are derived from entity tuples with known relations. As a result, it is a common practice to "dummify" entities (i.e., replace an entity with a generic tag such as $DRUG or $GENE) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b58">58]</ref>.</p><p>This risk remains in the standard supervised setting, such as in the tasks that comprise BLURB. We thus conducted a systematic evaluation of entity dummification and relation encoding, using two relation extraction tasks in BLURB.</p><p>For entity marking, we consider three variants: dummify the entities in question; use the original text; add start and end tags to entities in question. For relation encoding, we consider three schemes. In the [CLS] encoding introduced by the original BERT paper, the special token [CLS] is prepended to the beginning of the text span, and its contextual representation at the top layer is used as the input in the final classification. Another standard approach concatenates the BERT encoding of the given entity mentions, each obtained by applying max pooling to the corresponding token representations. Finally, following prior work, we also consider simply concatenating the top contextual representation of the entity start tag, if the entity markers are in use <ref type="bibr" target="#b6">[7]</ref>. <ref type="table" target="#tab_3">Table 14</ref> shows the results. Simply using the original text indeed exposes the neural methods to significant overfitting risk. Using [CLS] with the original text is the worst choice, as the relation encoding has a hard time to distinguish which entities in the text span are in question. Dummification remains the most reliable method, which works for either relation encoding method. Interestingly, using entity markers leads to slightly better results in both datasets, as it appears to prevent overfitting while preserving useful entity information. We leave it to future work to study whether this would generalize to all relation extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>Standard supervised learning requires labeled examples, which are expensive and time-consuming to annotate. Self-supervision using unlabeled text is thus a long-standing direction for alleviating the annotation bottleneck using transfer learning. Early methods focused on clustering related words using distributed similarity, such as Brown Clusters <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>. With the revival of neural approaches, neural embedding has become the new staple for transfer learning from unlabeled text. This starts with simple stand-alone word embeddings <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b46">46]</ref>, and evolves into more sophisticated pretrained language models, from LSTM in ULMFiT <ref type="bibr" target="#b22">[23]</ref> and ELMo <ref type="bibr" target="#b47">[47]</ref> to transformer-based models in GPT <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49]</ref> and BERT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>. Their success is fueled by access to large text corpora, advanced hardware such as GPUs, and a culmination of advances in optimization methods, such as Adam <ref type="bibr" target="#b29">[30]</ref> and slanted triangular learning rate <ref type="bibr" target="#b22">[23]</ref>. Here, transfer learning goes from the pretrained language models to fine-tuning task-specific models for downstream applications.</p><p>As the community ventures beyond the standard newswire and Web domains, and begins to explore highvalue verticals such as biomedicine, a different kind of transfer learning is brought into play by combining text from various domains in pretraining language models. The prevailing assumption is that such mixed-domain pretraining is advantageous. In this paper, we show that this type of transfer learning may not be applicable when there is a sufficient amount of in-domain text, as is the case in biomedicine. In fact, our experiments comparing clinical BERTs with PubMedBERT on biomedical NLP tasks show that even related text such as clinical notes may not be helpful, since we already have abundant biomedical text from PubMed. Our results show that we should distinguish different types of transfer learning and separately assess their utility in various situations.</p><p>There are a plethora of biomedical NLP datasets, especially from various shared tasks such as BioCreative <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">53]</ref>, BioNLP <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, SemEval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>, and BioASQ <ref type="bibr" target="#b42">[42]</ref>. The focus has evolved from simple tasks, such as named entity recognition, to more sophisticated tasks, such as relation extraction and question answering, and new tasks have been proposed for emerging application scenarios such as evidence-based medical information extraction <ref type="bibr" target="#b44">[44]</ref>. However, while comprehensive benchmarks and leaderboards are available for the general domains (e.g., GLUE <ref type="bibr" target="#b57">[57]</ref> and SuperGLUE <ref type="bibr" target="#b56">[56]</ref>), they are still a rarity in biomedical NLP. In this paper, inspired by prior effort towards this direction <ref type="bibr" target="#b45">[45]</ref>, we create the first leaderboard for biomedical NLP, BLURB -a comprehensive benchmark containing thirteen datasets for six tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we challenge a prevailing assumption in pretraining neural language models and show that domainspecific pretraining from scratch can significantly outperform mixed-domain pretraining such as continual pretraining from a general-domain language model, leading to new state-of-the-art results for a wide range of biomedical NLP applications. To facilitate this study, we create BLURB, a comprehensive benchmark for biomedical NLP featuring a diverse set of tasks such as named entity recognition, relation extraction, document classification, and question answering. To accelerate research in biomedical NLP, we release our state-of-the-art biomedical BERT models and setup a leaderboard based on BLURB.</p><p>Future directions include: further exploration of domain-specific pretraining strategies; incorporating more tasks in biomedical NLP; extension of the BLURB benchmark to clinical and other high-value domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Tagging scheme: BIO*; BIOUL; IO. • Classification layer: linear layer*; LSTM; CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>TransformInput: entity (dummification*; start/end marker; original); relation ([CLS]*; original). • Featurizer: entity (dummy token*; pooling); relation ([CLS] BERT encoding*; concatenation of the mention BERT encoding). • Classification layer: linear layer*; more sophisticated classifiers (e.g., MLP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of how domain-specific pretraining helps correct errors from mixed-domain pretraining. Top: attention for the leading word piece of the gene mention "epithelial-restricted with serine box" (abbreviation "ESX") in the BC2GM dataset. Bottom: attention for the [CLS] token in an instance of AGONIST relation between a pair of dummified chemical and protein. In both cases, we show the aggregate attention from the penultimate layer to the preceding layer, which tends to be most informative about the final classification. Note how BioBERT tends to shatter the relevant words by inheriting the general-domain vocabulary. The domain-specific vocabulary enables PubMedBERT to learn better attention patterns and make correct predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the biomedical datasets in prior language model pretraining studies and BLURB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Standard NLP tasks and their problem formulations and modeling choices.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>TransformInput: [CLS] 1 [SEP] 2 [SEP], for sentence pair 1 , 2 . • Featurizer: [CLS] BERT encoding. • Regression layer: linear regression.Document Classification. For each text span and category (an abstract and a cancer hallmark in HoC), the goal is to classify whether the text belongs to the category. By default, a [CLS] token is appended to the beginning of the text, and its BERT encoding is passed on by the Featurizer for the final classification, which typically uses a simple linear layer.</figDesc><table /><note>• TransformInput: [CLS] [SEP], for document .• Featurizer: returns [CLS] BERT encoding.• Classification layer: linear layer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Summary of pretraining details for the various BERT models used in our experiments. Statistics for prior BERT models are taken from their publications when available. The size of a text corpus such as PubMed may vary a bit, depending on downloading time and preprocessing (e.g., filtering out empty or very short abstracts). Both BioBERT and PubMedBERT also have a version pretrained with additional PMC full text; here we list the standard version pretrained using PubMed only.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Evaluation of the impact of vocabulary and whole word masking on the performance of PubMedBERT on BLURB. Comparison of the average input length in word pieces using general-domain vs in-domain vocabulary.</figDesc><table><row><cell>Vocab</cell><cell cols="2">Wiki + Books PubMed</cell></row><row><cell>BC5-chem</cell><cell>35.9</cell><cell>28.0</cell></row><row><cell>BC5-disease</cell><cell>35.9</cell><cell>28.0</cell></row><row><cell>NCBI-disease</cell><cell>34.2</cell><cell>27.4</cell></row><row><cell>BC2GM</cell><cell>38.5</cell><cell>30.5</cell></row><row><cell>JNLPBA</cell><cell>33.7</cell><cell>26.0</cell></row><row><cell>EBM PICO</cell><cell>30.7</cell><cell>25.1</cell></row><row><cell>ChemProt</cell><cell>75.4</cell><cell>55.5</cell></row><row><cell>DDI</cell><cell>106.0</cell><cell>75.9</cell></row><row><cell>GAD</cell><cell>47.0</cell><cell>35.7</cell></row><row><cell>BIOSSES</cell><cell>80.7</cell><cell>61.6</cell></row><row><cell>HoC</cell><cell>40.6</cell><cell>31.0</cell></row><row><cell>PubMedQA</cell><cell>343.1</cell><cell>293.0</cell></row><row><cell>BioASQ</cell><cell>702.4</cell><cell>541.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .Table 12 .Table 13 .</head><label>111213</label><figDesc>The top layer in the BERT model already captures many non-linear dependencies across Comparison of PubMedBERT performance on BLURB using standard and adversarial pretraining. Comparison of linear layers vs recurrent neural networks for task-specific fine-tuning in named entity recognition (entity-level F1) and relation extraction (micro F1), all using the standard PubMedBERT. Comparison of entity-level F1 for biomedical named entity recognition (NER) using different tagging schemes and the standard PubMedBERT.</figDesc><table><row><cell></cell><cell cols="2">PubMedBERT + adversarial</cell></row><row><cell>BC5-chem</cell><cell>93.33</cell><cell>93.17</cell></row><row><cell>BC5-disease</cell><cell>85.62</cell><cell>85.48</cell></row><row><cell>NCBI-disease</cell><cell>87.82</cell><cell>87.99</cell></row><row><cell>BC2GM</cell><cell>84.52</cell><cell>84.07</cell></row><row><cell>JNLPBA</cell><cell>80.06</cell><cell>79.83</cell></row><row><cell>EBM PICO</cell><cell>73.38</cell><cell>72.92</cell></row><row><cell>ChemProt</cell><cell>77.24</cell><cell>77.04</cell></row><row><cell>DDI</cell><cell>82.36</cell><cell>83.62</cell></row><row><cell>GAD</cell><cell>82.34</cell><cell>81.48</cell></row><row><cell>BIOSSES</cell><cell>92.30</cell><cell>94.11</cell></row><row><cell>HoC</cell><cell>82.32</cell><cell>82.20</cell></row><row><cell>PubMedQA</cell><cell>55.84</cell><cell>53.30</cell></row><row><cell>BioASQ</cell><cell>87.56</cell><cell>82.71</cell></row><row><cell>BLURB score</cell><cell>81.10</cell><cell>80.68</cell></row><row><cell cols="3">Task-Specific Model Linear Layer Bi-LSTM</cell></row><row><cell>BC5-chem</cell><cell>93.33</cell><cell>93.12</cell></row><row><cell>BC5-disease</cell><cell>85.62</cell><cell>85.64</cell></row><row><cell>JNLPBA</cell><cell>80.06</cell><cell>80.06</cell></row><row><cell>ChemProt</cell><cell>77.24</cell><cell>75.40</cell></row><row><cell>DDI</cell><cell>82.36</cell><cell>81.70</cell></row><row><cell>GAD</cell><cell>82.34</cell><cell>81.80</cell></row><row><cell cols="2">Tagging Scheme BIO BIOUL</cell><cell>IO</cell></row><row><cell>BC5-chem</cell><cell cols="2">93.33 93.37 93.11</cell></row><row><cell>BC5-disease</cell><cell cols="2">85.62 85.59 85.63</cell></row><row><cell>JNLPBA</cell><cell cols="2">80.06 79.98 80.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Evaluation of the impact of entity dummification and relation encoding in relation extraction, all using PubMedBERT. With entity dummification, the entity mentions in question are anonymized using entity type tags such as $DRUG or $GENE. With entity marker, special tags marking the start and end of an entity are appended to the entity mentions in question. Relation encoding is derived from the special [CLS] token appended to the beginning of the text or the special entity start token, or by concatenating the contextual representation of the entity mentions in question.</figDesc><table><row><cell>Encoding ChemProt DDI</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">• Gu, Tinn, Cheng, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Prior work also observed similar shattering for clinical words<ref type="bibr" target="#b52">[52]</ref>. 6 • Gu, Tinn, Cheng, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">• Gu, Tinn, Cheng, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://geneticassociationdb.nih.gov/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The original authors used cross-validation for their evaluation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://pubmed.ncbi.nlm.nih.gov/; downloaded in Feb. 2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/NVIDIA/DeepLearningExamples 7 For example, BioBERT started with the standard BERT, which was pretrained using 1M steps with batch size of 256, and ran another 1M steps in continual pretraining.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">• Gu, Tinn, Cheng, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.ncbi.nlm.nih.gov/pmc/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">• Gu, Tinn, Cheng, et al.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Publicly Available Clinical BERT Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
		<ptr target="https://doi.org/10.18653/v1/W19-1909" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shutova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/volumes/S18-1/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<editor>Steven Bethard, and Marine Carpuat</editor>
		<meeting>The 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BioCreative III interactive task: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><forename type="middle">N</forename><surname>Arighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoebe</forename><forename type="middle">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanmitra</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Cesareni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chatr-Aryamontri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">Gwinn</forename><surname>Giglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Harrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Huala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lois</forename><forename type="middle">J</forename><surname>Maltais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livia</forename><surname>Perfetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rune</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">E</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-12-S8-S4</idno>
		<ptr target="https://doi.org/10.1186/1471-2105-12-S8-S4" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011-10-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain Adaptation via Pseudo In-Domain Data Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D11-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilona</forename><surname>Silins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Högberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulla</forename><surname>Stenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3973" to="3981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic semantic classification of scientific literature according to the hallmarks of cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilona</forename><surname>Silins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Högberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulla</forename><surname>Stenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matching the Blanks: Distributional Similarity for Relation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1279" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1371" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/volumes/S17-2/" />
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<editor>Saif M. Mohammad, Daniel M. Cer, and David Jurgens</editor>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/volumes/S16-1/" />
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<editor>Preslav Nakov, and Torsten Zesch</editor>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àlex</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Piñero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Núria</forename><surname>Queralt-Rosinach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rautschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">I</forename><surname>Furlong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="480" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural network multi-task learning approach to biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamal</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">368</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<ptr target="https://www.aclweb.org/anthology/volumes/W19-50/" />
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<editor>Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou, and Junichi Tsujii</editor>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>BioNLP@ACL; Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Association for Computer Linguistics</title>
		<ptr target="https://www.aclweb.org/anthology/volumes/S13-2/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2013</title>
		<editor>Mona T. Diab, Timothy Baldwin, and Marco Baroni</editor>
		<meeting>the 7th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2013<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NCBI disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Rezarta Islamaj Doğan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ML-Net: multi-label classification of biomedical texts with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingcheng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocz085</idno>
		<ptr target="https://academic.oup.com/jamia/article-pdf/26/11/1279/36089060/ocz085.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Hanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The hallmarks of cancer. cell</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">María</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document-Level -ary Relation Extraction with Multiscale Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PubMedQA: A Dataset for Biomedical Research Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1259</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
		<ptr target="https://doi.org/10.1038/sdata.2016.35" />
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016-05-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the Bio-entity Recognition Task at JNLPBA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W04-1213" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP). COLING</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP). COLING<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overview of Genia Event Task in BioNLP Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Yonezawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioNLP Shared Task 2011 Workshop</title>
		<meeting>the BioNLP Shared Task 2011 Workshop<address><addrLine>Portland, Oregon; USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
	<note>BioNLP Shared Task &apos;11)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chatr-Aryamontri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">John</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">C</forename><surname>Comeau</surname></persName>
		</author>
		<title level="m">Overview of BioCreative V BioC Track</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overview of the BioCreative VI chemical-protein interaction Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Pérez Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santamaría</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gp Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth BioCreative challenge evaluation workshop</title>
		<meeting>the sixth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
		<ptr target="https://doi.org/10.18653/v1/D18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08994</idno>
		<title level="m">Adversarial Training for Large Neural Language Models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="912" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Van Auken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><forename type="middle">N</forename><surname>Arighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Thomas</forename><surname>Peter Mcquilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tweedie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mary</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">J F</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shur-Jen</forename><surname>Laulederkind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Jae</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Hsien</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-De</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Jung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Emadzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Ming</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Overview of the gene ontology task at BioCreative IV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Results of the Seventh Edition of the BioASQ Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="553" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependences in stochastic language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<idno type="DOI">10.1006/csla.1994.1001</idno>
		<ptr target="https://doi.org/10.1006/csla.1994.1001" />
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><forename type="middle">Jessy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. Meeting</title>
		<meeting>the conference. Association for Computational Linguistics. Meeting</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="0197" />
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5006</idno>
		<ptr target="https://doi.org/10.18653/v1/W19-5006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enhancing clinical concept extraction with contextual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Overview of BioCreative II gene mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lorraine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ju</forename><surname>Johnson Nee Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Fang</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shi</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedrich</surname></persName>
		</author>
		<idno>S2</idno>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">BIOSSES: a semantic sentence similarity estimation system for the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gizem</forename><surname>Soğancıoğlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakime</forename><surname>Öztürk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan</forename><surname>Özgür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1271</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1271" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2644" to="2655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A Review on Multi-Label Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2013.39</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2013.39" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Drug-drug interaction extraction via hierarchical RNNs on sequence and shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Dumontier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="828" to="835" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
