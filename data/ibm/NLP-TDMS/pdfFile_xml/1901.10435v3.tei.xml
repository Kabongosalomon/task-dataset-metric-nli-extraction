<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-movement modeling</term>
					<term>deep learning</term>
					<term>performance metrics</term>
					<term>physical rehabilitation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computer-aided assessment of physical rehabilitation entails evaluation of patient performance in completing prescribed rehabilitation exercises, based on processing movement data captured with a sensory system. Despite the essential role of rehabilitation assessment toward improved patient outcomes and reduced healthcare costs, existing approaches lack versatility, robustness, and practical relevance. In this paper, we propose a deep learning-based framework for automated assessment of the quality of physical rehabilitation exercises. The main components of the framework are metrics for quantifying movement performance, scoring functions for mapping the performance metrics into numerical scores of movement quality, and deep neural network models for generating quality scores of input movements via supervised learning. The proposed performance metric is defined based on the log-likelihood of a Gaussian mixture model, and encodes low-dimensional data representation obtained with a deep autoencoder network. The proposed deep spatio-temporal neural network arranges data into temporal pyramids, and exploits the spatial characteristics of human movements by using sub-networks to process joint displacements of individual body parts. The presented framework is validated using a dataset of ten rehabilitation exercises. The significance of this work is that it is the first that implements deep neural networks for assessment of rehabilitation performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ARTICIPATION in physical therapy and rehabilitation programs is often compulsory and critical in postoperative recovery or for treatment of a wide array of musculoskeletal conditions. However, it is infeasible and economically unjustified to offer patient access to a clinician for every single rehabilitation session <ref type="bibr" target="#b0">[1]</ref>. Accordingly, current healthcare systems around the world are organized such that an initial portion of rehabilitation programs is performed in an inpatient facility under direct supervision by a clinician, followed by a second portion performed in an outpatient setting, where patients perform a set of prescribed exercises in their own residence. Reports in the literature indicate that more than 90% <ref type="bibr" target="#b0">1</ref>  of all rehabilitation sessions are performed in a home-based setting <ref type="bibr" target="#b1">[2]</ref>. Under these circumstances, patients are tasked to record their daily progress and periodically visit the clinic for functional assessment. Still, numerous medical sources report low levels of patient adherence to the recommended exercise regimens in home-based rehabilitation, leading to prolonged treatment times and increased healthcare cost <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Although many different factors have been identified that contribute to the low compliance rates, the major impact factor is the absence of continuous feedback and oversight of patient exercises by a healthcare professional <ref type="bibr" target="#b4">[5]</ref>. Despite the development of a variety of tools and devices in support of physical rehabilitation, such as robotic assistive systems <ref type="bibr" target="#b5">[6]</ref>, virtual reality and gaming interfaces <ref type="bibr" target="#b6">[7]</ref>, and Kinect-based assistants <ref type="bibr" target="#b1">[2]</ref>, there is still a lack of versatile and robust systems for automatic monitoring and assessment of patient performance.</p><p>The article proposes a novel framework for assessment of home-based rehabilitation that encompasses formulation of metrics for quantifying movement performance, scoring functions for mapping the performance metrics into numerical scores of movement quality, and deep learning-based end-to-end models for encoding the relationship between movement data and quality scores. The employed performance metric is based on probabilistic modeling of the skeletal joints data with a Gaussian mixture model, and consequently, it employs the log-likelihood of the model for performance evaluation <ref type="bibr" target="#b7">[8]</ref>. Next, the article investigates the effectiveness of deep autoencoder neural networks for dimensionality reduction of captured data. Further, we propose a scoring function for scaling the values of the performance metric into movement quality scores in the [0, 1] range. The resulting scores are employed as the ground truth for training the proposed deep neural networks (NNs) for rehabilitation applications.</p><p>The paper introduces a deep NN model designed to handle spatial and temporal variability in human movements. Motivation for the proposed network structure was prior work on temporal pyramids <ref type="bibr" target="#b8">[9]</ref> and hierarchical recurrent networks for motion classification <ref type="bibr" target="#b9">[10]</ref>. Specifically, the proposed model aims to exploit spatial characteristics of human movements by hierarchical processing of the joint displacements of different body parts via a series of sub-networks that gradually merge the extracted feature vectors. Temporal pyramids are introduced using movement sequences at different time scales in order to learn data representations at multiple levels of abstraction. The network contains both convolutional layers for learning spatial dependencies and recurrent layers for encoding temporal correlations in movement data. The framework is validated on  <ref type="bibr" target="#b10">[11]</ref>. To the best of our knowledge, this is the first framework that employs deep NNs for assessment of rehabilitation exercises. The main contributions of the paper are: (1) A novel framework for computer-aided assessment of rehabilitation exercises; (2) A deep spatio-temporal NN model for outputing movement quality scores; and (3) A performance metric that employs probabilistic modeling and autoencoder NNs for dimensionality reduction of rehabilitation data.</p><p>The article is organized as follows. The next section provides an overview of related work. Section III first introduces the mathematical notation and afterward describes the components of the proposed framework for rehabilitation assessment, including dimensionality reduction, performance metric, scoring function, and deep learning model. The validation of the proposed framework on a dataset of rehabilitation exercises is presented in Section IV. The last two sections discuss the results and summarize the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human Movement Modeling</head><p>Conventional approaches for mathematical modeling and representation of human movements are broadly classified into two categories: top-down approaches that introduce latent states for describing the temporal dynamics of the movements, and bottom-up approaches that employ local features for representing the movements. Commonly used methods in the first category include Kalman filters <ref type="bibr" target="#b11">[12]</ref>, hidden Markov models <ref type="bibr" target="#b12">[13]</ref>, and Gaussian mixture models <ref type="bibr" target="#b13">[14]</ref>. The main shortcomings of these methods originate from employing linear models for the transitions among the latent states (as in Kalman filters), or from adopting simple internal structures of the latent states (typical for hidden Markov models). The approaches based on extracting local features employ predefined criteria for identifying key points <ref type="bibr" target="#b14">[15]</ref> and/or required body postures <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, or a collection of statistics of the movements (e.g., mean, standard deviation, mode, median) <ref type="bibr" target="#b17">[18]</ref>. Such local features are typically motion-specific, which limits the ability to efficiently handle arbitrary spatio-temporal variations within movement data.</p><p>Recent developments in artificial NNs stirred significant interest in their application for modeling and analysis of human motions. Numerous works employed NNs for motion classification and applied the trained models for activity recognition, gait identification, gesture recognition, action localization, and related applications. NN-based motion classifiers utilizing different computational units have been proposed, including convolutional units <ref type="bibr" target="#b18">[19]</ref>, long short-term memory (LSTM) recurrent units <ref type="bibr" target="#b19">[20]</ref>, gated recurrent units, and combinations <ref type="bibr" target="#b20">[21]</ref> or modifications of these computational units <ref type="bibr" target="#b21">[22]</ref>. Also, NNs with different layer structures have been implemented, such as encoder-decoder networks, spatio-temporal graphs <ref type="bibr" target="#b22">[23]</ref>, and attention mechanism models <ref type="bibr" target="#b23">[24]</ref>. Besides the task of classification, a body of work in the literature focused on modeling and representation of human movements for prediction of future motion patterns <ref type="bibr" target="#b24">[25]</ref>, synthesis of movement sequences <ref type="bibr" target="#b25">[26]</ref>, and density estimation <ref type="bibr" target="#b7">[8]</ref>. Conversely, little research has been conducted on the application of NNs for evaluation of movement quality, which can otherwise find use in various applications (physical rehabilitation being one of them).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Movement Assessment</head><p>Quantifying the level of correctness in completing prescribed exercises is important for the development of tools and devices in support of home-based rehabilitation. The movement assessment in existing studies is typically accomplished by comparing a patient's performance of an exercise to the desired performance by healthy participants.</p><p>Several studies in the literature on exercise evaluation employed machine learning methods to classify the individual repetitions into correct or incorrect classes of movements. Methods used for this purpose include Adaboost classifier, k-nearest neighbors, Bayesian classifier, and an ensemble of multi-layer perceptron NNs <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. The outputs in these approaches are discrete class values of 0 or 1 (i.e., incorrect or correct repetition). However, these methods do not provide the capacity to detect varying levels of movement quality or identify incremental changes in patient performance over the duration of the rehabilitation program.</p><p>The majority of related studies employ distance functions for deriving movement quality scores. Concretely, Houmanfar et al. <ref type="bibr" target="#b17">[18]</ref> used a variant of the Mahalanobis distance to quantify the level of correctness of rehabilitation movements, based on a calculated distance between patient-performed repetitions and a set of repetitions performed by a group of healthy individuals. Similarly, a body of work utilized the dynamic time warping (DTW) algorithm <ref type="bibr" target="#b29">[30]</ref> for calculating the distance between a patient's performance and healthy subjects' performance <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>. The advantage of the distance functions is that they are not exercise-specific, and thus can be applied for assessment of new types of exercises. The distance functions also have shortcomings, because they do not attempt to derive a model of the rehabilitation data, and the distances are calculated at the level of individual time-steps in the raw sensory measurements.</p><p>A line of research utilized probabilistic approaches for modeling and evaluation of rehabilitation movements. Studies based on hidden Markov models <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> and mixtures of Gaussian distributions <ref type="bibr" target="#b7">[8]</ref> typically perform a quality assessment based on the likelihood that the individual sequences are being drawn from a trained model. Although the utilization of probabilistic models is advantageous in handling the variability due to the stochastic character of human movements, models with abilities for a hierarchical data representation can produce more reliable outcomes for movement quality assessment, and better generalize to new exercises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>A block-diagram of the envisioned framework for assessing rehabilitation exercises is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. The skeletal joint coordinates acquired by the sensory system are processed via dimensionality reduction, performance quantification, and scoring mapping to obtain movement quality scores that are subsequently used for training an NN model. The trained NN model is afterward used to automatically generate movement quality scores for input movement data acquired by the sensory system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>In outpatient physical rehabilitation, a daily rehabilitation session requires completing a series of exercises, where the patient is instructed to complete a certain number of repetitions of each exercise during each session. The acquired data by the sensory system for one particular exercise performed by healthy subjects is denoted by , and hereafter they are referred to as reference movements. The symbol is used for the number or repetitions of the exercise by the -th subject. The combined data for all repetitions of the exercise by the -th subject is denoted ∆ . Similar, is used for the total number of all repetitions by the subjects, i.e., = ∑ =1 . Using the notation , for the collected data of the -th repetition by the -th subject, we have = {∆ } for ∈ , where ∆ = { , } for ∈ . For convenience, throughout the text the underscore symbol denotes a set of indices, e.g., = {1,2, … , } for any positive integer . The data for each repetition , is a temporal sequence of measurements,</p><formula xml:id="formula_0">therefore , = ( , (1) , ,<label>(2)</label></formula><p>, … , , ( ) ) , where the superscripts are used for indexing the temporal order of the joint displacement vectors within the repetition. Furthermore, the individual measurement , ( ) for ∈ is a D-dimensional vector, consisting of the values for all joint displacements in the human body, i.e. ,</p><formula xml:id="formula_1">( ) = [ , ( ,1) , , ( ,2) , … , ( , ) ].</formula><p>The collected data for the patient group are referred in the article as patient movements, and are denoted with the symbol . By analogy to the introduced notation for the reference movements, = { , }, where , is the data of the -th repetition by the -th subject. Analogously, the repetition</p><formula xml:id="formula_2">, = ( , (1) , ,<label>(2)</label></formula><p>, … , , ( ) ) is comprised of a sequence of multidimensional vectors , ( ) = [ , ( ,1) , , ( ,2) , … , , ( , ) ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dimensionality Reduction</head><p>The sensory systems for motion capturing typically track between 15 and 40 skeletal joints, depending on the sensor type. The measurement data consists of 3-dimensional spatial positions and/or orientations for each joint, and therefore the dimensionality of the data ranges between 45 and 120. Dimensionality reduction of recorded data is an essential step in processing human movements to suppress unimportant, redundant, or highly correlated dimensions. The aim is to project the data = { , = ( , ( ) ): ,</p><formula xml:id="formula_3">( ) ∈ ℝ } into a lower-dimensional representation ̃= {̃, = (̃, ( ) ): ̃, ( ) ∈ ℝ }, for ∈ , ∈ , ∈ , where &lt; .</formula><p>A common approach for dimensionality reduction of human movement data is maximum variance <ref type="bibr" target="#b35">[36]</ref>, which simply retains the first dimensions with the largest variance and discards the remaining dimensions. Principal component analysis (PCA) and its variants <ref type="bibr" target="#b36">[37]</ref> are also widely used for reducing the dimensionality of movement data, where a matrix containing the leading eigenvectors corresponding to the largest eigenvalues of the covariance matrix is used for projecting the data into a lower-dimensional space. Although PCA is one of the most common approaches for dimensionality reduction in general, it employs linear mapping of high-dimensional data into a lower-dimensional representation. Likewise, the shortcomings of maximum variance originate from its simplicity.</p><p>In the proposed framework, we introduce autoencoder NNs <ref type="bibr" target="#b37">[38]</ref> for dimensionality reduction. Autoencoder NN is a nonlinear technique for dimensionality reduction allowing extracting richer data representations for dimensionality reduction in comparison to the linear techniques (such as PCA). Furthermore, deep autoencoder NNs created by stacking multiple consecutive layers of hidden neurons, can additionally increase the representational capacity of the network.</p><p>Autoencoders are used for unsupervised learning of an alternative representation of input data, through a process of data compression and reconstruction. The data processing involves an encoding step of compressing input data through one or multiple hidden layers, followed by a decoding step of reconstructing the output from the encoded representation through one or multiple hidden layers. If denotes a class of mapping functions from ℝ to ℝ , and ℬ is a class of mapping functions from ℝ to ℝ , then for any function A ∈ and B ∈ ℬ, the encoder portion projects an input , ( ) ∈ ℝ into a lower-dimensional representation ̃, ( ) = B( , ( ) ) ∈ ℝ (referred to as a code), and the decoder portion converts the code into an output A (B( , ( ) )) ∈ ℝ . Autoencoders are trained to find functions A ∈ and B ∈ ℬ which minimize the mean squared deviation between the input data and output data, i.e.,</p><formula xml:id="formula_4">argmin A,B ‖A (B( , ( ) )) − , ( ) ‖.<label>(1)</label></formula><p>A graphical representation of the adopted architecture for the autoencoder network is presented in <ref type="figure">Fig. 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Metric</head><p>The metrics for quantifying the patient performance are classified into model-less and model-based groups of metrics <ref type="bibr" target="#b38">[39]</ref>. The model-less metrics employ distance functions, such as Euclidean, Mahalanobis distance, and dynamic time warping (DTW) <ref type="bibr" target="#b29">[30]</ref> deviation between data sequences. The model-based metrics apply probabilistic approaches for modeling the movement data, and employ the log-likelihood for performance evaluation <ref type="bibr" target="#b7">[8]</ref>.</p><p>We adopt a metric based on Gaussian mixture model (GMM) log-likelihood. The choice stems from the demonstrated capacity of statistical methods to encode the inherent random variability in human movements; this results in improved ability by the model-based metrics to handle spatio-temporal variations in rehabilitation data. Log-likelihood of a movement data for a given model is a natural choice for evaluation of data instances in probabilistic models.</p><p>GMM is a parametric probabilistic model for representing data with a mixture of Gaussian probability density functions <ref type="bibr" target="#b39">[40]</ref>. GMM is frequently used for modeling human movements. For a dataset consisting of multidimensional vectors , ( ) , a GMM with Gaussian components has the form</p><formula xml:id="formula_5">( , ( ) | ) = ∑ ( , ( ) | , Σ ) =1 ,<label>(2)</label></formula><p>where = { , , Σ } are the mixing coefficient, mean, and covariance of the c-th Gaussian component, respectively. The most popular method for estimating the model parameters in GMM is the expectation maximization (EM) algorithm <ref type="bibr" target="#b40">[41]</ref>; other approaches include maximum-a-posteriori estimation <ref type="bibr" target="#b41">[42]</ref> and mixture density networks <ref type="bibr" target="#b39">[40]</ref>. Subsequently, for a GMM model with parameters λ, the negative log-likelihood is used as a performance metrics, and for the repetition , is calculated as</p><formula xml:id="formula_6">( , | ) = − ∑ log{∑ ( , ( ) | , Σ ) =1 } =1 .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scoring Function</head><p>In the presented framework, a scoring function maps the values of the performance metrics into a movement quality score in the range between 0 and 1.</p><p>The resulting movement quality scores play a dual role in the framework. First, in a real-world exercise assessment setting, the quality scores allow for intuitive understanding of the calculated values of the used performance metric. For instance, a movement quality score of 88% presented to a patient is easy to understand, and it can also enable the patient to self-monitor his/her progress toward functional recovery based on received scores over a period of time. Second, the movement quality scores are used here for supervised training of the deep NN models.</p><p>For a sequence of performance metric values of the reference movements = ( 1 , 2 , ⋯ , ) and a sequence = ( 1 , 2 , … ) related to the patient movements, we propose the following scoring function:</p><formula xml:id="formula_7">̅ = (1 + +3 − 1 ) −1 ;</formula><p>(4)</p><formula xml:id="formula_8">̅ = (1 + ( +3 ) − 1 + − 2 ( +3 ) ) −1 ,<label>(5)</label></formula><p>where ∈ ,</p><formula xml:id="formula_9">= 1 ∑ | | =1 , = √ 1 ∑ (| | − ) 2 =1</formula><p>, and 1 , 2 are data-specific parameters. The proposed scoring function is monotonically decreasing, and is designed to preserve the distribution of the values of the performance metric. The values for the reference movements are scaled by + 3 in (4) to ensure that the resulting scores have values close to 1 for inputs in the range ( − 3 , + 3 ). Similarly, for the patient movements the scoring function in (5) is designed to preserve their distribution in mapping the performance metric values into movement quality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Deep Learning Architecture for Rehabilitation Assessment</head><p>We propose a novel deep learning model for spatio-temporal modeling of skeletal data, for application in rehabilitation assessment. A graphical representation of the NN architecture is provided in <ref type="figure" target="#fig_1">Fig. 3</ref>. The NN model is designed to exploit the spatial characteristics of human movements by dedicating sub-networks for processing joint displacements of individual body parts. In addition, the input data is arranged into temporal pyramids for processing multiple scaled version of the movement repetitions. The initial hierarchical layers in the model employ strided one-dimensional convolutional filters for learning spatial dependencies in human movements, and are followed by a series of LSTM recurrent layers for modeling temporal correlations in learned representations. <ref type="figure">Fig. 2</ref>. The proposed autoencoder architecture projects an input movement data into a code representation, and re-projects the code into the movement data.</p><p>Output Input <ref type="bibr" target="#b29">(30)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code</head><p>Legend:</p><p>The architecture of the NN draws inspiration from the hierarchical model proposed by Du et al. <ref type="bibr" target="#b9">[10]</ref> that employs five recurrent sub-networks taking as inputs joint displacements of the left arm, right arm, left leg, right leg, and torso, respectively. The outputs from the five sub-networks are merged into a single representation. Such hierarchical organization of the network layers allows low-level spatial information from joint movements to be exploited for obtaining a high-level representation of the body parts' movements in accomplishing required actions. Differently form the model proposed by Du et al. <ref type="bibr" target="#b9">[10]</ref> that consists of bidirectional layers with LSTM recurrent units, our proposed network uses convolutional units in the hierarchical layers and recurrent units in the succeeding layers. The presented ablation study and performance comparison in Section VI corroborate the advantage of the introduced modifications in our spatio-temporal model.</p><p>Similarly, the introduction of temporal pyramids for processing rehabilitation data was motivated by the concept of image pyramids in computer vision. Temporal pyramids have been used for processing video data by dynamically subsampling input videos at varying frame rate <ref type="bibr" target="#b42">[43]</ref>, temporal pooling of multi-scale data representations from extracted feature map <ref type="bibr" target="#b43">[44]</ref>, or by applying sliding windows with changeable scales to the sequences of images <ref type="bibr" target="#b44">[45]</ref>. In these works, the use of multi-scale video pyramids has been conducive to improve the detection and localization of human actions in videos.</p><p>In the proposed network, the temporal pyramids are composed of full-scaled input sequences, and three sub-sampled versions with a temporal length equal to one half, one quarter, and one eight of the sequence (see <ref type="figure" target="#fig_1">Fig. 3(b)</ref>). The resulting feature vectors are then concatenated and passed to the next layers. Such data processing enables recognizing movement patterns at different levels of abstraction, and led to improved performance of the deep model for movement assessment.</p><p>Inputs to the network are 117-dimensional sequences of the full-body joint angles corresponding to single repetitions of an exercise. The convolutional blocks consist of two convolutional layers followed by dropout layers with a rate of 0.25. For these layers we adopted the multi-branch design approach shown in <ref type="figure" target="#fig_1">Fig. 3(c)</ref>, popularized in the inception convolutional network architectures <ref type="bibr" target="#b45">[46]</ref>. Each layer contains three branches of 1D convolutional filters with different length, which outputs are concatenated and passed to the next layer. The use of multiple branches allows the model the select the most suitable filter length based on the input data. The recurrent portion of the model consists of four layers with 80, 40, 40, and 80 LSTM units, respectively. The last layer has linear activations, and outputs a numerical movement quality score for an input repetition. Mean-squared-error was used as a cost function for training the model parameters, with the Adam optimizer. A batch size of 5 repetitions was applied, with early stopping regularization. One can note that the proposed model is not particularly deep, as it comprises of a relatively low number of hidden layers; however, considering that the used dataset is also of relatively small size, larger and deeper networks would overfit and produce suboptimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>For validation of the presented framework, we created the UI-PRMD dataset <ref type="bibr" target="#b10">[11]</ref>. The dataset consists of skeletal data collected from 10 healthy subjects. Each subject completed 10 repetitions of 10 rehabilitation exercises, listed in <ref type="table" target="#tab_4">Table I</ref>. The data were acquired with a Vicon optical tracking system, and consist of 117-dimensional sequences of angular joint displacements. The subjects performed the exercises both in a correct manner, hereafter referred to as correct movements, and in an incorrect manner, i.e., simulating performance by patients with musculoskeletal constraints, hereafter referred to as incorrect movements. The research study related to the data collection was approved by the Institutional Review Boards at the University of Idaho under the identification code IRB 16-124. A written informed consent for participation in a research study was approved by the board, and was obtained from all participants in the study. A detailed description of the UI-PRMD dataset is provided in <ref type="bibr" target="#b10">[11]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Quantification</head><p>In this section, the adopted performance metric based on the log-likelihood of GMM is evaluated on the UI-PRMD dataset. For comparison, three common performance metrics for assessment of rehabilitation exercises based on Euclidean, Mahalanobis, and DTW distance are also evaluated.</p><p>Data scaling: To compare the performance metrics on the same basis, their values are first linearly scaled to the same range. In this study the range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> was selected based on an empirical understanding of the data. For the obtained values of the performance metrics related to repetitions of the correct movements denoted = ( 1 , 2 , … , ), and for the metrics of the incorrect movements = ( 1 , 2 , … , ) , the following scaling functions were used</p><formula xml:id="formula_11">′ = 19( − ) − + 1 ; ′ = 19( − ) − + 1 for ∈ ,<label>(6)</label></formula><p>where ′ , ′ ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>  The scaled values of the Euclidean distance for exercises E1 and E2 are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Green circle markers are used for the repetitions of the correct movements, whereas the red squares symbolize the repetitions of the incorrect movements. Note that inconsistent data (associated with measurement errors or subjects performing the exercise with their left-arm/leg in a set of mostly right arm/leg exercises) were manually removed from the original dataset, resulting in less than 100 repetitions per subject. E.g., there are 90 correct and incorrect movements for E1 in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>, and 55 correct and incorrect movements for E2 in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>.</p><note type="other">denote</note><p>Separation degree: For comparison of the scaled values of the performance metrics we propose the concept of separation degree. Specifically, for any positive real numbers , , their separation degree is defined as S D ( , ) = − + ∈ [−1, 1]. The separation degree between two positive sequences = ( 1 , 2 , … , ) and = ( 1 , 2 , … , ) is defined by</p><formula xml:id="formula_12">S D ( , ) = 1 ∑ ∑ ( , ) =1 =1 .<label>(7)</label></formula><p>Values of the separation degree close to 1 or −1 indicate good separation between the two sequences. Conversely, for values of the separation degree close to 0, the sequences don't separate well and they are almost mixed together.</p><p>When applied to the values of the distance metrics, the separation degree indicates greater ability of the used metric to differentiate between correct and incorrect repetitions of an exercise. For instance, in <ref type="figure" target="#fig_3">Fig. 4(b)</ref> one can observe a clearer differentiation between the correct and incorrect movements, in comparison to <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. This results in a larger value of the separation degree for the repetitions of exercise E2, which were calculated at 0.384 for E1, and 0.497 for E2, respectively.</p><p>The values for the separation degrees for the four studied performance metrics are presented in <ref type="table" target="#tab_4">Table II</ref>. Each cell in the table corresponds to the average separate degree values S D for the 10 exercises in the dataset. The shown values are the means and in parentheses are the standard deviations. For the comparison, scaled values of the metrics according to <ref type="bibr" target="#b5">(6)</ref> are used. Values for both between-subject and within-subject cases are presented. Table II also compares the values of the metrics for the cases of raw 117-dimensional data, and low-dimensional data obtained with the methods of maximum-variance, PCA, and GMM log-likelihood. The largest values for the separation degree are indicated in each row with a bold font.</p><p>Conclusively, the GMM log-likelihood metric applied on a low-dimensional data with the autoencoder NN resulted in the largest separation between the correct and incorrect movements for both between-and within-subject cases. The within-subject case provides improved separation because the repetitions performed by the same subject are characterized with a lower level of variability. The value of the GMM log-likelihood is not provided for the 117-dimensional data because GMM is commonly applied on low-dimensional data. Furthermore, the performance of the Euclidean and DTW distances in <ref type="table" target="#tab_4">Table II</ref> is comparable, and better than the Mahalanobis distance. Also, one can notice that the autoencoder NN lost less information in compressing the high-dimensional data sequences in comparison to maximum variance and PCA, because the separation degree values for all metrics using autoencoders are very close to the corresponding metric values of the 117-dimensions data without dimensionality reduction. In implementing GMM on the dataset, the number of Gaussian components C was set to 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neural Networks Performance</head><p>For training the deep neural networks, the movement quality scores based on the GMM log-likelihood calculated with autoencoder-reduced data are employed. Only the case of between-subject is considered, since for the within-subject case the number of repetitions per subject is too small to train NNs.</p><p>Scoring function: The scoring function presented in (4)-(5) is used to calculate the movement quality scores. The values of the parameters are empirically selected as 1 = 3.2 and 2 = 10. For example, <ref type="figure">Fig. 5</ref> depicts the values of the log-likelihood and the corresponding performance scores for exercise E1 (i.e., deep squat). The scores for the correct movements shown in <ref type="figure">Fig. 5(b)</ref> have values close to 1, whereas most of the scores for the incorrect movements are in the range between 0.7 and 0.9.</p><p>NN evaluation: The model was implemented on a Dell Precision 5810 workstation with Intel Xeon CPU, 32 GB RAM, 2 TB hard disk, and an NVIDIA Titan Xp GPU card. Inputs to the NNs are pairs of repetition data containing raw 117-dimensional angular joint measurements and quality scores. The networks are trained in a supervised regression manner, where the output is a predicted value of the movement quality for an input repetition. For each of the 10 exercises in the UI-PRMD dataset, a separate NN is trained and used for quality assessment. Each network model is run five times, and we report the average absolute deviation between the ground truth quality scores and the network prediction.</p><p>To evaluate the respective contributions of the individual components in the design of our spatio-temporal model we conducted an ablation study. The results for the 10 exercises in the dataset are displayed in <ref type="table" target="#tab_4">Table III</ref>. Lower values of the absolute deviation indicate low errors by the NN model in predicting the quality score for input data. The upper row in the table presents the aggregated mean deviation for all exercises E1 to E10. The results of the ablation study support our intuitive assumptions that the introduced components in the proposed model related to the multi-branch layers, temporal pyramids, hierarchical structure, and combination of convolutional and recurrent units all contribute to improved assessment of rehabilitation exercises. We further compared the performance of the proposed NN to state-of-the-art deep learning models for movement classification. We are not aware of any other deep NN models for movement assessment. On the other hand, there is a large body of research on using deep learning models for classification/recognition/detection of human movements (in a general context, rather than for biomedical purposes). Therefore, we adapted several recent NN classifiers that have achieved top performance, and we re-purposed the models for regressing movement quality scores. The selected models are: Co-occurrence <ref type="bibr" target="#b46">[47]</ref>, PA-LSTM <ref type="bibr" target="#b47">[48]</ref>, Two-stream CNN <ref type="bibr" target="#b48">[49]</ref>,  Hierarchical LSTM <ref type="bibr" target="#b9">[10]</ref>, as well as two basic Deep CNN and Deep LSTM architectures.</p><p>For these networks, we replaced the last softmax layer with a fully-connected layer with linear activations. Furthermore, we omitted all batch normalization layers (if any were present) in the original models, as they significantly degraded the capacity for movement assessment. Other than that, we closely followed the proposed implementation as described by the authors in the respective papers. Hierarchical LSTM is the network proposed by Du et al. <ref type="bibr" target="#b9">[10]</ref> that served as a motivation for our proposed deep learning model. We selected the architectures and hyperparameters of the basic Deep CNN and Deep LSTM models through an extensive grid-search; the resulting CNN network has three convolutional layers (60, 30, and 10 units) followed by two fully-connected layers (200 and 100 units), whereas the Deep LSTM network contains one LSTM layer <ref type="bibr">(20 units)</ref>, one fully-connected layer <ref type="bibr">(30 units)</ref>, and another LSTM layer <ref type="bibr">(10 units)</ref>. The values of the average absolute deviations are presented in <ref type="table" target="#tab_4">Table IV</ref>. With regards to the ability for movement quality assessment of all 10 exercises in the dataset, our proposed model outperformed the other deep learning classification models, although some of the models provided better performance on several of the exercises in the dataset (shown with a bold font in the table). The computational times for training the models averaged over all exercises are shown in the last row in <ref type="table" target="#tab_4">Table IV</ref>. The proposed spatio-temporal model is computationally less expensive than almost all compared models. The prediction of movement quality scores for input repetitions by the trained model is very fast, and it took about 10 milliseconds per repetition on average. Our approach Co-occu rrence <ref type="bibr" target="#b46">[47]</ref> Deep CNN PA-LS TM <ref type="bibr" target="#b47">[48]</ref> Two-strea m CNN <ref type="bibr" target="#b48">[49]</ref> Hierar. LSTM <ref type="bibr" target="#b9">[10]</ref> Deep The results of the proposed deep NN for assessment of exercise E1 are depicted in <ref type="figure" target="#fig_6">Fig. 6</ref>. The set of 90 correct and 90 incorrect repetitions was randomly split using a ratio of 0.7/0.3 into a training set of 124 and a validation set of 56 repetitions. The ground truth scores and predicted scores for the training and validation sets are shown in Figs. 6(a) and (b), respectively. In the two sub-figures the first half of the scores are for the correct sequences and have values close to one, and the second half of the scores pertain to the incorrect sequences and have lower quality scores. Conclusively, the network predictions closely follow the values of the input quality scores for all data instances. We also validate the proposed approach using leave-one-out cross-validation (i.e., testing on one subject a model trained on all other subjects). The performance was comparable to the presented results using random test data, with the predicted quality scores closely following the ground truth values.</p><p>The proposed model was next evaluated on the KIMORE dataset <ref type="bibr" target="#b50">[50]</ref>, which contains data for five rehabilitation exercises performed by 44 healthy subjects and 34 patients, and collected with a Kinect v2 sensor. We implemented our proposed deep learning model on the deep squat exercise. We employed full-body joint orientations data for 33 healthy subjects and 18 patients, and extracted 4 repetitions for each subject, resulting in 204 repetitions in total. The KIMORE dataset provides clinical scores for each subject's performance in the [0, 50] range. To train the model, we scaled the values in the [0, 1] range, and randomly selected 142 repetitions for training, and 62 for validation. The results are displayed in <ref type="figure" target="#fig_7">Figure 7</ref>, where the predicted movement quality scores by the deep learning model closely follow the ground truth scores provided by the clinicians. The obtained mean absolute deviation was 0.03786, which is greater than the deviation for the deep squat exercise in the UI-PRMD dataset, probably due to the lower accuracy of Kinect v2 compared to Vicon, and also in the KIMORE dataset the same clinical score is assigned for all repetitions performed by the same subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>The article introduces a novel framework for the assessment of rehabilitation exercises via deep NNs. The framework includes performance metrics, scoring functions, and NN models. Common metrics for quantifying the level of consistency in captured rehabilitation movements are compared. The metrics include Euclidean, Mahalanobis, DTW distance, and GMM log-likelihood. The concept of separation degree is proposed for metric comparison. GMM log-likelihood outperformed the model-less metrics on the UI-PRMD dataset. Such results confirm our hypothesis that efficient movement assessment is strongly predicated on the provision of models of human movements. Probabilistic approaches, such as the used GMM approach, have improved ability to handle the inherent variability and measurement uncertainty in human movement data, in comparison to the model-less approaches.</p><p>We compared the performance of PCA and maximum variance approaches for dimensionality reduction of human movements to autoencoder NNs. Expectedly, the provision of nonlinear functions for neuron activations in autoencoders provides richer representational capacity of the data into a lower dimensional space, in comparison to the linear technique of PCA and the simple concept of maximum variance.</p><p>We propose a deep learning architecture for hierarchical spatio-temporal modeling of rehabilitation exercises at multiple levels of abstraction. NNs are trained for each exercise via supervised regression, where for inputs comprising exercise repetitions the inferred outputs are quality scores. The network structure combines hierarchical merging of extracted feature vectors from different body parts, pyramidal processing of the movement sequences subsamples at multiple temporal scales, and multi-branch blocks for learning the structure of the used computational units. Although recurrent units are most commonly used for processing sequential time-series data as the considered rehabilitation movements, our proposed model employs convolutional filters in the initial layers and LSTM recurrent units in the later layers of the network. The reasons for such design stem from the following: (1) the employed dataset is fairly small, consisting of less than 200 repetitions per exercises, hence recurrent NNs can overfit the data due to the larger number of used parameters, and (2) a growing body of work report of improved performance by CNNs on time-series and movement data <ref type="bibr" target="#b51">[51]</ref>. The proposed deep learning model outperformed recent state-of-the-art deep NNs designed for movement classification.</p><p>Our presented research has several limitations. The validation is primarily performed on rehabilitation exercises performed by healthy subjects, where the measurements are acquired with an expensive optical motion capturing system. Additionally, the largest segment of the validation is based on movement data without a ground truth assessment of the movement quality by clinicians. The evaluation of the deep squat exercise in the KIMORE dataset provides a partial validation on patient data collected with a low-cost sensor.</p><p>In future work, we will attempt to address the above-listed shortcomings of this study, i.e., we will focus on a thorough validation of the framework on rehabilitation exercises performed by patients and labeled by a group of clinicians who will assign quality scores. We will validate the proposed approach by acquiring muscle activity measurements. Also, we have plans to implement the framework for assessment of patient performance in home-based rehabilitation using a Kinect sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>The article proposes a deep learning-based framework for assessment of rehabilitation exercises. The framework consists of algorithms for dimensionality reduction, performance metrics, scoring functions, and deep learning models. The framework is evaluated on a dataset of 10 rehabilitation exercises. The experimental results indicate that the quality scores generated by the proposed framework closely follow the ground truth quality scores for the movements.</p><p>This work demonstrates the potential of deep learning models for assessment of rehabilitation exercises. Such models can consistently outperform the approaches that employ distance functions for movement assessment where the data processing is performed on low-level measurements of joint coordinates at the individual time-steps, and the probabilistic approaches where the data modeling is typically performed at a single level of abstraction. The advantages of deep NNs for this task originate from the capacity for hierarchical modeling of human movements at multiple spatial and temporal levels of abstraction. This type of models provide improved abilities to "understand" the levels of hierarchy and the complex spatiotemporal correlations in human movement data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposed framework for assessment of rehabilitation exercises. with 10, 30, and 117 computational units, respectively. The input time-series data are 117-dimensional vectors of joint coordinates. The code representation of the proposed network is a temporal sequence of 4-dimensional vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 3 .</head><label>3</label><figDesc>(a) The proposed spatio-temporal model architecture. (b) Temporal pyramid sub-network. (c) Multi-branch convolutional block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the scaled values of the performance metrics,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Scaled values of the Euclidean distance for the between-subject case for: (a) First exercise E1 ( S D = 0.384); (b) second exercise E2 ( S D = 0.497).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) 0.309 (0.101) 0.063 (0.130) 0.310 (0.100) 0.344 (0.049) D=3 (PCA) 0.296 (0.103) 0.108 (0.169) 0.265 (0.093) 0.360 (0.060) D=4 (AE) 0.423 (0.092) 0.229 (0.102) 0.427 (0.094) 0.515 (0.106) Within-subject D=117 0.568 (0.058) 0.441 (0.118 0.570 (0.059) --D=3 (MV) 0.472 (0.048) 0.325 (0.118 0.455 (0.053) 0.471 (0.098) D=3 (PCA) 0.508 (0.032) 0.322 (0.169) 0.501 (0.031) 0.518 (0.057) D=4 (AE) 0.582 (0.057) 0.474 (0.133) 0.574 (0.060) 0.603 (0.073) D: data dimensions; MV: maximum variance; PCA: principal component analysis; AE: autoencoder neural networks (a) (b) Fig 5. (a) GMM log-likelihood values for exercise E1; (b) Corresponding quality scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>LSTM E1-E10 0.02527 0.02703 0.02615 0.04534 0.11044 0.08819 0.04059 E1 0.01077 0.01052 0.01357 0.01839 0.28798 0.03010 0.01670 E2 0.02824 0.02905 0.02953 0.04413 0.22349 0.07742 0.04934 E3 0.03980 0.05577 0.04141 0.08094 0.20493 0.13766 0.09382 E4 0.01185 0.01347 0.01640 0.02347 0.36033 0.03580 0.01609 E5 0.01870 0.01687 0.01300 0.03156 0.12332 0.06367 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Predictions on the training set for exercise E1; (b) Predictions on the validation set for exercise E1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>(a) Predictions on the training set for deep squat exercise; (b) Predictions on the validation set for deep squat exercise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript submitted June 14, 2019. This work was supported by the Center for Modeling Complex Interactions (CMCI) at the University of Idaho through NIH Award #P20GM104420. Yalin Liao, Aleksandar Vakanski, and Min Xian are with the Department of Computer Science, University of Idaho, 1776 Science Center Drive, Idaho</figDesc><table><row><cell>Falls,</cell><cell>ID,</cell><cell>83402,</cell><cell>USA</cell><cell>(e-mail:</cell><cell>liao4728@vandals.uidaho.edu;</cell></row><row><cell cols="5">vakanski@uidaho.edu; mxian@uidaho.edu).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. The encoder portion consists of three intermediate layers of LSTM recurrent units with 30, 10, and 4 computational units, and the corresponding decoder portion has three intermediate layers of LSTM units</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I EXERCISES</head><label>I</label><figDesc>IN THE UI-PRMD DATASET</figDesc><table><row><cell cols="2">Order Exercise</cell></row><row><cell>E1</cell><cell>Deep squat</cell></row><row><cell>E2</cell><cell>Hurdle step</cell></row><row><cell>E3</cell><cell>Inline lunge</cell></row><row><cell>E4</cell><cell>Side lunge</cell></row><row><cell>E5</cell><cell>Sit to stand</cell></row><row><cell>E6</cell><cell>Standing active straight leg raise</cell></row><row><cell>E7</cell><cell>Standing shoulder abduction</cell></row><row><cell>E8</cell><cell>Standing shoulder extension</cell></row><row><cell>E9</cell><cell>Standing shoulder internal-external rotation</cell></row><row><cell>E10</cell><cell>Standing shoulder scaption</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III ABLATION</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">STUDY: AVERAGE ABSOLUTE DEVIATION PER EXERCISE</cell></row><row><cell>Exercise</cell><cell>Our Approach</cell><cell>Without Branching Layers</cell><cell>Without Temporal Pyramids</cell><cell>Without Hierarch. Layers</cell><cell>Without Recurrent Layers</cell></row><row><cell cols="2">E1-E10 0.02527</cell><cell>0.02537</cell><cell>0.02594</cell><cell>0.02953</cell><cell>0.04729</cell></row><row><cell>E1</cell><cell>0.01077</cell><cell>0.01213</cell><cell>0.01162</cell><cell>0.01222</cell><cell>0.03631</cell></row><row><cell>E2</cell><cell>0.02824</cell><cell>0.02415</cell><cell>0.02785</cell><cell>0.03522</cell><cell>0.04322</cell></row><row><cell>E3</cell><cell>0.03980</cell><cell>0.04232</cell><cell>0.04286</cell><cell>0.05350</cell><cell>0.07876</cell></row><row><cell>E4</cell><cell>0.01185</cell><cell>0.01495</cell><cell>0.01226</cell><cell>0.01048</cell><cell>0.03654</cell></row><row><cell>E5</cell><cell>0.01870</cell><cell>0.01758</cell><cell>0.01569</cell><cell>0.01719</cell><cell>0.03716</cell></row><row><cell>E6</cell><cell>0.01779</cell><cell>0.02110</cell><cell>0.01930</cell><cell>0.01858</cell><cell>0.04104</cell></row><row><cell>E7</cell><cell>0.03819</cell><cell>0.03907</cell><cell>0.04241</cell><cell>0.04016</cell><cell>0.05699</cell></row><row><cell>E8</cell><cell>0.02305</cell><cell>0.02369</cell><cell>0.02418</cell><cell>0.02658</cell><cell>0.04589</cell></row><row><cell>E9</cell><cell>0.02271</cell><cell>0.02284</cell><cell>0.02296</cell><cell>0.02738</cell><cell>0.04130</cell></row><row><cell>E10</cell><cell>0.04162</cell><cell>0.03584</cell><cell>0.04027</cell><cell>0.05395</cell><cell>0.05565</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II SEPARATION</head><label>II</label><figDesc>DEGREE FOR THE PERFORMANCE METRICS: MEAN (ST. DEV.)</figDesc><table><row><cell>Metric</cell><cell>Euclidean distance</cell><cell>Mahalanobis distance</cell><cell>DTW distance</cell><cell>Log-likelihood GMM</cell></row><row><cell></cell><cell></cell><cell>Between-subject</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON: AVERAGE ABSOLUTE DEVIATION PER EXERCISE</figDesc><table><row><cell>Exercise</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Determinants of utilization and expenditures for episodes of ambulatory physical therapy among adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Machlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Zodet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Ther</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1018" to="1029" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quality and Quantity of Rehabilitation Exercises Delivered By A 3-D Motion Controlled Camera: A Pilot Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Komatireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chokshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Basnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Phys Med Rehabil</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Home-based physical therapy intervention with adherence-enhancing strategies versus clinic-based management for patients with ankle sprains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bassett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Prapavessis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Ther</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1132" to="1143" />
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Barriers to treatment adherence in physiotherapy outpatient clinics: A systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Moffett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gardiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Man Ther</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3-2</biblScope>
			<biblScope unit="page" from="220" to="228" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exercise after Stroke: Patient Adherence and Beliefs after Discharge from Rehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Debaun-Sprague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Puymbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Top Stroke Rehabil</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="148" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on robotic devices for upper limb rehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maciejasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eschweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gerlach-Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen-Troy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of NeuroEngineering and Rehabilitation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video Game Rehabilitation for Outpatient Stroke (VIGoROUS): protocol for a multi-center comparative effectiveness trial of in-home gamified constraint-induced movement therapy for rehabilitation of chronic upper extremity hemiparesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Neurology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mathematical Modeling and Evaluation of Human Motions in Physical Therapy Using Mixture Density Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Physiother Phys Rehabil</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatio-temporal Pyramid Matching for Sports Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Data Set of Human Body Movements for Physical Rehabilitation Exercises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design, Implementation, and Experimental Results of a Quaternion-Based Kalman Filter for Human Body Motion Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Bachmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1216" to="1227" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous Hidden Markov Model for Pedestrian Activity Classification and Gait Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panahandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Handel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Gaussian mixture model based classification scheme for myoelectric control of powered upper limb prostheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Englehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hudgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1801" to="1811" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trajectory Learning for Robot Programming by Demonstration Using Hidden Markov Model and Dynamic Time Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mantegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Janabi-Sharifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1039" to="1052" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CORDIC Framework for Quaternion-based Joint Angle Computation to Classify Arm Movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Mazomenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jöbges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maharatna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-30" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting Elementary Arm Movements by Tracking Upper Limb Joint Angles With MARG Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Mazomenos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1088" to="1099" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Movement Analysis of Rehabilitation Exercises: Distance Metrics for Measuring Patient Progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houmanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kulic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Systems Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1014" to="1025" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential Deep Learning for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Behavior Understanding</title>
		<imprint>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLSTM-RNN Based 3D Gesture Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berlemont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Ordóñez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02808</idno>
		<title level="m">NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</title>
		<imprint>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep Learning on Spatio-Temporal Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Representation Learning for Human Motion Prediction and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bütepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1591" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent Network Models for Human Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Template matching based motion classification for unsupervised post-stroke rehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Bioelectronics and Bioinformations</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="199" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A computerized recognition system for the home-based physiotherapy exercises using an RGBD camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1160" to="1171" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computational model for the recognition of lower limb movement using wearable gyroscope sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Maqbool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dehghani-Sanij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJSNet</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kinect-enabled home-based rehabilitation system using Dynamic Time Warping and fuzzy logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="652" to="666" />
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Objective Assessment of Upper-Limb Mobility for Poststroke Rehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="859" to="868" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exercise Recognition for Kinect-based Telerehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Antón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goñi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Illarramendi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Inf Med</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="145" to="155" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Hidden Semi-Markov Model based approach for rehabilitation exercise assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Capecci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Movement Primitive Segmentation for Human Motion Modeling: A Framework for Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kulić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="325" to="339" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online Segmentation of Human Motion for Automated Rehabilitation Exercise Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kulić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="180" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HMM-based motion recognition system using segmented PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schonfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1288</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Metrics for Performance Evaluation of Patient Exercises during Physical Therapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Physical Medicine &amp; Rehabilitation</title>
		<imprint>
			<biblScope unit="volume">05</biblScope>
			<biblScope unit="issue">03</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mixture models : inference and applications to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Basford</surname></persName>
		</author>
		<editor>M. Dekker</editor>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>New York, N.Y.</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic Temporal Pyramid Network: A Closer Look at Multi-scale Modeling for Activity Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="712" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal Action Detection with Structured Segment Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
	<note>NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman ; Z. Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The KIMORE dataset: KInematic assessment of MOvement and clinical scores for remote monitoring of physical REhabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Capecci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11790</idno>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
