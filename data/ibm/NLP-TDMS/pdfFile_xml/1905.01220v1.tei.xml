<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seamless Scene Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Colovic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mapillary</forename><surname>Research</surname></persName>
							<email>research@mapillary.com</email>
						</author>
						<title level="a" type="main">Seamless Scene Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we introduce a novel, CNN-based architecture that can be trained end-to-end to deliver seamless scene segmentation results. Our goal is to predict consistent semantic segmentation and detection results by means of a panoptic output format, going beyond the simple combination of independently trained segmentation and detection models. The proposed architecture takes advantage of a novel segmentation head that seamlessly integrates multi-scale features generated by a Feature Pyramid Network with contextual information conveyed by a lightweight DeepLab-like module. As additional contribution we review the panoptic metric and propose an alternative that overcomes its limitations when evaluating non-instance categories. Our proposed network architecture yields state-ofthe-art results on three challenging street-level datasets, i.e. Cityscapes, Indian Driving Dataset and Mapillary Vistas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding is one of the grand goals for automated perception that requires advanced visual comprehension of tasks like semantic segmentation (Which semantic category does a pixel belong to?) and detection or instance-specific semantic segmentation (Which individual object segmentation mask does a pixel belong to?). Solving these tasks has large impact on a number of applications, including autonomous driving or augmented reality. Interestingly, and despite sharing some obvious commonalities, both these segmentation tasks have been predominantly handled in a disjoint way ever since the rise of deep learning, while earlier works <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b56">56]</ref> already approached them in a joint manner. Instead, independent trainings of models, with separate evaluations using corresponding performance metrics, and final fusion in a post-processing step based on task-specific heuristics have seen a revival.</p><p>The work in <ref type="bibr" target="#b23">[24]</ref> introduces a so-called panoptic evaluation metric for joint assessment of semantic segmentation of stuff and instance-specific thing object categories, to encourage further research on this topic. Stuff is defined as non-countable, amorphous regions of similar texture or material while things are enumerable, and have a defined shape. Few works have started adopting the panoptic metric in their methodology yet, but reported results remain sig-nificantly below the ones obtained from fused, individual models. All winning entries on designated panoptic Segmentation challenges like e.g. the Joint COCO and Mapillary Recognition Workshop 2018 1 , were based on combinations of individual (pre-trained) segmentation and instance segmentation models, rather than introducing streamlined integrations that can be successfully trained from scratch.</p><p>The use of separate models for semantic segmentation and detection obviously comes with the disadvantage of significant computational overhead. Due to a lack of crosspollination of models, there is no way of enforcing labeling consistency between individual models. Moreover, we argue that individual models supposedly spend significant amounts of their capacity on modeling redundant information, whereas sensible architectural choices in a joint setting are leading to favorable or on par results, but at much reduced computational costs.</p><p>In this work we introduce a novel, deep convolutional neural network based architecture for seamless scene segmentation. Our proposed network design aims at jointly addressing the tasks of semantic segmentation and instance segmentation. We present ideas for interleaving information from segmentation and instance-segmentation modules and discuss model modifications over vanilla combinations of standard segmentation and detection building blocks. With our findings, we are able to train high-quality, seamless scene segmentation models without the need of pre-trained recognition models. As result, we obtain a state-of-theart, single model that jointly produces semantic segmentation and instance segmentation results, at a fraction of the computational cost required when combining independently trained recognition models.</p><p>We provide the following contributions in our work:</p><p>• Streamlined architecture based on a single network backbone to generate complete semantic scene segmentation for stuff and thing classes • A novel segmentation head integrating multi-scale features from a Feature Pyramid Network <ref type="bibr" target="#b32">[33]</ref>, with contextual information provided by a light-weight, DeepLab-inspired module • Re-evaluation of the panoptic segmentation metric and refinement for more adequate handling of stuff classes • Comparisons of the proposed architecture against in-1 http://cocodataset.org/workshop/coco-mapillary-eccv-2018.html dividually trained and fused segmentation models, including analyses of model parameters and computational requirements • Experimental results on challenging driving scene datasets like Cityscapes <ref type="bibr" target="#b9">[10]</ref>, Indian Driving Dataset <ref type="bibr" target="#b51">[51]</ref>, and Mapillary Vistas <ref type="bibr" target="#b38">[39]</ref>, demonstrating state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Semantic segmentation is a long-standing problem in computer vision research <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">48]</ref> that has significantly improved over the past five years, thanks in great part to advances in deep learning. The works in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref> have introduced encoder/decoder CNN architectures for providing dense, pixel-wise predictions by taking e.g. a fully convolutional approach. The more recent DeepLab <ref type="bibr" target="#b4">[5]</ref> exploits multi-scale features via parallel filters from convolutions with different dilation factors, together with globally pooled features. Another recent Deeplab extension <ref type="bibr" target="#b8">[9]</ref> integrates a decoder module for refining object boundary segmentation results. In <ref type="bibr" target="#b7">[8]</ref>, a meta-learning technique for dense prediction tasks is introduced, that learns how to design a decoder for semantic segmentation. The pyramid scene parsing network <ref type="bibr" target="#b59">[59]</ref> employs i) a pyramidal pooling module to capture sub-region representations at different scales, followed by upsampling and stacking with respective input features and ii) an auxiliary loss applied after the conv4 block of a ResNet-101 backbone. The works in <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b58">58]</ref> propose aggregation of multi-scale contextual information using dilated convolutions, which have proven to be particularly effective for dense prediction tasks, and are a generalization of the conventional convolution operator to expand its receptive field. RefineNet <ref type="bibr" target="#b31">[32]</ref> proposes a multi-path refinement network to exploit multiple abstraction levels of features for enhancing the segmentation quality of highresolution images. Other works like <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b53">53]</ref> are addressing the problem of class sample imbalance by introducing loss-guided, pixel-wise gradient reweighting schemes.</p><p>Instance-specific semantic segmentation has recently gained large attention in the field, with early, random-fieldbased works in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">49]</ref>. In <ref type="bibr" target="#b16">[17]</ref> a simultaneous detection and segmentation algorithm is developed that classifies and refines CNN features obtained from regions under R-CNN <ref type="bibr" target="#b15">[16]</ref> bounding box proposals. The work in <ref type="bibr" target="#b17">[18]</ref> emphasizes on refining object boundaries for binary segmentation masks initially generated from bounding box proposals. In <ref type="bibr" target="#b11">[12]</ref> a multi-task network cascade is introduced that, beyond sharing features from the encoder in all following tasks, subsequently adds blocks for i) bounding box generation, ii) instance mask generation and iii) mask categorization. Another approach <ref type="bibr" target="#b10">[11]</ref> introduces instance fully convolutional networks that assemble segmentations from position-sensitive score maps, generated by classifying pixels based on their relative positions. The follow-up work in <ref type="bibr" target="#b30">[31]</ref> builds upon Faster R-CNN <ref type="bibr" target="#b42">[43]</ref> for proposal generation and additionally includes position-sensitive outside score maps. InstanceCUT <ref type="bibr" target="#b24">[25]</ref> obtains instance segmentations by solving a Multi-Cut problem, taking instanceagnostic semantic segmentation masks and instance-aware, probabilistic boundary masks as inputs, provided by a CNN. The work in <ref type="bibr" target="#b0">[1]</ref> also introduces an approach where an instance Conditional Random Field (CRF) provides individual instance masks based on exploiting box, global and shape cues as unary potentials, together with instanceagnostic semantic information. In <ref type="bibr" target="#b35">[36]</ref>, sequential grouping networks are presented that run a sequence of simple networks for solving increasingly complex grouping problems, eventually yielding instance segmentation masks. Deep-Mask <ref type="bibr" target="#b39">[40]</ref> first produces an instance-agnostic segmentation mask for an input patch, which is then assigned to a score corresponding to how likely this patch it to contain an object. At inference, their approach generates a set of ranked segmentation proposals. The follow-up work Sharp-Mask <ref type="bibr" target="#b40">[41]</ref> augments the networks with a top-down refinement approach. Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> forms the basis of current state-of-the-art instance segmentation approaches. It is a conceptually simple extension of Faster R-CNN, adding a dedicated branch for object mask segmentation in parallel to the existing ones for bounding box regression and classification. Due to its importance in our work, we provide a more thorough review in the next section. The work in <ref type="bibr" target="#b36">[37]</ref> proposes to improve localization quality of objects in Mask R-CNN via integration of multi-scale information as bottom-up path augmentation.</p><p>Joint segmentation and instance-segmentation approaches date back to <ref type="bibr" target="#b50">[50]</ref>, introducing a Bayesian approach for scene representation by establishing a scene parsing graph to explain both, segmentation of stuff and things. Other works before the era of deep learning often built upon CRFs where <ref type="bibr" target="#b49">[49]</ref> alternatingly refined pixel labelings and object instance predictions, and <ref type="bibr" target="#b56">[56]</ref> framed holistic scene understanding as a structure prediction problem in a graphical model, defined over hierarchies of regions, scene types, etc. The recently proposed work in <ref type="bibr" target="#b21">[22]</ref> addresses automated loss balancing in a multi-task learning problem based on analysing the homoscedastic uncertainty of each task. Even though their work addresses three tasks at the same time (semantic segmentation, instance segmentation and depth estimation), it fails to demonstrate consistent improvements over semantic segmentation and instance segmentation alone and lacks of comparisons to comparable baselines. The supervised variant in <ref type="bibr" target="#b29">[30]</ref> generates panoptic segmentation results but i) requires separate (external) input for bounding box proposals and ii) exploits a CRF during inference, increasing the complexity of the model.</p><p>The work in <ref type="bibr" target="#b13">[14]</ref> attempts to introduce a unified architecture related to our ideas, however, the reported results remain significantly below those of reported state-of-the-art methods. Independently and simultaneously to our paper, a number of works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b55">55]</ref> have proposed panoptic segmentation provided by a single deep network, confirming the importance of this task to the field. While comparable in complexity and architecture, we obtain improved performance on challenging street-level image datasets like Cityscapes and Mapillary Vistas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Architecture</head><p>The proposed architecture consists of a backbone working as feature extractor and two task-specific branches addressing semantic segmentation and instance segmentation, respectively. Hereafter, we provide details about each component and refer to <ref type="figure" target="#fig_0">Fig. 1</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Shared Backbone</head><p>The backbone that we use throughout this paper is a slightly modified ResNet-50 <ref type="bibr" target="#b19">[20]</ref> with a Feature Pyramid Network (FPN) <ref type="bibr" target="#b32">[33]</ref> on top. The FPN network is linked to the output of the modules conv2, conv3, conv4 and conv5 of ResNet-50, which yield different downsampling factors, namely ×4, ×8, ×16 and ×32, respectively. Akin to the original FPN architecture, we have a variable number of additional, lower resolution scales covering downsampling factors of ×64 and ×128, depending on the dataset. The main modification in ResNet-50 is the replacement of all Batch Normalization (BN) + ReLU layers with synchronized Inplace Activated Batch Normalization (iABN sync ) proposed in <ref type="bibr" target="#b45">[46]</ref>, which uses LeakyReLU with slope 0.01 as activation function due to the need of invertible activation functions. This modification gives two important advantages: i) we gain up to 50% additional GPU memory since the layer performs in-place operations, and ii) the synchronization across GPUs ensures a better estimate of the gradients in multi-GPU trainings with positive effects on convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Segmentation Branch</head><p>The instance segmentation branch follows the state-ofthe-art Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> architecture. This branch is structured into a region proposal head and a region segmentation head.</p><p>Region Proposal Head (RPH). The RPH introduces the notion of an anchor. An anchor is a reference bounding box (a.k.a. region), centered on one of the available spatial locations of the RPH's input and having pre-defined dimensions. The set of pre-defined dimensions is chosen in advance, depending on the dataset and the scale of the FPN output (see details in Sec. 5). We denote by A all anchors that can be constructed by combining a position on an available, spatial location and a dimension from the pre-defined set, and which are entirely contained in the image. Given an anchor a we denote its position (in the image coordinate system) by (u a , v a ) and its dimensions by (w a , h a ). The role of RPH is to apply a transformation to each anchor in order to obtain a new bounding box proposal together with an objectness score, that assesses the validity of the region. To this end, RPH applies a 3 × 3 convolution with 256 output channels and stride 2 to the outputs of the backbone, followed by iABN sync , and a 1 × 1 convolution with 5N anchors channels, which provide a bounding box proposal with an objectness score for each anchor in A. In more details, for each anchor a ∈ A the transformed bounding box has center</p><formula xml:id="formula_0">(û,v) = (u a + o u w a , v a + o v h a ), dimensions (ŵ,ĥ) = (w a e ow , h a e o h ) and objectness scorê s = σ(o s ), where (o u , o v , o w , o h , o s )</formula><p>represents the output from the 1 × 1 convolution for anchor a, and σ(·) is the sigmoid function. The resulting set of bounding boxes are then fed to the region segmentation head, with distinct filtering steps for training and test time.</p><p>Region Segmentation Head (RSH). Each region proposal r = (û,v,ŵ,ĥ) obtained from RPH is fed to RSH, which applies ROIAlign <ref type="bibr" target="#b18">[19]</ref>, pooling features directly from the kth output of the backbone within regionr with a 14 × 14 spatial resolution, where k is selected based on the scale ofr according to the formula k = max(1, min(4, 3 + log 2 ( ŵĥ/224) )) <ref type="bibr" target="#b18">[19]</ref>. The result is forwarded to two parallel sub-branches: one devoted to predicting a class label (or void) for the region proposal together with classspecific corrections of the proposal's bounding box, and the other devoted to providing class-specific mask segmentations. The first sub-branch of RSH is composed of two fully-connected layers with 1024 channels, each followed by Group Normalization (GN) <ref type="bibr" target="#b52">[52]</ref> and LeakyReLU with slope 0.01, and a final fully-connected layer with 5N classes + 1 output units. The output units encode, for each possible class c, class-specific correction factors</p><formula xml:id="formula_1">(o c u , o c v , o c w , o c h ) that are used to compute a new bounding box centered in (û c ,v c ) = (û + o c uŵ ,v + o c vĥ ) with dimensions (ŵ c ,ĥ c ) = (ŵ e o c w ,ĥ e o c h )</formula><p>. This operation generates fromr and for each class c a new class-specific region proposals given bŷ r c = (û c ,v c ,ŵ c ,ĥ c ). In addition, we have N classes + 1 units providing logits for a softmax layer that gives a probability distribution over classes and void, the latter label assessing the invalidity of the proposal. The probability associated to class c is used as score functionŝ c for the class-specific region proposalr c . The second sub-branch applies four 3 × 3 convolution layers each with 256 output channels. As for the first sub-branch each convolution is followed by GN and LeakyReLU. This is followed by a 2 × 2 deconvolution layer with output stride 2 and 256 output channels, GN, LeakyReLU, and a final 1 × 1 convolution with N classes output channels. This yields, for each class, 28 × 28 logits that provide class-specific mask foreground probabilities for the given region proposal via a sigmoid. The resulting mask prediction is combined with the output of the segmentation branch described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Segmentation Branch</head><p>The semantic segmentation branch takes as input the outputs of the backbone corresponding to the first four scales of FPN. We apply independently to each input (not sharing parameters) a variant of the DeepLabV3 head <ref type="bibr" target="#b5">[6]</ref> that we call Mini-DeepLab (MiniDL, see <ref type="figure" target="#fig_1">Fig. 2</ref>) followed by an upsampling operation that yields an output downsampling factor of ×4 and 128 output channels. All the resulting streams are concatenated and the result is fed to a final 1 × 1 convolution layer with N classes output channels. The output is bilinearly upsampled to the size of the input image. This provides the logits for a final softmax layer that provides class probabilities for each pixel of the input image. Each convolution in the semantic segmentation branch, including MiniDL, is followed by iABN sync akin to the backbone.</p><p>MiniDL. The MiniDL module consists of 3 parallel subbranches. The first two apply a 3 × 3 convolution with 128 output channels with dilations 1 and 6, respectively. The third one applies a 64 × 64 average pooling operation with stride 1 followed by a padding with boundary replication to recover the spatial resolution of the input and a 1 × 1 convolution with 128 output channels. The outputs of the 3 sub-branches are concatenated and fed into a 3 × 3 convolution layer with 128 output channels, which delivers the final output of the MiniDL module.</p><p>As opposed to DeepLabV3, we do not perform the global pooling operation in our MiniDL module for two reasons: i) it breaks translation equivariance if we change the input resolution at test time, which is typically the case and ii) since we work with large input resolutions, it is preferable to limit the extent of contextual information. Instead, we replaced the global pooling operation with average pooling in the 3rd sub-branch with a fixed large kernel size and stride 1, but without padding. The lack of padding yields an output resolution which is smaller than the input resolution and we re-establish the input resolution by replicating the boundary of the resulting tensor, i.e. we employ a padding layer with boundary replication. By doing so, we generalize the solution originally implemented in DeepLabV3, for we obtain the same output at training time if we keep the kernel size equal to the training input resolution, but we preserve translation equivariance at test time, and can reduce the extent of contextual information by properly fixing the kernel size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training losses</head><p>The two branches of the architecture are supported with distinct losses, which are detailed below. We denote by Y = {1, . . . , N classes } the set of class labels, and assume for simplicity input images with fixed resolution H × W .</p><p>Semantic segmentation branch. Let Y ij ∈ Y be the semantic segmentation ground truth for a given image and pixel position (i, j) and let P ij (c) denote the predicted probability for the same pixel to be assigned class c ∈ Y. The per-image segmentation loss that we employ is a weighted per-pixel log-loss that is given by</p><formula xml:id="formula_2">L ss (P, Y ) = − ij ω ij log P ij (Y ij ) .</formula><p>The weights are computed following the simplest version of <ref type="bibr" target="#b44">[45]</ref> with p = 1 and τ = 4 W H . This corresponds to having a pixel-wise hard negative mining, which selects the 25% worst predictions, i.e. ω ij = τ for all (i, j) within the 25% pixels yielding the lowest probability P ij (Y ij ), and ω ij = 0 otherwise.</p><p>Instance segmentation branch. The losses for the instance segmentation branch and the training procedure are derived from the ones proposed in Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>. We start with the losses applied to the output of RPH. Let R be the set of ground truth bounding boxes for a given image I, letR be the set of bounding boxes generated from I by RPH and letr a ∈R be the proposal originated from anchor a ∈ A. We employ the same strategy adopted in Mask-RCNN to assign ground truth boxes in R to predictions inR. For each ground truth box r ∈ R, the closest anchor in A, i.e. the one with largest Intersection over Union (IoU), is selected and the corresponding predictionr a ∈R is regarded as positively matching r. For each anchor a ∈ A, we seek the closest ground truth box r ∈ R and regard the match between r andr a , i.e. the predicted box inR corresponding to anchor a, as positive match if IoU(r, a) &gt; τ H , or negative match if IoU(r, a) &lt; τ L , where τ H &gt; τ L are user-defined thresholds (we use τ H = 0.7, τ L = 0.3). We take a random subset M + ⊂ R ×R of all positive matches and a random subset M − ⊂ R ×R of all negative matches, where we typically set |M + | ≤ 128 and |M − | ≤ 256 − |M + | in order to have at most 256 matches. The objectness loss for the given image is</p><formula xml:id="formula_3">L ob RPH (M ± ) = − 1 |M| (r,r)∈M+ log sr − 1 |M| (r,r)∈M− log(1 − sr) ,</formula><p>where sr is the objectness score of the predicted bounding boxr, and M = M + ∪ M − , while the bounding box regression loss is defined only on positive matches M + and is given by</p><formula xml:id="formula_4">L bb RPH (M ± ) = 1 |M| (r,r)∈M+ x r − xr w ar S + y r − yr h ar S + log wr w r S + log hr h r S ,</formula><p>where |·| S is the smooth L 1 norm <ref type="bibr" target="#b42">[43]</ref>, r = (x r , y r , w r , h r ), r = (xr, yr, wr, hr) and ar is the anchor that originated predictionr. We next move to losses that pertain to RSH. Let again R be the ground truth boxes for a given image I and let R be the union of R and the set of bounding boxes generated by RPH from I, filtered with Non-Maxima Suppression (NMS), and clipped to the image area. For each r ∈Ȓ, we find the closest (in terms of IoU) ground truth box r ∈ R and regard it as a positive match if IoU(r,ȓ) &gt; η and as negative match otherwise (we use η = 0.5). Let N + and N − be random subsets or all positive and negative matches, respectively, where we typically set |N + | ≤ 128 and |N − | ≤ 512 − |N + | in order to have at most 512 matches. The region proposal classification loss is given by</p><formula xml:id="formula_5">L cls RSH (N ± ) = − 1 |N | (r,ȓ)∈N+ log s cȓ r − 1 |N | (r,ȓ)∈N− log s ∅ r ,</formula><p>where N = N + ∪ N − , ∅ denotes the void class, c r is the class of the ground truth bounding box r ∈ R, s c r is the probability given by RSH of the input proposalȓ to take class c. The bounding box regression loss is defined only on positive matches N + and is given by</p><formula xml:id="formula_6">L bb RSH (N ± ) = 1 |N | (r,ȓ)∈N+ x r − x cȓ r wȓ S + y r − y cȓ r hȓ S + log w cȓ r w r S + log h cȓ r h r S .</formula><p>Finally, the mask segmentation loss is given as follows. Letȓ be a region proposal entering RSH matching a groundtruth region r, i.e. (r,ȓ) ∈ N + . Let S r ∈ {0, 1, ∅} 28×28 be the corresponding ground truth binary mask with associated class label c, where ∅ denotes a void pixel, and let Sȓ ∈ [0, 1] 28×28 be the mask prediction for class c obtained from RSH with entries Sȓ ij denoting the probability of cell (i, j) to belong to the instance conditioned on class c and region ȓ. Then, the loss for the matched proposalȓ is given by</p><formula xml:id="formula_7">L msk RSH (S r , Sȓ) = − 1 |P r | (i,j)∈P r S r ij log Sȓ ij − 1 |P r | (i,j)∈P r (1 − S r ij ) log(1 − Sȓ ij ) ,</formula><p>where P r is the set of pixels being non-void in the groundtruth 28 × 28 mask S r . The mask losses corresponding to all matched proposals are finally summed and divided by |N | akin to the bounding box loss L bb RSH . All losses are weighted equally and gradient from RSH flows only to the backbone, i.e. the gradient component flowing from RSH to RPH is blocked, akin to Mask-RCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Testing and Panoptic Fusion</head><p>At test time, given an input image I we extract features F with the backbone and generate region proposals with corresponding objectness scores by applying RPH. We filter the resulting set of bounding boxes with Non-Maxima Suppression (NMS) guided by the objectness scores. The surviving proposals are fed to the RSH (first sub-branch) together with F in order to generate class-specific region proposals with corresponding class probabilities. A second NMS pass is applied on the resulting set of bounding boxes, this time independently per class guided by the class probabilities. The resulting class-specific bounding boxes are fed again to RSH together with F , but this time through the second sub-branch which provides the corresponding mask predictions. The extracted features F are fed in parallel to the segmentation branch, which provides class probabilities for each pixel. The output of RSH and the segmentation branch are finally fused using the strategy given below, in order to deliver the final panoptic segmentation. Fusion. The fusion operation is inspired by the one proposed in <ref type="bibr" target="#b23">[24]</ref>. We start iterating over predicted instances in reverse classification score order. For each instance we mark the pixels in the final output that belong to it and are still unassigned, provided that the latter number of pixels covers at least 50% of the instance. Otherwise we discard the instance thus resembling a NMS procedure. Remaining unassigned pixels take the most likely class according to the segmentation head prediction, if it belongs to stuff, or void if it belongs to thing. Finally, if the total amount of pixels of any stuff class is smaller than a given threshold (4096 in our case) we mark all those pixels to void.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Revisiting Panoptic Segmentation</head><p>In this section we review the panoptic segmentation metric <ref type="bibr" target="#b23">[24]</ref> (a.k.a. PQ metric), which evaluates the performance of a so-called panoptic segmentation, and discuss a limitation of this metric when it comes to stuff classes. PQ metric. A panoptic segmentation assigns each pixel a stuff class label or an instance ID. Instance IDs are further given a thing class label (e.g. pedestrian, car, etc.). As opposed to AP metrics used in detection, instances are not overlapping. The PQ metric is computed for each class independently and averaged over classes (void class excluded). This makes the metric insensitive to imbalanced class distributions. Given a set of ground truth segments S c and predicted segmentsŜ c for a given class c, the metric collects a set of True Positive matches as TP c = {(s,ŝ) ∈ S c ×Ŝ c : IoU(s,ŝ) &gt; 0.5} . This set contains all pairs of ground truth and predicted segments that overlap in terms of IoU more than 0.5. By construction, every ground truth segment can be assigned at most one predicted segment and vice versa. The PQ metric for class c is given by</p><formula xml:id="formula_8">PQ c = (s,ŝ)∈TPc IoU(s,ŝ) |TP c | + 1 2 |FP c | + 1 2 |FN c | ,</formula><p>where FP c is the set False Positives, i.e. unmatched predicted segments for class c, and FN c is the set False Negatives, i.e. unmatched segments from ground truth for class c.</p><p>The metric allows also specification of void classes, both in ground truth and actual predictions. Pixels labeled as void in the ground truth are not counted in IoU computations and predicted segments of any class c that overlap with void more than 50% are not counted in FP c . Also, ground truth segments for class c that overlap with predicted void pixels more than 50% are not counted in FN c . The final PQ metric is obtained by averaging the class-specific PQ scores:</p><formula xml:id="formula_9">PQ = 1 N classes c∈Y PQ c .</formula><p>We further denote by PQ Th and PQ St the average of thingspecific and stuff-specific PQ scores, respectively. The issue with stuff classes. One limitation of the PQ metric is that it over-penalizes errors related to stuff classes, which are by definition not organized into instances. This derives from the fact that the metric does not distinguish stuff and thing classes and applies indiscriminately the rule that we have a true positive if the ground truth and the predicted segment have IoU greater than 0.5. De facto it regards all pixels in an image belonging to a stuff class as a single big instance. To give an example of why we think this is sub-optimal, consider a street scene with two sidewalks and assume that the algorithm confuses one of the two with road (say the largest) then the segmentation quality on sidewalk for that image becomes 0. A real-world example is provided in <ref type="figure" target="#fig_2">Fig. 3</ref>, where several stuff segments are severely penalized by the PQ metric, not reflecting the real quality of the segmentation. The &gt;0.5-IoU rule for thing classes is convenient because it renders the matching between predicted and ground truth instances easy, but this is , are just below the PQ acceptance threshold, while the sidewalk class (IoU 0.62) is just above it. Thus, the former will be overly penalized (PQ → 0), while the latter will contribute positively (PQ → 0.62), even if they look qualitatively similar. Best viewed in color and with digital zoom. a problem to be solved only for thing classes. Indeed, predicted and ground truth segments belonging to stuff classes can be directly matched independently from their IoU because each image has at most one instance of them. Suggested alternative. We propose to maintain the PQ metric only for thing classes, but change the metric for stuff classes. Specifically, let S c be the set of ground truth segments of a given class c and letŜ c be the set of predicted segments for class c. Note that each image can have at most 1 ground truth segment and at most 1 predicted segment of the given stuff class. Let M c = {(s,ŝ) ∈ S c ×Ŝ c : IoU(s,ŝ) &gt; 0} be the set of matching segments, then the updated metric for class c becomes:</p><formula xml:id="formula_10">PQ † c = 1 |Sc| (s,ŝ)∈Mc IoU(s,ŝ) , if c is stuff class PQ c , otherwise.</formula><p>We denote by PQ † the final version of the proposed panoptic metric, which averages PQ † c over all classes, i.e.</p><formula xml:id="formula_11">PQ † = 1 N classes c∈Y PQ † c .</formula><p>Similarly to PQ, the proposed metric is bounded in [0, 1] and implicitly regards a stuff segment of an image as a single instance. However, we do not require the prediction of stuff classes to have IoU&gt;0.5 with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We assess the benefits of our proposed network architecture on multiple street-level image datasets, namely Cityscapes <ref type="bibr" target="#b9">[10]</ref>, Mapillary Vistas <ref type="bibr" target="#b38">[39]</ref> and the Indian Driving Dataset (IDD) <ref type="bibr" target="#b51">[51]</ref>. All experiments were designed to provide a fair comparison between baseline reference models and our proposed architecture design choices. To increase transparency of our proposed design contributions, we deliberately leave out model extensions like path aggregation network extensions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, deformable convolutions <ref type="bibr" target="#b12">[13]</ref> or Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref>. We do not apply test time data augmentation (multi-scale testing or horizontal flipping) or explicit use of model ensembles, etc., as we assume that such bells and whistles approximately equally increase recognition performances for all methods. All models were only pre-trained on ImageNet <ref type="bibr" target="#b46">[47]</ref>. We use the following terminology in the remainder of this section: Ours Independent refers to fused, but individually trained models ( <ref type="figure" target="#fig_0">Fig. 1  left)</ref> each following the proposed design, and Ours Combined refers to the unified architecture in <ref type="figure" target="#fig_0">Fig. 1 (right)</ref>.</p><p>Model and Training Hyperparameters. Unless otherwise noted, we take all the hyperparameters of the instance segmentation branch from <ref type="bibr" target="#b18">[19]</ref>. These hyperparameters are shared by all the models we evaluate in our experiments. We initialize our backbone model with weights extracted from PyTorch's ImageNet-pretrained ResNet-50 despite using a different activation function, motivated by findings in our prior work <ref type="bibr" target="#b45">[46]</ref>. We train all our networks with SGD, using a fixed schedule of 48k iterations and learning rate 10 −2 , decreasing the learning rate by a factor 10 after 36k and 44k iterations. At the beginning of training we perform a warm-up phase where the learning rate is linearly increased from 1 3 ·10 −2 to 10 −2 in 200 iterations. <ref type="bibr" target="#b1">2</ref> During training the networks receive full images as input, randomly flipped in the horizontal direction, and scaled such that their shortest side measures 1024 · t pixels, where t is randomly sampled from [0.5, 2.0]. Training is performed on batches of 8 images using a computing node equipped with 8 Nvidia V100 GPUs. At test time, images are scaled such that their shortest size measures 1024 pixels (preserving aspect ratio).</p><p>Differences with respect to <ref type="bibr" target="#b41">[42]</ref>. Some scores reported in this document differ from our corresponding CVPR 2019 paper, due to mistakenly copying numbers from the experiment logs and improper handling of void labels in Vistas. For fair comparisons please cite the numbers given here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Cityscapes</head><p>Cityscapes <ref type="bibr" target="#b9">[10]</ref> is a street-level driving dataset with images from 50 central-European cities. All images were recorded with a single camera type, image resolution of 1024 × 2048, and during comparable weather and lighting conditions. It has a total of 5,000 pixel-specifically annotated images (2,975/500/1,525 for training, validation and test, respectively), and additionally provides 19,998 images forming the coarse extra set, where only coarse annotations per image are available (which we have not used in our experiments). Images are annotated into 19 object classes (11 stuff and 8 instance-specific).</p><p>For Ours Independent, we trained each recognition model independently, using the hyperparameter settings described above (again, each with a ResNet-50+FPN backbone), and fused the outputs into a single panoptic prediction as described in Section 3.5. To assess the quality of our obtained panoptic results in terms of standard segmentation metrics, we drop the information about instance identity and retain only the pixel-wise class assignment. By doing so, we can evaluate the quality of Ours Independent in terms of mIoU (mean Intersection-over-Union <ref type="bibr" target="#b14">[15]</ref>) against other state-of-the-art semantic segmentation methods. We obtain a result of 75.4%, which is comparable or slightly better than 75.2% reported in <ref type="bibr" target="#b27">[28]</ref> (using a DenseNet-169 backbone), 73.6% using DeepLab2 in combination with a ResNet-101 backbone as reported in <ref type="bibr" target="#b44">[45]</ref>, or 74.6% with a ResNet-152 in <ref type="bibr" target="#b53">[53]</ref>. Similarly, we can evaluate the instancesegmentation results in terms of AP M (mean average precision on masks) by setting to void the stuff pixels. The result of our baseline is 31.9%, which is slightly above the reported baseline score in Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> (31.5% w/o COCO <ref type="bibr" target="#b33">[34]</ref> pre-training).</p><p>As for the PQ metrics, Ours Independent delivers PQ = 59.8%, PQ St = 64.5%, PQ Th = 53.4% and PQ † = 59.0%. We also provide results of Ours Combined in Tab. 1, per-forming slightly better on both PQ and PQ † . This is remarkable, given the significantly reduced number of model parameters (see <ref type="bibr">Section 5.4)</ref> and when assuming that the fusion of individually trained models could lead to an ensemble effect (often deliberately used to improve test results, at the cost of increased computational complexity).</p><p>In addition, we show results of jointly trained networks from independent, concurrently appearing works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b55">55]</ref>, with focus on comparability of network architectures and data used for pre-training. In Tab. 1 we abbreviate the network backbones as R50, R101 or X71 for ResNet50, ResNet101 or Xception Net71, respectively, and provide datasets used for pre-training (I = ImageNet and C = COCO). All our proposed variants outperform the direct competitors by a considerable margin, i.e. our baseline models as well as jointly trained architectures are better. Finally, the top row in <ref type="figure" target="#fig_3">Fig. 4</ref> shows some qualitative seamless segmentation results obtained with our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Indian Driving Dataset (IDD)</head><p>IDD <ref type="bibr" target="#b51">[51]</ref> was introduced for testing perception algorithm performance in India. It comprises 10,003 images from 182 driving sequences, divided in 6,993/981/2,029 images for training, validation and test, respectively. Images are either of 720p or 1080p resolution and were obtained from a front-facing camera mounted on a car roof. The dataset is annotated into 26 classes (17 stuff and 9 instance- specific), and we report results for level 3 labels. Following the same procedure described for Cityscapes, we evaluate Ours Independent on segmentation and instance segmentation yielding IoU = 68.0% and AP M = 32.1%, respectively. To contextualize the obtained results, the numbers reported as baselines in <ref type="bibr" target="#b51">[51]</ref> for semantic segmentation are 55.4% using ERFNet <ref type="bibr" target="#b43">[44]</ref> and 66.6% for dilated residual nets <ref type="bibr" target="#b58">[58]</ref> and again Mask R-CNN for instance-specific segmentation on a ResNet-101 body yielding AP M = 26.8%. Those numbers supposedly belong to the test set, while no numbers are reported for validation. In terms of panoptic metrics, Ours Independent yields PQ = 47.2%, PQ St = 46.6%, PQ Th = 48.3% and PQ † = 48.8%. For Ours Combined we obtain PQ = 46.9%, PQ St = 45.9%, PQ Th = 48.7%, PQ † = 48.6%, AP M = 29.8% and IOU = 68.2%. In the key metrics PQ and PQ † the results differ by ≤ 0.3 points, and we again stress that the numbers for Ours Combined are provided from a network with significantly less parameters.</p><p>The middle row in <ref type="figure" target="#fig_3">Fig. 4</ref> shows seamless segmentation results obtained by our combined architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Mapillary Vistas</head><p>Mapillary Vistas <ref type="bibr" target="#b38">[39]</ref> is one of the richest, publicly available street-level image datasets today. It comprises 25k high-resolution (on average 8.6 MPixels) images, split into sets of 18k/2k/5k images for training, validation and test, respectively. We only used the training set during model training while evaluating on the validation set. Vistas shows street-level images from all over the world, with images captured from driving cars as well as pedestrians taken them on a sidewalk. It also has large variability in terms of weather, lighting, capture time during day and season, sensor type, etc., making it a very challenging road scene segmentation benchmark. Accounting for this, we modify some of the model hyper-parameters and training schedule as follows: we use anchors with aspect ratios in {0.2, 0.5, 1, 2, 5} and area (2 × D) 2 , where D is the FPN level downsampling factor; we train on images with shortest side scaled to 1920 t , where t is randomly sampled from [0.8, 1.25]; we train for a total of 192k iterations, decreasing the learning rate after 144k and 176k iterations.</p><p>The results obtained with Ours Independent and Ours Combined are given in Tab. 1. The joint architecture besides using significantly less parameters achieves also slightly better results compared to the independently trained models (+0.5% PQ and +0.4% PQ † ). Compared to other state-of-the-art methods, we obtain +5.7% and +5.1% PQ score over DeeperLab <ref type="bibr" target="#b55">[55]</ref> and TASCNet <ref type="bibr" target="#b28">[29]</ref>, respectively, despite using a weaker backbone compared to DeeperLab and not pre-training on COCO as opposed to TASCNet.</p><p>Finally, we show seamless scene segmentation results in the bottom row of <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Computational Aspects</head><p>We discuss computational aspects when comparing the two individually trained recognition models against our combined model architecture. When fused, the two taskspecific models have ≈ 78.06M parameters, i.e. ≈ 51.8% more than our combined architecture (≈ 51.43M ). The majority of saved parameters belong to the backbone. The amount of computation is similarly reduced, i.e. the combined, independently trained models require ≈ 50.4% more FLOPs due to two inference steps per test image. In absolute terms, the individual models require ≈ 0.864 TFLOP while our combined architectures requires ≈ 0.514 TFLOP on 1024 × 2048 image resolution, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation of the New Pooling in DeeplabV3</head><p>In Section 3.3, we propose a modification of the DeepLabV3 global pooling operation in order to limit its extent and preserve translation equivariance. Specifically, DeepLabV3 applies global average spatial pooling to the backbone outputs and replicates the outcome to each spatial location. Instead, we replace the global pooling operation with average pooling with a fixed kernel size (in our experiments we use 96 × 96 corresponding to an image area of 768 × 768), with stride 1 and no padding. This is equivalent to a convolution with a smoothing kernel that yields a reduced spatial output resolution. To recover the original spatial resolution we apply padding with replicated boundary. Our motivation is twofold: i) if we train without limiting the input size (i.e. without crops) the context that is captured by global pooling is too wide and degrades the performance compared to having a reduced context as the one captured by our modification and ii) in case one trains with crops, the use of global pooling also at test time breaks translation equivariance and induces a substantial change of feature representation between test and training time, while our modification is not affected by this issue. To give an experimental evidence of the former motivation, we trained a segmentation model with the DeepLabV3 head and ResNet-50 backbone on Cityscapes at full resolution by keeping the original DeepLabV3 pooling strategy yielding 72.7% mIoU, whereas the same network and training schedule yields 74.5% (+1.8% absolute) with our new pooling strategy. To support the second motivation, we trained our baseline segmentation model with random 768 × 768 crops using the standard DeepLabV3 pooling strategy, yielding 73.3% mIoU, whereas the same network with our pooling strategy yields 74.2% (+0.9% absolute).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work we have introduced a novel CNN architecture for producing seamless scene segmentation results, i.e. semantic segmentation and instance segmentation modules jointly operating on top of a single network backbone. We depart from the prevailing approach of training individual recognition models, and instead introduce a multi-task architecture that benefits from interleaving network components as well as a novel segmentation module. We also revisit the panoptic metric used to assess combined segmentation and detection results and propose a relaxed alternative for handling stuff segments. Our findings include that we can generate state-of-the-art recognition results that are significantly more efficient in terms of computational effort and model sizes, compared to combined, individual models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of two architectures for panoptic segmentation. Left: Separate models (including bodies) for detection and segmentation. Both predictions are fused to obtain the final panoptic prediction. Right: Shared body between the heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Segmentation Head (top) and the architecture of the Mini Deeplab (MiniDL) module (bottom), which is used in the head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Prediction on a Cityscapes validation set image, where light colored areas highlight conducted errors. Several classes, e.g. pole (IoU 0.49) and traffic light (IoU 0.46)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results obtained by our proposed combined architecture. Top row: Cityscapes. Middle row: IDD. Bottom row: Vistas. Best viewed in color and with digital zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Cityscapes Mapillary Vistas Method Body Data PQ PQ St PQ Th PQ † AP M IoU PQ PQ St PQ Th PQ † AP M IoU de Geus et al. [14] R50 Comparison of validation set results on Cityscapes and Vistas with related works. Used network bodies include R101, R50 and X71 for ResNet-101, ResNet-50 and Xception-71, respectively. Data indicates datasets used for pre-training where I = ImageNet and C = COCO. All results in [%].</figDesc><table><row><cell></cell><cell></cell><cell>I</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">17.6 27.5 10.0</cell><cell>-</cell><cell>-</cell><cell>34.7</cell></row><row><cell cols="3">Supervised in [30] R101 I</cell><cell cols="3">47.3 52.9 39.6</cell><cell>-</cell><cell cols="2">24.3 71.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FPN-Panoptic [23] R50</cell><cell>I</cell><cell cols="3">57.7 62.2 51.6</cell><cell>-</cell><cell cols="2">32.0 75.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TASCNet [29]</cell><cell>R50</cell><cell cols="4">I+C 59.2 61.5 56.0</cell><cell>-</cell><cell cols="5">37.6 77.8 32.6 34.4 31.1</cell><cell>-</cell><cell>18.5</cell><cell>-</cell></row><row><cell>UPSNet [54]</cell><cell>R50</cell><cell>I</cell><cell cols="3">59.3 62.7 54.6</cell><cell>-</cell><cell cols="2">33.3 75.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeeperLab [55]</cell><cell>X71</cell><cell>I</cell><cell>56.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.3</cell></row><row><cell>Ours Independent</cell><cell>R50</cell><cell>I</cell><cell cols="12">59.8 64.5 53.4 59.0 31.9 75.4 37.2 42.5 33.2 38.6 16.3 50.2</cell></row><row><cell>Ours Combined</cell><cell>R50</cell><cell>I</cell><cell cols="12">60.3 63.3 56.1 59.6 33.6 77.5 37.7 42.9 33.8 39.0 16.4 50.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the warm-up phase is not strictly needed for convergence. Instead, we adopt it for compatibility with<ref type="bibr" target="#b18">[19]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Sun</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Daan De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubbelman</surname></persName>
		</author>
		<idno>abs/1809.02110</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Pascal visual object classes (VOC) challenge. (IJCV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An exemplar-based crf for multi-instance object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Panoptic feature pyramid networks. CoRR, abs/1901.02446</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Panoptic segmentation. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno>abs/1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Structured labels in random forests for semantic labelling and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Krapac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to fuse things and stuff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno>abs/1812.01192</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1611.07709</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: Common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An end-to-end network for panoptic segmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Loss maxpooling for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karphathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. (IJCV)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="2" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Indian driving dataset (IDD): A dataset for exploring problems of autonomous navigation in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbumani</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Group normalization. In (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Upsnet: A unified panoptic segmentation network. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deeperlab: Single-shot image parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1902.05093</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
