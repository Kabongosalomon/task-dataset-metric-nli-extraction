<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Twitter / Imperial College London United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Twitter / Imperial College London United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Twitter United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Twitter United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Twitter / Imperial College London United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Twitter United Kingdom</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motifinduced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-ofthe-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges. * Equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning on graphs, also known as geometric deep learning (GDL) <ref type="bibr" target="#b7">[9]</ref> or graph representation learning (GRL) <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b64">66]</ref>, has emerged in a matter of just a few years from a niche topic to one of the most prominent fields in machine learning. Graph deep learning models have recently scored successes in various applications relying on modeling relational data, see e.g. <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b41">43]</ref>. Graph Neural Networks (GNNs) seek to generalize classical convolutional architectures (CNNs) to graph-structured data, with a wide variety of convolution-like operations available in the literature <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b22">24]</ref>.</p><p>Until recently, most of the research in the field has focused on small-scale datasets, and relatively little effort has been devoted to scaling these methods to web-scale graphs such as the Facebook or Twitter social networks. Scaling is a major challenge precluding the wide application of graph deep learning methods in industrial settings. Compared to Euclidean neural networks where the training loss can be decomposed into individual samples and computed independently, graph convolutional networks diffuse information between nodes along the edges of the graph, making the loss computation interdependent for different nodes. Furthermore, in typical graphs the number of nodes grows exponentially with the increase of the filter receptive field, incurring significant computational and memory complexity. So far, various graph sampling approaches <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b68">70]</ref> have been proposed as a way to alleviate the cost of training graph neural networks by selecting a small number of neighbors that reduce the computational and memory complexity.</p><p>In this paper, we take a different approach for scalable deep learning on graphs. We propose SIGN, a simple scalable Graph Neural Network architecture inspired by the inception module <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b30">32]</ref>. SIGN combines graph convolutional filters of different types and sizes that are amenable to efficient precomputation, allowing extremely fast training and inference with complexity independent of the graph structure. Our architecture is able to scale to web-scale graphs without resorting to any sample technique and retaining sufficient expressiveness for effective learning: while being faster in training and, especially, inference (even one order of magnitude speedup), by employing SIGN with only one graph convolutional layer we are able to achieve results on par with state-of-the-art on several large-scale graph learning datasets. In particular, SIGN obtains state-of-the-art results on ogbn-papers100M, the largest public graph learning benchmark, with over 110 million nodes and 1.5 billion edges.</p><p>These results raise the important question on when deep graph neural network architectures are useful, especially when scalability is an important requirement, as in large-scale industrial systems. Significant effort has recently been devoted to methods allowing to design deep Graph Neural Networks with many graph convolutional layers <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b48">50]</ref>, which otherwise appear difficult to train <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b58">60]</ref>. While there is strong evidence in favor of depth on geometric graphs <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b19">21]</ref>, there has been almost no gain from depth on general irregular graphs like 'small-world' networks <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b48">50]</ref>. Given the abundance of such graphs e.g. in social network applications, it is important to take a step back and deliberate if deep architectures are the right approach. We conjecture that deep graph learning architectures are not useful for general irregular graphs and argue that future research in the field should focus on designing local more expressive operators <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b16">18]</ref> rather than going deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep learning on graphs</head><p>The goal of graph representation learning is to construct an embedding representing the structure of the graph and the data thereon. In node-wise prediction problems, we distinguish between Transductive setting, which assumes that the entire graph is known, and thus the same graph is used during training and testing (albeit different nodes are used for training and testing), and Inductive setting, in which training and testing are performed on different graphs. A typical graph neural network architecture consists of graph Convolution-like operators (discussed in Section 2.3) performing local aggregation of features by means of message passing with the neighbor nodes, and possibly Pooling amounting to fixed <ref type="bibr" target="#b14">[16]</ref> or learnable <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b6">8]</ref> graph coarsening. Additionally, graph Sampling schemes (detailed in Section 2.4) can be employed on large-scale graphs to reduce the computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Basic notions</head><p>Let G = (V = {1, . . . , n}, E, W) be an undirected weighted graph, represented by the symmetric n × n adjacency matrix W, where w ij &gt; 0 if (i, j) ∈ E and zero otherwise. The diagonal degree matrix D = diag( n j=1 w 1j , . . . , n j=1 w nj ) represents the number of neighbors of each node. We further assume that each node is endowed with a d-dimensional feature vector and arrange all the node features as rows of the n × d-dimensional matrix X. We denote by A = D −1/2 WD −1/2 the normalized adjacency matrix. The normalized graph Laplacian is an n × n positive semi-definite matrix ∆ = I − D −1/2 WD −1/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convolution-like operators on graphs</head><p>Spectral methods Bruna et al. <ref type="bibr" target="#b8">[10]</ref> used the analogy between the eigenvectors of the graph Laplacian and the Fourier transform to generalize convolutional neural networks (CNN) <ref type="bibr" target="#b35">[37]</ref> to graphs. Among the key drawbacks of this approach is a high (at least O(n 2 ) complexity), large (O(n)) number of filter parameters, no spatial localization, and no generalization of filters across graphs. Furthermore, the method explicitly assumes the underlying graph to be undirected, in order for the Laplacian to be a symmetric matrix with orthogonal eigenvectors.</p><p>ChebNet A way to address the shortcomings of <ref type="bibr" target="#b8">[10]</ref> is to model the filter as a transfer function g(λ), applied to the Laplacian asĝ(∆) = Φĝ(Λ)Φ . Filters computed in this manner are stable under graph perturbations <ref type="bibr" target="#b36">[38]</ref>. In the case whenĝ is expressed as simple matrix-vector operations (e.g. a polynomial <ref type="bibr" target="#b13">[15]</ref> or rational function <ref type="bibr" target="#b37">[39]</ref>), the eigendecomposition of the Laplacian can be avoided altogether. A particularly simple choice is a polynomial spectral filterĝ(λ) = r k=0 θ k λ k of degree r, allowing the convolution to be computed entirely in the spatial domain as</p><formula xml:id="formula_0">Y =ĝ(∆)X = r k=0 θ k ∆ k X.</formula><p>(1)</p><p>with O(r) parameters θ 0 , . . . , θ r , does not require explicit multiplication by Φ, and has a compact support of r hops in the node domain. Though originating from a spectral construction, the resulting filter is an operation in the node domain amounting to a successive aggregation of features in the neighbor nodes, which can be performed with complexity O(|E|r) ≈ O(nr). The polynomial filters can be combined with non-linearities, concatenated in multiple layers, and interleaved with pooling layers based on graph coarsening <ref type="bibr" target="#b13">[15]</ref>. The Laplacian in (1) can be replaced with other operators that diffuse information across neighbor nodes, e.g. the simple or normalized adjacency matrix, without affecting performance.</p><p>GCN In the case r = 1, equation (1) reduces to computing (I + D −1/2 WD −1/2 )X, which can be interpreted as a combination of the node features and the neighbors filtered features. Kipf and Welling <ref type="bibr" target="#b32">[34]</ref> proposed a model of graph convolutional networks (GCN) combining node-wise and graph diffusion operations:</p><formula xml:id="formula_1">Y =D −1/2WD−1/2 XΘ =ÃXΘ.<label>(2)</label></formula><p>HereW = I + W is the adjacency matrix with self-loops,D = diag( n j=1w 1j , . . . , n j=1w nj ) is the respective degree matrix, and Θ is a matrix of learnable parameters.</p><p>S-GCN Stacking L GCN layers with element-wise non-linearity σ and a final layer for node classification with activation ξ (e.g. softmax or sigmoid), it is possible to obtain filters with larger receptive fields on the graph nodes, Y = ξ(Ã · · · σ(ÃXΘ (1) ) · · · Θ (L) ).</p><p>Wu et al. <ref type="bibr" target="#b57">[59]</ref> argued that graph convolutions with large filters is practically equivalent to multiple convolutional layers with small filters and showed that all but the last non-linearities can be removed without practically harming the performance, resulting in the simplified GCN (S-GCN) model,</p><formula xml:id="formula_2">Y = ξ(Ã L XΘ (1) · · · Θ (L) ) = ξ(Ã L XΘ).<label>(3)</label></formula><p>MotifNet Monti et al. <ref type="bibr" target="#b42">[44]</ref> used adjacency matrices with weights proportional to the count of simple subgraphs (motifs) on edges in order to account for higher order structures. Related ideas have been explored using higher-order Laplacians on simplicial complexes <ref type="bibr" target="#b2">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph sampling</head><p>For Web-scale graphs such as Facebook or Twitter that typically have n = 10 8 ∼ 10 9 nodes and |E| = 10 10 ∼ 10 11 edges, the diffusion matrix cannot be stored in memory for training, making the straightforward application of graph neural networks impossible. Graph sampling has been shown to be a successful technique to scale GNNs to large graphs by approximating local connectivity with subsampled versions which are amenable for computation.</p><p>Node-wise sampling These strategies perform graph convolutions on partial node neighborhoods to reduce computational and memory complexity, and are coupled with minibatch training, where each training step is performed only on a batch of nodes rather than on the whole graph. A training batch </p><formula xml:id="formula_3">O(k Lc N ) O(k Lc N d 2 ) ClusterGCN O(|E|) O(L c |E|d + L ff N d 2 ) GraphSAINT O(kN ) O(L c |E|d + L ff N d 2 ) SIGN-r O(r|E|d) O(rL ff N d 2 )</formula><p>is assembled by first choosing b 'optimization' nodes and partially expanding their corresponding neighborhoods. In a single training step, the loss is computed and optimized only for optimization nodes. Node-wise sampling coupled with minibatch training was first introduced in GraphSAGE [24] to address the challenges of scaling GNNs. PinSAGE <ref type="bibr" target="#b60">[62]</ref> extended GraphSAGE by exploiting a neighbor selection method using scores from approximations of Personalized PageRank <ref type="bibr" target="#b24">[26]</ref> via random walks. VR-GCN <ref type="bibr" target="#b10">[12]</ref> uses control variates to reduce the variance of stochastic training and increase the speed of convergence with a small number of neighbors.</p><p>Layer-wise sampling A characteristic of many graphs, in particular 'small-world' social networks, is the exponential growth of the neighborhood size with number of hops L. <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b28">30]</ref> avoid overexpansion of neighborhoods to overcome the redundancy of node-wise sampling. Nodes in each layer only have directed edges towards nodes of the next layer, thus bounding the maximum amount of computation to O(b 2 ) per layer. Moreover, sharing common neighbors prevents feature replication across the batch, drastically reducing the memory complexity during training.</p><p>Graph-wise sampling In <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b62">64]</ref>, feature sharing is further advanced: each batch consists of a connected subgraph and at each training iteration the GNN model is optimized over all nodes in the subgraph. In ClusterGCN <ref type="bibr" target="#b11">[13]</ref>, non-overlapping clusters are computed as a pre-processing step and then sampled during training as input minibatches. GraphSAINT [64] adopts a similar approach, while also correcting for the bias and variance of the minibatch estimators when sampling subgraphs for training. It also explores different schemes to sample the subgraphs such as a random walk-based sampler, which is able to co-sample nodes having high influence on each other and guarantees each edge has a non-negligible probability of being sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scalable Inception Graph Neural Networks</head><p>In this work we propose SIGN, an alternative method to scale Graph Neural Networks to very large graphs. The key building block of our architecture is a set of linear diffusion operators represented as n × n matrices A 1 , . . . , A r , whose application to the node-wise features can be pre-computed. For node-wise classification tasks, our architecture has the form ( <ref type="figure" target="#fig_1">Figure 1)</ref>:</p><formula xml:id="formula_4">Z = σ ([XΘ 0 , A 1 XΘ 1 , . . . , A r XΘ r ]) Y = ξ (ZΩ) ,<label>(4)</label></formula><p>where Θ 0 , . . . , Θ r and Ω are learnable matrices respectively of dimensions d × d and d (r + 1) × c for c classes, and σ, ξ are non-linearities, the second one computing class probabilities, e.g. via softmax or sigmoid function, depending on the task at hand. We denote a model with r operators by SIGN-r.</p><p>A key observation is that matrix products A 1 X, . . . , A r X, in equation <ref type="formula">(</ref>    </p><formula xml:id="formula_5">B 1 , . . . , B r α Θ 0 , . . . , Θ r Ω ChebNet [15] ∆, . . . , ∆ r 1 Θ 0 , . . . , Θ r [I, . . . , I] GCN [34] r = 1,Ã 1 0, Θ [0, I] S-GCN [59] r = 1,Ã L 1 0, Θ [0, I]</formula><p>show experimentally that our model is significantly faster than all others due to the fact the forward and backward pass complexity of our model does not depend on the graph structure. Unlike the aforementioned scalable methods, SIGN is not based on sampling nodes or subgraphs, operations potentially introducing bias into the optimization procedure.</p><p>Inception-like module Within the SIGN framework, it is possible to choose one specific operator B and to define A k = B k for k = 1, . . . , r. In this setting the proposed model is analogous to the popular Inception module <ref type="bibr" target="#b54">[56]</ref> for classic CNN architectures: it consists of convolutional filters of different sizes determined by the parameter r, where r = 0 corresponds to 1 × 1 convolutions in the inception module (amounting to linear transformations of the features in each node without diffusion across nodes). Owing to this analogy, we refer to our model as the Scalable Inception Graph Network (SIGN). It is also easy to observe that various graph convolutional layers can be obtained as particular settings of (4). In particular, by setting the σ non-linearity to PReLU <ref type="bibr" target="#b25">[27]</ref>, ChebNet, GCN, and S-GCN can be automatically learnt if suitable diffusion operator B and activation ξ are used (see <ref type="table" target="#tab_2">Table 2</ref>).</p><p>Choice of the operators Generally speaking, the choice of the diffusion operators jointly depends on the task, graph structure, and the features. In complex networks such as social graphs, operators induced by triangles or cliques might help distinguishing edges representing weak or strong ties <ref type="bibr" target="#b20">[22]</ref>. In graphs with noisy connectivity, it was shown that diffusion operators based on Personalized PageRank (PPR) or Heat Kernel can boost performance <ref type="bibr" target="#b34">[36]</ref>. In our experiments, we choose three specific types of operators: simple (normalized) adjacency, Personalized PageRank-based adjacency, and triangle-based adjacency matrices, as well as their powers. We denote by SIGN(p,s,t) with r = p + s + t the configuration using up to the p-th, s-th, and t-th power of simple GCN-normalized, PPR-based, and triangle-based adjacency matrices, respectively. Lastly, when working on directed graphs, SIGN can be equipped with powers of the (properly normalized) directed adjacency matrix W d and its transpose W d , in addition to the standard operators built on top of its undirected counterpart W = 1 2 (W d + W d ).</p><p>SIGN and S-GCN SIGN and S-GCN are the only graph neural models which are inherently 'shallow': contrary to standard GNN architectures, graph convolutional layers are not sequentially stacked, but either collapsed into a single linear filtering operation (S-GCN) or applied in parallel to obtain multi-scale node representations capturing diverse connectivity patterns depending on the chosen operators (SIGN). This is the crucial feature that allows these models to naturally scale their training and inference to graphs of any size and family given that all graph operations can be conveniently pre-computed. While we notice that S-GCN can be considered as a specific configuration of SIGN-1 (it is sufficient to choose A 1 =Ã L and to constrain Θ 0 to 0, see <ref type="table" target="#tab_2">Table 2</ref>), we remark the fact that the more general SIGN architecture easily allows to incorporate more expressivity via parallel application of several, possibly, domain-specific, operators. We experimentally show this in the following section where we demonstrate that the S-GCN paradigm is indeed too limiting and that a more expressive SIGN model is not only able to perform on par with 'deeper' sampling-based models, but also to achieve state-of-the-art results on the largest publicly available graph learning benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets We evaluated the proposed method on node-wise classification tasks, both in transductive and inductive settings. Inductive experiments are performed using four datasets: Reddit <ref type="bibr" target="#b22">[24]</ref>, Flickr, Yelp <ref type="bibr" target="#b62">[64]</ref>, and PPI <ref type="bibr" target="#b67">[69]</ref>. To date, these are the largest graph learning inductive node classification benchmarks available in the public domain. Related tasks are multiclass node-wise classification for Reddit and Flickr and multilabel classification for Yelp and PPI. Transductive experiments were performed on the new ogbn-products and ogbn-papers100M datasets <ref type="bibr" target="#b27">[29]</ref>. The former represents an Amazon product co-purchasing network <ref type="bibr" target="#b5">[7]</ref> where the task is to predict the category of a product in a multi-class classification setup. The latter represents a directed citation network of ∼ 111 million academic papers, where the task is to leverage information from the entire citation network to infer the labels (subject areas) of a smaller subset of ArXiv papers. Overall, this dataset is orders-of-magnitude larger than any existing node classification dataset and is therefore the most important testbed for the scalability of SIGN and related methods.</p><p>Furthermore, we also test the scalability of our method on Wikipedia links [1], a large-scale network of links between articles in the English version of Wikipedia.</p><p>Statistics for all the datasets are reported in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Setup We tested several SIGN(p, s, t) configurations, with p the maximum power of the GCNnormalized adjacency matrix, s that of a random-walk normalized PPR diffusion operator <ref type="bibr" target="#b34">[36]</ref>, and t that of a row-normalized triangle-induced adjacency matrix <ref type="bibr" target="#b42">[44]</ref>, with weights proportional to edge occurrences in closed triads. PPR-based operators are computed from a symmetrically normalized adjacency transition matrix in an approximated form, with a restart probability of α = 0.01 for inductive datasets and α = 0.05 in the transductive case. To allow for larger model capacity in the inception modules and in computing final model predictions, we replace the single-layer projections performed by Θ i and Ω modules with multiple feedforward layers. Model parameters are found by minimizing the cross-entropy loss via minibatch gradient descent with the Adam optimizer <ref type="bibr" target="#b31">[33]</ref>. Early stopping is applied with a patience of 15. In order to limit overfitting, we apply the standard regularization techniques of weight decay and dropout <ref type="bibr" target="#b53">[55]</ref>. Additionally, batch-normalization <ref type="bibr" target="#b29">[31]</ref> was used in every layer to stabilize training and increase convergence speed. Architectural and optimization hyperparameters were estimated using Bayesian optimization with a tree Parzen estimator surrogate function <ref type="bibr" target="#b4">[6]</ref> over all inductive datasets. As for the the transductive setting, we employ standard exhaustive search on a predefined hyperparameter grid on ogbn-products, while on ogbn-papers100M we only test a basic configuration that can be found in Supplemetary Materials, along with further details on the hyperparameter search spaces. Given that this dataset represents a directed network, we experimented with operators built via asymmetric normalization of the original directed adjacency matrix and its transpose, as well as their powers. The Wikipedia dataset, due to the lack of node attributes and labels, is only used to assess scalability: to this end, we randomly generate 100-dimensional node feature vectors and scalar targets and consider the whole network for both training and inference. No hyperparameter tuning is required in this case.</p><p>Baselines On the inductive datasets, we compare our method with GCN <ref type="bibr" target="#b32">[34]</ref>, FastGCN <ref type="bibr" target="#b9">[11]</ref>, Stochastic-GCN <ref type="bibr" target="#b10">[12]</ref>, AS-GCN <ref type="bibr" target="#b28">[30]</ref>, GraphSAGE <ref type="bibr" target="#b22">[24]</ref>, ClusterGCN <ref type="bibr" target="#b11">[13]</ref>, and GraphSAINT <ref type="bibr" target="#b62">[64]</ref>, which constitute the current state-of-the-art. On ogbn-products we compare against scalable sampling-free baselines, i.e. a feed-forward network trained over node features only (MLP) and on their concatenation with structural Node2Vec embeddings <ref type="bibr" target="#b21">[23]</ref>, and the sampling-based approaches ClusterGCN <ref type="bibr" target="#b11">[13]</ref> and GraphSAINT <ref type="bibr" target="#b62">[64]</ref>. As for ogbn-papers100M, SIGN is compared with sampling-free baselines: an MLP trained on node features and an S-GCN model. Sampling-based methods have not been scaled yet to this benchmark. All results for OGB datasets are directly taken from the latest version of the arXiv paper <ref type="bibr" target="#b27">[29]</ref> (v4, at the time of writing). Lastly, being S-GCN an  important baseline for our model, we additionally report its performance on all the other datasets as well. In this case we choose power L of its (only) operator A L as the value p of the best corresponding SIGN(p,s,t) configuration and we tune its hyperparameters in the same space searched for SIGN.</p><p>Implementation SIGN is implemented using Pytorch <ref type="bibr" target="#b46">[48]</ref>. All experiments, including timings, were run on an AWS p2.8xlarge instance, with 8 NVIDIA K80 GPUs, 32 vCPUs, a processor Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz and 488GiB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Inductive <ref type="table" target="#tab_5">Table 5</ref> presents the results on the inductive dataset. In line with <ref type="bibr" target="#b62">[64]</ref>, we report the micro-averaged F1 score means and standard deviations computed over 10 runs. For each dataset we report the best performing SIGN configuration, specifying the maximum power for each of the three employed operators. SIGN outperforms other methods on Reddit and Flickr, and performs competitively to state-of-the-art on PPI. Our performance on Yelp is worse than in the other datasets; we hypothesize that a more tailored choice of operators is required to better suit the characteristics of this dataset. Interestingly, SIGN significantly outperforms S-GCN in all datasets, suggesting that the additional expressivity introduced by the different operators in our model is required for effective learning.</p><p>Transductive SIGN obtains state-of-the-art results on the ogbn-papers100M dataset ( <ref type="table" target="#tab_8">Table 8)</ref>, outperforming other sampling-free methods by at least 1.8%. This shows that SIGN can scale to massive graphs while retaining ample expressivity. Sampling based methods have not been scaled yet to this benchmark. In ogbn-papers100M only ∼ 1.35% of nodes are labeled; at each training and inference iteration these methods still need to perform computation on subgraphs where the majority of nodes are unlabeled and thus do not contribute to the computation of loss and evaluation metrics. On the contrary, at training and inference SIGN only processes the required labeled nodes given that the graph has already been employed during the one-time pre-computation phase, thus avoiding this redundant computation and memory usage.</p><p>ogbn-products results are reported in <ref type="table" target="#tab_6">Table 6</ref>. SIGN outperforms all other sampling-free methods by at least 2.7%. However, contrary to the inductive benchmarks, sampling methods outperform SIGN and appear to generally be more suitable to this dataset. We hypothesise that, on this particular task, sampling may implicitly act as a regularizer, making these methods generalize better to the held-out test set, which in this dataset is sampled from a different distribution w.r.t. training and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reddit</head><p>Flickr PPI Yelp GCN <ref type="bibr" target="#b32">[34]</ref> 0.933±0.000 0.492±0.003 0.515±0.006 0.378±0.001 FastGCN <ref type="bibr" target="#b9">[11]</ref> 0.924±0.001 0.504±0.001 0.513±0.032 0.265±0.053 Stochastic-GCN <ref type="bibr" target="#b10">[12]</ref> 0.964±0.001 0.482±0.003 0.963±0.010 0.640±0.002 AS-GCN <ref type="bibr" target="#b28">[30]</ref> 0.958±0.001 0.504±0.002 0.687±0.012 -GraphSAGE <ref type="bibr" target="#b22">[24]</ref> 0.953±0.001 0.501±0.013 0.637±0.006 0.634±0.006 ClusterGCN <ref type="bibr" target="#b11">[13]</ref> 0.954±0.001 0.481±0.005 0.875±0.004 0.609±0.005 GraphSAINT <ref type="bibr" target="#b62">[64]</ref> 0.966±0.001 0.511±0.001 0.981±0.004 0.653±0.003 S-GCN <ref type="bibr" target="#b57">[59]</ref> 0.949±0.000 0.502±0.001 0.892±0.015 0.358±0.006 SIGN 0.968±0.000 0.514±0.001 0.970±0.003 0.631±0.003 (p, s, t) (4, 2, 0) (4, 0, 1) (2, 0, 1) (2, 0, 1) validation nodes <ref type="bibr" target="#b27">[29]</ref>. This phenomenon, as well as its connection to the DropEdge method <ref type="bibr" target="#b48">[50]</ref> and the bottleneck problem <ref type="bibr" target="#b0">[2]</ref>, would be object of further investigation.</p><p>Runtime While performing on par or better than state-of-the-art methods on most benchmarks in terms of accuracy, our method has the advantage of being significantly faster than other methods for large graphs. We perform comprehensive timing comparisons on ogbn-products and Wikipedia datasets and report average training, inference, and preprocessing times in <ref type="table" target="#tab_4">Table 4</ref>. For these experiments, we run the implementations of ClusterGCN and GraphSAINT provided in the OGB code repository 2 .</p><p>We use these datasets rather than ogbn-papers100M so we can compare to ClusterGCN and Graph-SAINT. For completeness we report, however, that on ogbn-papers100M the best performing SIGN(3,3,3) model completes one evaluation pass on the validation set in 1.99 ± 0.05 seconds and on the test set in 3.34 ± 0.04 seconds (statistics are estimated over 10 runs and include the time required by device data transfers and by the computation of evalution metric).</p><p>Our model is faster than ClusterGCN and of comparable speed w.r.t. GraphSAINT in training 3 , while being by far the fastest approach in inference: all SIGN architectures are always at least one order of magnitude faster than other methods, with the largest one (8 operators) requiring no more than 30 seconds to perform inference on over 12M nodes. SIGN's preprocessing is slightly longer than other methods, but we notice that most of the calculations can be cast as sparse matrix multiplications and easily parallelized with frameworks for distributed computing. We envision to engineer faster and even more scalable SIGN preprocessing implementations in future developments of this work. Finally, in order to also study the convergence behavior of our proposed model, in <ref type="figure" target="#fig_2">Figure 2</ref> we plot the validation performance on ogbn-products from the start of the training as a function of run time for ClusterGCN, GraphSAINT and several SIGN configurations. We observe that SIGN does not only converge to a better validation accuracy than other methods, but also exhibits much faster convergence than ClusterGCN and comparable speed than to GraphSAINT.</p><p>Ablation study How do different operator combinations affect SIGN performance? Results obtained with different choices of operators and their powers are reported in <ref type="table" target="#tab_6">Tables 6, 8 and 7</ref> for, respectively, the the transductive ogbn-products and ogbn-papers100M and inductive datasets. We notice that best performance is obtained on each benchmark by a specific combination of operators, remarking the fact that each dataset features particular topological and content characteristics requiring suitable filters. Interestingly, we also observe that while PPR operators do not bring significant improvements in the inductive setting (being even harmful in certain cases), they are beneficial on the transductive ogbn-products. This finding is in accordance with <ref type="bibr" target="#b34">[36]</ref>, where the effectiveness of PPR diffusion operators in transductive settings has been extensively studied. Finally, we notice promising results attained in Flickr and PPI inductive settings by pairing standard adjacency matrices with a triangle-induced one. Studying the effect of operators induced by more complex network motifs is left for future research.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper we presented SIGN, a sampling-free Graph Neural Network model that is able to easily scale to gigantic graphs while retaining enough expressive power to attain competitive results in all large-scale graph learning benchmarks. SIGN attains state-of-the-art results on many of them, including the massive ogbn-papers100M, currently the largest publicly available node-classification benchmark with ∼ 111M nodes and ∼ 1.6B edges. Our experiments have further demonstrated that the ability of our model to flexibly incorporate diverse, possibly domain-specific, operators is crucial to overcome the expressivity limitations of other sampling-free scalable models such as S-GCN, which SIGN has constantly outperformed over all datasets. Overall, our architecture achieves an optimal trade-off between simplicity and expressiveness; as it has shown to attain competitive results with fast training and inference, it represents the most suitable architecture for scalable applications to web-scale graphs.</p><p>Depth vs. width for Graph Neural Networks Our results have shown that it is possible to obtain competitive -and often state-of-the-art -results with one single graph convolutional layer and hence a shallow architecture. An important question is, therefore, when one should apply deep architectures to graphs, where by 'depth' we refer to the number of stacked graph convolutional layers. Deep Graph Neural Networks are notoriously hard to train due to vanishing gradients and feature smoothing <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b58">60]</ref>, and, although recent works have shown that these issues can be addressed to some extent <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b48">50]</ref>, yet extensive experiments conducted in <ref type="bibr" target="#b48">[50]</ref> showed that depth often does not bring any significant gain in performance w.r.t. to shallow baselines. A promising direction for future investigation is, rather than 'going deep', to 'go wide', in the sense of exploring more expressive local operators. We believe this to be especially crucial in all those settings where scalability is a concern of paramount importance, such as in industrial large-scale systems.</p><p>Extensions In our experiments, triangle-based operators showed promising results. Possible extensions can employ operators that account for higher-order structures such as simplicial complexes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reddit</head><p>Flickr PPI Yelp SIGN(2,0,0) 0.966±0.003 0.503±0.003 0.965±0.002 0.623±0.005 SIGN(2,0,1) 0.966±0.000 0.510±0.001 0.970±0.003 0.631±0.003 SIGN(2,2,0) 0.967±0.000 0.495±0.002 0.964±0.003 0.617±0.005 SIGN(4,0,0) 0.967±0.000 0.508±0.001 0.959±0.002 0.623±0.004 SIGN(4,0,1) 0.967±0.000 0.514±0.001 0.965±0.003 0.623±0.004 SIGN(4,2,0) 0.968±0.000 0.500±0.001 0.930±0.010 0.618±0.004 SIGN(4,2,1) 0.967±0.000 0.508±0.002 0.969±0.001 0.620±0.004  <ref type="bibr" target="#b2">[4]</ref>, paths <ref type="bibr" target="#b16">[18]</ref>, or motifs <ref type="bibr" target="#b42">[44]</ref> that can be tailored to the specific problem. Furthermore, temporal information can be integrated e.g. in the form of temporal motifs <ref type="bibr" target="#b44">[46]</ref>.</p><p>Limitations While our method relies on linear graph aggregation operations of the form BX for efficient precomputation, it is possible to make the diffusion operator dependent on the node features (and edge features, if available). In particular, graph attention <ref type="bibr" target="#b55">[57]</ref> and similar mechanisms <ref type="bibr" target="#b40">[42]</ref> use B θ (X), where θ are learnable parameters. The limitation is that such operators preclude efficient precomputation, which is key to the efficiency of our approach. Attention can be implemented in our scheme by training on a small subset of the graph to first determine the attention parameters, then fixing them to precompute the diffusion operator that is used during training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Inductive datasets</head><p>Reddit <ref type="bibr" target="#b22">[24]</ref> and Flickr <ref type="bibr" target="#b62">[64]</ref> are multiclass classification problems, Yelp <ref type="bibr" target="#b62">[64]</ref> and PPI <ref type="bibr" target="#b67">[69]</ref> are multilabel classification instances. In Reddit, the task is to predict communities of online posts based on user comments. In Flickr the task is image categorization based on the description and common properties of online images. In Yelp the objective is to predict business attributes based on customer reviews; the task of PPI consists in predicting protein functions from the interactions of human tissue proteins. Further details on the generation of the Yelp and Flickr datasets can be found in <ref type="bibr" target="#b62">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Transductive dataset</head><p>ogbn-products <ref type="bibr" target="#b27">[29]</ref> represents an Amazon product co-purchasing network <ref type="bibr" target="#b5">[7]</ref> where the task is to predict the category of a product in a multi-class classification setup. Dataset splitting is not random, sales ranking (popularity) is instead used to split nodes into training/validation/test. Top 10% products in the ranking are assigned to the training set, next top 2% to validation and the remaining 88% of products are for testing.</p><p>ogbn-papers100M <ref type="bibr" target="#b27">[29]</ref> represents a directed citation network of ∼ 111 million academic papers, where the task is to leverage information from the entire citation network to infer the labels (subject areas) of a smaller subset of ArXiv papers. The splitting strategy is time-based. Specifically, the training nodes (with labels) are all ArXiv papers published until 2017, validation nodes are ArXiv papers published in 2018 and test nodes are ArXiv papers published since 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Wikipedia</head><p>Wikipedia links is a large-scale directed network of links between articles in the English version of Wikipedia. For the sake of our timing experiments the network has been turned into undirected. Node features have been randomly generated with a dimensionality of 100 as in ogbn-products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Selection and Hyperparameter Tuning</head><p>Tuning involved the following architectural and optimization hyperparameters: weight decay, dropout rate, batch size, learning rate, number of feedforward layers and units both in inception and classification modules. For each inductive experiment we chose the set of hyperparameters matching the best average validation loss calculated over 5 runs. For the the transductive setting we kept, instead, the set of hyperparameters with minimum validation loss over a single run. The hyperparameter search space for the inductive setting and grid for the transductive one are described in <ref type="table" target="#tab_9">Table 9</ref>. The estimated hyperparameters for each best SIGN configuration are reported in <ref type="table" target="#tab_0">Table 10</ref> for inductive datasets and <ref type="table" target="#tab_0">Table 11</ref> for the transductive ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Triangle-based Operators</head><p>The triangle operator encodes the concept of homophily with a stronger acceptation with respect to the adjacency matrix: two nodes are connected by an edge only if they are both part of the same closed triad, i.e. if they are connected together and are both connected to the same node. Edge weights are proportional to the amount of triangles an edge belongs to, and they are normalised row-wise so to represent, for each node in a neighbourhood, its relative importance with respect to all the other neighbors.</p><p>This brings us to two considerations: first of all, the triangle operator is not carrying information related to nodes which were not already in the neighborhood. Secondly, it emphasizes the connections with those neighbors which are more related to our source node in virtue of the relationship described above. We can thus envision this operator being more useful in those graphs where this kind of relationship can be more discriminative within a neigborhood.</p><p>To verify this, in <ref type="figure" target="#fig_3">Figure 3</ref> we plot the normalized frequency distribution of intra-neighborhood standard deviation for the weights of triangle operators. It is interesting to notice the significantly   different trends characterizing Flickr and Reddit, two datasets where triangle operators have experimentally brought, respectively, relative large and small performance improvement. Flickr tends to exhibit larger weight variations than other datasets, while, on the contrary, Reddit is the dataset where the smallest intra-neighborhood variation is observed. This suggests how, in Flickr, the triangle operator is able to restrict feature aggregation to a subset of the original neighbors -those co-occurring in the larger number of triangles-while in Reddit it mostly boils down to uniform averaging, making this operator not much more expressive than a simple adjacency matrix.</p><p>For replicability we report that, in the computation of triangle operators for PPI, we retained the self-loops already present in the original dataset. Investigations on how the presence of these edges affects the expressiveness of the triangle operator are left for future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 )</head><label>4</label><figDesc>do not depend on the learnable model parameters and can be easily precomputed. For large graphs, distributed computing infrastructures such as Apache Spark can speed up computation. This effectively reduces the computational complexity of the overall model to that of a Multi-Layer Perceptron (MLP), i.e. O(rL ff N d 2 ), where d is the number of features, N the number of nodes in the training/testing graph and L ff is the overall number of feed-forward layers in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The SIGN architecture for r generic graph filtering operators. Θ k represents the k-th dense layer transforming node-wise features downstream the application of operator k, | is the concatenation operation and Ω refers to the dense layer used to compute final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Convergence of different methods on ogbn-products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Normalized frequency distributions for row-wise variations on the diffusion weights of triangle operators over inductive datasets. Variations are measured as the standard deviation on the weight value over original neighborhoods from the test graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Theoretical time complexity where L c , L ff is the number of graph convolition and MLP layers, r is the filter size, N the number of nodes (in training or inference, respectively), |E| the number of edges, and d the feature dimensionality (assumed fixed for all layers). For GraphSAGE, k is the number of sampled neighbors per node. Forward pass complexity corresponds to an entire epoch where all nodes are seen.</figDesc><table><row><cell>Preproc.</cell><cell>Forward Pass</cell></row><row><cell>GraphSAGE</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>compares the complexity of our SIGN model to the other scalable architectures GraphSAGE, ClusterGCN, and GraphSAINT. While all models scale linearly w.r.t. the number of nodes N , we</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>By appropriate configuration, SIGN inception layer is able to replicate some popular graph convolutional layers. α represents the learnable parameter of a PReLU activation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of (s)ingle and (m)ulti-label dataset statistics. Wikipedia is used, with random features, for timing purposes only.</figDesc><table><row><cell></cell><cell>n</cell><cell>|E|</cell><cell cols="2">Avg. Deg. d</cell><cell cols="2">Classes Train / Val / Test</cell></row><row><cell>Wikipedia</cell><cell>12,150,976</cell><cell>378,142,420</cell><cell>62</cell><cell>100</cell><cell>2(s)</cell><cell>100% / -/ 100%</cell></row><row><cell cols="3">ogbn-papers100M 111,059,956 1,615,685,872</cell><cell>30</cell><cell cols="2">128 172(s)</cell><cell>78% / 8% / 14%</cell></row><row><cell>ogbn-products</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>51</cell><cell>100</cell><cell>47(s)</cell><cell>10% / 2% / 88%</cell></row><row><cell>Reddit</cell><cell>232,965</cell><cell>11,606,919</cell><cell>50</cell><cell>602</cell><cell>41(s)</cell><cell>66% / 10% / 24%</cell></row><row><cell>Yelp</cell><cell>716,847</cell><cell>6,977,410</cell><cell>10</cell><cell cols="3">300 100(m) 75% / 10% / 15%</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>10</cell><cell>500</cell><cell>7(s)</cell><cell>50% / 25% / 25%</cell></row><row><cell>PPI</cell><cell>14,755</cell><cell>225,270</cell><cell>15</cell><cell cols="3">50 121(m) 66% / 12% / 22%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean and standard deviation of preprocessing, training (one epoch) and inference times, in seconds, on ogbn-products and Wikipedia datasets, computed over 10 runs. SIGN-r denotes architecture with r precomputed operators. Preprocessing and training times for ClusterGCN on Wikipedia are not reported due to the clustering algorithm failing to complete.</figDesc><table><row><cell></cell><cell></cell><cell>ogbn-products</cell><cell></cell><cell></cell><cell>Wikipedia</cell></row><row><cell></cell><cell>Preprocessing</cell><cell>Training</cell><cell>Inference</cell><cell>Preprocessing</cell><cell>Training</cell><cell>Inference</cell></row><row><cell>ClusterGCN</cell><cell cols="3">36.93 ± 0.52 13.34 ± 0.16 93.00 ± 0.68</cell><cell>-</cell><cell>-</cell><cell>183.76 ± 3.01</cell></row><row><cell>GraphSAINT</cell><cell>52.06 ± 0.54</cell><cell cols="5">2.89 ± 0.05 94.76 ± 0.81 123.60 ± 1.60 135.73 ± 0.06 209.86 ± 4.73</cell></row><row><cell>SIGN-2</cell><cell>88.21 ± 1.33</cell><cell>1.04 ± 0.10</cell><cell cols="3">2.86 ± 0.10 192.88 ± 0.12 62.37 ± 0.17</cell><cell>13.40 ± 0.15</cell></row><row><cell>SIGN-4</cell><cell cols="2">160.16 ± 1.20 1.54 ± 0.04</cell><cell cols="3">3.79 ± 0.08 326.21 ± 1.14 93.84 ± 0.08</cell><cell>18.15 ± 0.05</cell></row><row><cell>SIGN-6</cell><cell cols="2">226.48 ± 1.43 2.05 ± 0.00</cell><cell cols="4">4.84 ± 0.08 459.24 ± 0.14 125.24 ± 0.03 22.94 ± 0.02</cell></row><row><cell>SIGN-8</cell><cell cols="2">297.92 ± 2.92 2.53 ± 0.04</cell><cell>5.88</cell><cell></cell><cell></cell></row></table><note>± 0.09 598.67 ± 0.82 154.73 ± 0.12 27.69 ± 0.11</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Micro-averaged F1 scores. For SIGN, we show the best performing configurations. The top three performance scores are highlighted as: First, Second, Third.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance on ogbn-products. SIGN(p,s,t) refers to a configuration using p, s, and t powers of simple, PPR-based, and triangle-based adjacency matrices. The top three performance scores are highlighted as: First, Second, Third.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell cols="3">84.03±0.93 75.54±0.14 61.06±0.08</cell></row><row><cell>Node2Vec</cell><cell cols="3">93.39±0.10 90.32±0.06 72.49±0.10</cell></row><row><cell cols="4">S-GCN(L=5) 92.54±0.09 91.38±0.07 74.87±0.25</cell></row><row><cell>ClusterGCN</cell><cell cols="3">93.75±0.13 92.12±0.09 78.97±0.33</cell></row><row><cell cols="4">GraphSAINT 92.71±0.14 91.62±0.08 79.08±0.24</cell></row><row><cell>SIGN(3,0,0)</cell><cell cols="3">96.21±0.31 92.99±0.05 76.52±0.14</cell></row><row><cell>SIGN(3,0,1)</cell><cell cols="3">96.46±0.29 92.93±0.04 75.73±0.20</cell></row><row><cell>SIGN(3,3,0)</cell><cell cols="3">96.87±0.23 93.02±0.04 77.13±0.10</cell></row><row><cell>SIGN(5,0,0)</cell><cell cols="3">95.99±0.69 92.98±0.18 76.83±0.39</cell></row><row><cell>SIGN(5,3,0)</cell><cell cols="3">96.92±0.46 93.10±0.08 77.60±0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Impact of various operator combinations on inductive datasets. Best results are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Results on ogbn-papers100M, the largest public graph dataset with over 110 million nodes. SIGN(p,d,f ) refers to a configuration using p, d, and f powers of simple undirected, directed and directed-transposed adjacency matrices. The top three performance scores are highlighted as: First, Second, Third.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>MLP</cell><cell cols="3">54.84±0.43 49.60±0.29 47.24±0.31</cell></row><row><cell>Node2Vec</cell><cell>-</cell><cell cols="2">55.60±0.23 58.07±0.28</cell></row><row><cell cols="4">S-GCN(L=3) 67.54±0.43 66.48±0.20 63.29±0.19</cell></row><row><cell>SIGN(3,0,0)</cell><cell cols="3">70.18±0.37 67.57±0.14 64.28±0.14</cell></row><row><cell>SIGN(3,1,1)</cell><cell cols="3">72.24±0.32 67.76±0.09 64.39±0.18</cell></row><row><cell>SIGN(3,3,3)</cell><cell cols="2">73.94±0.72 68.6±0.04</cell><cell>65.11±0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameter search space/grid. Ranges in the form [low, high] and sampling distributions. Inception Layers and Classification Layers are the number of feedforward layers in the representation part of the model (replacing Θ) and the classification part of the model (replacing Ω) respectively. The only exception is represented by Yelp, for which the Ω module was kept shallow (no hidden layers) to allow for lighter training and the left bounds on the dropout, learning rate and batch size intervals were lowered to, respectively, 0.0, 0.00001 and 60.</figDesc><table><row><cell></cell><cell>TRANSDUCTIVE</cell><cell cols="2">INDUCTIVE</cell></row><row><cell>HYPERPARAMETER</cell><cell>VALUES</cell><cell>SPACE</cell><cell>DISTRIBUTION</cell></row><row><cell>Learning Rate</cell><cell>0.0001, 0.001</cell><cell>[0.0001, 0.0025]</cell><cell>UNIFORM</cell></row><row><cell cols="2">Batch Size 4096, 8192, 16384</cell><cell>[128, 2048]</cell><cell>QUANTIZED UNIFORM</cell></row><row><cell>Dropout</cell><cell>0.5</cell><cell>[0.2, 0.8]</cell><cell>UNIFORM</cell></row><row><cell>Weight Decay</cell><cell>0.0, 0.00001</cell><cell>[0, 0.0001]</cell><cell>UNIFORM</cell></row><row><cell>Inception Layers</cell><cell>1</cell><cell>1, 2</cell><cell>-</cell></row><row><cell>Inception Units</cell><cell>256, 512</cell><cell>[128, 512]</cell><cell>QUANTIZED UNIFORM</cell></row><row><cell>Classification Layers</cell><cell>1</cell><cell>1, 2</cell><cell>-</cell></row><row><cell>Classification Units</cell><cell>256, 512</cell><cell>[512, 1024]</cell><cell>QUANTIZED UNIFORM</cell></row><row><cell>Activation</cell><cell>PRELU</cell><cell>RELU, PRELU</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters chosen for the best configuration of SIGN on inductive datasets.</figDesc><table><row><cell>HYPERPARAMETER</cell><cell>REDDIT</cell><cell>FLICKR</cell><cell>PPI</cell><cell>YELP</cell></row><row><cell cols="3">Learning Rate 0.00012278578238312588 0.0017230142114465549</cell><cell>0.0014386686616183625</cell><cell>0.00005</cell></row><row><cell>Dropout</cell><cell>0.707328910934901</cell><cell>0.7608352140584778</cell><cell>0.3085607444207686</cell><cell>0.05</cell></row><row><cell>Weight Decay</cell><cell>9.176773905054599E-05</cell><cell cols="3">9.419820474221673E-05 3.2571631135664696E-06 4.452466189193362E-07</cell></row><row><cell>Batch Size</cell><cell>830</cell><cell>330</cell><cell>210</cell><cell>90</cell></row><row><cell>Inception Layers</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Inception Units</cell><cell>460</cell><cell>465</cell><cell>315</cell><cell>320</cell></row><row><cell>Classification Layers</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>0</cell></row><row><cell>Classification Units</cell><cell>675</cell><cell>925</cell><cell>870</cell><cell>-</cell></row><row><cell>Activation</cell><cell>RELU</cell><cell>PRELU</cell><cell>RELU</cell><cell>RELU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters chosen for the best configuration of SIGN on ogbn-product and those used on the ogbn-product dataset.</figDesc><table><row><cell cols="3">HYPERPARAMETER ogbn-products ogbn-papers100M</cell></row><row><cell>Learning Rate</cell><cell>0.0001</cell><cell>0.001</cell></row><row><cell>Dropout</cell><cell>0.5</cell><cell>0.1</cell></row><row><cell>Weight Decay</cell><cell>0.0001</cell><cell>0.0</cell></row><row><cell>Batch Size</cell><cell>4096</cell><cell>256</cell></row><row><cell>Inception Layers</cell><cell>1</cell><cell>1</cell></row><row><cell>Inception Units</cell><cell>512</cell><cell>256</cell></row><row><cell>Classification Layers</cell><cell>1</cell><cell>3</cell></row><row><cell>Classification Units</cell><cell>512</cell><cell>256</cell></row><row><cell>Activation</cell><cell>PRELU</cell><cell>RELU</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/snap-stanford/ogb/tree/master/examples/nodeproppred/products 3 Traning time is measured as forward-backward time to complete one epoch.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno>arXiv:cs.LG/2006.05205</idno>
		<title level="m">On the Bottleneck of Graph Neural Networks and its Practical Implications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Sardellitti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11577</idno>
		<title level="m">Topological Signal Processing over Simplicial Complexes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithms for Hyper-Parameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The extreme classification repository: Multi-label datasets and code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="http://manikvarma.org/downloads/XC/XMLRepository.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00481</idno>
		<title level="m">Mincut pooling in graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stochastic Training of Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph neural networks for icecube signal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Choma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Ronaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhat</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wahid</forename><surname>Bhimji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Flam-Shepherd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10413</idno>
		<title level="m">Neural Message Passing on High Order Paths</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometrically Principled Connections in Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunwang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Strength of Weak Ties: A Network Theory Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granovetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sociological Theory</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="105" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank: A context-sensitive ranking algorithm for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="784" to="796" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<ptr target="https://arxiv.org/abs/2005.00687v4" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning ( Machine Learning Research<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>http</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">InceptionGCN: Receptive Field Aware Graph Convolutional Network for Disease Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Diffusion Improves Graph Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gitta</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutyniok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12972</idno>
		<title level="m">Transferability of Spectral Graph Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fake News Detection on Social Media using Geometric Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673</idno>
		<ptr target="http://arxiv.org/abs/1902.06673" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Motifnet: a motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSW</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML (JMLR Workshop and Conference Proceedings)</title>
		<editor>Maria-Florina Balcan and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motifs in temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Web Search and Data Mining</title>
		<meeting>Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disease prediction using graph convolutional networks: Application to Autism Spectrum Disorder and Alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><forename type="middle">Ira</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enzo</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ncRNA Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Deep Learning on Graphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Relational Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">HyperFoods: Machine intelligent mapping of cancer-beating molecules in foods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Veselkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Representation Learning on Graphs with Jumping Knowledge Networks</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<title level="m">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep learning on graphs: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">PairNorm: Tackling Oversmoothing in GNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx252</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btx252" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
