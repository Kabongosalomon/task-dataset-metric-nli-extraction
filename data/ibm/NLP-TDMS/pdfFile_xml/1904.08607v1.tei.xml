<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Attention Memory Network for Movie Story Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
							<email>cdyoo@kaist.ac.kr2ks0326.kim</email>
							<affiliation key="aff0">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Samsung Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Attention Memory Network for Movie Story Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes the progressive attention memory network (PAMN) for movie story question answering (QA). Movie story QA is challenging compared to VQA in two aspects: (1) pinpointing the temporal parts relevant to answer the question is difficult as the movies are typically longer than an hour, (2) it has both video and subtitle where different questions require different modality to infer the answer. To overcome these challenges, PAMN involves three main features: (1) progressive attention mechanism that utilizes cues from both question and answer to progressively prune out irrelevant temporal parts in memory, (2) dynamic modality fusion that adaptively determines the contribution of each modality for answering the current question, and (3) belief correction answering scheme that successively corrects the prediction score on each candidate answer. Experiments on publicly available benchmark datasets, MovieQA and TVQA, demonstrate that each feature contributes to our movie story QA architecture, PAMN, and improves performance to achieve the state-of-the-art result. Qualitative analysis by visualizing the inference mechanism of PAMN is also provided.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans have an innate cognitive ability to infer from different sensory inputs to answer questions of 5W's and 1H involving who, what, when, where, why and how, and it has been a quest of mankind to duplicate this ability on machines. In recent years, studies on question answering (QA) have successfully benefited from deep neural networks, and showed remarkable performance improvement on textQA <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>, imageQA <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref>, videoQA <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. This paper considers movie story QA <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> that aims at a joint understanding of vision and language by answering questions about movie contents and storyline after observing temporally-aligned video and subtitle. Movie * This research was supported by Samsung Research story QA is challenging compared to VQA in following two aspects: <ref type="bibr" target="#b0">(1)</ref> pinpointing the temporal parts relevant to answer the question is difficult as the movies are typically longer than an hour and (2) it has both video and subtitle where different questions require different modality to infer the answer.</p><p>The first challenge of movie story QA is that it involves long videos that are possibly longer than an hour which hinders pinpointing the required temporal parts. The information in the movie required to answer the question is not distributed uniformly across the temporal axis. To address this issue, memory networks <ref type="bibr" target="#b23">[24]</ref> have widely been accepted in QA tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. The attention mechanism have widely been adopted to retrieve the information relevant to the question. We observed that single-step attention on memory networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref> often generates blurred temporal attention map.</p><p>The second challenge of movie story QA is that it involves both video and subtitle where different questions require different modality to infer the answer. Each modality may convey essential information for different questions, and optimally fusing them is an important problem. For example in the movie Indiana Jones and the Last Crusade, answering the question "What does Indy do to the grave robbers at the beginning of the movie?" would require video modality rather than subtitle modality while the question "How has the guard managed to stay alive for 700 years?" would require subtitle modality. Existing multi-modal fusion methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> only focus on modeling rich interactions between the modalities. However, these methods are question-agnostic in that the fusion process is not conditioned on the question.</p><p>To address the aforementioned challenges, this paper proposes Progressive Attention Memory Network (PAMN) for movie story QA. PAMN contains three main features;</p><p>(1) progressive attention mechanism for pinpointing required temporal parts, (2) dynamic modality fusion for adaptively fusing modalities conditioned on question and (3) belief correction answering scheme. Progressive attention mechanism utilizes cues from both question and an-swers to prune out irrelevant temporal parts for each memory. While iteratively taking question and answers for temporal attention generation, memories are progressively updated to accumulate cues to locate relevant temporal parts for answering the question. Compared to stacked attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>, progressive attention considers multiple sources (e.g., Q and A) and multiple targets (e.g., video and subtitle memory) in a single framework. Dynamic modality fusion aggregates the outputs from each memory by adaptively determining the contribution of each modality. Conditioned on the current question, the contribution is obtained by softattention mechanism. Fusing multi-modal data by bilinear operations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> often requires heavy computation or large number of parameters. Dynamic modality fusion efficiently integrate video and subtitle modality by discarding worthless information from unnecessary modality. Belief correction answering scheme successively corrects the prediction score of each candidate answer. When humans solve questions, they typically read content, question and answers multiple times in an iterative manner <ref type="bibr" target="#b9">[10]</ref>. This observation is modeled by belief correction answering scheme. The prediction score (logits), which this paper refers to a belief, is equally likely initialized and successively corrected compared to existing answering scheme <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref> which uses single-step answering scheme.</p><p>The main contribution of this paper is summarized as follows. (1) This paper proposes a movie story QA architecture referred to as PAMN that tackles major challenges of movie story QA with three features; progressive attention, dynamic modality fusion and belief correction answering scheme. (2) PAMN achieves the state-of-the-art results on MovieQA dataset. Both the quantitative and qualitative results exhibit the benefits and potential of PAMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Question Answering</head><p>Despite the short history, imageQA enjoys large number of datasets including VQA <ref type="bibr" target="#b2">[3]</ref>, COCO-QA <ref type="bibr" target="#b22">[23]</ref> and Vi-sual7W <ref type="bibr" target="#b34">[35]</ref>. Attention mechanism is widely used to locate the visual clues relevant to the question. Stacked Attention Network (SAN) <ref type="bibr" target="#b30">[31]</ref> utilizes stacked attention module to query an image multiple times to infer the answer progressively. The Dual Attention Network (DAN) <ref type="bibr" target="#b21">[22]</ref> jointly leverages visual and textual attention mechanisms to localize key information from both image and question. Recently, applying bilinear operation showed promising results on imageQA. Multimodal Compact Bilinear pooling (MCB) <ref type="bibr" target="#b6">[7]</ref> utilized bilinear operation to fuse image and question features in imageQA. To reduce the computational complexity, MCB uses the sampling-based approximation. To further reduce the feature dimension, Multimodal Low-rank Bilinear Attention Network (MLB) <ref type="bibr" target="#b13">[14]</ref> utilizes Hadamard product in the common space with two low-rank projection matrices. Multimodal Tucker Fusion <ref type="bibr" target="#b3">[4]</ref> utilizes tucker decomposition <ref type="bibr" target="#b26">[27]</ref> to efficiently parameterize bilinear interactions between visual and textual representation.</p><p>VideoQA is a natural extension of imageQA as video can be seen as temporal extension of image. Large-scale videoQA benchmarks such as TGIF-QA <ref type="bibr" target="#b10">[11]</ref> and 'fill-inthe-blank' <ref type="bibr" target="#b33">[34]</ref> have boosted the research on videoQA. Spatio-temporal VQA (ST-VQA) <ref type="bibr" target="#b10">[11]</ref> generates spatial and temporal attention to localize which regions in a frame and which frames in a video to attend, respectively. Yu et al. <ref type="bibr" target="#b31">[32]</ref> proposed Joint Sequence Fusion (JSFusion) that measures semantic similarity between video and language. JS-Fusion utilizes hierarchical attention mechanism that learns matching representation patterns between modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Movie Story Question Answering</head><p>A recent direction in videoQA leverages text modality such as subtitle in addition to video modality for story understanding. To this end, various video story QA benchmarks such as PororoQA <ref type="bibr" target="#b15">[16]</ref>, MeMexQA <ref type="bibr" target="#b11">[12]</ref>, TVQA <ref type="bibr" target="#b16">[17]</ref> and MovieQA <ref type="bibr" target="#b25">[26]</ref> have been suggested. Numerous researches have tackled MovieQA benchmark which provides movie clip, subtitle and other various textual descriptions. Tapaswi et al. <ref type="bibr" target="#b25">[26]</ref> divided the movie into multiple sub-shots and utilized memory network (MemN2N) <ref type="bibr" target="#b23">[24]</ref> to store video and subtitle features into memory slots. Deep Embedded Memory Network (DEMN) <ref type="bibr" target="#b15">[16]</ref> reconstructs stories from a joint stream of scene-dialogue using a latent embedding space and retrieves information which is relevant to the question. Na et al. <ref type="bibr" target="#b20">[21]</ref> proposed Read-Write Memory Network (RWMN) which is a CNN-based memory network where video and subtitle features are first fused using bilinear operation, then write/read networks store/retrieve information, respectively.</p><p>Liang et al. <ref type="bibr" target="#b17">[18]</ref> proposed Focal Visual-Text Attention (FVTA) that utilizes the hierarchical attention applied to a three-dimensional tensor to localize evidential image and text snippets. Layered Memory Network (LMN) <ref type="bibr" target="#b28">[29]</ref> utilizes Static Word Memory module and Dynamic Subtitle Memory module to learn frame-level and clip-level representations. The hierarchically formed movie representation encodes the correspondence between words and frames, and the temporal alignment between sentences and frames. Multimodal Dual Attention Memory (MDAM) <ref type="bibr" target="#b14">[15]</ref> utilizes multi-head attention mechanism <ref type="bibr" target="#b27">[28]</ref> and question attention to learn the latent concepts of multimodal contents. Multimodal fusion is performed once after the attention process. Compared to existing architectures on movie story QA that adopt single-step reasoning, PAMN provides multi-step reasoning approach to localize necessary information from question, answers, and movie contents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Progressive Attention Memory Network</head><p>This section describes the proposed Progressive Attention Memory Network (PAMN). <ref type="figure" target="#fig_0">Fig. 1</ref> shows the overall architecture of PAMN, which fully utilizes diverse sources of information (video, subtitle, question and candidate answers) to answer the question. The pipeline of PAMN is as follows. First, video and subtitle are embedded into dual memory as in <ref type="figure" target="#fig_0">Fig. 1</ref>(a) that holds independent memories for each modality. Then, progressive attention mechanism pinpoints temporal parts that are relevant to answering the question as in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. To infer the correct answer, dynamic modality fusion in <ref type="figure" target="#fig_0">Fig. 1</ref>(c) adaptively integrates outputs of each memory by considering contribution of each modality. Belief correction answering scheme successively corrects the belief of each answer from equally likely initialized belief as in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setup</head><p>The formal definition of the problem is as follows. The inputs of PAMN are (1) a question representation q ∈ R 300 , (2) five candidate answer representations</p><formula xml:id="formula_0">{a i } 5 i=1 ∈ R 5×300 , (3) temporally aligned video (v) and subtitle (s) representation {(v i , s i )} T i=1</formula><p>on the whole movie. Each element of subtitle representation s i corresponds to the dialog sentence of a character and each element of video representation v i is extracted from temporally aligned video clip. The number of overall sentences of the movie is denoted as T . The detailed explanation on extracting visual and textual feature is provided in Section 4.2. The objective is to maximize the following likelihood:</p><formula xml:id="formula_1">arg max θ D log P (y|v, s, q, a; θ),<label>(1)</label></formula><p>where θ denotes learnable model parameters, D represents dataset and y represents the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dual Memory Embedding</head><p>As depicted in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, the inputs are first mapped to an embedding space. The question representation q and candidate answer representations {a i } 5</p><p>i=1 are embedded to a common space by weight-shared linear fully connected (FC) layer with parameters W ug ∈ R 300×d and b ug ∈ R d , to yield question embedding u ∈ R d and answer embedding g ∈ R 5×d where d denotes the memory dimension.</p><p>Video representation v and subtitle representation s are embedded independently to generate video memory M v and subtitle memory M s . This dual memory structure enables pinpointing different temporal parts for each modality. To reflect the observation that the adjacent video clips often have strong correlations, we utilized the average pooling (Avg.Pool) layer to store the adjacent representations into a single memory slot.</p><p>As the first step of dual memory embedding, feedforward neural network (FFN) composed of two linear FC layers with ReLU non-linearity in between is applied to embed video and subtitle representation. This operates on every element of v and s independently. Then, average pooling layer is applied to model neighboring representations together, forming video memory M v and subtitle memory M s , i.e. dual memory:</p><formula xml:id="formula_2">FFN(x) = ReLU(xW 1 + b 1 )W 2 + b 2 , (2) M v = Avg.Pool(FFN(v); θ p , θ s ), (3) M s = Avg.Pool(FFN(s); θ p , θ s ),<label>(4)</label></formula><p>where θ p and θ s denotes the size and stride of the pooling, x indicates the each input and W, b denotes the weight and bias of feed-forward neural network. Finally, generated video and subtitle memory are</p><formula xml:id="formula_3">M v , M s ∈ R N ×d where N = T /θ s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Progressive Attention Mechanism</head><p>The progressive attention mechanism in <ref type="figure" target="#fig_0">Fig. 1</ref>(b) takes dual memory M v , M s , question embedding u and answer embedding g as inputs, and progressively attends and updates the dual memory. While iteratively taking question and answers for temporal attention generation, memories are progressively updated to accumulate cues to locate relevant temporal parts for answering the question. We observed that single-step temporal attention on memory networks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref> often generates blurry attention map. The multi-step nature of progressive attention mechanism enables generating sharper attention distribution. Unnecessary information from memory is filtered out at each iteration.</p><p>The first step of progressive attention mechanism is temporal attention by question embedding u. The attention weights are obtained by calculating the cosine similarity between each memory slot and question embedding u as in Eqs.5, 6. The dual memory is multiplied by the attention weights and is followed by linear FC layer to be updated as in Eqs.7, 8. The attention operates independently for the video memory M v and the subtitle memory M s :</p><formula xml:id="formula_4">α v M u = softmax(uM v ),<label>(5)</label></formula><formula xml:id="formula_5">α s M u = softmax(uM s ),<label>(6)</label></formula><formula xml:id="formula_6">M v ← (α v M u M v )W v M u + b v M u ,<label>(7)</label></formula><formula xml:id="formula_7">M s ← (α s M u M s )W s M u + b s M u ,<label>(8)</label></formula><p>where α v M u , α s M u ∈ R N denote the temporal attention weight for M v , M s , respectively. The learnable parameters for linear FC layer is denoted by W M u , b M u , ← indicates the update operation and represents the elementwise multiplication with broadcasting on appropriate axis.</p><p>The second step of progressive attention mechanism is temporal attention by answers. This step is similar to the first step except it utilizes answer embedding g to attend updated dual memory M v , M s :</p><formula xml:id="formula_8">α v M g = softmax(gM v ), (9) α s M g = softmax(gM s ),<label>(10)</label></formula><formula xml:id="formula_9">M v ← (α v M g M v )W v M g + b v M g ,<label>(11)</label></formula><formula xml:id="formula_10">M s ← (α s M g M s )W s M g + b s M g ,<label>(12)</label></formula><p>where α v M g and α s M g ∈ R 5×N denote the temporal attention weights for dual memory and M v , M s ∈ R 5×N ×d represent the updated video and subtitle memory, respectively.</p><p>Multiple Hops Extension. As described above, the progressive attention mechanism attends the dual memory only once for each attention step. In this case, the dual memory may contain much irrelevant information and lack capability to query complicated semantics to answer the question. Progressive attention can be naturally extended to utilize multiple hops <ref type="bibr" target="#b23">[24]</ref> for fine-grained extraction of abstract concepts and reasoning of high-level semantics.</p><p>Different from the memory network <ref type="bibr" target="#b23">[24]</ref> that utilizes the sum of the output o k and query u k of k-th hop as the query of next hop, we use the same question embedding u with updated dual memory M (k) for k-th hop. Each attention step in Eqs. 5-8, 9-12 is repeated h M u , h M g times, respectively. Each attend and update operations can be expressed as:</p><formula xml:id="formula_11">α (k) = softmax(xM (k−1) ),<label>(13)</label></formula><formula xml:id="formula_12">M (k) ← (α (k) M (k−1) )W (k) + b (k) ,<label>(14)</label></formula><p>where the subscripts and superscripts corresponding to each equation are omitted to avoid repetition, and x indicates u or g for each step of progressive attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Modality Fusion</head><p>Dynamic modality fusion in <ref type="figure" target="#fig_0">Fig. 1(c)</ref> aggregates dual memory into fused output o at the end of each progressive attention step. Different question requires different modality to infer the answer. Consider the question "What drink bottle is at the table when Robin, Lily, Marshall and Ted are talking to each other?". In this case, the video modality would be more important than subtitle modality. Similar to modality attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, dynamic modality fusion is softattention based algorithm that determines the contribution of each modality for answering the question.</p><p>Given dual memory M v , M s , dynamic modality fusion first sum each memory along temporal axis and compute cosine similarity with question embedding u to calculate attention score.:</p><formula xml:id="formula_13">o m = N n=1 M m ,<label>(15)</label></formula><formula xml:id="formula_14">α DMF = softmax(u[o v ; o s ] ),<label>(16)</label></formula><p>where m indicates each modality v or s, o m represents the output of each memory, N denotes the temporal length of dual memory, and α DMF denotes attention weights. Finally, the fused output o is computed by weighted summing between attention weight and memory output:</p><formula xml:id="formula_15">o = m α m DMF o m .<label>(17)</label></formula><p>The learned attention weight can be interpreted as contribution or importance of each modality on answering the question. By regulating the ratio of each modality on fused output, dynamic modality fusion leads to stable learning by discarding information from unnecessary modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Belief Correction Answering Scheme</head><p>Belief correction answering scheme in <ref type="figure" target="#fig_0">Fig. 1(d)</ref> selects the correct answer among five candidate answers. Rather than determining the prediction score once, belief correction answering scheme successively corrects prediction score by observing diverse source of information. This mimics the multi-step reasoning process of human answering difficult questions <ref type="bibr" target="#b9">[10]</ref>. Combined with progressive attention and dynamic modality fusion, this multi-step reasoning approach of PAMN strengthens the model's ability to extract high-level meaning from the multimodal data.</p><p>Belief B ∈ R 5 denotes the prediction score on the candidate answers. The prediction probability z ∈ R 5 is computed by normalizing the belief, and the answer y is predicted with the highest probability:</p><formula xml:id="formula_16">z = softmax(B),<label>(18)</label></formula><p>y = arg max i∈ <ref type="bibr" target="#b4">[5]</ref> (z i ).</p><p>One way of initializing belief would be null initialization that endows all candidate answers with equal probabilities before observing any information. To reflect this unbiased initialization, the belief B is initialized as zero vector. Belief correction answering scheme adopts three-step belief correction; u-, Muand Mg-correction. For each correction step, the belief is corrected by accumulating the similarity between answer embedding g and the observed information. Belief is first corrected by only considering the question, i.e. u-correction. The intuition is that human often builds prior biases after skimming through only the question and candidate answers:</p><formula xml:id="formula_18">B u = ug ,<label>(20)</label></formula><formula xml:id="formula_19">B ← B + B u .<label>(21)</label></formula><p>Then for Muand Mg-correction, the outputs of first and second progressive attention steps, o M u and o M g , are considered. Again, the similarities between answer embedding g are computed:</p><formula xml:id="formula_20">B M u = o M u g , (22) B M g,i = o M g,i g i .<label>(23)</label></formula><p>Finally, the belief is corrected to infer correct answer:</p><formula xml:id="formula_21">B ← B + β M u B M u ,<label>(24)</label></formula><formula xml:id="formula_22">B ← B + β M g B M g ,<label>(25)</label></formula><p>where the correction weights β M u , β M g are hyper parameters that scales corresponding belief correction. Note that the belief is normalized to have unit norm after each correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>MovieQA <ref type="bibr" target="#b25">[26]</ref> benchmark is constructed for movie story QA which consists various sources of information such as movie clip, subtitle, plot synopses, scripts and DVS transcriptions. MovieQA dataset contains 408 movies with corresponding 14,944 multiple-choice questions. MovieQA benchmark consists of 6 tasks according to which sources to be used. This paper focuses on video+subtitles task which is the only task utilizing movie clip. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature extraction</head><p>For fair comparison, we extracted visual and textual features similar to previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref> and fixed them during training.</p><p>Textual feature Each sentence from question, candidate answers and subtitle are divided into sequence of words, then each word is embedded by skip-gram model <ref type="bibr" target="#b19">[20]</ref> provided by Tapaswi et al. <ref type="bibr" target="#b25">[26]</ref> which is trained on MovieQA plot synopses. In order to encode the order of words within a sentence, position encoding (PE) <ref type="bibr" target="#b23">[24]</ref> is utilized to obtain textual feature. For example in the case of question,  <ref type="table">Table 2</ref>. Accuracy comparison on the test set of TVQA benchmark without timestamp annotation. We utilized the video and text features extracted by Lei et al. <ref type="bibr" target="#b16">[17]</ref>. q = n PE(q n ) ∈ R 300 where each q n indicates word vector.</p><p>Visual feature Movies are divided into video clips that are temporally aligned with each sentence of the subtitle. The frames are sampled from each video clip with the rate of 1 fps. Then, frame feature of size 1536 is extracted from "Average Pooling" layer on Inception-v4 <ref type="bibr" target="#b24">[25]</ref>. Finally, mean-pooling over all frame features from the corresponding video clip produces the visual feature, v i ∈ R 1536 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>The entire architecture was implemented using Tensorflow <ref type="bibr" target="#b0">[1]</ref> framework. All the results reported in this paper were obtained using the Adagrad optimizer <ref type="bibr" target="#b4">[5]</ref> with a minibatch size of 32 and the learning rate of 0.001. All the experiments were performed under CUDA acceleration with single NVIDIA TITAN Xp (12GB of memory) GPU. In all the experiments, the recommended train / validation / test split was strictly observed. <ref type="table">Table 1</ref> compares the validation and test accuracy on the MovieQA benchmark of Video+Subtitles task. We compare the performance of PAMN with other state-of-the-art architecture. The ground-truth answers for MovieQA test set are not observable and the evaluation on the test set can only be performed once every 72 hours through an online evaluation. On MovieQA benchmark, PAMN exhibits the state-ofthe-art results by attaining test accuracy of 42.53%. It outperforms the runner-up, MDAM <ref type="bibr" target="#b14">[15]</ref> (41.41%) by 1.12% and the third place, LMN <ref type="bibr" target="#b28">[29]</ref> (39.03%) by 3.50%. Note the MDAM is an ensemble of 20 different models, while PAMN is a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Results</head><p>In order to evaluate the effectiveness of each modality, experiments based on using only video and subtitle were also conducted: PAMN w/o Sub and PAMN w/o Vid. From near random-guess performances of SSCB w/o Sub <ref type="bibr" target="#b25">[26]</ref> and MemN2N w/o Sub <ref type="bibr" target="#b25">[26]</ref> as shown in <ref type="table" target="#tab_2">Table.</ref> 1, it is noticed that movie story understanding is difficult using only video. The PAMN w/o Sub attains large performance gain of 19.23% compared to MemN2N w/o Sub. It even achieves performance comparable to LMN <ref type="bibr" target="#b28">[29]</ref> which exploits both video and subtitle. PAMN understands movie story even without observing subtitle. From <ref type="table" target="#tab_2">Table.</ref>1, it is noticed that PAMN performs better than PAMN w/o Vid and PAMN w/o Sub which indicates both video and subtitle provides conducive information in improving prediction. <ref type="table">Table 2</ref> shows performance comparison on TVQA benchmark without timestamp annotation. In this experiment, we utilized the video and text features extracted by Lei et al. <ref type="bibr" target="#b16">[17]</ref> (i.e. ImageNet and visual concept feature for video and GloVe feature for text) for fair comparison. Further, we encoded the sentence feature using LSTM instead of position encoding. On TVQA benchmark, PAMN outperforms state-of-the-art result by attaining test accuracy of 66.77% with visual concept feature.  <ref type="bibr" target="#b6">[7]</ref> 42.89 -0.45% PAMN w/ MFB <ref type="bibr" target="#b32">[33]</ref> 42.55 -0.79% PAMN w/ Tucker <ref type="bibr" target="#b3">[4]</ref> 42  <ref type="bibr" target="#b6">[7]</ref>, MFB <ref type="bibr" target="#b32">[33]</ref>, Tucker decomposition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref> instead of dynamic modality fusion, respectively. As shown in the second block of <ref type="table" target="#tab_2">Table.</ref> 3, fusing modalities by averaging or bilinear operations show lower performance than dynamic modality fusion. This implies that question dependent modality weighting (i.e. dynamic modality fusion) helps strengthens conducive modality. To measure the effectiveness of belief correction answering scheme, the third block of Table. 4 summarizes the performance variation depending on three sets of hyperparameters; the number of hops for attention by question u and answer g, θ p , θ s : size and stride of Avg. Pool layer, and β M u , β M g : correction weights for belief correction module. The multiple hops extension with 2-repetitions exhibits the best validation performance for PAMN. The multiple hops extension with more than three repetitions may suffer from overfitting due to the small size of dataset. The performance is positively affected by increasing θ p and θ s , but it degrades for large θ p and θ s due to information blurring of Avg. Pool. We observed that there is no best-performing optimal correction weights. If the question representation u has enough information about where in the movie to focus on, β M u should be higher, and vice versa. Furthermore, it is preferable to have smaller β M g than β M u since large value of β M g dilates the effect of ug and Mu correction since the normalization is applied in between every belief correction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative analysis</head><p>The <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the selected qualitative examples of PAMN. Each example provides the temporal attention map α v M g , α s M g from progressive attention mechanism, the ground-truth (GT) temporal part where the question was generated from, the attention weights α v DM F , α s DM F from dynamic modality fusion, and the inference path of belief correction answering scheme. The generated temporal attention well matches with the GT which indicates that PAMN successively learns where to attend. The weights α v DMF , α s DMF adaptively scales depending on the question type which implies that PAMN learns what modality to use without additional supervision. For some cases, PAMN predicts the correct answer at the u-correction step while for other cases the correct answer is determined at the last (Mg) step. PAMN is an interpretable architecture in that the inference path and the attention map provide the trace of where PAMN attends and what information source it used to answer the question.</p><p>The <ref type="figure" target="#fig_2">Fig. 3</ref> exhibits the accuracy comparison with respect to the first word of the question between MemN2N <ref type="bibr" target="#b25">[26]</ref>, RWMN <ref type="bibr" target="#b20">[21]</ref> and PAMN on the validation set of MovieQA benchmark. The results on 5W1H question types: Who, Where, When, What, Why and How are analyzed. Typically, answering who, where, when, what questions require pinpointing the temporal parts relevant to the question (e.g., When do the loyalists take over Air Force One?, What does Korshunov demand from Vice President Bennett?). On the other hand, answering why, how questions require understanding the contextual information over the  whole movie (e.g., How do Schmidt and Jenko's fake identities end up getting switched?, Why does Mozart's finan-cial situation get worse and worse?). We observed that PAMN outperforms MemN2N and RWMN on the majority of question types. Especially, PAMN attains 20% and 13% performance boosts on when, where questions, respectively which implies the superiority of PAMN to pinpoint the movie story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a movie story question answering (QA) architecture referred to as Progressive Attention Memory Network (PAMN) was proposed. The main challenges of movie story QA were summarized as: (1) pinpointing the temporal parts relevant to answer the question is difficult (2) different questions require different modality to infer the answer. Proposed PAMN make use of three main features to tackle aforementioned challenges: (1) progressive attention mechanism, (2) dynamic modality fusion and (3) belief correction answering scheme. We empirically demonstrated that proposed PAMN is valid by showing the state-of-the-art performance on MovieQA and TVQA benchmark dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the proposed PAMN. The pipeline of PAMN is as follows. (a) Question and candidate answers are embedded into a common space. Video and subtitle are embedded into dual memory that holds independent memories for each modality. (b) Progressive attention mechanism pinpoints temporal parts that are relevant to answering the question. To infer the correct answer, (c) dynamic modality fusion that adaptively integrates outputs of each memory by considering contribution of each modality. (d) Belief correction answering scheme successively corrects the belief of each answer from equally likely initialized belief.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Qualitative examples of MovieQA benchmark solved by PAMN (the last example is failure case). Green sentences and check symbols indicate correct answers and red dotted boxes highlight PAMN's prediction at each belief correction step. For failure cases, red 'x' symbols indicate the the incorrect selection. α v M g , α s M g represents temporal attention obtained by progressive attention mechanism, α v DMf , α s DMF denotes attention obtained by dynamic modality fusion. The temporal attention by PAMN matches well with groundtruth (GT) where the question is generated. Observing diverse source of information, PAMN successfully corrects the belief toward correct answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Accuracy comparison with respect to the first word of the question between MemN2N [26], RWMN [21] and PAMN on the validation set of MovieQA. PAMN outperforms on the majority of the question types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TVQA [17] benchmark is video story QA dataset on TV show domain. It consists of total 152.5k question-answer pairs on six TV shows: The Big Bang Theory, How I Met Your Mother, Friends, Grey's Anatomy, House, Castle. Each split of TVQA contains 122k, 15.25k, 15.25k for train, validation and test, respectively. Unlike MovieQA which considers whole movie as input, TVQA contains 21,793 short clips of 60/90 seconds segmented from the original TV show for question-answering.</figDesc><table><row><cell>Since only 140 movies</cell></row><row><cell>contain video clips, there are 6,462 question-answer pairs</cell></row><row><cell>which splits into 4,318 training, 886 validation and 1,258</cell></row><row><cell>test samples.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table . 3</head><label>.</label><figDesc>summarizes the ablation analysis of PAMN on the validation set of MovieQA benchmark in order to measure the validity of the key components of PAMN. To measure to effectiveness of progressive attention mechanism, each temporal attention step of PAMN w/o PA utilizes dual memory obtained in Eqs. 3,4, i.e. PAMN w/o PA do not accumulate cues and each attention step operates in a parallel manner. PAMN w/o Multiple Hop attends dual memory only once for each temporal attention step. As shown in the first block ofTable.3, PAMN w/o PA underperforms PAMN, which shows that the attention accumulation by progressive attention mechanism is important in understanding movie story. Multiple hops extension is also crucial in attaining the best possible performance.</figDesc><table><row><cell>Methods</cell><cell>valid Acc.</cell><cell>∆</cell></row><row><cell>PAMN w/o PA</cell><cell>42.03</cell><cell>-1.31%</cell></row><row><cell>PAMN w/o Multiple Hop</cell><cell>42.67</cell><cell>-0.67%</cell></row><row><cell>PAMN w/o DMF</cell><cell>42.09</cell><cell>-1.25%</cell></row><row><cell>PAMN w/ MCB</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>For ablating dynamic modality fusion, we experiment with</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table .</head><label>.</label><figDesc></figDesc><table><row><cell>3 shows the experimental results of three</cell></row><row><cell>variants: PAMN w/o Mu,Mg-correction, PAMN w/o Mg-</cell></row><row><cell>correction, and PAMN w/o Mu-correction. It is noteworthy</cell></row><row><cell>that only using QA pairs shows much higher performance</cell></row><row><cell>that the random baseline of 20%. Considering M u-and</cell></row><row><cell>M g-correction, PAMN w/o Mg-correction shows 2.26%</cell></row><row><cell>and PAMN w/o Mu-correction shows 1.36% performance</cell></row><row><cell>boosts, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance variation of PAMN on the validation set of MovieQA benchmark depending on three sets of hyper parameters. hMu, hMg: the number of hops for attention by question u and answer g, θp, θs: size and stride of Avg. Pool layer, and βMu, βMg: correction weights for belief correction module.</figDesc><table><row><cell cols="2"># hops</cell><cell cols="2">Avg. Pool</cell><cell cols="2">Correction</cell><cell>Acc.</cell></row><row><cell cols="3">h M u h M g θ p</cell><cell>θ s</cell><cell cols="2">β M u β M g</cell><cell></cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.5</cell><cell>38.94</cell></row><row><cell>1</cell><cell>1</cell><cell>12</cell><cell>8</cell><cell>1</cell><cell>0.5</cell><cell>40.18</cell></row><row><cell>1</cell><cell>1</cell><cell>24</cell><cell>16</cell><cell>0.5</cell><cell>0.5</cell><cell>40.07</cell></row><row><cell>1</cell><cell>1</cell><cell>24</cell><cell>16</cell><cell>1</cell><cell>0.1</cell><cell>42.10</cell></row><row><cell>1</cell><cell>1</cell><cell>24</cell><cell>16</cell><cell>1</cell><cell>0.5</cell><cell>42.67</cell></row><row><cell>1</cell><cell>1</cell><cell>40</cell><cell>30</cell><cell>0.5</cell><cell>0.5</cell><cell>40.97</cell></row><row><cell>1</cell><cell>1</cell><cell>40</cell><cell>30</cell><cell>1</cell><cell>0.1</cell><cell>42.66</cell></row><row><cell>1</cell><cell>1</cell><cell>40</cell><cell>30</cell><cell>1</cell><cell>0.5</cell><cell>42.55</cell></row><row><cell>1</cell><cell>1</cell><cell>80</cell><cell>60</cell><cell>1</cell><cell>0.5</cell><cell>41.20</cell></row><row><cell>2</cell><cell>2</cell><cell>24</cell><cell>16</cell><cell>1</cell><cell>0.5</cell><cell>43.34</cell></row><row><cell>2</cell><cell>2</cell><cell>40</cell><cell>30</cell><cell>1</cell><cell>0.1</cell><cell>42.89</cell></row><row><cell>3</cell><cell>3</cell><cell>24</cell><cell>16</cell><cell>1</cell><cell>0.5</cell><cell>42.55</cell></row><row><cell>3</cell><cell>3</cell><cell>40</cell><cell>30</cell><cell>1</cell><cell>0.1</cell><cell>42.77</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>OSDI 16</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MUTAN: Multimodal Tucker Fusion for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stacked latent attention for multimodal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sat reading analysis using eye-gaze tracking technology and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Tutoring Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01336</idno>
		<title level="m">Memexqa: Visual memex question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pivot correlational neural network for multimodal video categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal dual attention memory for video story question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepstory: Video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6135" to="6143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A readwrite memory network for movie story understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seil</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Movie question answering: Remembering the textual cues for layered visual contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chopra</forename><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multimodal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
