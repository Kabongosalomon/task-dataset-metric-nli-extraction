<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding and Improving Layer Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
							<email>jingjingxu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<email>xusun@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
							<email>zhaoguangxiang@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
							<email>linjunyang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding and Improving Layer Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets. backward gradients. Furthermore, it is beyond our expectation that the bias and gain do not work in most cases. The details of our findings are illustrated below.</p><p>The derivatives of the mean and variance are more important to LayerNorm than forward normalization. Many of the previous studies believe that the forward normalization is the only decisive factor to LayerNorm. It makes the input distribution more stable, thus brings better convergence. Unlike them, our experimental results show that forward normalization has little to do with the effectiveness and the derivatives of the mean and variance play a significant role in LayerNorm. To illustrate how these derivatives work, we propose DetachNorm, which adds an additional detaching operation to LayerNorm to change the mean and variance from variables to constants. It preserves the re-centering and re-scaling fact but cuts off the derivative of the mean and variance with respect to the input. DetachNorm performs worse than LayerNorm on six out of eight datasets. This proves that the derivatives of the mean and variance are useful to LayerNorm. Furthermore, to investigate the reason for the above observation, we analyze the gradients in LayerNorm and DetachNorm, and find that the derivatives of means re-center gradients and the derivatives of variances re-scale gradients.</p><p>The parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. The bias and gain are applied for affine transformation on normalized vectors. They are expected to enhance the expressive power by re-shaping the distribution. To evaluate their effects on results, we build a simple version of LayerNorm (LayerNorm-simple) by removing the bias and gain. Our experimental results show that LayerNorm-simple achieves better results than LayerNorm on four datasets. It even achieves the state-of-the-art performance on En-Vi machine translation. By comparing loss curves of LayerNorm with and without the bias and gain, we find that the bias and gain cause over-fitting. We speculate the reason of over-fitting is mainly that the bias and gain are learned from the training set and cannot adjust themself towards different input distributions when testing.</p><p>Motivated by this assumption, we propose a novel normalization method, Adaptive Normalization (AdaNorm). AdaNorm replaces the bias and gain with a new transformation function. This function adaptively adjusts scaling weights based on input values. We evaluate AdaNorm and LayerNorm on eight datasets, covering tasks of machine translation, language modeling, text classification, image classification, and dependency parsing. Results show that AdaNorm achieves better results on seven datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network training has long been a focus in Deep Learning research area. One of the prominent progress is the application of normalization methods. Initially, <ref type="bibr" target="#b11">Ioffe and Szegedy [2015]</ref> introduce the concept of normalizing layers with the proposed Batch Normalization (BatchNorm). It is widely believed that by controlling the mean and variance of layer inputs across mini-batches, BatchNorm stabilizes the distribution and improves training efficiency. Following this work, Lei <ref type="bibr" target="#b14">Ba et al. [2016]</ref> point out its limitation in Recurrent Neural Networks (RNN) and propose Layer Normalization (LayerNorm) that is performed across the neurons in a layer. LayerNorm is adaptive to RNN and self-attention-based models. A typical example is its application in the state-of-the-art framework, Transformer <ref type="bibr" target="#b25">[Vaswani et al., 2017]</ref>. LayerNorm enables faster training of Transformer and is irreplaceable in this framework.</p><p>Despite its great success, it is still unclear why LayerNorm is so effective. The widely accepted explanation is that forward normalization brings distribution stability <ref type="bibr" target="#b11">[Ioffe and</ref><ref type="bibr">Szegedy, 2015, Lei Ba et al., 2016]</ref>. Recent studies show that the effects of BatchNorm are not related to the stability of input distribution <ref type="bibr" target="#b28">[Zhang et al., 2017</ref><ref type="bibr" target="#b21">, Santurkar et al., 2018</ref>. They also propose that the reason why BatchNorm is effective is that normalization smooths the optimization landscape. However, it is still unclear whether these theories can explain the success of LayerNorm.</p><p>The main contribution of this paper is to explore how LayerNorm works. Through a series of analyses, we find that the derivatives of the mean and variance are important by re-centering and re-scaling 2 Preliminaries</p><p>In this section, we first review the algorithm of LayerNorm and then introduce the datasets and models used in the following analysis sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LayerNorm Algorithm</head><p>Let x = (x 1 , x 2 , . . . , x H ) be the vector representation of an input of size H to normalization layers. LayerNorm re-centers and re-scales input x as</p><formula xml:id="formula_0">h = g N (x) + b, N (x) = x − µ σ , µ = 1 H H i=1 x i , σ = 1 H H i=1 (x i − µ) 2<label>(1)</label></formula><p>where h is the output of a LayerNorm layer. is a dot production operation. µ and σ are the mean and standard deviation of input. Bias b and gain g are parameters with the same dimension H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Setup</head><p>To investigate how LayerNorm works, we conduct a series of experiments in this paper. Since LayerNorm is a default setting in Transformer <ref type="bibr" target="#b25">[Vaswani et al., 2017]</ref> and Transformer-XL <ref type="bibr" target="#b7">[Dai et al., 2019]</ref>, which have shown state-of-the-art results on a variety of tasks (e.g., machine translation), we primarily consider normalization on Transformer and Transformer-XL networks. Also, to avoid the impact of model architecture, we evaluate the effects of normalization on feed-forward neural networks and convolutional neural networks. Here list the datasets and models. More details can be found at the Appendix.</p><p>Machine translation includes three widely-used datasets, WMT English-German (En-De), IWSLT 14 German-English (De-En) <ref type="bibr" target="#b3">[Cettolo et al., 2014]</ref> and IWSLT 15 English-Vietnamese (En-Vi) <ref type="bibr" target="#b4">[Cettolo et al., 2015]</ref>. For all dataset, we use the setting of PreNorm where normalization is applied before each layer. We re-implement Transformer with the released code of Fairseq <ref type="bibr" target="#b16">[Ott et al., 2019]</ref> 2 . The evaluation metric is BLEU <ref type="bibr" target="#b18">[Papineni et al., 2002]</ref>.</p><p>For En-De dataset, we use the same dataset splits and the same compound splitting following previous work <ref type="bibr" target="#b25">[Vaswani et al., 2017]</ref>. BPE is used to get vocabularies. We use the shared embedding setting and the vocabulary size is 32,765. We use "transformer_wmt_en_de_big_t2t" as our basic model. The dropout rate is 0.3. The learning rate is 0.001. The training batch size is 4,096 tokens. We use optimizer Adam with β 1 = 0.9 and β 2 = 0.98. The number of warmup steps is 4K.</p><p>The De-En dataset is provided by the IWSLT 2014 Evaluation Campaign. We use the same dataset splits following previous work <ref type="bibr" target="#b16">[Ott et al., 2019</ref><ref type="bibr" target="#b20">, Ranzato et al., 2016</ref><ref type="bibr" target="#b26">, Wiseman and Rush, 2016</ref>. It contains 153K sentences for training, 7K sentences for validation, and 7K sentences for testing. BPE is used to get vocabularies. We use the shared embedding setting and the vocabulary size is 10,149.</p><p>We use "transformer_iwslt_de_en" as our basic model. The dropout rate is 0.3. The attention dropout rate is 0.1. The activation dropout is 0.1. The initialization learning rate is 1e-07 and the learning rate is 0.0015. The training batch size is 4,096 tokens. We update gradients for every 2 steps. The number of warmup steps is 8K.</p><p>The En-Vi dataset contains 133K training sentence pairs provided by the IWSLT 2015 Evaluation Campaign. We use TED tst2012 (1,553 sentences) as the validation set and TED tst2013 (1,268 sentences) as the test set. BPE is used to get input and output vocabularies. The English and Vietnamese vocabulary sizes are 7,669 and 6,669 respectively. The dropout rate is 0.1. The learning rate is 0.001. The training batch size is 4,096 tokens. The number of warmup steps is 8K. We use "transformer_wmt_en_de" as our basic model. We use optimizer Adam with β 1 = 0.9 and β 2 = 0.98.</p><p>Language modeling includes a large dataset, Enwiki8 3 that contains 100M bytes of unprocessed Wikipedia text. We implement a 12-layer Transformer-XL model. The dimension of each layer is 512. Multi-head attention contains 8 heads and the dimension of each head is 64. The dropout rate is 0.1. The batch size is 22. We use optimizer Adam with a learning rate 0.00025. We use the average number of Bits-Per-Character (BPC) as the evaluation metric <ref type="bibr" target="#b0">[Al-Rfou et al., 2018</ref><ref type="bibr" target="#b7">, Dai et al., 2019</ref>.</p><p>Text classification includes two sentence classification datasets: RT <ref type="bibr" target="#b17">[Pang and Lee, 2005]</ref>, and SST5 <ref type="bibr" target="#b22">[Socher et al., 2013]</ref>. RT is a binary sentiment classification dataset from online movie reviews. We randomly divide all examples into 8,608 for training, 964 for validation, and 1,089 for testing. SST5 is a single-sentence classification dataset built on movie reviews. We run experiments on a five label set. We build a Transformer model with a 4-layer encoder. The batch size is 4,096 tokens. The word embedding dimension is 128 and the hidden dimension is 128. The dropout rate is 0.2. We use optimizer Adam with β 1 = 0.9, β 2 = 0.998. Normalization is applied before each layer. Accuracy is the evaluation metric.</p><p>Image classification includes a widely-used dataset, <ref type="bibr">MNIST [LeCun et al., 1998</ref>]. It consists of 55,000 training images, 5,000 validation images, and additional 10,000 testing images. We implement a 3-layer convolutional neural network for classification. The first 2D-convolution layer has 1 inchannel, 20 out-channels. The second 2D-convolution layer has 20 in-channels, 50 out-channels. We flatten the output of the second 2D-convolution layer and send it to a linear layer. The batch size is 32. We use optimizer Adam with a learning rate of 0.001. We apply LayerNorm before the activation in every linear layer. We train the model for 20 epochs. Normalization is applied before each layer. Accuracy is the evaluation metric.</p><p>Dependency parsing includes a dataset, English Penn TreeBank (PTB) <ref type="bibr" target="#b15">[Marcus et al., 1993]</ref>. We follow the standard split of the corpus with sections 2-21 as the training set <ref type="formula">(</ref>  <ref type="bibr" target="#b5">[Chen and Manning, 2014]</ref>. The dimension of the hidden state is 512, the batch size is 1, 024, the dropout rate is 0.2. We use optimizer Adam and initialize the learning rate to 0.001. We apply normalization before activation in every linear layer.</p><p>Following the work <ref type="bibr" target="#b5">[Chen and Manning, 2014]</ref>, we use Unlabeled Attachment Score (UAS) as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Understanding LayerNorm</head><p>To investigate how LayerNorm facilitates training, we conduct ablation studies to observe each part's contribution to the performance. In this section, we analyse the effects of the bias and gain, forward normalization, and backward normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Effect of the Bias and Gain in LayerNorm</head><p>The bias and gain do not work in most cases. From <ref type="table" target="#tab_1">Table 1</ref>, it can be found that LayerNorm is an effective approach. It brings large performance improvements on six out of eight datasets compared with the naive baseline without LayerNorm ("w/o Norm"). By comparing LayerNorm and LayerNorm-simple, we find that dropping the bias and gain ("LayerNorm-simple") does not decrease the performance on six datasets. Surprisingly, LayerNorm-simple outperforms LayerNorm on four datasets, even with a 0.4 BLEU improvement on En-Vi and a 1.31 ACC improvement on SST-5. Also, it needs to notice that 31.6 achieved by LayerNorm-simple is the state-of-the-art result on En-Vi machine translation.</p><p>Furthermore, we find that the bias and gain increase the risk of over-fitting. Initially, considering that input information may be lost when normalizing input distributions, the bias and gain are designed for affine transformation on normalized vectors to enhance the expressive power. However, since the bias and gain are learned from the training set and they ignore the input distributions of the testing data, the risk of over-fitting may increase in LayerNorm. It is verified by convergence curves in <ref type="figure" target="#fig_0">Figure 1</ref>. LayerNorm achieves lower training loss (or BPC) but higher validation loss (or BPC) than LayerNorm-simple on En-Vi, Enwiki8. These results indicate that current affine transformation mechanism has a potential risk of over-fitting and needs to be further improved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Effect of Forward Normalization</head><p>For easier analysis, we only consider LayerNorm without the bias and gain here. Let y = (y 1 , y 2 , . . . , y H ) be the normalized vector, the calculation process of LayerNorm without the bias and gain can be written as</p><formula xml:id="formula_1">y = x − µ σ , µ = 1 H H i=1 x i , σ = 1 H H i=1 (x i − µ) 2 (2) where x = (x 1 , x 2 , . . . , x H )</formula><p>is the input vector and H is the dimension of x. µ and σ are the mean and standard deviation of x 1 , x 2 , . . . , x H . Then, supposeȳ and D y are the mean and variance of y 1 , y 2 , . . . , y H . It is easy to verifȳ</p><formula xml:id="formula_2">y = 1 H H i=1 y i = 1 H H i=1 (x i − µ) σ = 0, D y = 1 H H i=1 (x i − µ) 2 σ 2 = 1.<label>(3)</label></formula><p>Eq.</p><p>(3) shows that normalization re-centers and re-scales input vector x. By now, a widely accepted belief is that the effectiveness of LayerNorm comes from steady layer distributions brought by forward normalization [Lei <ref type="bibr" target="#b14">Ba et al., 2016]</ref>. To evaluate whether forward normalization explains the effectiveness of LayerNorm, we need to separate the effect on forward layer inputs and that on backward gradients. In this paper, we design a new method, called DetachNorm. The difference between LayerNorm and DetachNorm is that DetachNorm detaches the derivatives of the mean and variance 4 . Detaching derivatives means treating the mean and variance as changeable constants, rather than variables, which do not require gradients in backward propagation. The calculation of DetachNorm can be written as</p><formula xml:id="formula_3">y = x −μ σ ,μ = θ(µ),σ = θ(σ)<label>(4)</label></formula><p>where µ and σ are the mean and standard deviation of input x, as calculated in <ref type="figure">Eq.</ref> (2). The function θ(·) can be seen as a special copy function, which copies the values of µ and σ into constantsμ and σ. In all, DetachNorm keeps the same forward normalization fact as LayerNorm does, but cuts offs the derivatives of the mean and variance.</p><p>Since DetachNorm keeps the same re-centering and re-scaling way in forward propagation as LayerNorm-simple does, the gap between DetachNorm and "w/o Norm" shows the effect of forward normalization. As we can see, DetachNorm perform worse than "w/o Norm", showing that forward normalization has little to do with the success of LayerNorm.</p><p>Furthermore, the only difference between DetachNorm and LayerNorm-simple lies in that Detach-Norm detaches the derivatives of the mean and variance. As shown in <ref type="table" target="#tab_3">Table 2</ref>, DetachNorm performs worse than LayerNorm-simple on six datasets. It is mainly because that DetachNorm converges to much worse local optima compared with LayerNorm-simple, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The gap between DetachNorm and LayerNorm-simple shows the effectiveness of the derivatives of the mean and variance. By comparing the achieved improvements, we find that the derivatives of the mean and variance bring higher improvements than forward normalization does.</p><p>These results demonstrate that the derivatives of the mean and variance play a significant role. In addition, the extremely worse results of DetachNorm on En-De, De-En and En-Vi indicate that the derivatives of the mean and variance may be more important for deeper models. In the following section, we will give a detailed analysis of why and how the derivatives of the mean and variance contribute to the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Effect of the Derivatives of the Mean and Variance</head><p>To understand how the derivatives of the mean and variance work, we analyze the gradients of LayerNorm-simple and DetachNorm. According to the chain rule, the gradient of x is 5</p><formula xml:id="formula_4">∂ ∂x ← dy dx ∂ ∂y<label>(5)</label></formula><p>where is the loss function, x is the input vector and y is the normalized vector. We here analyze the effect of detaching the derivatives of the mean and variance on backward gradients. Our results are summarized in the following theorem, whose proof is listed in the Appendix.</p><p>Theorem 1. Given ∂ ∂y = (g 1 , g 2 , ..., g H ) T , letḡ and D g be the mean and variance of g 1 , g 2 , ..., g H . For the case of detaching the derivatives of µ and σ, suppose ∂ ∂x = (a 1 , a 2 , ..., a H ) T is the gradient of x with meanā and variance D a . We haveā =ḡ/σ and D a = D g /(σ 2 ).</p><p>(1) For the case of standard LayerNorm-simple, suppose ∂ ∂x = (b 1 , b 2 , ..., b H ) T is the gradient of x with meanb and variance D b .</p><p>We haveb = 0 and D b ≤ D g /(σ 2 ).</p><p>(2) For the case of detaching the derivative of µ, suppose ∂ ∂x = (c 1 , c 2 , ..., c H ) T is the gradient of x with meanc and variance D c .</p><p>We havec =ḡ/σ and D c ≤ D g /(σ 2 ).</p><p>(3) For the case of detaching the derivative of σ, suppose ∂ ∂x = (d 1 , d 2 , ..., d H ) T is the gradient of x with meand and variance D d .</p><p>We haved = 0 and D c = D g /(σ 2 ).</p><p>By comparing the case of detaching the derivative of µ with that of LayerNorm-simple in Theorem 1, we find that the derivative of µ re-centers ∂ ∂x to zero. By comparing the case of detaching the derivative of σ with of LayerNorm-simple, we find that the derivative of σ reduces the variance of ∂ ∂x , which can be seen a kind of re-scaling. We refer to gradient re-centering and re-scaling as gradient normalization.</p><p>To further evaluate the effect of gradient normalization on model performance, we test the derivatives of the mean and variance separately. <ref type="table">Table 3</ref> shows that detaching the derivative of variance decreases the performance significantly on deeper networks. Therefore, it is necessary to control the variance of gradients for deeper networks.</p><p>In conclusion, LayerNorm normalizes forward layer inputs and backward gradients. The derivatives of the mean and variance play more important roles than forward normalization in LayerNorm. Furthermore, unlike previous work <ref type="bibr" target="#b21">[Santurkar et al., 2018]</ref> only noticing that normalization smooths gradients, this paper provides deeper insight about how normalization impacts backward gradients. <ref type="table">Table 3</ref>: The derivative of variance is more important than that of mean for deeper networks. "(+)" means higher is better. "(-)" means lower is better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AdaNorm Algorithm</head><p>Formally, let y = N (x) = (x − µ)/ σ be the normalized vector where µ and σ are the mean and variance of the input x = (x 1 , x 2 , . . . , x H ). We use φ(y), a function with respect to input x, to replace the bias and gain with the following equation:</p><formula xml:id="formula_5">z = φ(y) y = φ(N (x)) N (x)<label>(6)</label></formula><p>where z = (z 1 , z 2 , . . . , z H ) is the output of AdaNorm and is a dot product operation. Unlike the bias and gain being fixed in LayerNorm, φ(y) can adaptively adjust scaling weights based on inputs. To keep the stability of training, we expect that φ(·) has some features. First, φ(·) must be differentiable. Second, we expect that the average scaling weight is fixed, namely the average of φ(y) is a constant C where C &gt; 0. Third, we expect that the average of z is bounded, which can avoid the problem of exploding loss. Namely, we require that there exists a constant M such</p><formula xml:id="formula_6">that | 1 H H i=1 z i | &lt; M .</formula><p>Theorem 2 proves that there exists a unique solution which can satisfy these requirements. The proof is listed in the Appendix.</p><formula xml:id="formula_7">Theorem 2. Suppose φ(y i ) is derivable, ∀y , 1 H H i=1 φ(y i ) = C &gt; 0, and ∃M, s.t. | 1 H H i=1 z i | &lt; M (M &gt; 0),</formula><p>where H is the hidden size. There exists only one solution:</p><formula xml:id="formula_8">φ(y i ) = C(1 − ky i )</formula><p>which can satisfy these requirements.</p><p>Since 1 − ky i &lt; 0 will undesirably change the direction of vector, we expect that φ(y i ) &gt; 0 holds, which means y i &lt; 1/k must hold. Due to the symmetry of y i , |y i | &lt; 1/k is required to hold too.</p><p>Based on Chebyshev's Inequality, we have</p><formula xml:id="formula_9">P (|y i | &lt; 1/k) = P (|y i − E(y i )| &lt; 1/k) ≥ 1 − D y (1/k) 2 = 1 − k 2 D y (7)</formula><p>where D y is the variance of y = (y 1 , y 2 , . . . , y H ) and H is the dimension of y. Based on Eq.</p><p>(3), we can verify D y = 1. If we expect that |y i | &lt; 1/k holds with a probability higher than 99%, k = 1/10 should be choose based on Eq. <ref type="formula">(7)</ref>. Namely, we choose</p><formula xml:id="formula_10">φ(y i ) = C(1 − y i 10 ).<label>(8)</label></formula><p>Given an input vector x, the complete calculation process of AdaNorm is</p><formula xml:id="formula_11">z = C(1 − ky) y, y = x − µ σ , µ = 1 H H i=1 x i , σ = 1 H H i=1 (x i − µ) 2 (9)</formula><p>where C is a hyper-parameter. is a dot product operation. k is recommended to set as 1/10. To prevent the introduced term C(1 − ky) dismissing the feature of gradient re-centering and re-scaling, we detach the gradient of C(1 − ky) and only treat it as a changeable constant in implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison between AdaNorm and LayerNorm</head><p>The comparison between LayerNorm and AdaNorm is shown in <ref type="table" target="#tab_5">Table 4</ref>. 7 AdaNorm outperforms LayerNorm on seven datasets, with 0.2 BLEU on En-De, 0.1 BLEU on De-En, 0.2 BLEU on En-Vi, 0.29 ACC on RT, 1.31 ACC on SST, 0.22 ACC on MNIST, and 0.11 UAC on PTB. Unlike LayerNorm-simple only performing well on bigger models, AdaNorm achieves more balanced results. <ref type="figure" target="#fig_2">Figure 3</ref> shows the loss curves of LayerNorm and AdaNorm on the validation set of En-Vi, PTB, and De-En. Compared to AdaNorm, LayerNorm has lower training loss but higher validation loss. Lower validation loss proves that AdaNorm has better convergence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Deep neural networks have outperformed shallow models in a variety of fields, such as natural language processing <ref type="bibr" target="#b23">[Sutskever et al., 2014</ref><ref type="bibr" target="#b1">, Bahdanau et al., 2015</ref><ref type="bibr" target="#b8">, Devlin et al., 2018</ref>, computer vision <ref type="bibr" target="#b9">[He et al., 2016</ref><ref type="bibr" target="#b10">, Huang et al., 2017</ref>, etc. The improvement mainly comes from the stronger expressive power of deep layers. However, with the increase of depth, the network training process becomes complicated and requires advanced architectural techniques. One of the important techniques of such advances is normalization.</p><p>Currently, it is widely accepted that normalization layers assist training by smoothing gradients, enabling large learning rates, accelerating convergence, and improving generalization results <ref type="bibr" target="#b29">[Zhang et al., 2019]</ref>. First introduced by Ioffe and Szegedy <ref type="bibr">[2015]</ref>, BatchNorm fixes layer distributions to reduce ICS (Internal Covariate Shift), a phenomenon that the upper layers need to continuously adapt to the new distributions of lower layers. Following this work, several normalization methods have been proposed, like instance normalization <ref type="bibr" target="#b24">[Ulyanov et al., 2016]</ref> and group normalization <ref type="bibr" target="#b27">[Wu and He, 2018]</ref>. In addition, there are several studies exploring better activation functions <ref type="bibr" target="#b12">[Klambauer et al., 2017]</ref> or initialization methods <ref type="bibr" target="#b29">[Zhang et al., 2019]</ref> to avoid the dependency on normalization layers.</p><p>LayerNorm is proposed to expand BatchNorm into RNN. LayerNorm normalizes the mean and variance of all summed inputs to the neurons in one layer. Unlike BatchNorm that depends on the size of mini-batch, LayerNorm has fewer limitations. LayerNorm is adaptive to RNN and self-attentionbased models. It has been applied to the state-of-the-art frameworks such as Transformer <ref type="bibr" target="#b25">[Vaswani et al., 2017]</ref>, BERT <ref type="bibr" target="#b8">[Devlin et al., 2018]</ref>, and Transformer-XL <ref type="bibr" target="#b7">[Dai et al., 2019]</ref>. LayerNorm brings better performance and is irreplaceable in these frameworks.</p><p>Despite the good performance, it is still unclear how layer normalization works. <ref type="bibr" target="#b11">Ioffe and Szegedy [2015]</ref> claim that the effectiveness of BatchNorm comes from reducing ICS. It has been a popular belief about BatchNorm <ref type="bibr" target="#b21">[Santurkar et al., 2018]</ref>. However, some recent studies point out that the success of BatchNorm relates to the smoother gradients and has little to do with reducing ICS <ref type="bibr" target="#b21">[Santurkar et al., 2018</ref><ref type="bibr" target="#b2">, Bjorck et al., 2018</ref>. Although these studies provide a pioneering perspective to understand BatchNorm, there still remain some unanswered questions, such as how BatchNorm helps smooth gradients. Also, there are little work studying whether these theories can explain the success of LayerNorm. In this paper, we take a further step to a better understanding of LayerNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigate how layer normalization works. Based on a series of experiments and theoretical analysis, we summarize some interesting conclusions. We find that the derivatives of the mean and variance are important to the success of LayerNorm by re-centering and re-scaling backward gradients. Furthermore, experiments show that the bias and gain increase the risk of over-fitting and do not work in most cases. To address this problem, we propose a normalization method AdaNorm. It replaces the bias and gain in LayerNorm with a new adaptive transformation function that can update scaling weights based on input values. Experiments show that AdaNorm outperforms LayerNorm on seven datasets. In the future work, we would like to explore more alternatives to LayerNorm from the perspective of gradient normalization. 7 Appendix 7.1 Experimental Settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Neural Machine Translation</head><p>For neural machine translation tasks, we re-implement Transformer with the released code of Fairseq <ref type="bibr">[Ott et al., 2019] 8</ref> . IWSLT 2015 English-Vietmanese Translation It contains 133K training sentence pairs provided by the IWSLT 2015 Evaluation Campaign. Following the pre-processing steps in the work of <ref type="bibr" target="#b19">Raffel et al. [2017]</ref>, we use TED tst2012 (1,553 sentences) as the validation set and TED tst2013 (1,268 sentences) as the test set. BPE is used to get input and output vocabularies. The English and Vietnamese vocabulary sizes are 7,669 and 6,669 respectively. The dropout rate is 0.1. The learning rate is 0.001. The training batch size is 4,096 tokens. The number of warmup steps is 8K. We use "transformer_wmt_en_de" as our basic model. The setting of PreNorm is adopted. We use optimizer Adam with β 1 = 0.9 and β 2 = 0.98. For AdaNorm, the hyper-parameter C is set to 1. We average the last 10 checkpoints for evaluation and set the beam size to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Language Modeling</head><p>Enwiki-8 9 This is a character-level language model dataset with 100M bytes. We use the same preprocessed dataset as in the work <ref type="bibr" target="#b6">[Chung et al., 2017]</ref>. We use the code provided by Transformer-XL 10 . We use the default hyper-parameters in the code. The model contains 12 decoder layers and the dimension of each layer is 512. Multi-head attention contains 8 heads and the dimension of each head is 64. The dropout rate is 0.1. The batch size is 22. We use optimizer Adam with a learning rate 0.00025. For AdaNorm, the hyper-parameter C is set to 1. We choose the best checkpoint on the validation set to evaluate the result on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Classification</head><p>RT The rating inference dataset <ref type="bibr" target="#b17">[Pang and Lee, 2005</ref>] is a binary sentiment classification dataset from online movie reviews. Due to the lack of the standard split, we randomly divide all examples into 8,608 for training, 964 for validation, and 1,089 for testing. We implement a 4-layer Transformer encoder. The setting of PreNorm is adopted. The batch size is 4,096 tokens. The word embedding dimension is 128, the hidden dimension is 128. The dropout rate is 0.2. The optimization method is Adam optimizer with β 1 = 0.9, β 2 = 0.998. For AdaNorm, the hyper-parameter C is set to 0.3.</p><p>SST The Stanford sentiment treebank <ref type="bibr" target="#b22">[Socher et al., 2013</ref>] is a single-sentence classification dataset built on movie reviews. We run experiments on a five label set. It provides the standard spit, with 8,544 for training, 1,101 for validation, and 2,210 for testing. We use the same model structure in RT. For AdaNorm, the hyper-parameter C is set to 0.3. The rest of parameters are set exactly the same as in RT settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST Image Recognition</head><p>The MNIST handwritten digit dataset <ref type="bibr" target="#b13">[LeCun et al., 1998</ref>] consists of 55,000 training images, 5,000 validation images, and additional 10,000 testing images. This task aims to recognize the numerical digit (0-9) of each image. We implement a CNN based classifier. The first 2D-convolution layer has 1 in-channel, 20 out-channels. The second 2D-convolution layer has 20 in-channels, 50 out-channels. We flatten the output of the second 2D-convolution layer and send it to a linear layer. The batch size is 32. We use Adam optimizer with a learning rate of 0.001. We apply LayerNorm before activation in every linear layer. When applying AdaNorm, we set hyper-parameter C to 2. We train the model for 20 epochs. We choose the best checkpoint on the validation set for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4">Dependency Parsing</head><p>Transition-based Dependency Parsing Following previous work, we use English Penn TreeBank (PTB) <ref type="bibr" target="#b15">[Marcus et al., 1993]</ref> for experiments. We follow the standard split of the corpus with sections 2-21 as the training set (39,832 sentences, 1,900,056 transition examples), section 22 as the validation set (1,700 sentences, 80,234 transition examples), and section 23 as the testing set (2,416 sentences, 113,368 transition examples). We implement a MLP-based parser following the work <ref type="bibr" target="#b5">[Chen and Manning, 2014]</ref>. The dimension of the hidden state is 512, the batch size is 1, 024, the dropout rate is 0.2. We use optimizer Adam and initialize the learning rate to 0.001. We apply LayerNorm before activation in every linear layer. When applying AdaNorm, we set hyper-parameter C to 1. We train 20 epochs on the training set. We evaluate the model on the development set every epoch and find the best checkpoint to evaluate the test results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Proof of Theorem 1</head><p>Proof. Define 1 H = (1, 1, · · · , 1) T . It is easy to verify</p><formula xml:id="formula_12">y T y = H i=1 y 2 i = H i=1 ( x i − µ σ i ) 2 = H 1 T H 1 H = H i=1 1 2 = H y T 1 H = H i=1 y i = H i=1 x i − µ σ i = 0<label>(10)</label></formula><p>The forward propagation</p><formula xml:id="formula_13">y = x − µ1 H σ<label>(11)</label></formula><p>Calculating the gradient in backward propagation</p><formula xml:id="formula_14">∂y ∂x = ∂ (Ix − µ1 H )/σ ∂x = 1 σ I T = 1 σ I ∂y ∂µ = ∂ (Ix − µ1 H )/σ ∂µ = − 1 σ 1 T H ∂y ∂σ = ∂ (Ix − µ1 H )/σ ∂σ = − (Ix − µ1 H ) T σ 2 = − 1 σ y T ∂µ ∂x i = H i=1 x i /H ∂x i = 1 H =⇒ ∂µ ∂x = 1 H H ∂σ ∂x i = ∂ ( H i=1 x 2 i − Hµ 2 )/H ∂x i = 1 2σ ∂(σ 2 ) ∂x i = 1 2σ ∂ ( H i=1</formula><p>x 2 i − Hµ 2 )/H </p><formula xml:id="formula_15">a i = H i=1 g i σ = Hḡ σ HD a = H i=1 (a i −ā) 2 = H i=1 ( g i −ḡ σ ) 2 = HD g σ 2<label>(16)</label></formula><p>To conclude,ā =ḡ/σ and D a = D g /(σ 2 ).</p><p>Therefore,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Convergence curves of LayerNorm and LayerNorm-simple on En-Vi, Enwiki8. Lower is better. The bias and gain increase the risk of over-fitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Convergence curves of LayerNorm-simple and DetachNorm on two translation datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Loss curves of LayerNorm and AdaNorm on En-Vi, PTB, and De-En.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The bias and gain do not work on six out of eight datasets. "w/o Norm" is a naive model without LayerNorm. "LayerNorm-simple" is a variant of LayerNorm that drops the bias and gain. "(+)" means higher is better. "(-)" means lower is better.</figDesc><table><row><cell>Models</cell><cell cols="3">Machine Translation</cell><cell>Language Modeling</cell><cell></cell><cell cols="2">Classification</cell><cell>Parsing</cell></row><row><cell></cell><cell cols="3">En-De(+) De-En(+) En-Vi(+)</cell><cell>Enwiki8(-)</cell><cell cols="4">RT(+) SST5(+) MNIST(+) PTB(+)</cell></row><row><cell>Model Layers</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>3</cell></row><row><cell>w/o Norm</cell><cell>Diverge</cell><cell>34.0</cell><cell>28.4</cell><cell>1.04</cell><cell cols="2">76.85 38.55</cell><cell>99.14</cell><cell>88.31</cell></row><row><cell>LayerNorm</cell><cell>28.3</cell><cell>35.5</cell><cell>31.2</cell><cell>1.07</cell><cell cols="2">77.21 39.23</cell><cell>99.13</cell><cell>89.12</cell></row><row><cell>LayerNorm-simple</cell><cell>28.4</cell><cell>35.5</cell><cell>31.6</cell><cell>1.07</cell><cell cols="2">76.66 40.54</cell><cell>99.09</cell><cell>89.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The derivatives of the mean and variance matter. "w/o Norm" is the naive model without normalization. "DetachNorm" is a variant of "LayerNorm-simple". It detaches the derivatives of the mean and variance. "(+)" means higher is better. "(-)" means lower is better. Thetop table shows the effect of forward normalization. The bottom table shows the effect of the derivatives of the mean and variance.</figDesc><table><row><cell>Models</cell><cell cols="3">Machine Translation</cell><cell>Language Modeling</cell><cell></cell><cell cols="2">Classification</cell><cell>Parsing</cell></row><row><cell></cell><cell cols="3">En-De De-En(+) En-Vi(+)</cell><cell>Enwiki8(-)</cell><cell cols="4">RT(+) SST5(+) MNIST(+) PTB(+)</cell></row><row><cell>Model Layers</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>3</cell></row><row><cell>w/o Norm</cell><cell>Diverge</cell><cell>34.0</cell><cell>28.4</cell><cell>1.04</cell><cell cols="2">76.85 38.55</cell><cell>99.14</cell><cell>88.31</cell></row><row><cell>DetachNorm</cell><cell>Diverge</cell><cell>33.9</cell><cell>27.7</cell><cell>1.12</cell><cell cols="2">76.40 40.04</cell><cell>99.10</cell><cell>89.79</cell></row><row><cell>Improvement</cell><cell>-</cell><cell>-0.1</cell><cell>-0.7</cell><cell>-0.08</cell><cell>-0.45</cell><cell>1.49</cell><cell>-0.04</cell><cell>1.48</cell></row><row><cell>Models</cell><cell cols="3">Machine Translation</cell><cell>Language Modeling</cell><cell></cell><cell cols="2">Classification</cell><cell>Parsing</cell></row><row><cell></cell><cell cols="3">En-De De-En(+) En-Vi(+)</cell><cell>Enwiki8(-)</cell><cell cols="4">RT(+) SST5(+) MNIST(+) PTB(+)</cell></row><row><cell>Model Layers</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>12</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>3</cell></row><row><cell>DetachNorm</cell><cell>Diverge</cell><cell>33.9</cell><cell>27.7</cell><cell>1.12</cell><cell cols="2">76.40 40.04</cell><cell>99.10</cell><cell>89.79</cell></row><row><cell cols="2">LayerNorm-simple 28.4</cell><cell>35.5</cell><cell>31.6</cell><cell>1.07</cell><cell cols="2">76.66 40.54</cell><cell>99.09</cell><cell>89.19</cell></row><row><cell>Improvement</cell><cell>-</cell><cell>1.6</cell><cell>3.9</cell><cell>0.05</cell><cell>0.26</cell><cell>0.50</cell><cell>-0.01</cell><cell>-0.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of LayerNorm and AdaNorm. "(+)" means higher is better. "(-)" means lower is better. AdaNorm outperforms LayerNorm on seven datasets.</figDesc><table><row><cell>Models</cell><cell cols="3">Machine Translation</cell><cell>Language Model</cell><cell cols="2">Classification</cell><cell>Parsing</cell></row><row><cell></cell><cell cols="3">En-De(+) De-En(+) En-Vi(+)</cell><cell>Enwiki8(-)</cell><cell cols="3">RT(+) SST5(+) MNIST(+) PTB(+)</cell></row><row><cell>w/o Norm</cell><cell>Diverge</cell><cell>34.0</cell><cell>28.4</cell><cell>1.04</cell><cell>76.85 38.55</cell><cell>99.14</cell><cell>88.31</cell></row><row><cell>LayerNorm</cell><cell>28.3</cell><cell>35.5</cell><cell>31.2</cell><cell>1.07</cell><cell>77.21 39.23</cell><cell>99.13</cell><cell>89.12</cell></row><row><cell>LayerNorm-simple</cell><cell>28.4</cell><cell>35.5</cell><cell>31.6</cell><cell>1.07</cell><cell>76.66 40.54</cell><cell>99.09</cell><cell>89.19</cell></row><row><cell>AdaNorm</cell><cell>28.5</cell><cell>35.6</cell><cell>31.4</cell><cell>1.07</cell><cell>77.50 40.54</cell><cell>99.35</cell><cell>89.23</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/pytorch/fairseq 3 http://mattmahoney.net/dc/text.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In our implementation, we detach the derivative of standard deviation, the square root of variance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">When calculating the gradient, we adopt the denominator layout.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Our code is released at https://github.com/lancopku/AdaNorm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For AdaNorm implementation, Kaiming initialization and the setting of prenorm are recommended.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/pytorch/fairseq 9 http://www.mattmahoney.net/dc/text.html 10 https://github.com/kimiyoung/transformer-xl</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all reviewers for providing the thoughtful and constructive suggestions. This work was supported in part by National Natural Science Foundation of China (No. 61673028).</p><p>IWSLT 2014 German-English Translation It is provided by the IWSLT 2014 Evaluation Campaign. We use the same dataset splits following previous work <ref type="bibr" target="#b16">[Ott et al., 2019</ref><ref type="bibr" target="#b20">, Ranzato et al., 2016</ref><ref type="bibr" target="#b26">, Wiseman and Rush, 2016</ref>. It contains 153K sentences for training, 7K sentences for validation, and 7K sentences for testing. BPE is used to get vocabularies. We use the shared embedding setting and the vocabulary size is 10,149. We use "transformer_iwslt_de_en" as our basic model. The setting of PreNorm is adopted. The dropout rate is 0.3. The attention dropout rate is 0.1. The activation dropout is 0.1. The initialization learning rate is 1e-07 and the learning rate is 0.0015. The training batch size is 4,096 tokens. We use optimizer Adam with β 1 = 0.9 and β 2 = 0.98. We update the gradients for every 2 steps. The number of warmup steps is 8K. For AdaNorm, the hyper-parameter C is set to 2. We average the last 10 checkpoints for evaluation and set the beam size to 5.</p><p>WMT English-German Translation Following previous work <ref type="bibr" target="#b25">[Vaswani et al., 2017]</ref>, we use the same dataset splits and the same compound splitting. The pre-processing code is provided by Fairseq. BPE is used to get vocabularies. We use the shared embedding setting and the vocabulary size is 32,765. We use "transformer_wmt_en_de_big_t2t" as our basic model. The setting of PreNorm is adopted. The dropout rate is 0.3. The learning rate is 0.001. The training batch size is 4,096 tokens. We use optimizer Adam with β 1 = 0.9 and β 2 = 0.98. The number of warmup steps is 4K. For AdaNorm, the hyper-parameter C is set to 2. We average the last 10 checkpoints for evaluation and set the beam size to 4.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of (1)</head><p>(1) In standard layernorm, we do not detach the gradients of µ and σ, in backward propagation </p><p>For any vector u vertical to 1 H and y ( 1 H is vertical to y), we have</p><p>We expand 1 H and y to a standard orthogonal basis</p><p>Therefore,</p><p>Proof of (2) (2) If we detach the gradients of µ, in backward propagation</p><p>Therefore,</p><p>Consider</p><p>Therefore,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of (3)</head><p>(3) If we detach the gradient of σ, in backward propagation</p><p>For any vector u vertical to 1 H</p><p>To concluded = 0 and D d = D a = D g /(σ 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Proof of Theorem 2</head><p>Proof. Assume dy = (dy 1 , dy 2 , ..., dy H ).</p><p>Because φ is derivable, asuume v = (φ (y 1 ), φ (y 2 ), ..., φ (y H )). It is easy to verify</p><p>Differential on both sides of following three equations</p><p>In H-dim Euclidean space, note that y T 1 H = H i=1 y i = 0, namely 1 H and y are vertical. We expand 1 H and y to an orthogonal basis u 1 = 1 H , u 2 = y, u 3 , ..., u H .</p><p>Accoring to Eq. 37, H i=3 α i β i u i 2 = 0. Because it holds in spite of α i , (i &gt; 2), β i = 0, (i &gt; 2).</p><p>If we set y 1 = 2 H/6, y 2 = y 3 = − H/6, y i = 0 (i &gt; 3),</p><p>Therefore |C 3 | &lt; |C2|+M √ H/6 holds for any H, when H approches infinity, we have |C 3 | = 0.</p><p>y i ) = C 1 H = CH, therefore C 1 = C. Let k = −C 2 /C, therefore φ(y i ) = C(1 − ky i ), then </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<idno>abs/1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="7705" to="7716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The iwslt 2015 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT 2014, International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The iwslt 2015 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT 2015, International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<title level="m">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online and linear-time attention by enforcing monotonic alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="2837" to="2846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fixup initialization: Residual learning without normalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
