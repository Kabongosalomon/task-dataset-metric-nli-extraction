<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learning of Entities and Documents from Knowledge Base Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">RIKEN AIP</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">RIKEN AIP</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Learning of Entities and Documents from Knowledge Base Descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe TextEnt, a neural network model that learns distributed representations of entities and documents directly from a knowledge base (KB). Given a document in a KB consisting of words and entity annotations, we train our model to predict the entity that the document describes and map the document and its target entity close to each other in a continuous vector space. Our model is trained using a large number of documents extracted from Wikipedia. The performance of the proposed model is evaluated using two tasks, namely fine-grained entity typing and multiclass text classification. The results demonstrate that our model achieves stateof-the-art performance on both tasks. The code and the trained representations are made available online for further academic research.</p><p>This work is licensed under a Creative Commons Attribution 4.0 International License. License details:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of learning distributed representations (or embeddings) from a knowledge base (KB) has recently attracted considerable attention. These representations enable us to use the large-scale, humanedited information of a KB in machine learning models, and can be applied in various natural language tasks such as entity linking <ref type="bibr" target="#b8">(Hu et al., 2015;</ref><ref type="bibr" target="#b32">Yamada et al., 2016;</ref><ref type="bibr" target="#b33">Yamada et al., 2017)</ref>, entity search <ref type="bibr" target="#b8">(Hu et al., 2015)</ref>, and link prediction <ref type="bibr" target="#b0">(Bordes et al., 2013;</ref><ref type="bibr" target="#b27">Wang et al., 2014)</ref>.</p><p>In this paper, we describe TextEnt, a simple neural network model that learns distributed representations of entities and documents from a KB. Specifically, given a document in a KB consisting of words and contextual entities (i.e., entities referred from entity annotations in the document), our model predicts the target entity explained by the document (see <ref type="figure" target="#fig_0">Figure 1)</ref>, and maps the document and its target entity close to each other in a continuous vector space. Here, words, contextual entities, and target entities are mapped into continuous vectors that are updated throughout the training. In this study, we train the model using documents retrieved from Wikipedia.</p><p>One key characteristic of our model is that it enables us to combine the semantic signals obtained from both words and entities in a straightforward manner. The main motivation for using entities in addition to words is to address the problems of ambiguity (i.e., the same words or phrases may have different meanings) and variety (i.e., the same meaning may be expressed using different words or phrases) in natural language. For example, the word Washington is ambiguous because it can refer to a US state, or the capital city of the US, or the first US president George Washington, and so on. Further, New York is sometimes referred to as NY or by its nickname, the Big Apple. Obviously, entities do not have these problems, because they are uniquely identified in the KB.</p><p>To evaluate our model, we address two important tasks using the proposed representations. Firstly, we consider a fine-grained entity typing task <ref type="bibr">(Yaghoobzadeh and Schutze, 2015)</ref> to evaluate the quality of the learned entity representations. In this task, the aim is to infer one or more types of each entity (e.g., athlete, airport, sports team) from a predefined type set. We perform this task using the simple multilayer perceptron (MLP) classifier with the learned entity representations as features. The results show that our method outperforms the state-of-the-art methods by a wide margin.</p><p>Saturn is the sixth planet from the Sun and the second-largest in the Solar System, after Jupiter. It is a gas giant with an average radius about nine times that of Earth. <ref type="bibr">Planet, Sun, Solar System, Jupiter, Gas giant, Earth</ref> Target entity: Saturn Secondly, we consider a multiclass text classification task, which aims to classify documents into a set of predefined classes. This task examines the ability of our model as a generic encoder of arbitrary documents. One important approach adopted here is that we automatically annotate entities appearing in the target documents using a publicly available entity linking system and encode the documents to the document representations in the same manner as the documents in the KB. For this task, the logistic regression classifier is applied to the document representations. Because of the quality of semantic signals obtained from the entities, our method outperforms strong state-of-the-art methods on two popular datasets (i.e., the 20 newsgroups dataset <ref type="bibr" target="#b13">(Lang, 1995)</ref> and R8 dataset <ref type="bibr" target="#b1">(Debole and Sebastiani, 2005)</ref>). To facilitate further research, our code and the trained representations are available online at https://github.com/studio-ousia/textent/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual entities:</head><p>Our contributions can be summarized as follows:</p><p>• We propose TextEnt, a simple neural network model that learns distributed representations of entities and documents from a KB. Given a document in a KB consisting of words and contextual entities, our model learns the representations by predicting the target entity explained by the document (see <ref type="figure" target="#fig_0">Figure 1</ref>). We train our model using large-scale documents extracted from Wikipedia.</p><p>• Our proposed model allows us to effectively combine the semantic signals retrieved from both words and entities in a straightforward manner. We demonstrate the effectiveness of this feature by addressing two important tasks: fine-grained entity typing and text classification. Despite the simplicity of our approach, we achieve state-of-the-art results in both tasks.</p><p>• We have published our code and the trained representations online to facilitate further academic research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Method</head><p>In this section, we describe our approach of learning distributed representations of entities and documents from a KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>Given a document D in a KB consisting of a set of words w 1 , ..., w N and a set of contextual entities e 1 , ..., e K , we train our model to predict the target entity that the document is explaining. We first derive two vector representations of document D: the word-based representation v Dw and the contextual entity-based representation v De . For simplicity, we compute v Dw and v De by averaging the vector representations of words and those of contextual entities, respectively.</p><formula xml:id="formula_0">v Dw = 1 N N n=1 a wn , v De = 1 K K n=1 b en ,<label>(1)</label></formula><p>where a w ∈ R d and b e ∈ R d are the vector representations of words and contextual entities, respectively. We define a probability that represents the likelihood of entity e t being the target entity of document D as the following softmax function: where E KB is a set of all entities in the KB, c e ∈ R d denotes the vector representation of target entity e, and v D ∈ R d is the vector representation of document D.</p><formula xml:id="formula_1">P (e t |D) = exp(c et v D ) e ∈E KB exp(c e v D ) ,<label>(2)</label></formula><p>Here, v D is computed using a fully connected hidden layer with v Dw and v De as inputs:</p><formula xml:id="formula_2">v D = W[v Dw , v De ]<label>(3)</label></formula><p>where W ∈ R d×2d is a weight matrix, and [v i , v j ] is the concatenation of v i and v j . This layer projects the input vector ([v Dw , v De ]) down to d dimensions, and captures the interactions between v Dw and v De . We use the categorical cross-entropy loss to train the model:</p><formula xml:id="formula_3">L = − (D,et)∈Γ log P (e t |D),<label>(4)</label></formula><p>where Γ represents a set of pairs consisting of a document D and its target entity e t in the KB. When training our model, the denominator in Eq.</p><p>(2) is computationally expensive because it involves summation over all KB entities. To address this, we use negative sampling <ref type="bibr" target="#b23">(Mikolov et al., 2013b)</ref>; specifically, we replace E KB in Eq. (2) with a set consisting of the target entity e t and k randomly chosen negative entities. Furthermore, to avoid overfitting, we use word dropout <ref type="bibr" target="#b9">(Iyyer et al., 2015)</ref>, which randomly excludes words and contextual entities with a probability p during the training.</p><p>We also test models trained using only words (denoted by TextEnt-word) and only contextual entities (denoted by TextEnt-entity) in our experiments. These variants are created by replacing v D in Eq.</p><p>(2) with v Dw (TextEnt-word) and v De (TextEnt-entity). Hereafter, our original model is referred to as TextEnt-full.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>We trained our model using documents obtained from the April 2016 version of the DBpedia NIF abstract dataset 1 , which contains the texts and entity annotations in the first introductory sections of Wikipedia articles.</p><p>For computational efficiency, we limited the size of our dataset. In particular, we excluded documents with fewer than five incoming links from other documents if the corresponding entity of the document is not contained in the dataset used in our fine-grained entity typing experiments, presented in Section 3.1. As a result, the number of target documents was 702,388.</p><p>We also modified all words to lowercase, and excluded words that make fewer than five appearances and contextual entities that make fewer than three appearances in the documents. Thus, the final dataset contained 242,771 unique words and 327,263 unique contextual entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameters</head><p>The parameters to be trained in our model are the weight matrix W in the fully connected layer and the vector representations of the words, contextual entities, and target entities. The weight matrix was initialized at random and the vector representations were initialized using pre-trained representations. The pre-trained representations of words and entities were learned jointly using the skip-gram model <ref type="bibr" target="#b22">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013b)</ref> with negative sampling 2 . The corpus was automatically generated by replacing the name of each entity annotation in the Wikipedia documents with a unique identifier of the entity corresponding to that annotation. Note that we used the same pre-trained entity representations to initialize the representations of the contextual entities and the target entities. Additionally, we used all Wikipedia documents obtained from the July 2016 version of Wikipedia dump 3 to build the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation Details</head><p>The proposed model was implemented using PyTorch 4 and trained with mini-batch stochastic gradient descent (SGD). The mini-batch size was fixed at 100 and the learning rate was automatically controlled by Adadelta <ref type="bibr" target="#b34">(Zeiler, 2012)</ref>. We trained the model by iterating over the documents in the KB in random order for 50 epochs 5 . For computational efficiency, we used only the first 2,000 words and first 300 entities in the documents. The training took approximately 25 h on an NVIDIA GTX 1080 Ti GPU. Regarding the other hyper-parameters, the representations were set to have d = 300 dimensions, the size of the negative entities was k = 100, and the dropout probability was set to p = 0.5, as recommended in <ref type="bibr" target="#b26">Srivastava et al. (2014)</ref> 3 Experiments To evaluate the models described in the previous section, we conducted fine-grained entity typing and text classification tasks using the learned representations. A description of each task is given in the following subsections. Finally, we qualitatively analyze the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-grained Entity Typing</head><p>This section describes the task of fine-grained entity typing (Yaghoobzadeh and Schutze, 2015; Neelakantan and Chang, 2015; Yaghoobzadeh and Schütze, 2017) using the entity representations learned by our proposed models. The aim of this task is to assign each entity with one or more fine-grained types such as musician and film. Because an entity typing model is capable of predicting the entity types that are missing from the KB, this can be seen as a knowledge base completion problem. The task is important because entity type information is often missing from KBs, but is known to be beneficial for various downstream natural language tasks such as entity linking <ref type="bibr" target="#b19">(Ling et al., 2015)</ref>, coreference resolution <ref type="bibr" target="#b5">(Hajishirzi et al., 2013)</ref>, and semantic parsing .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Our experimental setup follows that of <ref type="bibr">Yaghoobzadeh and Schutze (2015)</ref>. In particular, we use their entity dataset of 201,933 Freebase 6 entities mapped to 102 entity types based on the FIGER type set <ref type="bibr" target="#b18">(Ling and Weld, 2012)</ref>. The dataset consists of a training set (50%), development set (20%), and test set (30%). Because the dataset is constructed based on Freebase, we preprocessed the data by mapping each entity to the corresponding entry in Wikipedia and excluded those entities that did not exist in Wikipedia. 7 As a result, we successfully mapped approximately 92% of the entities to Wikipedia, and obtained training, development, and test sets containing 93,350, 37,036, and 55,715 entities, respectively. We publicized the dataset and the code used to generate the dataset at https://github.com/studio-ousia/ textent/.</p><p>Following Yaghoobzadeh and Schutze <ref type="formula" target="#formula_0">(2015)</ref>, we evaluated the models using ranking and classification measures. The ranking measures test how well a model ranks entity types. In particular, we ranked the entity types based on the probabilities assigned by the model and evaluated the ranked list using the precision at 1 (P@1) and breakeven point (BEP) 8 .</p><p>The classification measures evaluate the quality of the thresholded assignment decisions of a model. The assignment decisions are based on thresholding the probability assigned to each type. The threshold is selected per type by maximizing the F1 score of entities assigned to the type in the development set. We used the accuracy (an entity is correct if all its types and no incorrect types are assigned to it), microaverage F1 (F1 score of all type-entity assignment decisions), and macro-average F1 (F1 score of types assigned to an entity, averaged over entities). These ranking and classification measures are exactly the same as those used in Yaghoobzadeh and Schutze <ref type="formula" target="#formula_0">(2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We used an MLP classifier with the entity representations as inputs to predict the probability of entity e being a member of type t in the set of possible types T . In particular, we used an MLP with a single hidden layer and the tanh activation function, and an output layer that contains, for each possible type t ∈ T , a logistic regression classifier that predicts the probability of t:</p><formula xml:id="formula_4">P (t 1 |e), ..., P (t |T | |e) = σ W o tanh (W h c e ) ,<label>(5)</label></formula><p>where c e ∈ R d is the vector representation of entity e, σ is the sigmoid function, and W h ∈ R h×d and W o ∈ R |T |×h are the weight matrices corresponding to the hidden layer and the output layer, respectively. The model was trained to minimize the binary cross-entropy loss summed over all entities and types: − e t y e,t log p e,t + (1 − y e,t ) log(1 − p e,t ) ,</p><p>where y e,t ∈ {0, 1} and p e,t denote the ground-truth label and predicted probability, respectively, of entity e being type t. The parameters in W h and W o are updated in the training stage. Note that the model described here is equivalent to that proposed in Yaghoobzadeh and Schutze (2015). The model was trained using mini-batch SGD, with the learning rate controlled by Adam <ref type="bibr" target="#b11">(Kingma and Ba, 2014)</ref> and the mini-batch size set to 32. The model was trained using the training set and evaluated using the test set. Following Yaghoobzadeh and Schutze <ref type="formula" target="#formula_0">(2015)</ref>, the number of hidden units was set to 200. We also measured P@1 on the development set to locate the best epoch for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>The performance of our models is compared with that of the following three entity representation models.</p><p>• Figment-GM (Yaghoobzadeh and Schutze, 2015) is based on the skip-gram model <ref type="bibr" target="#b22">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013b)</ref> trained using a large corpus with automatically generated entity annotations (i.e., FACC1 <ref type="bibr" target="#b3">(Gabrilovich et al., 2013)</ref>). In this experiment, we used the entity representations publicized by the authors 9 .</p><p>• Skip-Gram-Wiki is equivalent to Figment-GM, except that Wikipedia is used as the entityannotated corpus. This model is also the same as our pre-trained representations described in Section 2.3.</p><p>• Wikipedia2Vec <ref type="bibr" target="#b32">(Yamada et al., 2016)</ref> extends the skip-gram model to learn entity representations based on the contextual words of link anchors in Wikipedia and the internal link structure of Wikipedia entities. We used the entity representations trained using the code publicized by the authors 10 and the Wikipedia dump used to train the Skip-Gram-Wiki model. <ref type="bibr">11</ref> We used the entity typing method presented above with the entity representations of each baseline model as inputs. Note that, because the Wikipedia2Vec and Skip-Gram-Wiki models were trained using the link anchors in Wikipedia, they do not contain entities that do not appear or are very rare as the link anchor destinations in Wikipedia. To address this, we evaluated these models in the following two settings: (1) using only the entities that exist in the model, and (2) using all entities, including nonexistent ones. In the latter setting, we used the zero vector as the representation of non-existent entities. Similar to the latter setting, the former is not a fair comparison because it is typically more difficult to learn good entity representations of rare entities than those of popular entities (Yaghoobzadeh and Schutze, 2015). <ref type="table">Table 1</ref> compares the results of our models with those of the baseline models. Our TextEnt-full model outperforms the baseline models in all measures. In particular, the TextEnt-full model achieves a strong P@1 score of 93.2%, which clearly shows the effectiveness of our entity typing model for many downstream NLP tasks. Moreover, the TextEnt-full model generally performs better than both the TextEntword and TextEnt-entity models. This demonstrates the effectiveness of combining the semantic signals obtained from words and entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiclass Text Classification</head><p>This section describes the multiclass text classification task, which tests the ability of our proposed representations to encode arbitrary documents. Our key assumption here is that, because our proposed representations are trained to predict the corresponding entity of a given document in the KB, they can also classify non-KB documents into classes that are much more coarse-grained than entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Following Jin et al. <ref type="formula" target="#formula_0">(2016)</ref>, we used two standard text classification datasets: the 20 newsgroups dataset 12 (denoted by 20NG) <ref type="bibr" target="#b13">(Lang, 1995)</ref> and the R8 dataset <ref type="bibr" target="#b1">(Debole and Sebastiani, 2005)</ref>. The 20NG dataset consists of 11,314 training documents and 7,532 test documents retrieved from 20 different newsgroups. The documents are partitioned nearly equally across the classes. The R8 dataset contains documents from the eight most frequent classes of the Reuters-21578 corpus <ref type="bibr" target="#b15">(Lewis, 1992)</ref>, which consists of labeled news articles from the 1987 Reuters newswire. The R8 dataset contains 5,485 documents for training and 2,189 documents for testing. Unlike the 20NG dataset, the R8 dataset is imbalanced; the largest class contains 3,923 documents and the smallest class contains 51 documents. For both datasets, we report the accuracy and macro-average F1 score. Furthermore, the development set was formed by selecting 10% of the documents in the training set at random for both datasets.</p><p>As preprocessing, we lowercased all words and removed words and entities appearing fewer than five times. Furthermore, we automatically annotated entity mentions in the documents using an entity linking system. In particular, we used TAGME 13 <ref type="bibr" target="#b2">(Ferragina and Scaiella, 2010)</ref>, a state-of-the-art entity linking system that is freely available and has been frequently used in recent studies <ref type="bibr" target="#b29">(Xiong et al., 2016;</ref><ref type="bibr" target="#b6">Hasibi et al., 2016)</ref>. However, TAGME returned many irrelevant entity mentions that would act as noise (e.g., I like refers to an entity I Like <ref type="figure">(Keri Hilson song)</ref>). Thus, we excluded mentions having relevance scores 14 of less than 0.05 15 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>For this task, we simply stacked a logistic regression layer onto our TextEnt model to classify documents into the predefined classes. First, we encoded each document (words with entity annotations) and used the resulting document representation (i.e., v D in the TextEnt-full model, v Dw in the TextEnt-word model, and v De in the TextEnt-entity model) as the feature of the logistic regression classifier.</p><p>We trained the classifier using the training set of each dataset, and evaluated the classification performance using the corresponding test set. The classifier was trained using mini-batch SGD, with the learning rate controlled by Adam <ref type="bibr" target="#b11">(Kingma and Ba, 2014)</ref> and the mini-batch size set to 32. The accuracy on the development set of each dataset was used to locate the best epoch for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We adopted the following state-of-the-art models as our baselines. We also used the Wikipedia2Vec and Skip-Gram-Wiki models described in Section 3.1 as baselines. For this experiment, we simply input the representations of words and entities in these models to our text classification model described in the previous section.</p><p>Results <ref type="table" target="#tab_1">Table 2</ref> compares the results of our proposed models with those of the baseline models. We obtained the BoW-SVM and BoE results from Jin et al. <ref type="bibr">(2016)</ref>. Our TextEnt-full model outperforms the state-of-theart models in terms of accuracy and macro F1 score on both the 20NG and R8 datasets. Furthermore, similar to the results of our previous experiment, the TextEnt-full model generally performs better than both the TextEnt-word and TextEnt-entity models. This shows that combining semantic signals obtained from words and entities is also beneficial for text classification tasks. Furthermore, we conducted a detailed comparison of the BoW-SVM model, BoE model, and TextEntfull model using the class-level F1 scores on the 20NG dataset <ref type="table" target="#tab_2">(Table 3</ref>) and the R8 dataset <ref type="table" target="#tab_4">(Table 4)</ref>. On the 20NG dataset, our model achieves the best scores in more than half of the classes and provides comparable performance in the other classes. Moreover, our model achieves strong performance in classes with relatively few documents on the R8 dataset. This is because our model successfully captures the strong semantic signals that can only be obtained from entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20NG R8</head><p>Acc. F1 Acc. F1    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Qualitative Analysis</head><p>To investigate how our model encodes documents and entities into the same continuous vector space, we extracted five example sentences from the 20NG dataset and encoded each sentence into a vector using our model. The closest entities to this vector based on the cosine similarity are presented in <ref type="table" target="#tab_3">Table 5</ref>. We automatically annotated the entity mentions using TAGME 16 , and fed the words and detected entities into the TextEnt-full model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In recent years, various models for computing distributed representations of text (e.g., sentences and documents) have been proposed <ref type="bibr" target="#b14">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b12">Kiros et al., 2015;</ref><ref type="bibr" target="#b28">Wieting et al., 2016;</ref><ref type="bibr" target="#b7">Hill et al., 2016)</ref>. These models typically use large, unstructured corpora for training; however, certain models attempt to learn text representations from structured data. For instance, <ref type="bibr" target="#b7">Hill et al. (2016)</ref> proposed a neural network model that learns text representations from online public dictionaries by predicting each dictionary word from its description. Further, <ref type="bibr" target="#b28">Wieting et al. (2016)</ref> used a large set of paraphrase pairs obtained from the Paraphrase Database <ref type="bibr" target="#b4">(Ganitkevitch et al., 2013)</ref> to learn text representations. A number of recent models have attempted to learn distributed representations of entities from a KB. For example, <ref type="bibr" target="#b8">Hu et al. (2015)</ref> extended the skip-gram model <ref type="bibr" target="#b22">(Mikolov et al., 2013a)</ref> to learn entity representations using the hierarchical structure of the KB, and <ref type="bibr" target="#b16">Li et al. (2016)</ref> modified the model by Hu et al. to learn both the category representations and entity representations using the category information of the KB. Additionally, relational embedding models <ref type="bibr" target="#b0">(Bordes et al., 2013;</ref><ref type="bibr" target="#b27">Wang et al., 2014;</ref><ref type="bibr" target="#b17">Lin et al., 2015)</ref> learn the entity representations for link prediction tasks.</p><p>Furthermore, some models learn the representations of both words and entities from the KB. A simple method reported in the literature <ref type="bibr">(Yaghoobzadeh and Schutze, 2015;</ref><ref type="bibr" target="#b33">Yamada et al., 2017)</ref> is used to derive the pre-trained representations in this study (i.e., preprocessing an entity-annotated corpus by replacing the name of each annotation with the unique identifier of the entity and feeding the corpus into a word embedding model (e.g., skip-gram)). <ref type="bibr" target="#b32">Yamada et al. (2016)</ref> proposed Wikipedia2Vec, which extends this idea by using neighboring entities in the internal link graph of the KB as additional contexts for training the model. Note that we used Wikipedia2Vec as a baseline method in the two experiments conducted in this study. Similarly, in their subsequent work <ref type="bibr" target="#b33">(Yamada et al., 2017)</ref>, they proposed a neural network model that takes entity-annotated text as input and learns word and entity representations by predicting the annotated entities contained in each text. Furthermore, <ref type="bibr" target="#b21">Mancini et al. (2017)</ref> proposed a model that maps words and entities in a lexical dictionary (i.e., BabelNet <ref type="bibr" target="#b24">(Navigli and Ponzetto, 2012)</ref>) to a single vector space by extending the CBOW model. Unlike our proposed model, these models require users to design a composition function (e.g., vector averaging) to model the semantics of a document using words and entities in it. Moreover, we showed that our approach is highly effective for the two important tasks of fine-grained entity typing and multiclass text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we described TextEnt, a simple neural network model that learns distributed representations of entities and documents from large-scale KB descriptions. We evaluated the performance of the proposed model on fine-grained entity typing and text classification tasks, and achieved state-of-the-art results in both cases, which clearly demonstrates the effectiveness of our approach. In future work, we will explore the applicability of our model to broader NLP tasks such as entity search and KB-based question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of a KB document with entity annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of TextEnt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>BoW-SVM is based on a linear support vector machine (SVM) classifier with bag-of-words (BoW) features as inputs. This model outperforms the conventional naive Bayes model (Jin et al., 2016). • BoE (Jin et al., 2016) is an extension of the skip-gram model that learns different word representations per target class. A linear model based on learned word representations was used to classify documents. This model achieves state-of-the-art results on both the 20NG and R8 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of the text classification task.</figDesc><table><row><cell>Class</cell><cell cols="2">SVM BoE TextEnt</cell></row><row><cell>alt.atheism</cell><cell>.699 .712</cell><cell>.783</cell></row><row><cell>comp.graphics</cell><cell>.702 .724</cell><cell>.773</cell></row><row><cell cols="2">comp.os.ms-windows.misc .714 .724</cell><cell>.742</cell></row><row><cell cols="2">comp.sys.ibm.pc.hardware .673 .706</cell><cell>.721</cell></row><row><cell>comp.sys.mac.hardware</cell><cell>.778 .792</cell><cell>.840</cell></row><row><cell>comp.windows.x</cell><cell>.779 .853</cell><cell>.846</cell></row><row><cell>misc.forsale</cell><cell>.846 .852</cell><cell>.829</cell></row><row><cell>rec.autos</cell><cell>.817 .910</cell><cell>.909</cell></row><row><cell>rec.motorcycles</cell><cell>.900 .942</cell><cell>.943</cell></row><row><cell>rec.sport.baseball</cell><cell>.895 .947</cell><cell>.941</cell></row><row><cell>rec.sport.hockey</cell><cell>.935 .967</cell><cell>.960</cell></row><row><cell>sci.crypt</cell><cell>.890 .926</cell><cell>.934</cell></row><row><cell>sci.electronics</cell><cell>.721 .737</cell><cell>.757</cell></row><row><cell>sci.med</cell><cell>.803 .869</cell><cell>.891</cell></row><row><cell>sci.space</cell><cell>.892 .885</cell><cell>.900</cell></row><row><cell>soc.religion.christian</cell><cell>.823 .877</cell><cell>.904</cell></row><row><cell>talk.politics.guns</cell><cell>.781 .833</cell><cell>.810</cell></row><row><cell>talk.politics.mideast</cell><cell>.837 .920</cell><cell>.944</cell></row><row><cell>talk.politics.misc</cell><cell>.699 .687</cell><cell>.678</cell></row><row><cell>talk.religion.misc</cell><cell>.590 .676</cell><cell>.672</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Class-level F1 scores in each class on the 20NG dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>presents the sentences, nearest entities, and their corresponding classes in the 20NG dataset. Our model successfully encodes the sentences into vectors that are close to their relevant entities. For example, all nearest entities of the first sentence "At one time there was speculation that the first spacewalk (Alexei Leonov?) was a staged fake" are strongly related to the historic Soviet space program. Similar results can be observed in the other four examples.</figDesc><table><row><cell>Class</cell><cell cols="3">Count SVM BoE TextEnt</cell></row><row><cell>grain</cell><cell>51</cell><cell>.824 .818</cell><cell>.889</cell></row><row><cell>ship</cell><cell>144</cell><cell>.781 .783</cell><cell>.829</cell></row><row><cell>interest</cell><cell>271</cell><cell>.745 .832</cell><cell>.873</cell></row><row><cell>money-fx</cell><cell>293</cell><cell>.687 .853</cell><cell>.876</cell></row><row><cell>trade</cell><cell>326</cell><cell>.897 .879</cell><cell>.918</cell></row><row><cell>crude</cell><cell>374</cell><cell>.929 .958</cell><cell>.929</cell></row><row><cell>acq</cell><cell cols="2">2,292 .956 .978</cell><cell>.977</cell></row><row><cell>earn</cell><cell cols="2">3,923 .986 .990</cell><cell>.988</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Class-level F1 scores with the number of documents in each class on the R8 dataset.</figDesc><table><row><cell>Class</cell><cell>Sentence</cell><cell>Nearest entities</cell></row><row><cell>sci.space</cell><cell>At one time there was specula-</cell><cell>Sputnik 1 (0.39), Soviet space program</cell></row><row><cell></cell><cell>tion that the first spacewalk (Alexei</cell><cell>(0.38), Soyuz 5 (0.38), Vostok 1 (0.37)</cell></row><row><cell></cell><cell>Leonov?) was a staged fake.</cell><cell></cell></row><row><cell>rec.autos</cell><cell>I prefer a manual to an automatic as</cell><cell>Manual transmission (0.45), Automatic trans-</cell></row><row><cell></cell><cell>it should be.</cell><cell>mission (0.45), Dual-clutch transmission</cell></row><row><cell></cell><cell></cell><cell>(0.43), Semi-automatic transmission (0.41)</cell></row><row><cell>sci.crypt</cell><cell>I change login passwords every cou-</cell><cell>Password (0.49), Login (0.46), Privilege</cell></row><row><cell></cell><cell>ple of months.</cell><cell>(computing) (0.44), Privilege escalation</cell></row><row><cell></cell><cell></cell><cell>(0.43)</cell></row><row><cell>soc.religion.</cell><cell>Which version of the Bible do you</cell><cell>Bible translations (0.37), King James Only</cell></row><row><cell>christian</cell><cell>consider to be the most accurate</cell><cell>movement (0.37), Biblical poetry (0.36), The</cell></row><row><cell></cell><cell>translation?</cell><cell>Living Bible (0.36)</cell></row><row><cell>sci.med</cell><cell>The blood tests have shown that I</cell><cell>Blood (0.38), Introduction to genetics (0.38),</cell></row><row><cell></cell><cell>have a little too much Hemoglobin</cell><cell>Hemoglobin (0.37), Blood transfusion (0.35)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Five example sentences with their top nearest entities using the TextEnt model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://wiki.dbpedia.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We used the skip-gram model implemented in the open-source Gensim library with size = 300, window = 10, negative = 15, min count = 3, and iter = 5. Default values were used for other parameters.3  We obtained the Wikipedia dump from Wikimedia Downloads: https://dumps.wikimedia.org/ 4 http://pytorch.org5  We experimented using 10, 20, 30, and 50 epochs. All numbers achieved similar performance in our experiments. We used the model trained for 50 epochs because it achieved the best P@1 performance in our fine-grained entity typing task. 6 https://developers.google.com/freebase/ 7 We used the wikipedia.en title property contained in the Freebase dump to create the mapping.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">BEP is the F1 score at the point in the ranked list at which the precision and recall have the same value. 9 https://github.com/yyaghoobzadeh/figment</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/wikipedia2vec/wikipedia2vec 11 We trained the representations with dim size = 300, window = 10, negative = 15, min entity count = 3, and iteration = 5. Default values were used for other parameters.12  We used the by-date version of the dataset obtained from http://qwone.com/˜jason/20Newsgroups/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">We used the public Web API service available at https://services.d4science.org/. 14 We used the ρ scores assigned by TAGME.15  Excluding entity mentions using the relevance scores is the recommended practice described in the documentation: https://services.d4science.org/web/tagme/documentation16  We used the same configuration as described in Section 3.2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their careful reading of our manuscript and their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Analysis of the Relative Hardness of Reuters-21578 Subsets: Research Articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franca</forename><surname>Debole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="584" to="596" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Debole and Sebastiani2005</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Scaiella2010</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">FACC1: Freebase Annotation of ClueWeb Corpora, Version 1 (Release date 2013-06-26</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gabrilovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Format version 1, Correction level 0</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ganitkevitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting Entity Linking in Queries for Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hasibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Understand Phrases by Embedding the Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entity Hierarchy Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2015</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag-of-embeddings for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2824" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980v9</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kingma and Ba2014</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skip-Thought Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">NewsWeeder: Learning to Filter Netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning</title>
		<meeting>the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-Grained Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weld2012] Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Design Challenges for Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
	<note>Topical Word Embeddings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embedding Words and Senses Together via Joint Knowledge-Enhanced Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mancini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="100" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Learning Representations</title>
		<meeting>the 2013 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ponzetto2012] Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="515" to="525" />
		</imprint>
	</monogr>
	<note>Neelakantan and Chang2015</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge Graph and Text Jointly Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards Universal Paraphrastic Sentence Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wieting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Learning Representations</title>
		<meeting>the 2016 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bag-of-Entities Representation for Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Corpus-level Fine-grained Entity Typing Using Contextual Information</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="715" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="578" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Texts and Entities from Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="397" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701v1</idno>
		<title level="m">ADADELTA: An Adaptive Learning Rate Method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
