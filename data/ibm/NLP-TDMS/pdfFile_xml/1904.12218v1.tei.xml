<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Kernels: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
							<email>nikolentzos@lix.polytechnique.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">École Polytechnique Palaiseau</orgName>
								<address>
									<postCode>91120</postCode>
									<country>France Giannis Siglidis</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution">UPMC Université</orgName>
								<address>
									<addrLine>Paris 6</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Universités Paris</orgName>
								<address>
									<postCode>75005</postCode>
									<country>France Michalis Vazirgiannis</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">LIX,École Polytechnique Palaiseau</orgName>
								<address>
									<postCode>91120</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Kernels: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. During the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. The goal of this survey is to provide a unifying view of the literature on graph kernels. In particular, we present a comprehensive overview of a wide range of graph kernels. Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, data that can be naturally modeled as graphs has augmented significantly. Such types of data have become ubiquitous in many application domains, ranging from social networks to biology and chemistry. A large portion of the available graph representations corresponds to data derived from social networks. These networks represent the interactions between a set of individuals such as friendships in a social website, and collaborations in a network of film actors or scientists. In chemistry, molecular compounds are traditionally modeled as graphs where vertices represent atoms and edges represent chemical bonds. Biology constitutes another primary source of graph-structured data. Protein-protein interaction networks, metabolic networks, regulatory networks, and phylogenetic networks are all examples of graphs that arise in this domain. Graphs are also well-suited to representing technological networks. For example, the World Wide Web can be modeled as a graph where vertices correspond to webpages and edges to hyperlinks between these webpages. The use of graph representations is not limited to the above application domains. In fact, most complex systems are usually represented as compositions of entities along with their interactions, and can thus be modeled as graphs. Interestingly, graphs are very flexible and rich as a means of data representation. It is not thus surprising that they can also represent data that do not inherently possess an underlying graph structure. For instance, sequential data such as text can be mapped to graph structures <ref type="bibr" target="#b34">(Filippova, 2010)</ref>. From the above, it becomes clear that graphs emerge in many real-world applications, and hence, they deserve no less attention than feature vectors which is the dominant representation in data mining and machine learning.</p><p>The aforementioned abundance of graph-structured data raised requirements for automated methods that can gain useful insights. This often requires applying machine learning techniques to graphs. In chemistry and biology, some experimental methods are very expensive and time-consuming, and machine learning methods can serve as cost-effective alternatives. For example, identifying experimentally the function of a protein with known sequence and structure is a very expensive and tedious process. Therefore, it is often desirable to be able to use computational approaches in order to predict the function of a protein.</p><p>By representing proteins as graphs, the problem can be formulated as a graph classification problem where the function of a newly discovered protein is predicted based on structural similarity to proteins with known function <ref type="bibr" target="#b18">(Borgwardt, Ong, Schönauer, Vishwanathan, Smola, &amp; Kriegel, 2005)</ref>. Besides the need for more efficient methods, there is also a need for automating tasks that were traditionally handled by humans and which involve large amounts of data. For instance, in cybersecurity, humans used to manually inspect code samples to identify if they contain malicious functionality. However, due to the rapid increase of the number of malicious applications in the past years, humans are no longer capable of meeting the demands of this task <ref type="bibr" target="#b119">(Suarez-Tangil, Tapiador, Peris-Lopez, &amp; Ribagorda, 2014)</ref>. Hence, there is a need for methods that can accumulate the knowledge and experience of humans, and that can successfully detect malicious behavior in code samples. It turns out that machine learning approaches are particularly suited to this task since most of the newly discovered malware samples are variations of existing malware. By representing code samples as function call graphs, detecting such variations becomes less problematic. Hence, the problem of detecting malicious software can be formulated as a graph classification problem where unknown code samples are compared against known malware samples and clean code <ref type="bibr" target="#b6">(Anderson, Quist, Neil, Storlie, &amp; Lane, 2011)</ref>. From the above example, it becomes thus clear that performing machine learning tasks on graph-structured data is of critical importance for many applications.</p><p>A central issue for machine learning is modelling and computation of similarity among objects. In the case of graphs, graph kernels have received a lot of attention in recent years, and have been established as the dominant approach for learning on graph-structured data. A graph kernel is a symmetric, positive semidefinite function defined on the space of graphs G. This function can be expressed as an inner product in some Hilbert space. Specifically, given a kernel k, there exists a map φ : G → H into a Hilbert space H such that k(G 1 , G 2 ) = φ(G 1 ), φ(G 2 ) for all G 1 , G 2 ∈ G. Roughly speaking, a graph kernel is a measure of similarity between graphs. Comparing graphs is a fundamental problem which has numerous applications in many disciplines <ref type="bibr" target="#b24">(Conte, Foggia, Sansone, &amp; Vento, 2004)</ref>. However, the problem is far from trivial and requires considerable computational time. Graph kernels tackle the graph comparison problem by trying to both capture as much as possible the semantics inherent in the graph but also to remain computationally efficient. One of the most important factors behind the success of graph kernels is that they allow the large family of kernel methods to work directly on graphs. Therefore, graph kernels can bring to bear several machine learning algorithms to real-world problems on graph-structured data. The field of graph kernels has been intensively developed recently.</p><p>Interestingly, dozens of graph kernels have been proposed in the past 20 years. Some of these kernels have achieved state-of-the-art results on several datasets. This paper is a survey of kernels that operate on graph-structured data. We present a comprehensive study of these approaches. We begin with well-known kernels that established the foundations of the field, and we proceed with more recent kernels that are considered the state-of-the-art for many graph-related machine learning tasks. Besides the detailed description of the kernels, we also provide an extensive experimental evaluation of most of them. As we show in the survey, graph kernels are powerful tools with a wide range of applications. We expect these methods to gain soon more attention in a wealth of applications due to their attractive properties. Importantly, this study aims to assist both practitioners and researchers who are interested in applying machine learning tasks on graphs. Furthermore, it should be of interest to all researchers who deal with the problems of graph similarity and graph comparison. The abundance of applications related to the above problems stresses the value of the survey. We should note that a similar survey reviewing work on graph kernels became very recently available <ref type="bibr" target="#b42">(Ghosh, Das, Goncalves, Quaresma, &amp; Kundu, 2018)</ref>. One may thus ask the question: why another survey within such a short period of time? The answer is that in contrast to the existing survey, this survey is much more thorough and covers a larger number of kernels. Moreover, it presents kernels in a more comprehensive way allowing researchers to identify open problems and areas for further exploration, and practitioners to gain a deeper understanding of kernels so that they can decide which kernel suits best to their needs. This survey also provides a much more meaningful taxonomy of graph kernels. Specifically, kernels are grouped into classes based on different criteria such as the type of data on which they operate, and the design paradigm that they follow. Finally, to the best of our knowledge, we provide the most complete evaluation of a large number of graph kernels. The need for such a wider, and more extensive experimental comparison of graph kernels is very strong since it can provide useful insights into the strengths and weaknesses of the different kernels.</p><p>The rest of this manuscript is organized as follows. In Section 2, we discuss why the use of graphs as a means of object representation is vital and necessary in many domain areas, and we also present the challenges of applying learning algorithms on graphs. In Section 3, we introduce notation and background material that we need for the remainder of the paper, including some fundamental concepts from graph theory and from kernel methods. In Section 4, we discuss the core concepts of graph kernels, and we give an overview of the literature on graph kernels. We begin by describing important kernels that were developed in the early days of the field. We next present kernels that are based on neighborhood aggregation mechanisms. We then describe more recent kernels that do not employ neighborhood aggregation mechanisms. Subsequently, we present kernels that are based on assignment, and methods that can handle continuous node attributes. Finally, we give details about frameworks that work on top of graph kernels and aim to improve their performance. The grouping of the reported studies is designed to make it easier for the reader to follow the analysis of the literature, and to obtain a complete picture of the different graph kernels that have been proposed throughout the years. In Section 5, we present applications of graph kernels in many different domain areas. In Section 6, we experimentally evaluate the performance of many graph kernels on several widely-used graph classification benchmark datasets. Furthermore, we measure the running times of the kernels. Based on the obtained results, we provide guidelines for the successful application of graph kernels in different classification problems. Finally, Section 7 contains the summary of the survey, along with a discussion about future research directions in the field of graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation and Challenges</head><p>In this Section, we present the main reasons that motivate the use of graphs instead of feature vectors as a means of data representation. Furthermore, we describe the problem of learning on graphs which arises in many application domains. We focus on the instance of the problem where every graph is an input example, and highlight its relationship to the graph comparison problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Why Graphs</head><p>Graphs are a powerful and flexible means of representing structured data. The power of graphs stems from the fact that they represent both entities, and the relationships between them. Typically, the vertices of a graph correspond to some entities, and the edges model how these entities interact with each other. It is important to note that several fundamental structures for representing data can be seen as instances of graphs <ref type="bibr" target="#b15">(Borgwardt, 2007)</ref>. This highlights the generality of graphs as a form of representation. For example, a vector can be naturally thought of as a graph where vertices correspond to components of the vector and consecutive components within the vector are joined by an edge. Associative arrays can be modeled as graphs, with keys and values represented as vertices, and directed edges connecting keys to their corresponding values. Strings can also be represented as graphs, with one vertex per character and edges between consecutive characters. Due to the power and the generality of graphs as representational models, in some cases, even data that does not exhibit graph-like structure is mapped to graph representations. A very common example is that of textual data, where graphs are usually employed to model the relationships between sentences or terms <ref type="bibr" target="#b83">(Mihalcea &amp; Tarau, 2004)</ref>.</p><p>In data mining and machine learning, observations traditionally come in the form of vectors. However, vector representations suffer from a series of limitations. Specifically, vectors have limited capability to model complex objects since they are unable to capture relationships that may exist between different entities of an object. Furthermore, all the input objects are usually represented as vectors of the same length, despite their size and complexity. On the other hand, as discussed above, graphs are characterized by increased flexibility which allows them to adequately model a variety of different objects. Graphs model both the entities and the relationships between them. Moreover, they are allowed to vary in the number of vertices and in the number of edges. Therefore, graphs address several of the limitations inherent to vectors. It is thus clear that the need for methods that perform learning tasks on graphs is intense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning on Graphs and Challenges</head><p>Learning on graphs has gained extensive attention in the past years. This is mainly due to the representational power of graphs which has established them as a major structure for modeling data from various disciplines. Hence, it is not surprising that a plethora of learning problems have been defined on graphs. Most of these learning problems focus either on the node level or on the graph level. Node classification belongs to the former set of problems, while graph classification belongs to the latter set of problems. In this survey, we focus exclusively on tasks performed at the graph level. Therefore, all the kernels that are presented correspond to functions between graphs.</p><p>The representation of data is a key issue in the fields of data mining and machine learning. Algorithms can only handle data in a specific representation. Due to the appealing properties of graphs, one would expect that there would be great progress in the development of algorithms that can handle graph-structured data. However, the combinatorial nature of graphs acts as a "barrier" since it is very likely that algorithms that operate directly on graphs will be computationally expensive and will not scale to large datasets. This is why research in these areas focused mainly on algorithms operating on vectors, as vectors possess many desirable mathematical properties and can be dealt with much more efficiently. Hence, it is not surprising that the most popular learning algorithms are designed for data represented as vectors. As a consequence, it has become common practice to represent any type of data as feature vectors. Even in application domains where data is naturally represented as graphs, attempts were made to transform the graphs into feature vectors instead of designing algorithms that operate directly on graphs. Ideally, we would like to have a method capable of transforming graphs to feature vectors without losing their representational power. Unfortunately, such a method does not exist. Directly representing data as vectors is thus suboptimal since vectors fail to preserve the rich topological information encoded in a graph. Hence, it would be much more preferable to devise algorithms that operate directly on graphs.</p><p>The problem of learning on graphs (at the graph level) is directly related to that of graph comparison. The ability to compute meaningful similarity or distance measures is often a prerequisite to perform machine learning tasks. Such similarity and distance measures are at the core of many machine learning algorithms. Examples include the k-nearest neighbor classifier, and algorithms that learn decision functions in proximity spaces <ref type="bibr" target="#b46">(Graepel, Herbrich, Bollmann-Sdorra, &amp; Obermayer, 1999)</ref>. These algorithms are very flexible since they require only a distance or similarity function to be defined as the sole mathematical structure on the set of input objects. Hence, by defining a meaningful distance function d : G × G → R + between graphs, we can immediately use one of the above algorithms to perform tasks such as graph classification and graph clustering. However, it turns out that graph comparison is a very complex problem. Specifically, graphs lack the convenient mathematical context of vector spaces, and many operations on them, though conceptually simple, are either not properly defined or computationally expensive. Perhaps the most striking example of these operations is to determine if two objects are identical. In the case of vectors, it requires comparing all their corresponding components, and it can thus be accomplished in linear time with respect to the size of the vectors. For the analogous operation on graphs, known as graph isomorphism, no polynomial-time algorithm has been discovered so far <ref type="bibr" target="#b37">(Garey &amp; Johnson, 1979)</ref>. In general, the problem of comparing two objects is much less well-defined on graphs compared to vectors. For vectors, distance can be computed efficiently using the universally accepted Euclidean distance metric. Unfortunately, there exists no such metric on graphs. Several fundamental problems in graph theory related to <ref type="bibr">(1,</ref><ref type="bibr">3,</ref><ref type="bibr">0)</ref> (1, 1, 2) (0, 2, 5) (2, 0, 4) (1, 3, 0)</p><p>(1, 1, 2) (0, 2, 5) (2, 0, 4) α γ β β (1, 3, 0)</p><p>(1, 1, 2) (0, 2, 5) (2, 0, 4) graph comparison such as the subgraph isomorphism problem and the maximum common subgraph problem are NP-complete <ref type="bibr" target="#b37">(Garey &amp; Johnson, 1979)</ref>. Furthermore, identifying common parts in two graphs is computationally infeasible. Given a graph consisting of n vertices, there are 2 n possible subsets of nodes. There are hence exponentially many in the size of the graphs pairs of subsets to consider. It becomes thus clear that although graphs offer a very intuitive way of modeling data from diverse sources, their power and flexibility do not come without a price. Due to these limitations, graphs have failed to become the major data structure in computer science.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Before we delve into the details of graph kernels, we outline some fundamental aspects of graph theory and kernel methods. We first introduce basic concepts from graph theory, and define our notation. We also give a short introduction to kernel functions and kernel methods in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions and Notations</head><p>Definition 1 (Graph). A graph is a pair G = (V, E) consisting of a set of vertices (or nodes) V and a set of edges E ⊆ V × V which connect pairs of vertices.</p><p>The size of the graph corresponds to its number of vertices denoted by |V | or n. As regards the number of edges of the graph, we will denote it as |E| or m. An example of a graph is given in <ref type="figure" target="#fig_0">Figure 1</ref> (left). A graph may have labels on its nodes and edges. This is often necessary for capturing the semantics of complex objects. For instance, most graphs derived from chemistry (e. g. molecules) are annotated by categorical labels from a finite set.</p><p>Definition 2 (Labeled Graph). A labeled graph is a graph G = (V, E) endowed with a function : V ∪ E → Σ that assigns labels to the vertices and edges of the graph from a discrete set of labels Σ.</p><p>A graph with labels on its vertices is called node-labeled. Similarly, a graph with labels on edges is called edge-labeled. A graph with labels on both the vertices and edges is called fully-labeled. An example of a node-labeled graph is given in <ref type="figure" target="#fig_0">Figure 1</ref> (center). In many settings, vertex and edge annotations are in the form of vectors. For example, vertices and edges may be annotated with multiple categorical or real-valued properties. These graphs are known as attributed graphs.</p><p>Definition 3 (Attributed Graph). An attributed graph is a graph G = (V, E) endowed with a function f : V ∪ E → R d that assigns real-valued vectors to the vertices and edges of the graph.</p><p>An example of a node-attributed graph is given in <ref type="figure" target="#fig_0">Figure 1</ref> (right). Note that labeled graphs are a special case of attributed graphs. We can represent labeled graphs as attributed graphs if we map the discrete labels to one-hot vector representations. A graph G = (V, E) can be represented by its adjacency matrix A.</p><p>Definition 4 (Adjacency Matrix). Let A ij be the element in the i-th row and j-th column of matrix A. Then, the adjacency matrix A of a graph G = (V, E) can be defined as follows</p><formula xml:id="formula_0">A ij = 1 if {v i , v j } ∈ E, 0 otherwise Matrix A is of dimensionality n × n (or |V | × |V |). The neighborhood N (v i ) of vertex v i is the set of all vertices adjacent to v i . Hence, N (v i ) = {v j : {v i , v j } ∈ E} where {v i , v j } is an edge between vertices v i and v j of V . A concept closely related to the neighborhood of a vertex v i is its degree deg G (v i ).</formula><p>Definition 5 (Degree). Given an undirected graph G = (V, E) and a vertex v i ∈ V , the degree of v i in G is the number of edges incident to v i , and is defined as</p><formula xml:id="formula_1">deg G (v i ) = |{v j : {v i , v j } ∈ E}| = |N (v i )|<label>(1)</label></formula><p>Besides the adjacency matrix A, a graph G = (V, E) can also be represented by its Laplacian matrix L.</p><p>Definition 6 (Laplacian Matrix). Let A be the adjacency matrix of a graph G = (V, E) and D a diagonal matrix with D ii = j A ij . Then, the Laplacian matrix L of a graph G = (V, E) can be defined as follows</p><formula xml:id="formula_2">L = D − A<label>(2)</label></formula><p>Similarly to the adjacency matrix A, the dimensionality of the Laplacian matrix is n×n. A subgraph of a graph G is a graph whose set of vertices and set of edges are both subsets of those of G. Let G ⊆ G denote that G is a subgraph of G.</p><p>Definition 7 (Induced Subgraph). Given a graph G = (V, E) and a subset of vertices S ⊆ V , the subgraph G(S) = (S, E(S)) induced by S consists of the set of vertices S and the set of edges E(S) that have both end-points in S defined as follows</p><formula xml:id="formula_3">E(S) = {{v i , v j } ∈ E : v i , v j ∈ S} (3) List of key symbols G Set of graphs G A graph V Set of vertices E Set of edges n Number of vertices m Number of edges N (v) Neighbors of v degG(v) Degree of v in G G(S) Subgraph of G induced by set of vertices S Nr(v) r-hop neighborhood of v A</formula><p>Adjacency matrix of graph L Laplacian matrix of graph Function that assigns labels to vertices and edges δ</p><p>Diameter of graph f Function that assigns attributes to vertices and edges The degree of a vertex v i ∈ S, deg G(S) (v i ), is equal to the number of vertices that are adjacent to v i in G(S). The density of a graph G is δ(G) = m/ n 2 , the number of edges m over the total possible edges. A graph G with density δ(G) = 1 is called a complete graph. In a complete graph, every pair of distinct vertices are adjacent. A clique is a subset of vertices such that every pair of them are connected by an edge, that is, their induced subgraph is complete.</p><formula xml:id="formula_4">Definition 8 (Walk, Path, Cycle). A walk in a graph G = (V, E) is a sequence of vertices v 1 , v 2 , . . . , v k+1 where v i ∈ V for all 1 ≤ i ≤ k + 1 and {v i , v i+1 } ∈ E for all 1 ≤ i ≤ k.</formula><p>The length of the walk is equal to the number of edges in the sequence, i. e. k in the above case.</p><formula xml:id="formula_5">A walk in which v i = v j ⇔ i = j is called a path. A cycle is a path with {v k+1 , v 1 } ∈ E.</formula><p>Definition 9 (Shortest Path). A shortest path from vertex v i to vertex v j of a graph G is a path from v i to v j such that there exist no other path between these two vertices with smaller length.</p><p>The diameter of a graph G is the length of the longest shortest path between any pair of vertices of G. The neighborhood of radius r (or r-hop neighborhood) of vertex v i is the set of vertices whose shortest path distance from v i is less than or equal to r and is denoted by N r (v i ). <ref type="table" target="#tab_0">Table 1</ref> gives a list of the most commonly used symbols along with their definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Kernel Functions and Kernel Methods</head><p>We next give an introduction to kernel functions and kernel methods.</p><p>Definition 10 (Gram Matrix). Given a set of inputs x 1 , . . . , x N ∈ X and a function k : X × X → R, the N × N matrix K defined as</p><formula xml:id="formula_6">K ij = k(x i , x j )<label>(4)</label></formula><p>is called the gram matrix (or kernel matrix) of k with respect to the inputs x 1 , . . . , x N .</p><p>In what follows, we will refer to gram matrices as kernel matrices.</p><formula xml:id="formula_7">Definition 11 (Positive Semidefinite Matrix). A real N ×N symmetric matrix K satisfying N i=1 N j=1 c i c j K ij ≥ 0<label>(5)</label></formula><p>for all c i ∈ R is called positive semidefinite.</p><p>Definition 12 (Positive Semidefinite Kernel). Let X be a nonempty set. A function k : X × X → R which for all N ∈ N and all x 1 , . . . , x N ∈ X gives rise to a positive semidefinite kernel matrix is called a positive semidefinite kernel, or just a kernel.</p><p>Informally, a kernel function measures the similarity between two objects. Furthermore, kernel functions can be represented as inner products between the vector representations of these objects. Specifically, if we define a kernel k on X × X , then there exists a mapping φ : X → H into a Hilbert space with inner product ·, · , such that:</p><formula xml:id="formula_8">∀x i , x j ∈ X : k(x i , x j ) = φ(x i ), φ(x j )<label>(6)</label></formula><p>A Hilbert space is an inner product space which also possesses the completeness property that every Cauchy sequence of points taken from the space converges to a point in the space. Furthermore, the Hilbert space H has the following property known as the reproducing property:</p><formula xml:id="formula_9">∀f ∈ H, ∀x ∈ X : f (x) = f, k(x, ·)<label>(7)</label></formula><p>By virtue of this property, H is called a reproducing kernel Hilbert space (RKHS) associated with kernel k. It is interesting to note that every kernel function on X × X is associated with an RKHS and vice versa <ref type="bibr" target="#b9">(Aronszajn, 1950)</ref>. Kernel methods are a class of machine learning algorithms which operate on input data after they have been mapped into an implicit feature space using a kernel function. One of the major advantages of kernel methods is that they can operate on very general types of data <ref type="bibr" target="#b106">(Schölkopf &amp; Smola, 2002)</ref>. The input space X does not have to be a vector space, but it can represent any structured domain, such as the space of strings or graphs <ref type="bibr" target="#b38">(Gärtner, 2003)</ref>. Kernel methods can still be applied to such types of data, as long as we can find a mapping φ : X → H, where H is an RKHS. This mapping is not neccasary to be explicitly determined. These methods implicitly represent data in a feature space and compute inner products between them in that space using a kernel function. These inner products can be interpreted as the similarities between the corresponding objects. Machine learning tasks such as classification and clustering can be carried out by using only the inner products computed in that feature space. Kernel methods are very popular and have been successfully used in a wide variety of applications. Here, we need to stress that the optimization problem of several kernel methods such as the Support Vector Machines is convex only if the employed function is positive semidefinite.</p><p>To make these ideas clear, consider a binary classification problem with training set</p><formula xml:id="formula_10">D = {(x i , y i )} N i=1 where (x i , y i ) ∈ X × Y, X is an inner product space (e. g. R d )</formula><p>, and Y = {−1, +1}. As mentioned above, given the training set D, the goal is to learn a function f : X → Y such that the generalization error of f is as low as possible. Hence, we are not only interested in minimizing the training error, but we are also interested in performing accurate predictions/classifications on previously unseen instances.</p><p>Large margin methods such as the Support Vector Machines seek a hyperplane that separates the instances belonging to class −1 from those belonging to class 1. Therefore, f can take the form f (x) = sgn( w, x + b) where sgn(·) takes the value 1 if its argument is positive, −1 otherwise. Then, given an instance x, the decision function f outputs a prediction depending on the location of x with respect to the hyperplane w, x + b = 0. Let us first assume that the training data is linearly separable. In such a case, there exists a hyperplane such that points belonging to different classes are on opposing sides of the hyperplane. In fact, there exist infinitely many of these hyperplanes. Large margin classifiers select among all these hyperplanes the one that maximizes the margin between the two classes of training data, that is, the one whose distance from the nearest data points of the two classes is maximum. To find this optimal hyperplane, classifiers solve a convex quadratic optimization problem. The solution vector w of this problem is a linear combination of the training instances:</p><formula xml:id="formula_11">w = N i=1 α i y i x i<label>(8)</label></formula><p>where α i ∈ R + . Using the above result, which is known as the representer theorem <ref type="bibr" target="#b105">(Schölkopf, Herbrich, &amp; Smola, 2001)</ref>, the linear classifier f can be written as</p><formula xml:id="formula_12">f (x) = sgn( N i=1 α i y i x i , x + b)<label>(9)</label></formula><p>In case that the training data is not linearly separable, we search for the hyperplane that maximizes the margin and at the same time, minimizes a quantity proportional to the number of misclassification errors. Computing this hyperplane can again be formulated as a convex quadratic optimization problem, and the solution vector w is still a linear combination of the training inputs.</p><p>For complex classification problems, there may be no hyperplanes that can separate the positive from the negative instances and provide good classification performance. The answer to this problem, which is commonly known as the kernel trick <ref type="bibr" target="#b4">(Aizerman, 1964;</ref><ref type="bibr" target="#b19">Boser, Guyon, &amp; Vapnik, 1992)</ref>, is to map the inputs into a (usually higher-dimensional) feature space H, and to find the separating hyperplane in that space. Let φ : X → H be a mapping from the input space X to a feature space H, equipped with an inner product. To compute the optimal hyperplane in the feature space we can use our previous formulation, simply by replacing x i , x with φ(x i ), φ(x) . Let k : X × X → R be a kernel function with the following property:</p><formula xml:id="formula_13">k(x i , x j ) = φ(x i ), φ(x j )<label>(10)</label></formula><p>The decision function f can now be written as</p><formula xml:id="formula_14">f (x) = sgn( N i=1 α i y i φ(x i ), φ(x) + b) = sgn( N i=1 α i y i k(x i , x) + b)<label>(11)</label></formula><p>From the above analysis, it is clear that by defining a kernel function k, we implicitly map all data points into a feature space H. Kernel methods can then perform machine learning tasks such as classification without ever explicitly performing the mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Kernels</head><p>In this Section, we give an overview of the graph kernel literature. Our study is not exhaustive, however, we have tried to cover the most representative approaches that have appeared in the literature of graph kernels., We first present some fundamental aspects of graph kernels, and we then proceed by discussing the details of several graph kernel instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Kernels between Graphs</head><p>Kernels on graphs can be divided into two categories: (1) those that compare nodes in a graph, and (2) those that compare graphs. As mentioned above, in this survey, we focus on the second category, that is, kernels between graphs and thus we exclusively use the term graph kernel for describing such kernel functions. As regards the first category, we refer the interested reader to the work of <ref type="bibr" target="#b64">Kondor and Lafferty (2002)</ref> which was later extended by <ref type="bibr" target="#b115">Smola and Kondor (2003)</ref>. Graph kernels have recently emerged as a promising approach for learning on graph-structured data. These methods exhibit several attractive statistical properties. They combine the representative power of graphs and the discrimination power of kernel-based methods. Hence, they constitute powerful tools for tackling the graph similarity and learning tasks at the same time.</p><p>From the previous Section, it is clear that the application of kernel methods consists of two steps. First, a kernel function is designed, and based on this function the kernel matrix is constructed. Second, a learning algorithm is employed to compute the optimal manifold in the feature space (e. g. a hyperplane in binary classification problems). Since several mature kernel-based classifiers are available in the literature, research on graph kernels has focused on the first step. Hence, the main effort has been devoted to developing expressive and efficient graph kernels capable of accurately measuring the similarity between input graphs. These kernels implicitly (or explicitly sometimes) project graphs into a feature space H as illustrated in <ref type="figure">Figure 2</ref>. As regards the second step, it is common to employ off-the-shelf algorithms such as the Support Vector Machines classifier <ref type="bibr" target="#b19">(Boser et al., 1992)</ref>, and thus, we will not enter into more details here. The interested reader is referred to <ref type="bibr" target="#b106">Schölkopf and Smola (2002)</ref> or to <ref type="bibr" target="#b109">Shawe-Taylor and Cristianini (2004)</ref>.</p><p>Concluding, the main challenge in applying kernel methods to graphs is to define appropriate positive semidefinte kernel functions on the set of input graphs which are able to reliably assess the similarity among them. We next present, for illustration purposes, two very simple kernels that compare node and edge labels of the two involved graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simple Kernels</head><p>The vertex histogram and edge histogram kernels are very simple instances of graph kernels which generate explicit graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Vertex Histogram Kernel</head><p>The vertex histogram kernel is a basic linear kernel on vertex label histograms. The kernel assumes node-labeled graphs. Let G be a collection of graphs, and assume that each of their vertices comes from an abstract vertex space V. Given a set of node labels Σ, : <ref type="figure">Figure 2</ref>: Feature space and map defined by graph kernels. Any kernel on a space of graphs G can be represented as an inner product after graphs are mapped into a Hilbert space H.</p><formula xml:id="formula_15">V → Σ G 1 G 2 G 3 G H φ(G 1 ) φ(G 2 ) φ(G 3 )</formula><p>is a function that assigns labels to the vertices of the graphs. Assume that there are d labels in total, that is d = |Σ|. Furthermore, assume that Σ = {1, . . . , d}. Then, the vertex label histogram of a graph G =</p><formula xml:id="formula_16">(V, E) is a vector f = (f 1 , f 2 , . . . , f d ) , such that f i = |{v ∈ V : (v) = i}| for each i ∈ Σ.</formula><p>Let f, f be the vertex label histograms of two graphs G, G , respectively. The vertex histogram kernel is then defined as the linear kernel between f and f , that is</p><formula xml:id="formula_17">k(G, G ) = f, f<label>(12)</label></formula><p>The complexity of the vertex histogram kernel is linear in the number of vertices of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Edge Histogram Kernel</head><p>The edge histogram kernel is a basic linear kernel on edge label histograms. The kernel assumes edge-labeled graphs. Let G be a collection of graphs, and assume that each of their edges comes from an abstract edge space E. Given a set of node labels Σ, : E → Σ is a function that assigns labels to the edges of the graphs. Assume that there are d labels in total, that is d = |Σ|. Furthermore, assume that Σ = {1, . . . , d}. Then, the edge label histogram of a graph G = (V, E) is a vector f = (f 1 , f 2 , . . . , f d ) , such that</p><formula xml:id="formula_18">f i = |{(v, u) ∈ E : (v, u) = i}| for each i ∈ Σ.</formula><p>Let f, f be the edge label histograms of two graphs G, G , respectively. The edge histogram kernel is then defined as the linear kernel between f and f , that is</p><formula xml:id="formula_19">k(G, G ) = f, f<label>(13)</label></formula><p>The complexity of the edge histogram kernel is linear in the number of edges of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Expressiveness vs Efficiency</head><p>The two kernels defined above are indeed postive semidefinite, but they both correspond to rather naive concepts -as a distribution of values is. A question that may arise at this point is how expressive can graph kernels be in practice. Let us first define the class of kernels which are capable of distinguishing between all (non-isomorphic) graphs in the feature space. Such kernels are called complete. <ref type="bibr" target="#b39">Gärtner, Flach and Wrobel (2003)</ref> showed that computing any complete graph kernel is at least as hard as deciding whether two graphs are isomorphic. The above result, in effect, prohibits the use of complete graph kernels in practical applications. Instead, by using kernels that are not complete, it is not further guaranteed that non-isomorphic graphs will not be mapped into the same point in the feature space. This is a negative result since it implies that to develop expressive kernels, it is necessary to sacrifice some of their efficiency. More recently, <ref type="bibr" target="#b68">Kriege et al. (2018)</ref> showed that several established graph kernels, such as the Weisfeiler-Lehman subtree kernel, cannot distinguish essential graph properties such as connectivity, planarity and bipartiteness. Considering that the Weisfeiler-Lehman subtree kernel achieves state-of-the-art results on most benchmark datasets, this result blurs even more the already vague issue of choosing a graph kernel a practitioner is faced with when dealing with a particular application. In fact, devising a good trade-off between efficiency and effectiveness is an issue of vital importance when designing a graph kernel.</p><formula xml:id="formula_20">Definition 13 (Complete Graph Kernel). A graph kernel k(G i , G j ) = φ(G i ), φ(G j ) is complete if φ is injective.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Taxonomy of Graph Kernels</head><p>There exist many different criteria we can use to divide the various graph kernels into different categories. For instance, graph kernels are traditionally grouped into some major families, each focusing on a different structural aspect of graphs such as random walks, subtrees, cycles, paths, and small subgraphs. Alternatively, graph kernels can be divided into groups according to their ability to handle unlabeled graphs, node-labeled or nodeattributed graphs. Furthermore, graph kernels can be divided into approaches that employ explicit computation schemes and approaches that employ implicit computation schemes <ref type="bibr" target="#b66">(Kriege, Neumann, Kersting, &amp; Mutzel, 2014)</ref>. Graph kernels can also be grouped into categories based on the design paradigm that they follow (i. e. if they are R-convolution, assignment or intersection kernels). Note that groups emerging from different criteria may be related to each other. For instance, graph kernels that can handle node-attributed graphs usually employ implicit computation schemes. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the taxonomy of graph kernels. The devised taxonomy is based on some of the criteria mentioned above. However, in what follows, we do not adopt exclusively any of these criteria. We begin our treatment with approaches that were proposed in the early days of graph kernels, starting from the well-studied random walk kernel till the very popular Weisfeiler-Lehman subtree kernel. We next present some approaches that were inspired from the neighborhood aggregation schmeme of the Weisfeiler-Lehman subtree kernel, and then kernels that do not fall into either of the previous two categories. The subequent subsections are devoted to assignment kernels, and to kernels that can handle continuous node attributes. The final subsections deals with frameworks and approaches that can be applied on top of existing graph kernels. An overview of the graph kernels that are presented in this survey and their properties is given in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Early Days of Graph Kernels</head><p>While early studies on kernel functions and kernel methods focused almost exclusively on input data represented as vectors, it soon became clear that these methods could handle more complex structured objects such as strings, trees and graphs. One of the most popular methods for defining kernels between such objects is to decompose the objects into their "parts", and to compare all pairs of these "parts" by applying existing kernels on them. Kernels constructed using the above framework are called R-convolution kernels <ref type="bibr" target="#b51">(Haussler, 1999)</ref>. Most graph kernels in the literature are instances of the R-convolution framework. These kernels decompose graphs into their substructures and add up the pairwise similarities between these substructures.</p><p>The most intuitive example of an R-convolution kernel is probably a kernel that decomposes each graph into the set of all of its subgraphs, and compares them pairwise. Gärtner,  <ref type="bibr" target="#b39">Flach and Wrobel (2003)</ref> showed that the problem of computing the kernel that compares all the subgraphs of two graphs is NP-hard. Based on this result, it becomes clear that we need to consider alternative, less powerful graph kernels that are computable in polynomial time. However, as discussed above, it is necessary that these kernels provide an expressive measure of similarity on graphs. Over the years, several graph kernels have been proposed, each focusing on a different structural aspect of graphs. Such aspects involve comparing graphs based on random walks, subtrees, cycles, paths, and small subgraphs, to name a few. We next look at some kernels that date back to the early days of this field. Furthermore, we present kernels that were motivated by problems encountered by the above instances, and were proposed as more advanced alternatives.</p><formula xml:id="formula_21">Graph Kernel Exp. φ Node Node Type Labels Attributes Vertex Histogram √ √ × R-convolution Edge Histogram √ √ × R-convolution Random Walk × √ √ R-convolution Subtree × √ √ R-convolution Cyclic Pattern √ √ × intersection Shortest Path × √ √ R-convolution Graphlet √ × × R-convolution Weisfeiler-Lehman Subtree √ √ × R-convolution Neighborhood Hash √ √ × intersection Neighborhood Subgraph Pairwise Distance √ √ × R-convolution Lovász ϑ √ × × R-convolution SVM-ϑ √ × × R-convolution Ordered Decomposition DAGs √ √ × R-convolution Pyramid Match × √ × assignment Weisfeiler-Lehman Optimal Assignment × √ × assignment Subgraph Matching × √ √ R-convolution GraphHopper × √ √ R-convolution Graph Invariant Kernels × √ √ R-convolution Propagation √ √ √ R-convolution Multiscale Laplacian × √ √ R-convolution</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Random Walk Kernel</head><p>The random walk kernels are perhaps of the first successful efforts to design kernels between graphs that can be computed in polynomial time. The members of this well-studied family of graph kernels quantify the similarity between a pair of graphs based on the number of common walks in the two graphs <ref type="bibr" target="#b60">(Kashima, Tsuda, &amp; Inokuchi, 2003;</ref><ref type="bibr" target="#b39">Gärtner et al., 2003;</ref><ref type="bibr" target="#b78">Mahé, Ueda, Akutsu, Perret, &amp; Vert, 2004;</ref><ref type="bibr" target="#b129">Vishwanathan, Schraudolph, Kondor, &amp; Borgwardt, 2010;</ref><ref type="bibr" target="#b120">Sugiyama &amp; Borgwardt, 2015;</ref><ref type="bibr" target="#b140">Zhang, Wang, Xiang, Huang, &amp; Nehorai, 2018)</ref>. Kernels belonging to this family have concentrated mainly on counting matching walks in the two input graphs. There are several variations of random walk kernels. The k-step random walk kernel compares random walks up to length k in the two graphs. The most widely-used kernel from this family is the geometric random walk kernel  which compares walks up to infinity assigning a weight λ k (λ &lt; 1) to walks of length k in order to ensure convergence of the corresponding geometric series. We next give the formal definition of the geometric random walk kernel. Given two node-labeled graphs G = (V, E) and G = (V , E ), their direct product G × = (V × , E × ) is a graph with vertex set:</p><formula xml:id="formula_22">V × = {(v, v ) : v ∈ V ∧ v ∈ V ∧ (v) = (v )}<label>(14)</label></formula><p>and edge set:</p><formula xml:id="formula_23">E × = {{(v, v ), (u, u )} : {v, u} ∈ E ∧ {v , u } ∈ E }<label>(15)</label></formula><p>An example of the product graph of two graphs is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. Performing a random walk on G × is equivalent to performing a simultaneous random walk on G i and G j . The geometric random walk kernel counts common walks (of potentially infinite length) in two graphs and is defined as follows.</p><p>Definition 14 (Geometric Random Walk Kernel). Let G and G be two graphs, let A × denote the adjacency matrix of their product graph G × , and let V × denote the vertex set of the product graph G × . Then, the geometric random walk kernel is defined as</p><formula xml:id="formula_24">K ∞ × (G, G ) = |V × | p,q=1 ∞ l=0 λ l A l × pq = e (I − λA × ) −1 e (16)</formula><p>where I is the identity matrix, e is the all-ones vector, and λ is a positive, real-valued weight. The geometric random walk kernel converges</p><formula xml:id="formula_25">only if λ &lt; 1 λ × where λ × is the largest eigenvalue of A × .</formula><p>Direct computation of the geometric random walk kernel requires O(n 6 ) time. The computational complexity of the method severely limits its applicability to real-world applications. To account for this, <ref type="bibr" target="#b129">Vishwanathan et al. (2010)</ref> proposed four efficient methods to compute random walk graph kernels which generally reduce the computational complexity from O(n 6 ) to O(n 3 ). <ref type="bibr" target="#b78">Mahé et al. (2004)</ref> proposed some other extensions of random walk kernels. Specifically, they proposed a label enrichment approach which increases specificity and in most cases also reduces computational complexity. They also employed a second order Markov random walk to deal with the problem of "tottering". <ref type="bibr" target="#b120">Sugiyama and Borgwardt (2015)</ref> focused on a different problem of random walk kernels, a phenomenon referred to as "halting". assicMore recently, <ref type="bibr" target="#b140">Zhang et al. (2018)</ref> proposed a kernel that capitalizes on the isomorphism-invariance property of the return probabilities of random walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Subtree Kernel</head><p>Due to problems with the expressiveness of the random walk kernels that they identified, <ref type="bibr" target="#b99">Ramon and Gärtner (2003)</ref> worked on designing new kernels. Their research efforts resulted in the development of the subtree kernel, an algorithm that counts the number of common subtree patterns in two graphs. The kernel is more expressive, but also more computationally expensive than the random walk kernels.</p><p>The subtree patterns that the subtree kernel considers correspond to rooted subgraphs. Every subtree pattern has a tree-structured signature, and the kernel associates each possible subtree pattern signature to a feature. Given a graph, the value of each feature is the number of times that a subtree of the signature that corresponds to this feature occurs in the graph. Let k h (v, v ) be a kernel that counts the pairs of subtrees of the same signature of height less than or equal to h, where the first subtree is rooted at v and the second one</p><formula xml:id="formula_26">is rooted at v . The kernel k h (v, v ) is equal to: k h (v, v ) = δ( (v), (v )) if h = 1 λ v λ v R∈M (v,v ) (u,u )∈R k h−1 (u, u ) if h &gt; 1<label>(17)</label></formula><p>where λ v and λ v are positive values smaller than 1 to cause higher trees to have a smaller weight in the overall sum, and δ is the dirac kernel. Therefore, if h = 1 and the two nodes share the same label, then it holds that k 1 (v, v ) = 1. If h = 1 and the two nodes have different labels, we have k 1 (v, v ) = 0. For h ≥ 1, one can compute k h (v, v ) using a recursive scheme. Specifically, we define the set of all matchings from N (v) to N (v ) as follows</p><formula xml:id="formula_27">M (v, v ) = R ⊆ N (v) × N (v )| ∀(u, u ), (w, w ) ∈ R : u = w ⇔ u = w ∧ ∀(u, u ) ∈ R : (u) = (u ) (18) Each element R of M (v, v )</formula><p>is a set of pairs of nodes from the neighborhoods of v ∈ V and v ∈ V , such that nodes in each pair have identical labels and no node is contained in more than one pair. The subtree kernel compares all pairs of vertices from two graphs by iteratively comparing their neighborhoods.</p><p>Definition 15 (Subtree Kernel). Let G = (V, E) and G = (V , E ) be two graphs. Then, the subtree kernel is defined as</p><formula xml:id="formula_28">k(G, G ) = v∈V v ∈V k h (v, v )<label>(19)</label></formula><p>The computational complexity of the subtree kernel for a pair of graphs is O(n 2 4 deg * h) where deg * is the maximum degree. Although in the worst-case scenario, the runtime complexity of the subtree kernel is very high, in practice, it can be quite low if the input graphs are sparse or if there is sufficient diversity in the labels of the vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Cyclic Pattern Kernel</head><p>The cyclic pattern kernel is also one of the earliest approaches developed in the area of graph kernels. This kernel decomposes a graph into cyclic and tree patterns, and counts the number of common patterns which occur in two graphs <ref type="bibr" target="#b54">(Horváth, Gärtner, &amp; Wrobel, 2004)</ref>. More specifically, let G = (V, E) be a graph. Let also S(G) denote the set of cycles of G. Let C = (v 1 , v 2 , . . . , v k , v 1 ) be a sequence of vertices that forms a cycle in G, i. e. C ∈ S(G). The canonical representation of a cycle C is the lexicographically smallest string π(C) among the strings obtained by concatenating the labels along the vertices of the cyclic permutations of C and its reverse. Formally, denoting by ρ(s) the set of cyclic permutations of a sequence s and its reverse, the canonical representation of C is defined by</p><formula xml:id="formula_29">π(C) = min{w : w ∈ ρ (v 1 ), (v 2 ), . . . , (v k ) }<label>(20)</label></formula><p>where is a function that assigns labels to the vertices of the graph. In case of edge-labeled graphs, edgle labels can also be taken into account. The set of cyclic patterns of G is then defined by</p><formula xml:id="formula_30">C(G) = {π(C) : C ∈ S(G)}<label>(21)</label></formula><p>The kernel then extracts from G all the edges that do not belong to any cycle (a.k.a bridges) by removing from G all the edges of all cycles. The set of bridges of G forms a set of trees (each tree is a connected component composed of bridges). Then, similarly to cycles, the kernel computes the canonical representation π(T ) of each tree T . The set of tree patterns of G is then defined by</p><formula xml:id="formula_31">T (G) = {π(T ) : T is a tree}<label>(22)</label></formula><p>Then, given two graphs, the kernel computes the intersection of their sets of cyclic and tree patters.</p><p>Definition 16 (Cyclic Pattern Kernel). Let G, G be two graphs, and C(G), C(G ) and T (G), T (G ) be the sets of cyclic patterns and tree patters of the two graphs, respectively. Then, the cyclic pattern kernel is defined as</p><formula xml:id="formula_32">k(G, G ) = |C(G) ∪ C(G )| + |T (G) ∪ T (G )|<label>(23)</label></formula><p>Unfortunately, computing the cyclic pattern kernel is an NP-hard problem. The cardinality of the set of cyclic and tree patterns of a graph can be exponential in the number of vertices of the graph. However, the cyclic pattern kernel can prove useful for practical problem classes where the number of cycles in the input graphs is bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Shortest-Path Kernel</head><p>The high computational complexity of the graph kernels based on walks, subtrees and cycles renders them impractical for most real-world scenarios.  worked on developing more efficient kernels based on paths. However, computing all the paths in a graph and computing the longest paths in a graph are both NP-hard problems. Instead, shortest paths can be computed in polynomial time, and they gave rise to the shortest-path kernel, one of the most popular kernels at present.</p><p>The shortest-path kernel decomposes graphs into shortest paths and compares pairs of shortest paths according to their lengths and the labels of their endpoints. The first step of the shortest-path kernel is to transform the input graphs into shortest-paths graphs. Given an input graph G = (V, E), the algorithm creates a new graph S = (V, E s ) (i. e. its shortestpath graph). The shortest-path graph S contains the same set of vertices as the graph from which it originates. The edge set of the former is a superset of that of the latter, since in the shortest-path graph S, there exists an edge between all vertices which are connected by a walk in the original graph G. To complete the transformation, the algorithm assigns labels to all the edges of the shortest-path graph S. The label of each edge is set equal to the shortest distance between its endpoints in the original graph G.</p><p>Given the above procedure for transforming a graph into a shortest-path graph, the shortest-path kernel is defined as follows.</p><p>Definition 17 (Shortest-Path Kernel). Let G, G be two graphs, and S = (V, E), S = (V , E ) their corresponding shortest-path graphs. The shortest-path kernel is then defined as</p><formula xml:id="formula_33">k(G, G ) = e∈E e ∈E k (1) walk (e, e )<label>(24)</label></formula><p>where k</p><p>(1)</p><p>walk (e, e ) is a positive semidefinite kernel on edge walks of length 1.</p><p>In labeled graphs, the k</p><p>walk (e, e ) kernel is designed to compare both the lengths of the shortest paths corresponding to edges e and e , and the labels of their endpoint vertices. Let e = {v, u} and e = {v , u }. Then, k</p><p>walk (e, e ) is usually defined as</p><formula xml:id="formula_36">k (1) walk (e, e ) = k v (v), (v ) k e (e), (e ) k v (u), (u ) + k v (v), (u ) k e (e), (e ) k v (u), (v )<label>(25)</label></formula><p>where k v is a kernel comparing vertex labels, and k e a kernel comparing shortest path lengths. Vertex labels are usually compared via a dirac kernel, while shortest path lengths may also be compared via a dirac kernel or, more rarely, via a brownian bridge kernel . When k v and k e both are dirac kernels, an explicit computation scheme can be employed as shown in <ref type="figure">Figure 5</ref>. In terms of runtime complexity, the shortest-path kernel can be computed in O(n 4 ) time. <ref type="figure">Figure 5</ref>: Example of explicit computation of the shortest path kernel. Each triple is a feature and corresponds to: (label of source vertex; label of sink vertex; shortest path length between the two vertices).</p><formula xml:id="formula_37">a a b c b (a; a; 2), (a; b; 1), (a; b; 2), (a; c; 2), (b; a; 1), (b; a; 2), (b; b; 1), (b; c; 2), (c; a; 2), (c; b; 1), (c; b; 2) G i Triples: φ(G i ) = (2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0) φ(G j ) = (0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1) a b G j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Graphlet Kernel</head><p>The graphlet kernel decomposes graphs into graphlets (i. e. small subgraphs with k nodes where k ∈ {3, 4, 5}) <ref type="bibr" target="#b97">(Pržulj, 2007)</ref> and counts matching graphlets in the input graphs. The set of graphlets of size 4 is shown in <ref type="figure">Figure 6</ref>. The kernel was originally designed to address scalability issues experienced by earlier approaches. In fact, the graphlet kernel was one of the first kernels that could cope with very large graphs using a simple sampling scheme. However, apart from the scalability issue, the graphlet kernel was also motivated by the graph reconstruction conjecture <ref type="bibr" target="#b14">(Bondy &amp; Hemminger, 1977)</ref>, which states that any graph of size n can be reconstructed from the set of all its subgraphs of size n − 1. This could possibly be interpreted as indicating that kernels that compare graphs based on their subgraphs should reflect graph similarity better than approaches that are defined based on random walks, subtrees, cyclic patterns or shortest paths. However, even if graphs that have similar distributions of graphlets are very likely to be similar themselves, there is no theoretical justification on why such a substructure (i. e. graphlets) is better than the others. As mentioned above, the graphlet kernel computes the distribution of small subgraphs in a graph. Let G = {graphlet 1 , graphlet 2 , . . ., graphlet d } be the set of size-k graphlets. Let also f G ∈ N d be a vector such that its i-th entry is equal to the frequency of occurrence of graphlet i in G, f G,i = #(graphlet i G). Then, the graphlet kernel is defined as follows.</p><p>Definition 18 (Graphlet of size k Kernel). Let G, G be two graphs of size n ≥ k, and f G , f G vectors that count the occurrence of each graphlet of size k (not necessarily connected) in the two graphs. Then the graphlet kernel is defined as</p><formula xml:id="formula_38">k(G, G ) = f G f G (26)</formula><p>As is evident from the above definition, the graphlet kernel is computed by explicit feature maps. First, the representation of each graph in the feature space is computed. And then, the kernel value is computed as the dot product of the two feature vectors. The main problem of graphlet kernel is that an exaustive enumeration of graphlets is very <ref type="figure">Figure 6</ref>: All graphlets of size 4.</p><formula xml:id="formula_39">G 1 G 2 G 3 G 4 G 5 G 6 G 7 G 8 G 9 G 10 G 11</formula><p>expensive. Since there are n k size-k subgraphs in a graph, computing the feature vector for a graph of size n requires O(n k ) time. To account for that, <ref type="bibr" target="#b111">Shervashidze et al. (2009)</ref> resorted to sampling. Following <ref type="bibr" target="#b134">Weissman et al. (2003)</ref>, they showed that by sampling a fixed number of graphlets the empirical distribution of graphlets will be sufficiently close to their actual distribution in the graph. An alternative proposed strategy that reduces the expressivity of the kernel is to enumerate only the connected graphlets of k vertices, and not all the possible graphlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.6">Weisfeiler-Lehman Subtree Kernel</head><p>The Weisfeiler-Lehman subtree kernel is a very popular algorithm, and is considered the state-of-the-art in graph classification. It belongs to the family of subtree kernels, and was motivated by the need for a fast subtree kernel that scales up to large, labeled graphs. The kernel is an instance of the Weisfeiler-Lehman framework. This framework operates on top of existing graph kernels and is inspired by the Weisfeiler-Lehman test of graph isomorphism <ref type="bibr" target="#b133">(Weisfeiler &amp; Lehman, 1968)</ref>. The key idea of the Weisfeiler-Lehman algorithm is to replace the label of each vertex with a multiset label consisting of the original label of the vertex and the sorted set of labels of its neighbors. The resultant multiset is then compressed into a new, short label. This relabeling procedure is then repeated for h iterations. Note that this procedure is performed simultaneously on all input graphs. Therefore, two vertices from different graphs will get identical new labels if and only if they have identical multiset labels.</p><p>More formally, given a graph G = (V, E) endowed with a labeling function = 0 , the Weisfeiler-Lehman graph of G at height i is a graph G i = (V, E) endowed with a labeling function i which has emerged after i iterations of the relabeling procedure described above. The Weisfeiler-Lehman sequence up to height h of G consists of the Weisfeiler-Lehman graphs of G at heights from 0 to h, {G 0 , G 1 , . . . , G h }.</p><p>Definition 19 <ref type="bibr">(Weisfeiler-Lehman Framework)</ref>. Let k be any kernel for graphs, that we will call the base kernel. Then the Weisfeiler-Lehman kernel with h iterations with the base kernel k between two graphs G and G is defined as</p><formula xml:id="formula_40">k W L (G, G ) = k(G 0 , G 0 ) + k(G 1 , G 1 ) + . . . + k(G h , G h )<label>(27)</label></formula><p>where h is the number of Weisfeiler-Lehman iterations, and {G 0 , G 1 , . . . , G h } and {G 0 , G 1 , . . . , G h } are the Weisfeiler-Lehman sequences of G and G respectively.</p><p>From the above definition, it is clear that any graph kernel that takes into account discrete node labels can take advantage of the Weisfeiler-Lehman framework and compare graphs based on the whole Weisfeiler-Lehman sequence.</p><p>When the base kernel compares subtrees extracted from two graphs, the computation involves counting common original and compressed labels in the two graphs. The emerging Weisfeiler-Lehman subtree kernel is a byproduct of the Weisfeiler-Lehman test of isomorphism.</p><p>Definition 20 (Weisfeiler-Lehman Subtree Kernel). Let G, G be two graphs. Define Σ i ⊆ Σ as the set of letters that occur as node labels at least once in G or G at the end of the i th iteration of the Weisfeiler-Lehman algorithm. Let Σ 0 be the set of original node labels of G and G . Assume all Σ i are pairwise disjoint. Without loss of generality, assume that every</p><formula xml:id="formula_41">Σ i = {σ i1 , . . . , σ i|Σ i | } is ordered. Define a map c i : {G, G } × Σ i → N such that c i (G, σ ij )</formula><p>is the number of occurrences of the letter σ ij in the graph G.</p><p>The Weisfeiler-Lehman subtree kernel on two graphs G and G with h iterations is defined as</p><formula xml:id="formula_42">k(G, G ) = φ(G), φ(G ) (28) where φ(G) = (c 0 (G, σ 01 ), . . . , c 0 (G, σ 0|Σ 0 | ), . . . , c h (G, σ h1 ), . . . , c h (G, σ h|Σ h | ))<label>(29)</label></formula><p>and</p><formula xml:id="formula_43">φ(G ) = (c 0 (G , σ 01 ), . . . , c 0 (G , σ 0|Σ 0 | ), . . . , c h (G , σ h1 ), . . . , c h (G , σ h|Σ h | ))<label>(30)</label></formula><p>An illustration of the Weisfeiler-Lehman subtree kernel is given in <ref type="figure">Figure 7</ref>. It can be shown that the above definition is equivalent to comparing the number of shared subtrees between the two input graphs <ref type="bibr" target="#b110">(Shervashidze, Schweitzer, Leeuwen, Mehlhorn, &amp; Borgwardt, 2011)</ref>. In contrast to the subtree kernel that was proposed by Ramon and Gärtner and was presented above, the Weisfeiler-Lehman subtree kernel considers all subtrees up to height h, instead of subtrees of exactly height h. Furthermore, the Weisfeiler-Lehman subtree kernel checks whether the neighborhoods of two vertices match exactly, while the subtree kernel considers all pairs of matching subsets of the neighborhoods of two vertices. It is interesting to note that the Weisfeiler-Lehman subtree kernel exhibits a very attractive computational complexity since it can be computed in O(hm) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Neighborhood Aggregation Approaches</head><p>The Weisfeiler-Lehman subtree kernel triggered a lot of activity in the field of graph kernels. The relabeling procedure of the Weisfeiler-Lehman algorithm can be viewed as a neighborhood aggregation scheme. The main idea behind neighborhood aggregation algorithms (a.k.a. message-passing algorithms) is that each vertex receives messages from its neighbors and utilizes these messages to update its representation. Following the success of this kernel, several variations of it were proposed. All these variations employ a neighborhood aggregation scheme similar to that of the Weisfeiler-Lehman algorithm. The goal of most of these works is to speed-up the computation time of the Weisfeiler-Lehman subtree   2, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, ) 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, ) <ref type="figure">Figure 7</ref>: Illustration of the computation of the Weisfeiler-Lehman subtree kernel with h = 1 for two graphs G and G . Here, 1, 2, . . . , 13 ∈ Σ are letters that occur as node labels.</p><formula xml:id="formula_44">G G φ(G) = ( φ(G )<label>(</label></formula><p>Compressed labels map to subtree patterns. For example, if a node has label 6, this means that there is a subtree pattern of height 1 rooted at this node, where the root has label 1 and its single neighbor has label 4.</p><p>kernel <ref type="bibr" target="#b53">(Hido &amp; Kashima, 2009;</ref><ref type="bibr" target="#b61">Kataoka &amp; Inokuchi, 2016)</ref>. However, other types of variations were also proposed such as a streaming version of the Weisfeiler-Lehman algorithm <ref type="bibr" target="#b72">(Li, Zhu, Chi, &amp; Zhang, 2012)</ref>, and a kernel that uses the k-dimensional Weisfeiler-Lehman test of isomorphism <ref type="bibr" target="#b86">(Morris, Kersting, &amp; Mutzel, 2017)</ref>. We next present the neighborhood hash kernel, a kernel that was born out of these research efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Neighborhood Hash Kernel</head><p>Similar to the Weisfeiler-Lehman subtree kernel, the neighborhood hash kernel also assumes node-labeled graphs <ref type="bibr" target="#b53">(Hido &amp; Kashima, 2009</ref>). It compares graphs by updating their node labels and counting the number of common labels. The kernel replaces the discrete node</p><formula xml:id="formula_45">a(#1000) c(#1100) b(#1110) #1110 #1100 ⊕ #0010 ⊕ #0001 ROT1(#1000 ) = #0011</formula><p>Figure 8: Example of computation of the simple neighborhood hash for a vertex (in green). The vertex has two adjacent vertices (in red). The three vertices have different labels from each other. The algorithm uses XOR and ROT operations to compute the neighborhood hash of the vertex (#0011).</p><p>labels with binary arrays of fixed length, and it then employs logical operations to update the labels so that they contain information about the neighborhood structure of each vertex. Let : V → Σ be a function that maps vertices to an alphabet Σ which is the set of possible discrete node labels. Hence, given a vertex v, (v) ∈ Σ is the label of vertex v. The algorithm first transforms each discrete node label to a bit label. A bit label is a binary array consisting of d bits as</p><formula xml:id="formula_46">s = (b 1 , b 2 , . . . , b d ) (31) where the constant d satisfies 2 d − 1 |Σ| and b 1 , b 2 , . . . , b d ∈ {0, 1}.</formula><p>The most important step of the algorithm involves a procedure that updates the labels of the vertices. To achieve that, the kernel makes use of two very common bit operations:</p><p>(1) the exclusive or (XOR) operation, and (2) the bit rotation (ROT ) operation. Let XOR(s i , s j ) = s i ⊕ s j denote the XOR operation between two bit labels s i and s j (i. e. the XOR operation is applied to all their components). The output of the operation is a new binary array whose components represent the XOR value between the corresponding components of the s i and s j arrays. The ROT o operation takes as input a bit array and shifts its last o bits to the left by o bits and moves the first o bits to the right end as shown below</p><formula xml:id="formula_47">ROT o (s) = {b o+1 , b o+2 , . . . , b d , b 1 , . . . , b o }<label>(32)</label></formula><p>Below, we present in detail two procedures for updating the labels of the vertices: (1) the simple neighborhood hash, and (2) the count-sensitive neighborhood hash.</p><p>Simple Neighborhood Hash. Given a graph G = (V, E) with bit labels, the simple neighborhood hash update procedure computes a neighborhood hash for each vertex using the logical operations XOR and ROT . More specifically, given a vertex v ∈ V , let N (v) = {u 1 , . . . , u d } be the set of neighbors of v. Then, the kernel computes the neighborhood hash as</p><formula xml:id="formula_48">N H(v) = ROT 1 (v) ⊕ (u 1 ) ⊕ . . . ⊕ (u d )<label>(33)</label></formula><p>The resulting hash N H(v) is still a bit array of length d, and we regard it as the new label of v. This new label represents the distribution of the node labels around v. Hence, if v i and v j are two vertices that have the same label (i. e. (v i ) = (v j )) and the label sets of their neighborhors are also identical, their hash values will be the same (i. e. N H(v i ) = N H(v j )). Otherwise, they will be different except for accidental hash collisions. The main idea behind this update procedure is that the hash value is independent of the order of the neighborhood values due to the properties of the XOR operation. Hence, one can check whether or not <ref type="figure">Figure 9</ref>: Example of computation of the count-sensitive neighborhood hash for a vertex (in green). The vertex has three adjacent vertices (in red). Two of these three vertices have identical labels. The algorithm uses XOR and ROT operations to compute the countsensitive neighborhood hash of the vertex (#0111).</p><formula xml:id="formula_49">a(#1000) c(#1100) #0110 ⊕ #0001 ROT1(#1000 ) = #0111 d(#0101) d(#0101) #0101 #1100 #0010) = #0001 = ⊕ ⊕ ROT2(#0111) = #1101 ROT1(#1101) = #1011 ROT2( ROT1( ⊕</formula><p>the distributions of neighborhood labels of two vertices are equivalent without sorting or matching these two label sets. <ref type="figure">Figure 8</ref> illustrates how the simple neighborhood hash is computed for a given vertex.</p><p>Count-sensitive Neighborhood Hash. The simple neighborhood hash update procedure described above suffers from some problematic hash collisions. Specifically, the neighborhood hash values for two independent nodes have a small probability of being the same even if there is no accidental hash collision. Such problematic hash collisions may affect the positive semidefiniteness of the kernel. To address that problem, the count-sensitive neighborhood hash update procedure counts the number of occurences of each label in the set. More specifically, it first uses a sorting algorithm (e. g. radix sort) to align the bit labels of the neighbors, and then, it extracts the unique labels (set { 1 , . . . , l } in the case of l unique labels) and for each label counts its number of occurences. Then, it updates each unique label based on its number of occurences as follows</p><formula xml:id="formula_50">i = ROT o i ⊕ o<label>(34)</label></formula><p>where i , i is the initial and updated label respectively, and o is the number of occurences of that label in the set of neighbors. The above operation makes the hash values unique by depending on the number of label occurrences. Then, the count-sensitive neighborhood hash is computed as <ref type="figure">Figure 9</ref> illustrates the operations of the count-sensitive neighborhood hash for a given vertex. Both the simple and the count-sensitive neighborhood hash can be seen as general approaches for enriching the labels of vertices based on the label distribution of their neighborhood vertices.</p><formula xml:id="formula_51">CSN H(v) = ROT 1 (v) ⊕ 1 ⊕ . . . ⊕ l<label>(35)</label></formula><p>Kernel Calculation. The neighborhood hash update procedures presented above aggregate the information of the neighborhood vertices to each vertex. Then, given two graphs G and G , the updated labels of their vertices are compared using the following function</p><formula xml:id="formula_52">κ(G, G ) = c |V | + |V | − c<label>(36)</label></formula><p>where c is the number of labels the two graphs have in common. This function is equivalent to the Tanimoto coefficent which is commonly used as a similarity measure between sets of discrete values and which has been proven to be positive semidefinite <ref type="bibr" target="#b45">(Gower, 1971</ref>).</p><p>The label-update procedures is not necessary to be applied once, but they can be applied iteratively. By updating the bit labels several times, the new labels can capture high-order relationships between vertices. For instance, if the procedure is performed h times in total, the updated label (v) of a vertex v represents the label distribution of its h-neighbors. Hence, two vertices v i , v j with identical labels and connections among their r-neighbors will be assigned the same label.</p><p>Definition 21 (Neighborhood Hash Kernel). Let G and G be two graphs, and let G 1 , . . . , G h and G 1 , . . . , G h denote their updated graphs where the node labels have been updated 1, . . . , h times based on one of the two procedures presented above, respectively. Then, the neighborhood hash kernel is defined as</p><formula xml:id="formula_53">k(G, G ) = 1 h h i=1 κ(G i , G i )<label>(37)</label></formula><p>The computational complexity of the neighborhood hash kernel is O(deg nhd) where n = |V | is the number of vertices of the graphs and deg is the average degree of their vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Other Approaches</head><p>There are several kernels that were proposed recently which belong to the R-convolution framework, but do not perform neighborhood aggregation. There are, for instance, kernels specially designed for graphs with ordered neighborhoods <ref type="bibr" target="#b30">(Draief, Kutzkov, Scaman, &amp; Vojnovic, 2018)</ref>, kernels that compare pairs of rooted subgraphs containing nodes up to a certain distance from the root <ref type="bibr" target="#b25">(Costa &amp; De Grave, 2010)</ref>, kernels that extract directed acyclic graphs from the input graphs (Da San Martino, Navarin, &amp; Sperduti, 2012), and kernels that use the orthonormal representations of vertices introduced by Lovász (Johansson, <ref type="bibr" target="#b58">Jethava, Dubhashi, &amp; Bhattacharyya, 2014)</ref>. We next present some of these kernels in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Neighborhood Subgraph Pairwise Distance Kernel</head><p>The neighborhood subgraph pairwise distance kernel extracts pairs of rooted subgraphs from each graph whose roots are located at a certain distance from each other, and which contain vertices up to a certain distance from the root. It then compares graphs based on these pairs of rooted subgraphs. To avoid isomorphism checking, graph invariants are employed to encode each rooted subgraph <ref type="bibr" target="#b25">(Costa &amp; De Grave, 2010)</ref>.</p><p>Let G = (V, E) be a graph. The distance between two vertices u, v ∈ V , denoted D(u, v), is the length of the shortest path between them. The neighborhood of radius r of a vertex v is the set of vertices at a distance less than or equal to r from v, that is {u ∈ V : D(u, v) ≤ r}. Given a subset of vertices S ⊆ V , let E(S) be the set of edges that have both end-points in S. Then, the subgraph with vertex set S and edge set E(S) is known as the subgraph induced by S. The neighborhood subgraph of radius r of vertex v is the subgraph induced by the neighborhood of radius r of v and is denoted by N r (v). Let also R r,d (A v , B u , G) be a relation between two rooted graphs A v , B u and a graph G = (V, E) that is true if and only if both</p><formula xml:id="formula_54">A v and B u are in {N r (v) : v ∈ V }, where we require A v , B u</formula><p>to be isomorphic to some N r (v) to verify the set inclusion, and that D(u, v) = d. We denote with R −1 (G) the inverse relation that yields all the pairs of rooted graphs A v , B u satisfying the above constraints. Hence, R −1 (G) selects all pairs of neighborhood graphs of radius r whose roots are at distance d in a given graph G.</p><p>Definition 22 (Neighborhood Subgraph Pairwise Distance Kernel). Let G, G be two graphs. The neighborhood subgraph pairwise distance kernel extracts from the two graphs pairs of rooted subgraphs of radius r whose roots are located at distance d from each other. It then utilizes the following kernel to compare them</p><formula xml:id="formula_55">k r,d (G, G ) = Av,Bv∈R −1 r,d (G) A v ,B v ∈R −1 r,d (G ) δ(A v , A v ) δ(B v , B v )<label>(38)</label></formula><p>where δ is 1 if its input subgraphs are isomorphic, and 0 otherwise. The above kernel counts the number of identical pairs of neighboring graphs of radius r at distance d between two graphs. Then, the neighborhood subgraph pairwise distance kernel is defined as</p><formula xml:id="formula_56">k(G, G ) = r * r=0 d * d=0k r,d (G, G )<label>(39)</label></formula><p>wherek r,d is a normalized version of k r,d , that iŝ</p><formula xml:id="formula_57">k r,d (G, G ) = k r,d (G, G ) k r,d (G, G)k r,d (G , G )<label>(40)</label></formula><p>The above version ensures that relations of all orders are equally weighted regardless of the size of the induced part sets. The neighborhood subgraph pairwise distance kernel includes an exact matching kernel over two graphs (i. e. the δ kernel) which is equivalent to solving the graph isomorphism problem. Solving the graph isomorphism problem is not feasible. Therefore, the kernel produces an approximate solution to it instead. Given a subgraph G S induced by the set of vertices S, the kernel computes a graph invariant encoding for the subgraph via a label function g : G → Σ * , where G is the set of rooted graphs and Σ * is the set of strings over a finite alphabet Σ. The function g makes use of two other label functions: (1) a function n for vertices, and (2) a function e for edges. The n function assigns to vertex v the concatenation of the lexicographically sorted list of distance-distance from root-label triplets D(v, u), D(v, h), (u) for all u ∈ S, where h is the root of the subgraph and is a function that maps vertices/edges to their label symbol. Hence, the above function relabels each vertex with a string that encodes the initial label of the vertex, the vertex distance from all other labeled vertices, and the distance from the root vertex. The e {u, v} function assigns to edge {u, v} the label n (u), n (v), {u, v} . The e {u, v} function thus annotates each edge based on the new labels of its endpoints, and its initial label, if any. Finally, the function g (G S ) assigns to the rooted graph induced by S the concatenation of the lexicographically sorted list of e {u, v} for all {u, v} ∈ E(S). The kernel then employs a hashing function from strings to natural numbers H : Σ * → N to obtain a unique identifier for each subgraph. Hence, instead of testing pairs of subgraphs for isomorphism, the kernel just checks if the subgraphs share the same identifier.</p><p>The computational complexity of the neighborhood subgraph pairwise distance kernel is O(n|S||E(S)| log |E(S)|) and is dominated by the repeated computation of the graph invariant for each vertex of the graph. Since this is a constant time procedure, for small values of d * and r * , the complexity of the kernel is in practice linear in the size of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Lovász ϑ Kernel</head><p>The Lovász number ϑ(G) of a graph G = (V, E) is a real number that is an upper bound on the Shannon capacity of the graph. It was introduced by László Lovász in 1979 <ref type="bibr" target="#b76">(Lovász, 1979)</ref>. The Lovász number is intimately connected with the notion of orthonormal representations of graphs. An orthonormal representation of a graph G consists of a set of unit vectors U G = {u i ∈ R d : ||u i || = 1} i∈V where each vertex i is assigned a unit vector u i such that (i, j) ∈ E =⇒ u i u j = 0. Specifically, the Lovász number of a graph G is defined as</p><formula xml:id="formula_58">ϑ(G) = min c,U G max i∈V 1 (c u i ) 2<label>(41)</label></formula><p>where c ∈ R d is a unit vector and U G is an orthonormal representation of G. Geometrically, ϑ(G) is defined by the smallest cone enclosing a valid orthonormal representation U G . The Lovász number ϑ(G) of a graph G can be computed to arbitrary precision in polynomial time by solving a semidefinite program. The Lovász ϑ kernel utilizes the orthonormal representations associated with the Lovász number to compare graphs <ref type="bibr" target="#b58">(Johansson et al., 2014)</ref>. The kernel is applicable only to unlabeled graphs. Given a collection of graphs, it first generates orthonormal representations for the vertices of each graph by computing the Lovász ϑ number. Hence, U G is a set that contains the orthonormal representations of G. Let S ⊆ V be a subset of the vertex set of G. Then, the Lovász value of the set of vertices S is defined as</p><formula xml:id="formula_59">ϑ S (G) = min c max i∈S 1 (c u i ) 2<label>(42)</label></formula><p>where c ∈ R d is a unit vector and u i is the representation of vertex i obtained by computing the Lovász number ϑ(G) of G. The Lovász value of a set of vertices S represents the angle of the smallest cone enclosing the set of orthonormal representations of these vertices (i. e. subset of U G defined as</p><formula xml:id="formula_60">{u i : u i ∈ U G , i ∈ S}).</formula><p>Definition 23 (Lovász ϑ Kernel). Let G = (V, E) and G = (V , E ) be two graphs. The Lovász ϑ kernel between the two graphs is defined as follows</p><formula xml:id="formula_61">k(G, G ) = S⊆V S ⊆V δ(|S|, |S |) 1 Z |S| k ϑ S (G), ϑ S (G )<label>(43)</label></formula><p>where Z |S| = |V | |S| |V | |S| , δ(|S|, |S |) is a delta kernel (equal to 1 if |S| = |S |, and 0 otherwise), and k is a positive semi-definite kernel between Lovász values (e. g. linear kernel, gaussian kernel).</p><p>The Lovász ϑ kernel consists of two main steps: (1) computing the Lovász number ϑ of each graph and obtaining the associated orthonormal representations, and (2) computing the Lovász value for all subgraphs (i. e. subsets of vertices S ⊆ V ) of each graph. Exact computation of the Lovász ϑ kernel is in most real settings infeasible since it requires computing the minimum enclosing cones of 2 n sets of vertices.</p><p>When dealing with large graphs, it is thus necessary to resort to sampling. Given a graph G, instead of evaluating the Lovász value on all 2 n sets of vertices, the algorithm evaluates it in on a smaller number of subgraphs induced by sets of vertices contained in L ⊂ 2 V . Then, the Lovász ϑ kernel is defined as followŝ</p><formula xml:id="formula_62">k(G, G ) = S∈L S ∈L δ(|S|, |S |) 1 Z |S| k ϑ S (G), ϑ S (G )<label>(44)</label></formula><p>whereẐ |S| = |L |S| ||L |S| | and L |S| denotes the subset of L consisting of all sets of cardinality |S|, that is</p><formula xml:id="formula_63">L |S| = {B ∈ L : |B| = |S|}. The time complexity of computingk(G, G ) is O(n 2 m −1 + s 2 T (k) + sn) where T (k)</formula><p>is the complexity of computing the base kernel k, n = |V |, m = |E| and s = max(|L|, |L |). The first term represents the cost of solving the semi-definite program that computes the Lovász number ϑ. The second term corresponds to the worst-case complexity of computing the sum of the Lovász values. And finally, the third term is the cost of computing the Lovász values of the sampled subsets of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.3">SVM-ϑ Kernel</head><p>The SVM-ϑ kernel is very related to the Lovász ϑ kernel <ref type="bibr" target="#b58">(Johansson et al., 2014)</ref>. The Lovász ϑ kernel suffers from high computational complexity, and the SVM-ϑ kernel was developed as a more efficient alternative. Similar to the Lovász ϑ kernel, this kernel also assumes unlabeled graphs.</p><p>Given a graph G = (V, E) such that |V | = n, the Lovász number of G can be defined as</p><formula xml:id="formula_64">ϑ(G) = min K∈L ω(K)<label>(45)</label></formula><p>where ω(K) is the one-class SVM given by</p><formula xml:id="formula_65">ω(K) = max α i &gt;0 2 n i=1 α i − n i=1 n j=1 α i α j K ij<label>(46)</label></formula><p>and L is a set of positive semidefinite matrices defined as</p><formula xml:id="formula_66">L = {K ∈ S + n : K ii = 1, K ij = 0 ∀(i, j) ∈ E}<label>(47)</label></formula><p>where S + n is the set of all n × n positive semidefinite matrices. The SVM-ϑ kernel first computes the matrix K LS which is equal to</p><formula xml:id="formula_67">K LS = A ρ + I<label>(48)</label></formula><p>where A is the adjacency matrix of G, I is the n × n identity matrix, and ρ ≥ −λ n with λ n the minimum eigenvalue of A. The matrix K LS is positive semidefinite by construction and it has been shown in <ref type="bibr" target="#b55">(Jethava, Martinsson, Bhattacharyya, &amp; Dubhashi, 2013)</ref> that</p><formula xml:id="formula_68">ω(K LS ) = n i=1 α i<label>(49)</label></formula><p>where α i are the maximizers of Equation 46. Furthermore, it was shown that on certain families of graphs (e. g. Erdös Rényi random graphs), ω(K LS ) is with high probability a constant factor approximation to ϑ(G).</p><p>Definition 24 (SVM-ϑ Kernel). Let G = (V, E) and G = (V , E ) be two graphs. Then, the SVM-ϑ kernel is defined as follows</p><formula xml:id="formula_69">k(G, G ) = S⊆V S ⊆V δ(|S|, |S |) 1 Z |S| k i∈S α i , j∈S α j<label>(50)</label></formula><p>where Z |S| = |V | |S| |V | |S| , δ(|S|, |S |) is a delta kernel (equal to 1 if |S| = |S |, and 0 otherwise), and k is a positive semi-definite kernel between real values (e. g. linear kernel, gaussian kernel).</p><p>The SVM-ϑ kernel consists of three main steps: (1) constructing matrix K LS of G which takes O(n 3 ) time (2) solving the one-class SVM problem in O(n 2 ) time to obtain the α i values, and (3) computing the sum of the α i values for all subgraphs (i. e. subsets of vertices S ⊆ V ) of each graph. Computing the above quantity for all 2 n sets of vertices is not feasible in real-world scenarios.</p><p>To address the above issue, the SVM-ϑ kernel employs sampling schemes. Given a graph G, the kernel samples a specific number of subgraphs induced by sets of vertices contained in L ∈ 2 V . Then, the SVM-ϑ kernel is defined as followŝ</p><formula xml:id="formula_70">k(G, G ) = S∈L S ∈L δ(|S|, |S |) 1 Z |S| k i∈S α i , j∈S α j<label>(51)</label></formula><p>whereẐ |S| = |L |S| ||L |S| | and L |S| denotes the subset of L consisting of all sets of cardinality |S|, that is L |S| = {B ∈ L : |B| = |S|}.</p><p>The time complexity of computingk(G, G ) is O(n 3 + s 2 T (k) + sn) where T (k) is the complexity of computing the base kernel k and s = max(|L|, |L |). The first term represents the cost of computing K LS (dominated by the eigenvalue decomposition). The second term corresponds to the worst-case complexity of comparing the sums of the α i values. And finally, the third term is the cost of computing the sum of the α i values for the sampled subsets of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.4">Ordered Decomposition DAGs Kernel</head><p>In contrast to the above two kernels, the ordered decomposition DAGs kernel can handle node-labeled graphs. The kernel decomposes graphs into multisets of directed acyclic graphs (DAGs), and then uses existing tree kernels to compare these DAGs <ref type="bibr" target="#b26">(Da San Martino et al., 2012)</ref>. (1)</p><p>(2) (3) (4) <ref type="figure" target="#fig_0">Figure 10</ref>: Example of decomposition of a graph into its four DAGs (one for each vertex).</p><p>Given a graph G = (V, E), the kernel generates one unordered rooted DAG, say DD v , for each vertex v ∈ V . To generate the DAG, the kernel keeps only those edges belonging to the shortest paths between v and any vertex u ∈ V \ {v}. Furthermore, a direction is given to each edge, while edges connecting vertices visited at level l to vertices visited at level l &lt; l are also removed. <ref type="figure" target="#fig_0">Figure 10</ref> gives an example of the decomposition of a graph into a set of DAGs.</p><p>Definition 25 (Ordered Decomposition DAGs Kernel). Let G = (V, E) and G = (V , E ) be two graphs. Let also DD(G) and DD(G ) be multisets defined as {DD v : v ∈ V } and {DD v : v ∈ V }, respectively. Then, the ordered decomposition DAGs kernel is defined as</p><formula xml:id="formula_71">k(G, G ) = D∈DD(G) D ∈DD(G ) k DAG (D, D )<label>(52)</label></formula><p>where k DAG is a kernel between DAGs.</p><p>The kernel is thus defined as the sum of the computation of a local kernel for DAGs, over all pairs of DAGs in the multiset. Note that these DAGs are unordered. Moreover, there is a vast literature on kernels for ordered trees, but only a few kernel functions for unordered trees. Hence, the ordered decomposition DAGs kernel transforms the unordered DAGs to ordered DAGs, and then applies a kernel for ordered trees. More specifically, the kernel defines a strict partial order among the vertices of each DAG. This partial order takes into account the labels of the vertices, the outdegrees of the vertices (in case of identical node labels), and the relation between the sequence of successors of each vertex (in case of identical node labels and equal outdegrees). Let ODD v denote the DAG of v ∈ V ordered according to the above relation. Let a tree visit be a function T (u) that, given a vertex u of a ODD v , returns the tree resulting from the visit of the DAG starting in u. <ref type="figure" target="#fig_0">Figure 11</ref> gives an example of tree visits. Then, the ordered decomposition DAGs kernel uses tree visits to project subdags to a tree space and applies tree kernels on the visits</p><formula xml:id="formula_72">k DAG (D, D ) = v∈V D v ∈V D k tree root(T (v)), root(T (v ))<label>(53)</label></formula><p>where V D , V D are the set of vertices of D and D , respectivevly, and k tree is a kernel between ordered trees. The time complexity of the ordered decomposition DAGs kernel depends on the employed tree kernel k tree . For instance, using the subtree and subset tree kernel leads to a time complexity of O(n 3 log n) and O(n 4 ), respectively. To reduce the time complexity, the kernel employs a strategy that allows it to compute k tree once for each unique pair of subtrees appearing in different DAGs. Furthermore, in case the subtree kernel is employed, some other strategies can be applied to speed up the computation such as for instance limiting the depth of the visits during the generation of the multiset of DAGs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Assignment Kernels</head><p>The majority of the kernels that have been presented so far belong to the family of Rconvolution kernels. Besides this family of kernels, another family that has received a lot of attention recently are the assignment kernels. In general, these kernels compute a matching between substructures of one object and substructures of a second object such that the overall similarity of the two objects is maximized. Such a matching can reveal structural correspondences between the two objects. However, defining graph kernels that follow this design paradigm is not trivial. For example, an optimal assignment kernel that was proposed in the early days of graph kernels to compute a correpondence between the atoms of molecules <ref type="bibr" target="#b36">(Fröhlich, Wegner, Sieker, &amp; Zell, 2005</ref>) was later proven not to always be positive semidefinite <ref type="bibr" target="#b128">(Vert, 2008)</ref>. Despite these design difficulties, there is a handful of valid assignment graph kernels. For instance, there is a method that capitalizes on the well-known pyramid match kernel to match the node embeddings of graphs <ref type="bibr" target="#b93">(Nikolentzos, Meladianos, &amp; Vazirgiannis, 2017b)</ref>, while another approach uses multi-graph matching techniques to obtain valid assignment kernels <ref type="bibr" target="#b103">(Schiavinato, Gasparetto, &amp; Torsello, 2015)</ref>. More importantly, it was recently shown that there exists a class of base kernels used to compare substructures that guarantees positive semidefinite optimal assignment kernels <ref type="bibr" target="#b67">(Kriege, Giscard, &amp; Wilson, 2016)</ref>. We next present some of the above instances of assignment kernels in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Pyramid Match Graph Kernel</head><p>The pyramid match kernel is a very popular algorithm in Computer Vision, and has proven useful for many applications including object recognition and image retrieval <ref type="bibr" target="#b47">(Grauman &amp; Darrell, 2007;</ref><ref type="bibr" target="#b70">Lazebnik, Schmid, &amp; Ponce, 2006)</ref>. The pyramid match graph kernel extends its applicability to graph-structured data <ref type="bibr" target="#b93">(Nikolentzos et al., 2017b)</ref>. The kernel can handle unlabeled graphs as well as graphs that contain discrete node labels.</p><p>The pyramid match graph kernel first embeds the vertices of each graph into a lowdimensional vector space using the eigenvectors of the d largest in magnitude eigenvalues of the graph's adjacency matrix. Since the signs of these eigenvectors are arbitrary, it replaces all their components by their absolute values. Each vertex is thus a point in the d-dimensional unit hypercube. To find an approximate correspondence between the sets of vertices of two graphs, the kernel maps these points to multi-resolution histograms, and compares the emerging histograms with a weighted histogram intersection function.</p><p>Initially, the kernel partitions the feature space into regions of increasingly larger size and takes a weighted sum of the matches that occur at each level. Two points match with each other if they fall into the same region. Matches made within larger regions are weighted less than those found in smaller regions. The kernel repeatedly fits a grid with cells of increasing size to the d-dimensional unit hypercube. Each cell is related only to a specific dimension and its size along that dimension is doubled at each iteration, while its size along the other dimensions stays constant and equal to 1. Given a sequence of levels from 0 to L, then at level l, the d-dimensional unit hypercube has 2 l cells along each dimension and D = 2 l d cells in total. Given a pair of graphs G, G , let H l G and H l G denote the histograms of G and G at level l, and H l G (i), H l G (i), the number of vertices of G, G that lie in the i th cell. The number of points in two sets which match at level l is then computed using the histogram intersection function</p><formula xml:id="formula_73">I(H l G , H l G ) = D i=1 min H l G (i), H l G (i)<label>(54)</label></formula><p>The matches that occur at level l also occur at levels 0, . . . , l − 1. The algorithm takes into account only the new matches found at each level which is given by I(H l G 1 , H l G 2 ) − I(H l+1 G 1 , H l+1 G 2 ) for l = 0, . . . , L − 1. Furthermore, the number of new matches found at each level in the pyramid is weighted according to the size of that level's cells. Matches found within smaller cells are weighted more than those that occur in larger cells. Specifically, the weight for level l is set equal to 1 /2 L−l . Hence, the weights are inversely proportional to the length of the side of the cells that varies in size as the levels increase.</p><p>Definition 26 (Pyramid Match Graph Kernel). Let G = (V, E) and G = (V , E ) be two graphs. The pyramid match kernel is defined as follows</p><formula xml:id="formula_74">k(G, G ) = I(H L G , H L G ) + L−1 l=0 1 2 L−l I(H l G , H l G ) − I(H l+1 G , H l+1 G )<label>(55)</label></formula><p>where L is the number of different levels.</p><p>The complexity of the pyramid match kernel is O(dnL) where n is the number of nodes of the graphs under comparison.</p><p>In the case of labeled graphs, the kernel restricts matchings to occur only between vertices that share same labels. It represents each graph as a set of sets of vectors, and matches pairs of sets of two graphs corresponding to the same label using the pyramid match kernel. The emerging kernel for labeled graphs corresponds to the sum of the separate kernels</p><formula xml:id="formula_75">k(G, G ) = |Σ| i=1 k i (G, G )<label>(56)</label></formula><p>where |Σ| is the number of distinct labels and k i (G, G ) is the pyramid match kernel between the sets of vertices of the two graphs which are assigned the label i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Weisfeiler-Lehman Optimal Assignment Kernel</head><p>The Weisfeiler-Lehman optimal assignment kernel is currently one of the state-of-the-art algorithms for learning on graphs . The kernel capitalizes on the theory of valid assignment kernels to improve the performance of the Weisfeiler-Lehman subtree kernel. Before we delve into the details of the kernel, it is necessary to introduce the theory of valid optimal assignment kernels. Let X be a set, and [X ] n denote the set of all n-element subsets of X . Let also X, X ∈ [X ] n for n ∈ N, and B(X, X ) denote the set of all bijections between X and X . The optimal assignment kernel on [X ] n is defined as</p><formula xml:id="formula_76">K k B (X, X ) = max B∈B(X,X ) (x,x )∈B k(x, x )<label>(57)</label></formula><p>where k is a kernel between the elements of X and X . <ref type="bibr" target="#b67">Kriege et al. (2016)</ref> showed that the above function K B (X , X ) is a valid kernel only if the base kernel k is strong.</p><p>Definition 27 (Strong Kernel). A function k : X × X → R ≥0 is called strong kernel if k(x, y) ≥ min{k(x, z), k(z, y)} for all x, y, z ∈ X .</p><p>Strong kernels are equivalent to kernels obtained from a hierarchy defined on set X . More specifically, let T be a rooted tree such that the leaves of T are the elements of X . Let V (T ) be the set of vertices of T . Each inner vertex v in T corresponds to a subset of X comprising all leaves of the subtree rooted at v. Let w : V (T ) → R ≥0 be a weight function such that w(v) ≥ w(p(v)) for all v in T where p(v) is the parent of vertex v. Then, the tuple (T, w) defines a hierarchy. Let LCA(u, v) be the lowest common ancestor of vertices u and v, that is, the unique vertex with maximum depth that is an ancestor of both u and v.</p><p>Definition 28 (Hierarchy-induced Kernel). Let H = (T, w) be a hierarchy on X , then the function defined as k(x, y) = w(LCA(x, y)) for all x, y in X is the kernel on X induced by H.</p><p>Interestingly, strong kernels are equivalent to kernels obtained from a hierarchical partition of the domain of the kernel. Hence, by constructing a hierarchy on X , we can derive a strong kernel k and ensure that the emerging assignment function is a valid kernel.</p><p>Based on the property that every strong kernel is induced by a hierarchy, we can derive explicit feature maps for strong kernels.  <ref type="figure" target="#fig_0">Figure 12</ref> shows an example of a strong kernel, an associated hierarchy and the derived feature vectors. Let H = (T, w) be a hierarchy on X . As mentioned above, the hierarchy H induces a strong kernel k. Since k is strong, the function K k B defined in Equation 57 is a valid kernel. The kernel K k B can be computed in linear time in the number of vertices n of the tree T using the histogram intersection kernel <ref type="bibr" target="#b122">(Swain &amp; Ballard, 1991)</ref> as follows</p><formula xml:id="formula_77">v ∈ V (T ) are φ(v) = ω(u) if u ∈ P (v), 0 otherwise (58)</formula><formula xml:id="formula_78">K k B (X, X ) = n i=1 min H X (i), H X (i)<label>(59)</label></formula><p>which is known to be a valid kernel on R n <ref type="bibr" target="#b11">(Barla, Odone, &amp; Verri, 2003)</ref>. Hence, the complexity of the proposed kernel depends on the size of the tree T . <ref type="figure" target="#fig_0">Figure 13</ref> illustrates the relation between the optimal assignment kernel employing a strong base kernel and the histogram intersection kernel. We next present the Weisfeiler-Lehman optimal assignment kernel.</p><p>Definition 29 (Weisfeiler-Lehman Optimal Assignment Kernel). Let G = (V, E) and G = (V , E ) be two graphs. The Weisfeiler-Lehman optimal assignment kernel is defined as</p><formula xml:id="formula_79">k(G, G ) = K k B (V, V )<label>(60)</label></formula><p>where k is the following base kernel</p><formula xml:id="formula_80">k(v, v ) = h i=0 δ(τ i (v), τ i (v ))<label>(61)</label></formula><p>where τ i (v) is the label of node v at the end of the i th iteration of the Weisfeiler-Lehman relabeling procedure. The base kernel value reflects to what extent two vertices v and v have a similar neighborhood. It can be shown that the colour refinement process of the Weisfeiler-Lehman algorithm defines a hierarchy on the set of all vertices of the input graphs. Specifically, the sequence (τ i ) 0≤i≤h gives rise to a family of nested subsets, which can naturally be represented by a hierarchy (T, w). When assuming ω(v) = 1 for all vertices v ∈ V (T ), the hierarchy induces the kernel defined above. Such a hierarchy for a graph on six vertices is illustrated in <ref type="figure" target="#fig_0">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Kernels for Graphs with Continuous Attributes</head><p>Most existing graph kernels are designed to operate on both unlabeled and node-labeled graphs. However, many real-world graphs contain continuous real-valued node attributes. One example comes from the field of cybersecurity where the function call graphs extracted from the source code of application programs typically contain multi-dimensional node labels. Such types of graphs do not appear only in cybersecurity, but also in computer vision <ref type="bibr" target="#b50">(Harchaoui &amp; Bach, 2007)</ref> or even in bioinformatics , where labels may represent RGB values of colors or physical properties of protein secondary structure elements, respectively. Research in graph kernels has achieved a remarkable progress in recent years. However, it has focused mainly on unlabeled graphs and graphs with discrete node labels. For such kind of graphs, there are several highly scalable graph kernels available which can handle graphs with thousands of vertices (e. g. the Weisfeiler-Lehman subtree kernel). However, the same does not happen in the case of datasets where node labels correspond to vectors. Some existing graph kernels for node-labeled graphs such as the shortest-path kernel can be extended to handle continuous labels. However, by taking into account these labels, their computational complexity becomes prohibitive. Designing graph kernels for graphs with continuous node labels is a much less well studied problem which started to gain some attention recently <ref type="bibr" target="#b65">(Kriege &amp; Mutzel, 2012;</ref><ref type="bibr" target="#b32">Feragen, Kasenburg, Petersen, de Bruijne, &amp; Borgwardt, 2013;</ref><ref type="bibr" target="#b96">Orsini, Frasconi, &amp; De Raedt, 2015;</ref><ref type="bibr" target="#b89">Neumann, Garnett, Bauckhage, &amp; Kersting, 2016;</ref><ref type="bibr" target="#b118">Su, Han, Harang, &amp; Yan, 2016;</ref><ref type="bibr" target="#b63">Kondor &amp; Pan, 2016)</ref>. There are mainly two categories of approaches for graphs with continuous node labels: (1) those that directly handle the continuous node labels, and (2) those that first discretize the node labels and then employ existing kernels for graphs with discrete node labels. We will next present some kernels belonging to the first category. With regards to the second category, worthy of mention is the work of <ref type="bibr" target="#b87">Morris et al. (2016)</ref> where the authors propose the hash graph kernel framework which iteratively transforms continuous attributes into discrete labels using randomized hash functions, thus allowing kernels that support discrete node labels to handle node-attributed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.1">Subgraph Matching Kernel</head><p>The subgraph matching kernel counts the number of matchings between subgraphs of bounded size in two graphs <ref type="bibr" target="#b65">(Kriege &amp; Mutzel, 2012)</ref>. The kernel is very general since it can be applied to graphs that contain node labels, edge labels, node attributes or edge attributes.</p><p>Let G be a set of graphs. We assume that the graphs that are contained in the set are labeled or attributed. Specifically, let be a labeling function that assigns either discrete labels or continuous attributes to vertices and edges. A graph isomorphism between two labeled/attributed graphs G = (V, E) and G = (V , E ) is a bijection φ : V → V that preserves adjacencies, i. e. ∀v, u ∈ V : {v, u} ∈ E ⇔ {φ(v), φ(u)} ∈ E , and labels, i. e. if ψ ∈ V × V → V × V is the mapping of vertex pairs implicated by the bijection φ such that ψ({v, u}) = {φ(v), φ(u)}, then, the conditions ∀v ∈ V : (v) ≡ (φ(v)) and ∀e ∈ E :</p><p>(e) ≡ (ψ(e)) must hold, where ≡ denotes that two labels are considered equivalent.</p><p>Definition 30 (Subgraph Matching Kernel). Given two graphs G = (V, E) and G = (V , E ), let B(G, G ) denote the set of all bijections between sets S ⊆ V and S ⊆ V , and let λ : B(G, G ) → R + be a weight function. The subgraph matching kernel is defined as</p><formula xml:id="formula_81">k(G, G ) = φ∈B(G,G ) λ(φ) v∈S κ V (v, φ(v)) e∈S×S κ E (e, ψ(e))<label>(62)</label></formula><p>where S = dom(φ) and κ V , κ E are kernel functions defined on vertices and edges, respectively.</p><p>The instance of the subgraph matching kernel that is obtained if we set the κ V , κ E functions as follows</p><formula xml:id="formula_82">κ V (v, v ) = 1, if (v) ≡ (v ), 0, otherwise and κ E (e, e ) = 1, if e ∈ E ∧ e ∈ E ∧ (e) ≡ (e ) or e ∈ E ∧ e ∈ E , 0, otherwise.<label>(63)</label></formula><p>is known as the common subgraph isomorphism kernel. This kernel counts the number of isomorphic subgraphs contained in two graphs.</p><p>To count the number of isomorphisms between subgraphs, the kernel capitalizes on a classical result of <ref type="bibr" target="#b71">Levi (1973)</ref> which makes a connection between common subgraphs of two graphs and cliques in their product graph. More specifically, each maximum clique in the product graph is associated with a maximum common subgraph of the factor graphs. This allows someone to compute the common subgraph isomorphism kernel by enumerating the cliques of the product graph.</p><p>The general subgraph matching kernel extends the theory of Levi and builds a weighted product graph to allow a more flexible scoring of bijections. Given two graphs G = (V, E), G = (V , E ), and vertex and edge kernels κ V and κ E , the weighted product graph G P = (V P , E P ) of G and G is defined as</p><formula xml:id="formula_83">V P = {(v, v ) ∈ V × V : κ V (v, v ) &gt; 0} E P = {{(v, v ), (u, u )} ∈ V P × V P : v = u ∧ v = u ∧ κ E ({v, u}, {v , u }) &gt; 0} c(u) = κ V (v, v ) ∀u = (v, v ) ∈ V P c(e) = κ E ({v, u}, {v , u }) ∀e ∈ E P , where e = ({v, u}, {v , u })<label>(64)</label></formula><p>After creating the weighted product graph, the kernel enumerates its cliques. The kernel starts from an empty clique and extends it stepwise by all vertices preserving the clique property. Let w be the weight of a clique C. Whenever the clique C is extended by a new vertex v, the weight of the clique is updated as follows: first it is multiplied by the weight of the vertex w = w c(v), and then, it is multiplied by all the edges connecting v to a vertex in C, that is w = u∈C w c <ref type="bibr">({v, u})</ref>. The algorithm effectively avoids duplicates by removing a vertex from the candidate set after all cliques containing it have been exhaustively explored.</p><p>The runtime of the subgraph matching kernel depends on the number of cliques in the product graph. The worst-case runtime complexity of the kernel when considering subgraphs of size up to k is O(kn k+1 ), where n = |V | + |V | is the sum of the number of vertices of the two graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.2">GraphHopper Kernel</head><p>The GraphHopper kernel is very related to the shortest path kernel. In the case of graphs with discrete node labels, the kernels k v and k e of the shortest-path kernel which compare vertex labels and path lengths correspond typically to dirac kernels. Hence, nodes and shortest path lengths are considered similar if they are completely identical. That specific instance of the shortest path kernel allows the use of an explicit computation scheme which is very efficient, even for larger datasets. However, for attributed graphs, such an explicit mapping is no longer possible. This has a large impact on the runtime of the algorithm which is generally O(n 4 ), and makes the kernel unfeasible for many real-world applications. GraphHopper is a kernel which also compares shortest paths between node pairs from the two graphs, but with a different path kernel <ref type="bibr" target="#b32">(Feragen et al., 2013)</ref>. The kernel takes into account both path lengths and the vertices encountered while "hopping" along shortest paths. The kernel is equivalent to a weighted sum of node kernels. Moreover, it can handle both labeled and attributed graphs, and is much more efficient than the shortest-path kernel.</p><p>Let G = (V, E) be a graph. The graph contains discrete node labels, continuous node attributes or both. Let be a labeling function that assigns either discrete labels or continuous attributes to vertices. The kernel compares node labels/attributes using a kernel k n (e. g. delta kernel in the case of node labels, and linear or gaussian kernel in the case of node attributes). Given two vertices v, u ∈ V , a path π from v to u in G is defined as a sequence of vertices</p><formula xml:id="formula_84">π = [v 1 , v 2 , v 3 , . . . , v l ]<label>(65)</label></formula><p>where v 1 = v, v l = u and (v i , v i+1 ) ∈ E for all i = 1, . . . , l − 1. Let π(i) = v i denote the i th vertex encountered when "hopping" along the path. Denote by l(π) the weighted length of π and by |π| its discrete length, defined as the number of vertices in π. The shortest path π ij from v i to v j is defined in terms of weighted length. The diameter δ(G) of G is the maximal number of nodes in a shortest path in G, with respect to the weighted path length.</p><p>Definition 31 (GraphHopper Kernel). The GraphHopper kernel is defined as a sum of path kernels k p over the families P, P of shortest paths in G, G</p><formula xml:id="formula_85">k(G, G ) = π∈P π ∈P k p (π, π )<label>(66)</label></formula><p>The path kernel k p (π, π ) is a sum of node kernels k n on vertices simultaneously encountered while simultaneously hopping along paths π and π of equal discrete length, that is k p (π, π ) = |π| j=1 k n (π(j), π (j)), if |π| = |π |, 0, otherwise.</p><p>The k(G, G ) kernel can be decomposed into a weighted sum of node kernels</p><formula xml:id="formula_87">k(G, G ) = v∈V v ∈V w(v, v )k n (v, v )<label>(68)</label></formula><p>where w(v, v ) counts the number of times v and v appear at the same hop, or coordinate, i of shortest paths π, π of equal discrete length |π| = |π |. We can decompose the weight w(v, v ) as</p><formula xml:id="formula_88">w(v, v ) = δ j=1 δ i=1 |{(π, π ) : π(i) = v, π (i) = v , |π| = |π | = j}| = δ j=1 δ i=1 M v ij M v ij (69)</formula><p>where M v is a δ × δ matrix whose entry M v ij counts how many times v appears at the i th coordinate of a shortest path in G of discrete length j, and δ = max(δ(G), δ(G )). The components of these matrices can be computed efficiently using recursive message-passing algorithms. The total complexity of computing the GraphHopper kernel is O(n 2 (m + log n + d + δ 2 )) where n is the number of vertices, m is the number of edges and d is the dimensionality of the node attributes (d = 1 in the case of discrete node labels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.3">Graph Invariant Kernels</head><p>Kernels for attributed graphs have received increased attention recently, and research efforts have focused not only on new kernels, but also on frameworks for building kernels that can handle such continuous node attributes. The graph invariant kernels are instances of such a framework <ref type="bibr" target="#b96">(Orsini et al., 2015)</ref>. These kernels decompose graphs into sets of vertices, and compare them to each other using a kernel that measures their similarity both in terms of their attributes and in terms of their structural roles.</p><p>Let G be a graph. Let R be a decomposition relation that specifies a decomposition of G into its parts. Then, we denote by R −1 (G) the multiset of all patterns in G. An example of such a decomposition relation is the one that generates neighborhood subgraphs. Graph invariant kernels compare vertices of graphs based on their attributes, but also based on their structural role in subgraphs obtained using a decomposition relation.</p><p>Definition 32 (Graph Invariant Kernel). Given two attributed graphs G = (V, E) and G = (V , E ), the graph invariant kernels compare the attributes of all pairs of vertices of the two graphs using a kernel</p><formula xml:id="formula_89">k(G, G ) = v∈V v ∈V w(v, v ) k attr (v, v )<label>(70)</label></formula><p>where k attr is a kernel between vertex attributes, and w(v, v ) is a weight function defined as follows</p><formula xml:id="formula_90">w(v, v ) = g∈R −1 (G) g ∈R −1 (G ) k inv (v, v ) δ m (g, g ) |V g ||V g | 1{v ∈ V g ∧ v ∈ V g } (71)</formula><p>where δ m is a dirac function that determines whether two patterns match, V g , V g are the set of vertices of patterns g, g , and 1 is an indicator function.</p><p>If g, g are subgraphs of G, G , δ m can be a dirac function that compares the canonical representations of the subgraphs obtained by applying a labeling function which produces efficient string encodings of the subgraphs along with a hash function from strings to natural numbers. The indicator function 1{v ∈ V g ∧ v ∈ V g } from all the subgraphs extracted from the two graphs selects only those in which vertices v and v are involved into. The kernel function k inv is used to measure the similarity between the colors produced by a vertex invariant L and encodes the extent to which the vertices play the same structural role in the two subgraphs. By employing different graph invariants L, different instances of graph invariant kernels emerge. Some common graph invariants include the Weisfeiler-Lehman relabeling procedure and coloring methods that capitalize on diffusion updates. For kernels that decompose graphs into sets of subgraphs, their complexity is O n 2 (d attr +d inv n 2 |V g | 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.4">Propagation Kernel</head><p>The propagation kernel is another instance of the neighborhood aggregation framework, and in contrast to most other instances, it can handle continuous node attributes . The kernel leverages quantization in order to transform continuous node attributes to discrete labels. Similarly to the Weisfeiler-Lehman subtree kernel, the propagation kernel applies an iterative procedure which updates the node attributes, places the nodes into bins based on their attributes, and counts nodes that fall into the same bins in two graphs.</p><p>Let G = (V, E) be a node-attributed graph. Let also P 0 be a matrix whose i th row contains the intial attribute of vertex v i ∈ V . The propagation kernel first uses a hash function that maps the node attributes to integer-valued bins, such that vertices with similar attributes end up in the same bin. Hence, this function maps each row of matrix P 0 to an integer. Then, the kernel employs a propagation scheme to update the attributes of the vertices. Different schemes can be employed. A common scheme updates node attributes as follows</p><formula xml:id="formula_91">P t+1 = T P t<label>(72)</label></formula><p>where T t is the transition matrix, i. e. the row-normalized adjacency matrix T t = D −1 A, and D is a diagonal matrix with D ii = j A ij . The above two steps (hashing and update of node attributes) are performed for T iterations.</p><p>Definition 33 (Propagation Kernel). Let G, G be two node-attributed graphs. Define n i as the number of integer bins occupied by nodes of G and G after applying the hashing function to the node attributes at the i th iteration of the algorithm. Let also c t (G, i) be the number of nodes of G placed into bin i at the t th iteration of the algorithm. Then, the propagation kernel on two graphs G and G with T iterations is defined as</p><formula xml:id="formula_92">k(G, G ) = φ(G), φ(G )<label>(73)</label></formula><p>where φ(G) = (c 0 <ref type="figure" target="#fig_0">(G, 1)</ref>, . . . , c 0 (G, n 0 ), . . . , c T <ref type="figure" target="#fig_0">(G, 1)</ref>, . . . , c T (G, n T )) (74) and φ(G ) = (c 0 <ref type="figure" target="#fig_0">(G , 1)</ref>, . . . , c 0 (G , n 0 ), . . . , c T <ref type="figure" target="#fig_0">(G , 1)</ref>, . . . , c T (G , n T )) (75)</p><p>An illustration of the propagation kernel is given in <ref type="figure" target="#fig_0">Figure 15</ref>. The total runtime complexity of the propagation kernel is O (T − 1)m + T n . ) <ref type="figure" target="#fig_0">Figure 15</ref>: Propagation kernel computation. Distributions, bins, count features, and kernel contributions for two graphs G and G with binary node labels and one iteration of label propagation. Node-label distributions are decoded by color.</p><formula xml:id="formula_93">G G φ(G) = ( φ(G ) (a) Input</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.5">Multiscale Laplacian Graph Kernel</head><p>The multiscale Laplacian graph kernel can handle unlabeled graphs, graphs with discrete node labels and graphs with continuous node attributes <ref type="bibr" target="#b63">(Kondor &amp; Pan, 2016)</ref>. It takes into account structure in graphs at a range of different scales by building a hierarchy of nested subgraphs. These subgraphs are compared to each other using another graph kernel, called the feature space Laplacian graph kernel. This kernel is capable of lifting a base kernel defined on the vertices of two graphs to a kernel between the graphs themselves.</p><p>Since exact computation of the multiscale Laplacian graph kernel is a very expensive operation, the kernel uses a randomized projection procedure similar to the popular Nyström approximation for kernel matrices <ref type="bibr" target="#b135">(Williams &amp; Seeger, 2001)</ref>.</p><p>Let G = (V, E) be an undirected graph such that n = |V | and let L be the Laplacian of G. Given two graphs G 1 and G 2 of n vertices, we can define the kernel between them to be a kernel between the corresponding normal distributions p 1 = N (0, L −1 1 ) and p 2 = N (0, L −1 2 ) where 0 is the n-dimensional all-zeros vector. Note that the Laplacian matrices of the two graphs have a zero eigenvalue eigenvector. Hence, in order to be able to invert them, the algorithm adds a small constant "regularizer" ηI to them. In the following, we denote the regularized Laplacians of G 1 and G 2 by L 1 and L 2 , respectively. More specifically, given two graphs G 1 and G 2 of n vertices with regularized Laplacians L 1 and L 2 respectively, the Laplacian graph kernel with parameter γ between the two graphs is <ref type="bibr">76)</ref> where S 1 = L −1 1 + γI, S 2 = L −1 2 + γI and I is the n × n identity matrix. The Laplacian graph kernel captures similarity between the overall shapes of the two graphs. However, it assumes that both graphs have the same size, and it is not invariant to permutations of the vertices.</p><formula xml:id="formula_94">k LG (G 1 , G 2 ) = |( 1 2 S −1 1 + 1 2 S −1 2 ) −1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(</label></formula><p>To achieve permutation invariance, the multiscale Laplacian graph kernel represents each vertex as a d-dimensional vector whose components correspond to local and permutation invariant vertex features. Such features may include for instance the degree of the vertex or the number of triangles in which it participates. Then, it performs a linear transformation and represents each graph as a distribution of the considered features instead of a distribution of its vertices. Let U 1 , U 2 ∈ R d×n be the feature mapping matrices of the two graphs, that is the matrices whose columns contain the vector representations of the vertices of the two graphs. Then, the feature space Laplacian graph kernel is defined as</p><formula xml:id="formula_95">k F LG (G 1 , G 2 ) = |( 1 2 S −1 1 + 1 2 S −1 2 ) −1 | 1/2 |S 1 | 1/4 |S 2 | 1/4 (77) where S 1 = U 1 L −1 1 U 1 + γI, S 2 = U 2 L −1 2 U 2 + γI and I is the d × d identity matrix.</formula><p>Since the vertex features are local and invariant to vertex reordering, the feature space Laplacian graph kernel is permutation invariant. Furthermore, since the distributions now live in the space of features rather than the space of vertices, the feature space Laplacian graph kernel can be applied to graphs of different sizes.</p><p>Let φ(v) be the representation of vertex v constructed from local vertex features as described above. The base kernel κ between two vertices v 1 and v 2 corresponds to the dot product of their feature vectors</p><formula xml:id="formula_96">κ(v 1 , v 2 ) = φ(v 1 ) φ(v 2 )<label>(78)</label></formula><p>Let G 1 and G 2 be two graphs with vertex sets V 1 = {v 1 , . . . , v n 1 } and V 2 = {u 1 , . . . , u n 2 } respectively, and letV = {v 1 , . . . ,v n 1 +n 2 } be the union of the two vertex sets. Let also K ∈ R (n 1 +n 2 )×(n 1 +n 2 ) be the kernel matrix defined as</p><formula xml:id="formula_97">K ij = κ(v i ,v j ) = φ(v i ) φ(v j )<label>(79)</label></formula><p>Let u 1 , . . . , u p be a maximal orthonormal set of the non-zero eigenvalue eigenvectors of K with corresponding eigenvalues λ 1 , . . . , λ p . Then the vectors</p><formula xml:id="formula_98">ξ i = 1 √ λ i n 1 +n 2 l=1 [u i ] l φ(v l )<label>(80)</label></formula><p>where [u i ] l is the l th component of vector u i form an orthonormal basis for the subspace {φ(v 1 ), . . . , φ(v n 1 +n 2 )}. Moreover, let Q = [λ 1/2 1 u 1 , . . . , λ 1/2 p u p ] ∈ R p×p and Q 1 , Q 2 denote the first n 1 and last n 2 rows of matrix Q respectively. Then, the generalized feature space Laplacian graph kernel induced from the base kernel κ is defined as</p><formula xml:id="formula_99">k κ F LG (G 1 , G 2 ) = |( 1 2 S −1 1 + 1 2 S −1 2 ) −1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(81)</label></formula><p>where S 1 = Q 1 L −1 1 Q 1 + γI and S 2 = Q 2 L −1 2 Q 2 + γI where I is the p × p identity matrix. The multiscale Laplacian graph kernel builds a hierarchy of nested subgraphs, where each subgraph is centered around a vertex and computes the generalized feature space Laplacian graph kernel between every pair of these subgraphs. Let G be a graph with vertex set V , and κ a positive semi-definite kernel on V . Assume that for each v ∈ V , we have a nested sequence of L neighborhoods</p><formula xml:id="formula_100">v ∈ N 1 (v) ⊆ N 2 (v) ⊆ . . . ⊆ N lmax (v)<label>(82)</label></formula><p>and for each N l (v), let G l (v) be the corresponding induced subgraph of G. The multiscale Laplacian subgraph kernels are defined as K 1 , . . . , K lmax : V × V → R as follows 1. K 1 is just the generalized feature space Laplacian graph kernel k κ F LG induced from the base kernel κ between the lowest level subgraphs (i. e. the vertices)</p><formula xml:id="formula_101">K 1 (v, u) = k κ F LG (v, u)<label>(83)</label></formula><p>2. For l = 2, 3, . . . , l max , K l is the the generalized feature space Laplacian graph kernel induced from K l−1 between G l (v) and G l (u)</p><formula xml:id="formula_102">K l (v, u) = k K l−1 F LG G l (v), G l (u)<label>(84)</label></formula><p>Definition 34 (Multiscale Laplacian Graph Kernel). Let G 1 , G 2 be two graphs. The multiscale Laplacian graph kernel between the two graphs is defined as follows</p><formula xml:id="formula_103">k(G 1 , G 2 ) = k K lmax F LG (G 1 , G 2 )<label>(85)</label></formula><p>The multiscale Laplacian graph kernel computes K 1 for all pairs of vertices, then computes K 2 for all pairs of vertices, and so on. Hence, it requires O(n 2 l max ) kernel evaluations. At the top levels of the hierarchy each subgraph centered around a vertex G l (v) may have as many as n vertices. Therefore, the cost of a single evaluation of the generalized feature space Laplacian graph kernel may take O(n 3 ) time. This means that in the worst case, the overall cost of computing k is O(n 5 l max ). Given a dataset of N graphs, computing the kernel matrix requires repeating this for all pairs of graphs, which takes O(N 2 n 5 l max ) time and is clearly problematic for real-world settings.</p><p>The solution to this issue is to compute for each level l = 1, 2, . . . , l max + 1 a single joint basis for all subgraphs at the given level across all graphs. Let G 1 , G 2 , . . . , G N be a collection of graphs, V 1 , V 2 , . . . , V N their vertex sets, and assume that V 1 , V 2 , . . . , V N ⊆ V for some general vertex space V. The joint vertex feature space of the whole graph collection</p><formula xml:id="formula_104">is W = span N i=1 v∈V i {φ(v)} . Let c = N i=1</formula><p>|V i | be the total number of vertices and V = (v 1 , . . . ,v c ) be the concatenation of the vertex sets of all graphs. Let K be the corresponding joint kernel matrix and u 1 , . . . , u p be a maximal orthonormal set of non-zero eigenvalue eigenvectors of K with corresponding eigenvalues λ 1 , . . . , λ p and p = dim(W ). Then the vectors</p><formula xml:id="formula_105">ξ i = 1 √ λ i c l=1 [u i ] l φ(v l ) i = 1, . . . , p<label>(86)</label></formula><p>form an orthonormal basis for W . Moreover, let Q = [λ 1/2 1 u 1 , . . . , λ 1/2 p u p ] ∈ R p×p and Q 1 denote the first n 1 rows of matrix Q, Q 2 denote the next n 2 rows of matrix Q and so on. For any pair of graphs G i , G j of the collection, the generalized feature space Laplacian graph kernel induced from κ can be expressed as</p><formula xml:id="formula_106">k κ F LG (G i , G j ) = |( 1 2S −1 i + 1 2S −1 j ) −1 | 1/2 |S i | 1/4 |S j | 1/4<label>(87)</label></formula><p>whereS i = Q i L −1 i Q i + γI,S j = Q j L −1 j Q j + γI and I is the p × p identity matrix. Computing the kernel matrix between all vertices of all graphs (c vertices in total) and storing it is a very costly procedure. Computing its eigendecomposition is even worse in terms of the required runtime. Morever, p is also very large. Hence, managing thē S 1 , . . . ,S N matrices (each of which is of size p × p) becomes infeasible. Hence, the multiscale Laplacian graph kernel replaces W with a smaller, approximate joint features space. LetṼ = (ṽ 1 , . . . ,ṽc) bec c vertices sampled from the joint vertex set. Then, the corresponding subsampled vertex feature space isW = span{φ(v) : v ∈Ṽ }. Letp = dim(W ). Similarly to before, the kernel constructs an orthonormal basis {ξ 1 , . . . , ξp} forW by forming the (now much smaller) kernel matrix K ij = κ(ṽ i ,ṽ j ), computing its eigenvalues and eigenvectors, and setting ξ i = 1</p><formula xml:id="formula_107">√ λ i c l=1 [u i ] l φ(ṽ l ).</formula><p>The resulting approximate generalized feature space Laplacian graph kernel is</p><formula xml:id="formula_108">k κ F LG (G 1 , G 2 ) = |( 1 2S −1 1 + 1 2S −1 2 ) −1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(88)</label></formula><p>whereS 1 =Q 1 L −1 1Q 1 +γI,S 2 =Q 2 L −1 2Q 2 +γI are the projections ofS 1 andS 2 toW and I is thep×p identity matrix. Finally, the kernel introduces a further layer of approximation by restrictingW to be the space spanned by the firstp &lt;p basis vectors (ordered by descending eigenvalue), effectively doing kernel PCA on {φ(ṽ)}ṽ ∈Ṽ . The combination of these two factors makes computing the entire stack of kernels feasible, reducing the complexity of computing the kernel matrix for a dataset of N graphs to O(Nc 2p3 l max + Nc 3 l max + N 2p3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Frameworks</head><p>Besides the kernels themselves, research on graph kernels has also focused on frameworks and approaches that can be applied to existing graph kernels and increase their performance. The most popular of all frameworks is perhaps the Weisfeiler-Lehman framework which has been already presented <ref type="bibr" target="#b110">(Shervashidze et al., 2011)</ref>. Interestingly, any kernel that can handle discrete node labels can be plugged into that framework. Recently, two other frameworks were presented for deriving variants of popular R-convolution graph kernels <ref type="bibr" target="#b138">(Yanardag &amp; Vishwanathan, 2015b</ref><ref type="bibr" target="#b137">, 2015a</ref>. Inspired by recent advances in NLP, these frameworks offer a way to take into account similarity between substructures. In addition, a method that combines several kernels using the multiple kernel learning framework was also recently proposed <ref type="bibr" target="#b0">(Aiolli, Donini, Navarin, &amp; Sperduti, 2015)</ref>. Another recently proposed framework generates a hierarchy of subgraphs and compares the corresponding according to the hierarchy subgraphs using graph kernels <ref type="bibr" target="#b91">(Nikolentzos, Meladianos, Limnios, &amp; Vazirgiannis, 2018)</ref>. Moreover, a recent approach employs graph kernels and performs a series of successive embeddings in order to derive more expressive kernels . Some of these frameworks are described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.1">Frameworks Dealing with Diagonal Dominance</head><p>We next present two frameworks that are inspired by recent advances in natural language processing, namely the deep graph kernels framework <ref type="bibr" target="#b138">(Yanardag &amp; Vishwanathan, 2015b)</ref> and the structural smoothing framework <ref type="bibr" target="#b137">(Yanardag &amp; Vishwanathan, 2015a)</ref>. These two frameworks were developed to address the problem of diagonal dominance which is inherent to R-convolution kernels. The feature space of these kernels is usually large (i. e. grows exponentially) and we encounter the sparsity problem: only a few substructures will be common across graphs, and therefore each graph is similar to itself, but not to any other graph in the dataset. However, the substructures used to define a graph kernel are often related to each other, but commonly-used R-convolution kernels respect only exact matchings. For example, when the features correspond to large graphlets (e. g. k ≥ 5), two graphs may be composed of many similar graphlets, but not any identical. As a consequence, the kernel value between the two graphs (i. e. inner product of their feature representations) will be equal to 0 even though the two graphs are similar to each other.</p><p>Ideally, we would like the kernels to output large values for pairs of graphs that belong to the class, and lower values for pairs of graphs that belong to different classes. To deal with the aforementioned problem, the deep graph kernels framework computes the kernel between two graphs G and G as follows</p><formula xml:id="formula_109">k(G, G ) = φ(G) M φ(G )<label>(89)</label></formula><p>where M represents a positive semidefinite matrix that encodes the relationship between substructures and φ(G), φ(G ) are the representations of graphs G, G according to a graph kernel which contains counts of atomic substructures. Therefore, one can design an M matrix that respects the similarity of the substructure space. Clearly, the deep graph kernels framework can be applied only to graph kernels whose feature maps φ can be computed explicitly.</p><p>Matrix M can be generated by manually defining functions to compare substructures or alternatively, it can be learned using techniques inspired from the field of natural language processing. When substructures exhibit a clear mathematical relationship, one can define a function to measure the similarities between them (e. g. edit distance in the case of graphlets). However, the above approach requires manually designing the similarity functions. Furthermore, in many cases, it becomes prohibitively expensive to compare all pairs of substructures. On the other hand, learning the latent representations of substructures is more efficient and does not involve any manual intervention. Matrix M can then be computed based on the learned representations. To learn a latent representation for each substructure, the framework utilizes recent approaches for generating word embeddings such as the continuous bag-of-words (CBOW) and Skip-gram models <ref type="bibr" target="#b84">(Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013)</ref>. These models generate semantic representations from word co-occurrence statistics derived from large text corpora. However, unlike words in a traditional text corpora, substructures of graphs do not have a linear co-occurrence relationship. Hence, these co-occurrence relationships need to be manually defined. <ref type="bibr" target="#b138">Yanardag and Vishwanathan (2015b)</ref> propose a methodology on how to generate corpora where co-occurrence relationship is meaningful on three popular kernels, namely the Weisfeiler-Lehman subtree kernel, the graphlet kernel, and the shortest path kernel.</p><p>The structural smoothing framework is inspired by recent smoothing techniques in natural language processing. Similar to the deep graph kernels framework, this framework also can be applied only to graph kernels whose feature maps φ can be computed explicitly. The framework takes structural similarity into account by constructing a directed acyclic graph (DAG) that encodes the relationships between lower and higher order substructures. Each vertex of the DAG corresponds to a substructure (and also to a feature in the explicit graph representation). For each substructure s of size k, the framework determines all possible substructures of size k − 1 into which s can be reduced. These substructures are the parents of s, and a weighted directed edge is drawn from each parent to its children vertices. Since all descendants of a given substructure at depth k − 1 are at depth k, the emerging graph is indeed a DAG. <ref type="bibr" target="#b138">Yanardag and Vishwanathan (2015b)</ref> present how such a DAG can be constructed for three popular graph kernels, namely the Weisfeiler-Lehman subtree kernel, the graphlet kernel, and the shortest path kernel. Given the DAG, the structural smoothing for a substructure s at level k is defined as</p><formula xml:id="formula_110">P k SS (s) = max(c s − d, 0) m + dm d m p∈Ps P k−1 SS (p) w ps c∈Cp w pc<label>(90)</label></formula><p>where c i denotes the number of times substructure i appears in the graph, m = i c i denotes the total number of substructures present in the graph, d &gt; 0 is a discount factor, m d = |{i : c i &gt; d}| is the number of substructures whose counts are larger than d, w ij denotes the weight of the edge connecting vertex i to vertex j, P s denotes the parents of vertex s, and C p the children of vertex p. The above equation subtracts a fixed discount factor d from every substructure that appears in the graph, and accumulates it to a total mass of dm d . Each substructure s receives some portion of this accumulated probability mass from its parents. The proportion of the mass that a parent p at level k − 1 transmits to a given child a depends on the weight w ps between the parent and the child, and the probability mass P k−1 SS (p) that is assigned to the parent. It is thus clear that, even if a graph does not contain a substructure s (i. e. c s = 0), its value in the feature vector may become greater than 0 (i. e. P SS (s) &gt; 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.2">Core Framework</head><p>The core framework is another tool for improving the performance of graph kernels . The framework is not restricted to graph kernels, but can be applied to any graph comparison algorithm. It capitalizes on the k-core decomposition which is capable of uncovering topological and hierarchical properties of graphs. Specifically, the k-core decomposition is a powerful tool for network analysis and it is commonly used as a measure of importance and well connectedness for vertices in a broad spectrum of applications. The notion of k-core was first introduced by Seidman to study the cohesion of social networks <ref type="bibr" target="#b107">(Seidman, 1983)</ref>. In recent years, the k-core decomposition has been established as a standard tool in many application domains such as in network visualization (Alvarez-Hamelin, Dall'Asta, <ref type="bibr" target="#b5">Barrat, &amp; Vespignani, 2006)</ref>.</p><p>Core Decomposition. Let G = (V, E) be an undirected and unweighted graph. Given a subset of vertices S ⊆ V , let E(S) be the set of edges that have both end-points in S. Then, G = (S, E(S)) is the subgraph induced by S. We use G ⊆ G to denote that G is a subgraph of G. The degree of a vertex v ∈ S, d G (v), is equal to the number of vertices that are adjacent to v in G . Let G be a graph and G a subgraph of G induced by a set of vertices S. Then, G is defined to be a k-core of G, denoted by C k , if it is a maximal subgraph of G in which all vertices have degree at least k. Hence, if G is a k-core of G, then ∀v ∈ S, d G (v) ≥ k. Each k-core is a unique subgraph of G, and it is not necessarily connected. The core number c(v) of a vertex v is equal to the highest-order core that v belongs to. In other words, v has core number c(v) = k, if it belongs to the k-core but not to the (k + 1)-core. The degeneracy δ * (G) of a graph G is defined as the maximum k for which graph G contains a non-empty k-core subgraph, δ * (G) = max v∈V c(v). Furthermore, assuming that C = {C 0 , C 1 , . . . , C δ * (G) } is the set of all k-cores, then C forms a nested chain</p><formula xml:id="formula_111">C δ * (G) ⊆ . . . ⊆ C 1 ⊆ C 0 = G<label>(91)</label></formula><p>Therefore, the k-core decomposition is a very useful tool for discovering the hierarchical structure of graphs. The k-core decomposition of a graph can be computed in O(n + m) time <ref type="bibr" target="#b82">(Matula &amp; Beck, 1983;</ref><ref type="bibr" target="#b12">Batagelj &amp; Zaveršnik, 2011)</ref>. The underlying idea is that we can obtain the i-core of a graph if we recursively remove all vertices with degree less than i and their incident edges from the graph until no other vertex can be removed.</p><p>Core Kernels. The k-core decomposition builds a hierarchy of nested subgraphs, each having stronger connectedness properties compared to the previous ones. The core framework measures the similarity between the corresponding according to the hierarchy subgraphs and aggregates the results. Let G = (V, E) and G = (V , E ) be two graphs. Let also k be any kernel for graphs. Then, the core variant of the base kernel k is defined as</p><formula xml:id="formula_112">k c (G, G ) = k(C 0 , C 0 ) + k(C 1 , C 1 ) + . . . + k(C δ * min , C δ * min )<label>(92)</label></formula><p>where δ * min is the minimum of the degeneracies of the two graphs, and C 0 , C 1 , . . . , C δ * min and C 0 , C 1 , . . . , C δ * min are the 0-core, 1-core,. . ., δ * min -core subgraphs of G and G , respectively. By decomposing graphs into subgraphs of increasing importance, the algorithm is capable of more accurately capturing their underlying structure.</p><p>The computational complexity of the core framework depends on the complexity of the base kernel and the degeneracy of the graphs under comparison. Given a pair of graphs G, G and an algorithm A for comparing the two graphs, let O A be the time complexity of algorithm A. Let also δ * min = min δ * (G), δ * (G ) be the minimum of the degeneracies of the two graphs. Then, the complexity of computing the core variant of algorithm A is</p><formula xml:id="formula_113">O c = δ * min O A .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications of Graph Kernels</head><p>In the past years, graph kernels have been applied successfully to a series of real-world problems. Most of these problems come from the fields of bioinformatics and chemoinformatics. However, graph kernels are not limited only to these two fields, but they have been applied to problems arising in other domains as well. We list below some examples of such fields of application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Chemoinformatics</head><p>Traditionally, chemistry is one of the richest sources of graph-structured data. A common problem in this field is to find chemical compounds with a specific property or activity. The experimental characterization of molecules is often an expensive and time-consuming process, and thus people usually resort to computational methods. Specifically, they model chemical compounds as graphs where vertices correspond to atoms and edges to bonds, and then they apply computational methods to identify a small set of potentially interesting molecules for a given property or activity, which are then tested experimentally. Graph kernels have been used extensively for predicting the mutagenicity, toxicity and anti-cancer activity of small molecules <ref type="bibr" target="#b123">(Swamidass, Chen, Bruand, Phung, Ralaivola, &amp; Baldi, 2005;</ref><ref type="bibr" target="#b98">Ralaivola, Swamidass, Saigo, &amp; Baldi, 2005;</ref><ref type="bibr" target="#b79">Mahé, Ueda, Akutsu, Perret, &amp; Vert, 2005;</ref><ref type="bibr" target="#b22">Ceroni, Costa, &amp; Frasconi, 2007;</ref><ref type="bibr" target="#b80">Mahé &amp; Vert, 2009;</ref><ref type="bibr" target="#b114">Smalter, Huan, &amp; Lushington, 2009</ref>) Furthermore, graph kernels have been applied to other problems such as the prediction of the atomization energies of organic molecules <ref type="bibr" target="#b33">(Ferré, Haut, &amp; Barros, 2017)</ref>, the predicition of the boiling points of molecules <ref type="bibr" target="#b41">(Gaüzère, Brun, &amp; Villemin, 2011)</ref>, the prediction of the activity against HIV <ref type="bibr" target="#b41">(Gaüzère et al., 2011)</ref>, and the prediction of properties of stereoisomers <ref type="bibr" target="#b20">(Brown, Urata, Tamura, Arai, Kawabata, &amp; Akutsu, 2010;</ref><ref type="bibr" target="#b48">Grenier, Brun, &amp; Villemin, 2017)</ref>. A review of the applications of graph kernels in chemoinformatics is provided by <ref type="bibr" target="#b101">Rupp and Schneider (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bioinformatics</head><p>Bioinformatics is also one of the major application domains of graph representations and therefore, of graph kernels. Recent advances in technology have delivered a step change in our ability to sequence genomes, measure gene expression levels, and test large numbers of potential regulatory interactions between genes. Despite these advancements, some problems of high interest such as the experimental determination of the function of a protein still remain both expensive and time-consuming. Interestingly, the above-mentioned processes produce large volumes of data which can give rise to various types of graphs, such as protein structures, protein and gene co-expression networks, or protein-protein interaction networks. These graphs can then be processed by computational approaches such as graph kernels, and provide solutions to some of these challenging problems. Among others, in the field of bioinformatics, graph kernels have been applied to the prediction of the function of proteins with known sequence and structure <ref type="bibr" target="#b104">Schietgat, Fannes, &amp; Ramon, 2015)</ref>, to the identification of the interactions that are involved in disease outbreak and progression <ref type="bibr" target="#b17">(Borgwardt, Kriegel, Vishwanathan, &amp; Schraudolph, 2007)</ref>, to the analysis of functional non-coding RNA sequences <ref type="bibr" target="#b102">(Sato, Mituyama, Asai, &amp; Sakakibara, 2008)</ref>, to the identification of temporally localized relationships among genes <ref type="bibr" target="#b8">(Antoniotti, Carreras, Farinaccio, Mauri, Merico, &amp; Zoppis, 2010)</ref>, and to the prediction of domain-peptide interactions <ref type="bibr" target="#b69">(Kundu, Costa, &amp; Backofen, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Computer Vision</head><p>Graph representations have been investigated a lot in the fields of image processing and computer vision. There exist many different approaches to represent images as graphs. For instance, vertices usually correspond to pixels or to segmented regions, while edges join neighboring pixels or neighboring regions with each other. Graph kernels have served as an effective tool for many computer vision tasks such as for classifying images <ref type="bibr" target="#b50">(Harchaoui &amp; Bach, 2007;</ref><ref type="bibr" target="#b77">Mahboubi, Brun, &amp; Dupé, 2010;</ref><ref type="bibr" target="#b7">Antanas, Frasconi, Costa, Tuytelaars, &amp; De Raedt, 2012;</ref><ref type="bibr" target="#b139">Zhang, Song, Liu, Bu, &amp; Chen, 2013)</ref>, for detecting objects represented as point clouds <ref type="bibr" target="#b10">(Bach, 2008;</ref><ref type="bibr" target="#b90">Neumann, Moreno, Antanas, Garnett, &amp; Kersting, 2013)</ref>, for achieving place recognition <ref type="bibr" target="#b117">(Stumm, Mei, Lacroix, Nieto, Hutter, &amp; Siegwart, 2016)</ref>, for achieving action recognition <ref type="bibr" target="#b132">(Wang &amp; Sahbi, 2013;</ref><ref type="bibr" target="#b136">Wu, Yuan, &amp; Hu, 2014;</ref><ref type="bibr" target="#b73">Li, Leung, Liu, &amp; Zhou, 2016a)</ref>, for scene modeling <ref type="bibr" target="#b35">(Fisher, Savva, &amp; Hanrahan, 2011)</ref>, and for matching observations of persons across different cameras <ref type="bibr" target="#b21">(Brun, Conte, Foggia, &amp; Vento, 2011)</ref>.</p><p>Besides the above applications, graphs are also used increasingly often in biomedical imaging. Different types of graphs such as connectivity graphs are usually extracted from functional magnetic resonance imaging (fMRI) data. Then, graph kernels capitalize on these graphs to address various tasks such as to distinguish between different brain states <ref type="bibr" target="#b108">(Shahnazian, Mokhtari, &amp; Hossein-Zadeh, 2012;</ref><ref type="bibr" target="#b85">Mokhtari &amp; Hossein-Zadeh, 2013;</ref><ref type="bibr" target="#b126">Vega-Pons &amp; Avesani, 2013;</ref><ref type="bibr" target="#b127">Vega-Pons, Avesani, Andric, &amp; Hasson, 2014)</ref>, to determine whether a subject is cocaine-addicted or not <ref type="bibr" target="#b43">(Gkirtzou &amp; Blaschko, 2016)</ref>, or to predict mild cognitive impairment, a prodromal stage of Alzheimer's disease <ref type="bibr" target="#b57">(Jie, Zhang, Wee, &amp; Shen, 2014;</ref><ref type="bibr" target="#b56">Jie, Liu, Jiang, &amp; Zhang, 2016)</ref>. There have also been proposed kernels that can handle intersubject variability in fMRI data <ref type="bibr" target="#b124">(Takerkart, Auzias, Thirion, &amp; Ralaivola, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cybersecurity and Software Verification</head><p>The number of malicious applications targeting desktop and mobile devices has exploded in the past few years. Due to this unprecedented increase in the number of malicious applications, malware detection has recently become a very active area of research. It has been observed that most newly discovered malware samples are variations of existing malware. Furthermore, it has been shown that it is easier to detect these variations if high-level code representations, such as function call graphs or control flow graphs, are employed. It should be mentioned that these graphs can prove useful not only for detecting malware, but also for retrieving similar application programs. Therefore, graph kernels can be applied to such graphs, and have served as a common tool for detecting malware <ref type="bibr" target="#b6">(Anderson et al., 2011;</ref><ref type="bibr" target="#b40">Gascon, Yamaguchi, Arp, &amp; Rieck, 2013;</ref><ref type="bibr" target="#b88">Narayanan, Meng, Yang, Liu, &amp; Chen, 2016)</ref>, but also for analyzing execution traces obtained from dynamic analysis <ref type="bibr" target="#b130">(Wagner, Wagener, State, &amp; Engel, 2009</ref>), for measuring the similarity between programs <ref type="bibr" target="#b74">(Li, Saidi, Sanchez, Schäf, &amp; Schweitzer, 2016b)</ref>, and for predicting metamorphic relations <ref type="bibr" target="#b59">(Kanewala, Bieman, &amp; Ben-Hur, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Natural Language Processing</head><p>Although textual documents do not exhibit an underlying graph structure, in many cases, they are also modeled as graphs. A vertex corresponds to some meaningful linguistic unit such as a sentence, a word, or even a character, while an edge corresponds to some relationship between two vertices which can be statistical, syntactic, or semantic among others. A common representation is the word co-occurence network, where vertices correspond to terms and edges represent co-occurrences between the terms within a fixed-size sliding window. This representation addresses some of the limitations of the bag-of-words representation which treats terms as independent of one another. Graph kernels have proven useful for several text mining applications such as for recognizing identical real-world events modeled as event graphs <ref type="bibr" target="#b44">(Glavaš &amp;Šnajder, 2013)</ref>, for classifying biomedical text documents represented as concept graphs <ref type="bibr" target="#b13">(Bleik, Mishra, Huan, &amp; Song, 2013)</ref>, for extracting protein-protein interactions from scientific literature <ref type="bibr" target="#b1">(Airola, Pyysalo, Björne, Pahikkala, Ginter, &amp; Salakoski, 2008a</ref><ref type="bibr" target="#b2">, 2008b</ref>, and for measuring the similarity between documents represented as word co-occurence networks <ref type="bibr" target="#b92">(Nikolentzos, Meladianos, Rousseau, Stavrakas, &amp; Vazirgiannis, 2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Others</head><p>Graph kernels have been applied to many other practical problems involving graph representations such as for classifying Resource Description Framework (RDF) data <ref type="bibr" target="#b75">(Lösch, Bloehdorn, &amp; Rettinger, 2012;</ref><ref type="bibr" target="#b27">De Vries &amp; de Rooij, 2015)</ref>, for entity disambiguation in anonymized graphs <ref type="bibr" target="#b52">(Hermansson, Kerola, Johansson, Jethava, &amp; Dubhashi, 2013)</ref>, for classifying architectural designs into architectural styles <ref type="bibr" target="#b116">(Strobbe, Verstraeten, De Meyer, Van Campenhout, et al., 2016)</ref>, and for estimating the similarity of relational states in relational reinforcement learning <ref type="bibr" target="#b31">(Driessens, Ramon, &amp; Gärtner, 2006;</ref><ref type="bibr" target="#b49">Halbritter &amp; Geibel, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Comparison</head><p>In this Section, we experimentally evaluate many of the graph kernels presented above and compare them to each other. Although there are approaches that measure the expressiveness of graph kernels by exploiting recent results in the field of statistical learning theory <ref type="bibr" target="#b95">(Oneto, Navarin, Donini, Sperduti, Aiolli, &amp; Anguita, 2017)</ref>, empirically evaluating the graph kernels can provide insights into their utility in real-world scenarios. We first present the problem of graph classification. We then describe the datasets that we used for our experiments, and give details about the experimental settings. Finally, we report on the performance and running time of the different kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Graph Classification</head><p>Classification is perhaps the most frequently encountered machine learning problem. In classification, the goal is to learn a mapping from inputs to outputs, given a training set. When the input objects are graphs, the problem is called graph classification. More formally, in this setting, we are given a training set D = {(G i , y i )} N i=1 consisting of N graphs along with their class labels. The goal is to learn a function f : G → Y, where G is the input  space of graphs and Y the set of graph labels. This function can then be used to assign class labels to new previously unseen graphs, such as those contained in the test set. The problem of graph classification has become a popular area of research in recent years because it finds numerous applications in a wide variety of fields. Several of these application have already been discussed above. For example, graph classification arises in applications which range from predicting the mutagenicity of a chemical compound , and predicting the function of a protein given its amino acid sequence , to detecting if a software object is infected with malware <ref type="bibr" target="#b130">(Wagner et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Datasets</head><p>We next briefly describe the graph datasets used in our experiments. We have considered data from different domains, including chemoinformatics, bioinformatics and social networks. All graphs are undirected. Furtermore, the graphs contained in the chemoinformatics and bioinformatics datasets are node-labeled, node-attributed or both. All datasets are publicly available <ref type="bibr" target="#b62">(Kersting, Kriege, Morris, Mutzel, &amp; Neumann, 2016)</ref>. <ref type="table" target="#tab_6">Table 3</ref> provides a summary of the employed datasets.</p><p>AIDS. It consists of molecular compounds represented as graphs. The compounds were obtained from the AIDS Antiviral Screen Database of Active Compounds. Vertices correspond to atoms and edges to covalent bonds. Vertices are labeled with the corresponding chemical symbol and edges with the valence of the linkage. The task is to predict whether or not each compound is active against HIV <ref type="bibr" target="#b100">(Riesen &amp; Bunke, 2008)</ref>.</p><p>BZR. It contains 405 chemical compounds (ligands for the benzodiazepine receptor) which are modeled as graphs. The task is to predict whether a compound is active or inactive <ref type="bibr" target="#b121">(Sutherland, O'Brien, &amp; Weaver, 2003)</ref>.</p><p>COLLAB. This is a scientific collaboration dataset consisting of the ego-networks of several researchers from three subfields of Physics (High Energy Physics, Condensed Matter Physics and Astro Physics). The task is to determine the subfield of Physics to which the ego-network of each researcher belongs <ref type="bibr" target="#b138">(Yanardag &amp; Vishwanathan, 2015b)</ref>.</p><p>D&amp;D. This dataset contains over a thousand protein structures. Each protein is a graph whose vertices correspond to amino acids and a pair of amino acids are connected by an edge if they are less than 6Ångstroms apart. The task is to predict if a protein is an enzyme or not <ref type="bibr" target="#b29">(Dobson &amp; Doig, 2003)</ref>.</p><p>ENZYMES. It comprises of 600 protein tertiary structures obtained from the BRENDA enzyme database. Each enzyme is a member of one of the Enzyme Commission top level enzyme classes (EC classes) and the task is to correctly assign the enzymes to their classes .</p><p>IMDB-BINARY and IMDB-MULTI. These datasets were created from IMDb (www. imdb.com), an online database of information related to movies and television programs. The graphs contained in the two datasets correspond to movie collaborations. The vertices of each graph represent actors/actresses and two vertices are connected by an edge if the corresponding actors/actresses appear in the same movie. Each graph is the ego-network of an actor/actress, and the task is to predict which genre an ego-network belongs to <ref type="bibr" target="#b138">(Yanardag &amp; Vishwanathan, 2015b)</ref>.</p><p>MUTAG. This dataset consists of 188 mutagenic aromatic and heteroaromatic nitro compounds. The task is to predict whether or not each chemical compound has mutagenic effect on the Gram-negative bacterium Salmonella typhimurium <ref type="bibr" target="#b28">(Debnath, Lopez de Compadre, Debnath, Shusterman, &amp; Hansch, 1991)</ref>.</p><p>NCI1. This dataset contains a few thousand chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines <ref type="bibr" target="#b131">(Wale, Watson, &amp; Karypis, 2008)</ref>.</p><p>PROTEINS, PROTEINS full. They contain proteins represented as graphs where vertices are secondary structure elements and there is an edge between two vertices if they are neighbors in the amino-acid sequence or in 3D space. The task is to classify proteins into enzymes and non-enzymes .</p><p>PTC-MR. This dataset contains 344 organic molecules represented as graphs. The task is to predict their carcinogenic effects on male rats <ref type="bibr" target="#b125">(Toivonen, Srinivasan, King, Kramer, &amp; Helma, 2003)</ref>.</p><p>REDDIT-BINARY, REDDIT-MULTI-5K, REDDIT-MULTI-12K. The graphs contained in these three datasets represent social interaction between users of Reddit (www. reddit.com), one of the most popular social media websites. Each graph represents an online discussion thread. Specifically, each vertex corresponds to a user, and two users are connected by an edge if one of them responded to at least one of the other's comments. The task is to classify graphs into either communities or subreddits <ref type="bibr" target="#b138">(Yanardag &amp; Vishwanathan, 2015b)</ref>.</p><p>SYNTHETICnew. It comprises of 300 synthetic graphs divided into two classes of equal size. Each graph is obtained by adding noise to a random graph with 100 nodes and 196 edges, whose vertices are endowed with normally distributed scalar attributes sampled from N (0, 1). The graphs of the first class were generated by rewiring 5 edges and permuting 10 node attributes, while the graphs of the second class were generated by rewiring 10 edges and permuting 5 node attributes. After the generation of all graphs, noise from N (0, 0.45 2 ) was also added to every node attribute in every graph <ref type="bibr" target="#b32">(Feragen et al., 2013)</ref>.</p><p>Synthie. This dataset consists of 400 synthetic graphs, subdivided into four classes, with 15 real-valued node attributes. Two types of graphs and two types of attributes were generated, and each combination of those gave rise to a class (four classes in total). All graphs were generated by randomly adding edges between 10 perturbed instances of two Erdös Rényi graphs. To generate graphs of the first type, perturbed instances of the first Erdös Rényi graph were choosen with probability 0.8, while perturbed instances of the second Erdös Rényi graph were choosen with probability 0.2. To generate graphs of the second type, the two probabilities were reversed. The vertices of each graph were then annotated by attributes drawn either from the first or from the second type of attributes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Setup</head><p>We evaluated the performance of the graph kernels on the datasets presented above. Specifically, we made use of the GraKeL library which contains implementations of a large number of graph kernels <ref type="bibr" target="#b112">(Siglidis, Nikolentzos, Limnios, Giatsidis, Skianis, &amp; Vazirgiannis, 2018)</ref>. We used the following 16 kernels in our experimental evaluation: (1) vertex histogram kernel (VH), (2) random walk kernel (RW), (3) shortest path kernel (SP), (4) Weisfeiler-Lehman subtree kernel (WL-VH), (5) Weisfeiler-Lehman shortest path kernel (WL-SP), (6) Weisfeiler-Lehman pyramid match kernel (WL-PM), (7) neighborhood hash kernel (NH), (8) neighborhood subgraph pairwise distance kernel (NSPDK), (9) ordered decompositional DAGs with subtree kernel (ODD-STh), (10) pyramid match kernel (PM), (11) GraphHopper kernel (GH), (12) subgraph matching kernel (SM), (13) propagation kernel (PK), (14) multiscale Laplacian kernel (ML), (15) core Weisfeiler-Lehman subtree kernel (CORE-WL), (16) core shortest path kernel (CORE-SP). Note that some of the kernels (e. g. WL-SP, CORE-SP) correspond to frameworks applied to graph kernels. Furthermore, since some kernels can handle different types of graphs than others, we conduct three distinct experiments.</p><p>The three experiments are characterized by the types of graphs contained in the employed datasets: (1) datasets with unlabeled graphs, (2) datasets with node-labeled graphs, and (3) datasets with node-attributed graphs. It is important to mention that kernels that are  designed for node-labeled graphs can also be applied to unlabaled graphs by initializing the node labels of all vertices of the unlabaled graphs to the same value. Hence, we evaluate these kernels on datasets that contain node-labeled graphs, but also on datasets that contain unlabeled graphs. Moreover, kernels that are designed for node-attributed graphs can be applied to unlabeled graphs and to graphs that contain dicrete node labels. To achieve that, in the case of unlabeled graphs, all the vertices of all graphs are assigned the same attribute, while in the case of node-labeled graphs, node labels are transformed into feature vectors (e. g. using a "one-hot" encoding scheme). Hence, we evaluated these kernels on all three experimental scenarios.</p><p>In all cases, to perform graph classification, we employed a Support Vector Machine (SVM) classifier and in particular, the LIB-SVM implementation <ref type="bibr" target="#b23">(Chang &amp; Lin, 2011)</ref>. To evaluate the performance of the different kernels, we performed 10-fold cross-validation. Furthermore, since the size of most of the above datasets is not large, the whole process was repeated 10 times in order to exclude random effects of the fold assignments. Within each fold, the parameter C of the SVM and the hyperparameters of the kernels (see below) were chosen based on a validation experiment on a single 90% − 10% split of the training data. We chose the value of parameter C from {10 −7 , 10 −5 , . . . , 10 5 , 10 7 }. Moreover, we normalized all kernel matrices. All experiments were performed on a cluster of 80 Intel c Xeon c CPU E7−4860 @ 2.27GHz with 1TB RAM. Note that each kernel was computed on a single thread of the cluster. We set a time limit of 24 hours for each kernel to compute the kernel matrix. Hence, we denote by TIMEOUT kernel computations that did not finish within one day. We also set a memory limit of 64GB, and we denote by OUT-OF-MEM computations that exceeded this limit.</p><p>As mentioned above, to choose the hyperparameters of the kernels, we performed a validation experiment on a single 90% − 10% split of the training set. Hence, given a kernel, for each combination of hyperparameter values, we generated a seperate kernel matrix.</p><p>The hyperparameter values that result into the classifier with the best performance on the validation set are the ones selected for the final model learning. The values of the different hyperparameters of the kernels are shown in <ref type="table" target="#tab_8">Table 4</ref>. It is interesting to mention that some kernels such as the vertex historgram kernel (VH) lack hyperparameters, while other kernels such as the multiscale Laplacian kernel (ML) contain a large number of hyperparameters. Hence, for the vertex historgram (VH) kernel, we computed only a single kernel matrix in each experiment, while for the multiscale Laplacian (ML) kernel, we computed 24 different kernel matrices in each experiment. Note also that instead of performing cross-validation to identify the best combination of hyperparameter values, we could have applied multiple kernel learning to the generated kernel matrices <ref type="bibr" target="#b81">(Massimo, Navarin, &amp; Sperduti, 2016)</ref>.</p><p>For each experiment, we report the average accuracy over the 10 runs of the crossvalidation procedure. Furthermore, we report running times averaged over the 10 independent runs. For each run, we compute running times as follows: for each fold of a 10-fold cross-validation experiment, the running time of the kernel corresponds to the running time for the computation of the kernel matrix that performed best on the validation experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Results</head><p>We next present our experimental results. As mentioned above, we evaluate the graph kernels by performing graph classification on unlabeled, node-labeled and node-attributed benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Node-Labeled Graphs</head><p>Tables 5 and 6 illustrate average prediction accuracies and average running times of the compared kernels on the datasets that contain node-labeled graphs. We observe that the kernels that employ some neighborhood aggregation mechanism (e. g. the Weisfeiler-Lehman framework) yield very good performance. Specifically, the WL-PM kernel outperforms all the other kernels on 4 out of the 7 datasets (MUTAG, ENZYMES, NCI1, and PTC-MR). Moreover, the WL-VH, CORE-WL and NH kernels also achive high accuracies on most datasets. Surprisingly, WL-SP, although equipped with a neighborhood aggregation scheme, performs much worse than the other kernels which employ the same framework and also much worse than the SP and CORE-SP kernels. The core framework leads to performance improvements on most datasets. For instance, in the case of the SP kernel, it leads to better accuracies on all but one dataset. It is worth mentioning that CORE-SP provides the highest accuracies on two datasets (D&amp;D and PROTEINS). As regards the kernels for graphs with continuous attributes (GH, SM, PK, and ML), most of them failed to produce results comparable to the best-performing kernels. The only exception is the ML kernel which yielded good results on most datasets. Moreover, the GH kernel reached the highest accuracy on the AIDS dataset. Finally, VH and RW achieved very low accuracy levels.</p><p>On most datasets, the variability in the performance of the different kernels is low. The ENZYMES dataset is an exception to that, since the average accuracy of the best-  <ref type="table">Table 5</ref>: Average classification accuracy (± standard deviation) on the 7 classification datasets containing node-labeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the better the overall performance of the kernel.</p><p>performing kernel is equal to 57.72, while that of the worst-performing kernel is equal to 12.90. Furthermore, the AIDS dataset is almost perfectly classified by several kernels, and this raises some concerns about the value of this dataset for graph kernel comparison. In terms of running time, as expected, VH is the fastest kernel on all datasets. This kernel computes the dot product on vertex label histograms, hence, its complexity is linear to the number of vertices. The running time of WL-VH, NH, SP and PK is also low compared to the other kernels on most datasets. Hence, the effectiveness of WL-PM comes at a price, as computing the kernel requires a large amount of time. Note also that while the worst-case complexity of SP is very high, by employing an explicit computation scheme,  <ref type="table">Table 6</ref>: Average CPU running time for kernel matrix computation on the 7 classification datasets containing node-labeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.</p><p>the running time of the kernel in real scenarios is very attractive. We also observe that the ML, RW, SM and WL-PM kernels are very expensive in terms of runtime. Specifically, the SM kernel failed to compute the kernel matrix on NCI1 within one day, while it exceeded the maximum available memory on two other datasets (D&amp;D and PROTEINS). It should be mentioned that the size of the graphs (i. e. number of nodes) and the size of the dataset (i. e. number of graphs) have a different impact on the running time of the kernels. For instance, the average running time of the PM kernel is relatively high on datasets that contain small graphs. However, this kernel is much more competitive on datasets which contain large graphs such as the D&amp;D dataset on which it was the third fastest kernel.  <ref type="table">Table 7</ref>: Average classification accuracy (± standard deviation) on the 6 classification datasets containing unlabeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the better the overall performance of the kernel.</p><p>Overall, when dealing with tasks that involve node-labeled graphs, we suggest to use a kernel that utilizes some neighborhood aggregation mechanism. For instance, the WL-VH and NH kernels achieve high accuracies and are very efficient even when the size of the graphs and/or the dataset is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Unabeled Graphs</head><p>Tables 7 and 8 illustrate average prediction accuracies and average running times of the compared kernels on the 6 datasets that contain unlabeled graphs. We observe that NH is the best-performing method. It outperforms all the other kernels on 3 out of the 6 datasets (IMDB-BINARY, REDDIT-MULTI-5K and COLLAB). Furthermore, on the remaining datasets, it reached the second, the fourth and the fifth best accuracy levels among all methods considered. The CORE-SP, ML, CORE-WL, PM and WL-VH kernels also yielded high performance on most datasets. We should note that the core framework improved significantly the performance of the SP kernel on several datasets. On the other hand, the Weisfeiler-Lehman framework did not provide such a large increase in the performance of the SP kernel. Lo-ϑ was the worst-performing kernel, followed by VH, WL-SP, SVM-ϑ and PK, in that order. It is interesting to mention that most of the kernels that reached the highest accuracies can also handle graphs with dicrete node labels. For those kernels, the label of each vertex was set equal to its degree. The kernels that can only handle unlabeled graphs (GR, Lo-ϑ, SVM-ϑ) failed to achieve accuracies competitive to the best-performing kernels. As regards the kernels that can handle graphs with continuous attributes (GH, SM, PK, and ML), as mentioned above, ML yielded very good results.  <ref type="table">Table 8</ref>: Average CPU running time for kernel matrix computation on the 6 classification datasets containing unlabeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.</p><p>GH and PK achieved low accuracy levels, while SM failed to generate even a single kernel matrix due to running time or memory issues. On most datasets, the variability in the performance of the different kernels is low. The kernels achieve higher performance on binary classification tasks (IMDB-BINARY and REDDIT-BINARY) than on multi-class classification tasks. For instance, on IMDB-MULTI, REDDIT-MULTI-5K and REDDIT-MULTI-12K, the highest average accuracies obtained by the considered kernels are 50.68, 49.36 and 41.15, respectively. Hence, it is clear that these three datasets are very challenging even for state-of-the-art methods.</p><p>In terms of running time, similar to the labeled case, VH is again the fastest kernel on all datasets. The running time of PK, ODD-STh, and WL-VH is also low compared to the other kernels on most datasets. The SVM-ϑ, NH, PM, SP and CORE-WL kernels were also competitive in terms of running time. Besides achieving low accuracy levels, the Lo-ϑ kernel is also very computationally expensive. The RW, NSPDK, CORE-SP, WL-SP and GH are also very expensive in terms of running time. The above 6 kernels did not manage to calculate any kernel matrix on REDDIT-MULTI-5K and REDDIT-MULTI-12K within one day. The SM kernel failed to compute the kernel matrix on IMDB-BINARY, IMDB-MULTI and COLLAB within one day, while it exceeded the maximum available memory on the remaining three datasets. The WL-VH and CORE-WL kernels also exceeded the maximum available memory on REDDIT-MULTI-5K and REDDIT-MULTI-12K. It should be mentioned that these two datasets contain several thousands of graphs, while the size of the graphs is also large (i. e. several hundreds of vertices on average).</p><p>When dealing with tasks that involve unlabeled graphs, we suggest to assign discrete node labels to the vertices of the graphs (e. g. set the label of each vertex equal to its degree), and then to again employ kernels that utilize some neighborhood aggregation mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATASETS</head><p>Avg <ref type="table">.  ENZYMES  PROTEINS full SYNTHETICnew  Synthie  BZR  Rank   SP  TIMEOUT  TIMEOUT  TIMEOUT  TIMEOUT  TIMEOUT  -SM  TIMEOUT  OUT-OF-MEM  TIMEOUT  TIMEOUT  80</ref>.52 <ref type="bibr">(± 0.43)</ref> 3.0 GH 66.25 <ref type="bibr">(± 1.24)</ref> 72.49 <ref type="bibr">(± 0.34)</ref> 76.43 <ref type="bibr">(± 1.97)</ref> 71.75 <ref type="bibr">(± 1.65)</ref> 82.58 (± 1.05) 1.0 PK 15.42 (± 1.00) 59.56 (± 0.01) 47.90 <ref type="bibr">(± 3.26)</ref> 48.90 <ref type="bibr">(± 2.05)</ref> 78.76 (± 0.02) 3.0 ML 65.55 <ref type="bibr">(± 0.93)</ref> 70.55 <ref type="bibr">(± 0.99)</ref> 47.90 <ref type="bibr">(± 2.13)</ref> 69.42 (± 1.98) 82.33 <ref type="bibr">(± 1.29)</ref> 2.0 <ref type="table">Table 9</ref>: Average classification accuracy (± standard deviation) on the 5 classification datasets containing node-attributed graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the better the overall performance of the kernel.  <ref type="table" target="#tab_0">Table 10</ref>: Average CPU running time for kernel matrix computation on the 5 classification datasets containing node-attributed graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.</p><p>For instance, the NH and WL-VH kernels achieve high accuracies and their computational complexity is low, although WL-VH is not always efficient in terms of memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Node-Attributed Graphs</head><p>As mentioned above, the majority of graph kernels can handle graphs that are either unlabeled or contain discrete node labels. On the other hand, the number of graph kernels that can handle graphs that contain continuous vertex attributes is limited. Moreover, most of these kernels do not scale even to relatively small datasets. Tables 9 and 10 illustrate average prediction accuracies and average runtimes of 5 graph kernels on datasets that contain node-attributed graphs. Note that although the graphs of some of these datasets contain discrete node labels, we did not take these discrete labels into account since our main aim was to evaluate the ability of the kernels to properly handle continuous node attributes. GH is the best-performing kernel since it outperforms all the other kernels on all datasets. Interestingly, on PROTEINS full, it outperforms the other kernels by wide margins. GH is followed by ML and PK in terms of performance in that order. ML yields average accuracies comparable to those of GH on most datasets, while PK is less competitive. One of the most striking findings of this set of experiments is that the SP kernel did not manage to compute the kernel matrix even on a single dataset within one day, while the SM kernel finished its computations within one day only on the BZR dataset on which it was outperformed by GH and ML.</p><p>In terms of running time, PK is the most efficient kernel since it handled all datasets in less than two minutes. GH and ML are much slower than PK on all datasets. For instance, the average computation time of ML and GH was greater than 4 hours and 5 hours on PROTEINS full, respectively. The SP and SM kernels, as already discussed, are very expensive in terms of running time, and hence, their usefulness in real-world problems is limited.</p><p>To sum up, it is clear that the running time of most kernels for node-attributed graphs is prohibitive, especially considering the relatively small size of the datasets. Although the running time of PK is attractive, it achieved low accuracies on almost all datasets. An open challenge in the field of graph kernels is thus to develop scalable kernels for graphs with continuous vertex attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Recent years have witnessed a tremendous increase in the availability of graph-structured data. Graphs arise in many different contexts where it is necessary to represent relationships between entities. Specifically, graphs are the commonly employed structure for representing data in various domains including bioinformatics, chemoinformatics, social networks and information networks. The abundance of graph-structured data and the need to perform machine learning tasks on this kind of data led to the development of several sophisticated approaches such as graph kernels. In this survey, we provided a detailed overview of graph kernels. Furthermore, we empirically evaluated the effectiveness of several graph kernels, and measured their running time. We hope that this survey will provide a better understanding of the current progress on graph kernels and graph classification, and offer some guidelines on how to apply these approaches in order to solve real-world problems.</p><p>Although graph kernels have achieved remarkable results in many tasks, there are still some challenges to be addressed, while there is also still some room for improvement. For example, the majority of the kernels that can handle graphs with continuous attributes are either very expensive in terms of computational complexity or fail to produce competitive results. Hence, we believe that an important direction of research is the development of scalable graph kernels for graphs annotated with continuous attributes which will also provide improvements over the state-of-the-art approaches. Another useful direction of research is to capitalize on the framework for designing valid assignment kernels presented above, and to develop new kernels which compute an optimal assignment between substructures extracted from graphs. In general, the complexity of the assignment kernels is more attractive than that of kernels that belong to the R-convolution framework, and hence, it is our belief that this framework can pave the way for the development of more efficient graph kernels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of different types of graphs. A simple undirected graph (left), a labeled graph (center), and an attributed graph (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Taxonomy of graph kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Two graphs (top left and right) and their direct product (bottom). Each vertex of the direct product graph is labeled with a pair of vertices; an edge exists in the direct product if and only if the corresponding vertices are adjacent in both original graphs. For instance, nodes 1 − 4 and 3 − 5 are adjacent because there is an edge between vertices 1 and 3 in the first, and 4 and 5 in the second graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Two DAGs (left) and their associated tree visits T (u) starting from each node u (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Let ω : V (T ) → R ≥0 be an additive weight function defined as ω(v) = w(v) − w(p(v)) and ω(r) = w(r) for the root r. Note that the property of a hierarchy assures that the values of the ω function are nonnegative. For v ∈ V (T ), let P (v) ⊆ V (T ) denote the vertices on the path from v to the root r. The strong kernel matrixFigure 12: The matrix of a strong kernel on objects a, b and c (a) induced by the hierarchy (b) and the derived feature vectors (c). A vertex v in (b) is annotated by its weights w(v); ω(v). k induced by the hierarchy H can be defined using the mapping φ : X → R n , where n = |V (T )| and the components indexed by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 14 :</head><label>14</label><figDesc>Figure 13: An assignment instance (a) for X, Y ∈ [X ] 5 and the derived histograms (b). The set X contains three distinct vertices labeled a and the set Y two distinct vertices labeled b and c. Taking the multiplicities into account the histograms are obtained from the hierarchy of the base kernel k depicted inFigure 12. The optimal assignment yields a value of K k B (X, Y ) = n i=1 min H X (i), H Y (i) = min{5, 5} + min{8, 6} + min{3, 1} + min{2, 4} + min{1, 2} = 15. A graph G with uniform initial labels τ 0 and refined labels τ i for i ∈ {1, . . . , 3} (a), and the associated hierarchy (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Commonly Used Symbols and Notations</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of selected graph kernels regarding computation by explicit feature mapping (Exp. φ), support for node-labeled and node-attributed graphs, and type.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Summary of the 17 datasets used in our experiments. The "Max Class Imbalance" column indicates the ratio of the size of the smallest class of the dataset to the size of its largest class.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>∈ {1, . . . , 6}, d * ∈ {3, . . . , 7}</figDesc><table><row><cell>Kernels</cell><cell></cell><cell cols="2">Fixed</cell><cell cols="2">Hyperparameters Chosen based on validation set performance</cell></row><row><cell>VH</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>RW</cell><cell>λ = 10</cell><cell>log 10 (</cell><cell>1 deg 2 max</cell><cell>)</cell><cell>k ∈ {2, . . . 10, ∞}</cell></row><row><cell>SP</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>GR</cell><cell></cell><cell cols="2">k = 5</cell><cell></cell><cell>nsamples = {200, 500, 1000, 2000, 5000}</cell></row><row><cell>WL</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>h ∈ {4, . . . , 8}</cell></row><row><cell>NH</cell><cell cols="4">Count-sensitive neighborhood hash</cell><cell>h ∈ {1, . . . , 6}</cell></row><row><cell cols="6">NSPDK r  Lo-ϑ -2 ≤ |S| ≤ 8 nsamples = {100, 200, 500, 1000}</cell></row><row><cell>SVM-ϑ</cell><cell cols="3">2 ≤ |S| ≤ 8</cell><cell></cell><cell>nsamples = {100, 200, 500, 1000}</cell></row><row><cell>ODD-STh</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>h ∈ {1, . . . , 11}</cell></row><row><cell>PM</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>L ∈ {2, 4, 6}, d ∈ {4, 6, 8, 10}</cell></row><row><cell>GH</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>linear kernel/gaussian kernel</cell></row><row><cell>SM</cell><cell></cell><cell cols="2">k = 3</cell><cell></cell><cell>-</cell></row><row><cell>PK</cell><cell cols="3">w = 10 −5</cell><cell></cell><cell>T ∈ {1, . . . , 6}</cell></row><row><cell>ML</cell><cell cols="4">γ = 0.01, η = 0.01,p = 10</cell><cell>lmax ∈ {0, . . . , 5},c ∈ {50, 100, 200, 300}</cell></row><row><cell>CORE</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Values of the hyperparameters of the graphs kernels and frameworks included in our experimental comparison. Note that for some kernels, only a subset of the hyperparameters was optimized, while the rest of the hyperparameters were kept fixed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>(± 0.80)   29.59(± 0.40)   47.32 (± 0.66) 17.92 (± 0.42) 21.73 (± 0.00) 52.00 (± 0.00)</figDesc><table><row><cell>Kernels</cell><cell>IMDB BINARY</cell><cell>IMDB MULTI</cell><cell cols="2">DATASETS REDDIT REDDIT BINARY MULTI-5K</cell><cell>REDDIT MULTI-12K</cell><cell>COLLAB</cell><cell>Avg. Rank</cell></row><row><cell>VH</cell><cell cols="7">46.54 12.4</cell></row><row><cell>RW</cell><cell>63.87 (± 1.06)</cell><cell>45.75 (± 1.03)</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>OUT-OF-MEM</cell><cell>68.00 (± 0.07)</cell><cell>7.6</cell></row><row><cell>SP</cell><cell>55.18 (± 1.23)</cell><cell cols="3">39.37 (± 0.84) 81.67 (± 0.23) 47.90 (± 0.13)</cell><cell>TIMEOUT</cell><cell>58.80 (± 0.08)</cell><cell>8.3</cell></row><row><cell>GR</cell><cell cols="6">65.19 (± 0.97) 39.82 (± 0.89) 76.80 (± 0.27) 34.06 (± 0.38) 23.08 (± 0.11) 70.63 (± 0.25)</cell><cell>7.0</cell></row><row><cell>WL-VH</cell><cell cols="3">72.47 (± 0.50) 50.76 (± 0.30) 67.96 (± 1.01)</cell><cell>OUT-OF-MEM</cell><cell>OUT-OF-MEM</cell><cell>78.12 (± 0.17)</cell><cell>4.2</cell></row><row><cell>WL-SP</cell><cell cols="2">55.87 (± 1.19) 39.63 (± 0.68)</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>58.80 (± 0.06)</cell><cell>10.8</cell></row><row><cell>NH</cell><cell cols="6">73.34 (± 0.98) 50.68 (± 0.50) 81.65 (± 0.28) 49.36 (± 0.18) 39.62 (± 0.19) 79.99 (± 0.39)</cell><cell>2.3</cell></row><row><cell>NSPDK</cell><cell cols="2">68.81 (± 0.71) 45.10 (± 0.63)</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>7.5</cell></row><row><cell>Lo-ϑ</cell><cell cols="2">49.21 (± 1.33) 39.33 (± 0.95)</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>15.0</cell></row><row><cell>SVM-ϑ</cell><cell cols="6">51.35 (± 1.54) 38.40 (± 0.60) 74.54 (± 0.27) 29.65 (± 0.53) 23.04 (± 0.18) 55.72 (± 0.31)</cell><cell>10.1</cell></row><row><cell>ODD-STh</cell><cell cols="6">64.70 (± 0.73) 46.80 (± 0.51) 50.61 (± 1.06) 42.99 (± 0.09) 29.83 (± 0.08) 52.00 (± 0.00)</cell><cell>7.5</cell></row><row><cell>PM</cell><cell cols="6">66.67 (± 1.45) 45.25 (± 0.79) 86.77 (± 0.42) 48.22 (± 0.29) 41.15 (± 0.17) 74.57 (± 0.34)</cell><cell>4.1</cell></row><row><cell>GH</cell><cell cols="2">57.69 (± 1.31) 40.04 (± 0.91)</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>60.21 (± 0.10)</cell><cell>9.3</cell></row><row><cell>SM</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>OUT-OF-MEM</cell><cell>OUT-OF-MEM</cell><cell>OUT-OF-MEM</cell><cell>TIMEOUT</cell><cell>-</cell></row><row><cell>PK</cell><cell cols="6">51.15 (± 1.67) 33.15 (± 1.08) 63.41 (± 0.77) 34.32 (± 0.61) 24.07 (± 0.11) 58.67 (± 0.15)</cell><cell>10.1</cell></row><row><cell>ML</cell><cell cols="4">70.94 (± 0.93) 47.92 (± 0.87) 89.44 (± 0.30) 35.01 (± 0.65)</cell><cell>OUT-OF-MEM</cell><cell>75.29 (± 0.49)</cell><cell>3.8</cell></row><row><cell cols="4">CORE-WL 73.31 (± 1.06) 50.79 (± 0.54) 72.82 (± 1.05)</cell><cell>OUT-OF-MEM</cell><cell>OUT-OF-MEM</cell><cell>OUT-OF-MEM</cell><cell>3.8</cell></row><row><cell>CORE-SP</cell><cell cols="3">69.37 (± 0.68) 50.79 (± 0.57) 90.76 (± 0.14)</cell><cell>TIMEOUT</cell><cell>OUT-OF-MEM</cell><cell>TIMEOUT</cell><cell>2.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple Graph-Kernel Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Symposium Series on Computational Intelligence</title>
		<meeting>the 2015 Symposium Series on Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Graph Kernel for Protein-Protein Interaction Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing</title>
		<meeting>the Workshop on Current Trends in Biomedical Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning</title>
		<idno>S2</idno>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aizerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="821" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale networks fingerprinting and visualization using the k-core decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alvarez-Hamelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dall&amp;apos;asta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vespignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based malware detection using dynamic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Storlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal in Computer Virology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="247" to="258" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Relational Kernel-Based Framework for Hierarchical Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</title>
		<meeting>the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An application of kernel methods to gene cluster temporal meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antoniotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farinaccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zoppis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1361" to="1368" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph kernels between point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Histogram intersection kernel for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 International Conference on Image Processing</title>
		<meeting>the 2003 International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="513" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast algorithms for determining (generalized) core groups in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaveršnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Data Analysis and Classification</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text Categorization of Biomedical Data Sets using Graph Kernels and a Controlled Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bleik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1211" to="1217" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph Reconstruction-A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bondy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hemminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="268" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Ludwig-Maximilian University of Munich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining</title>
		<meeting>the 5th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph kernels for disease outcome prediction from protein-protein interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Training Algorithm for Optimal Margin Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Workshop on Computational Learning Theory</title>
		<meeting>the 5th Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compound analysis via graph kernels incorporating chirality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Urata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bioinformatics and Computational Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">supp01</biblScope>
			<biblScope unit="page" from="63" to="81" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">People re-identification by Graph Kernels Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification of small molecules by two-and three-dimensional decomposition kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ceroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2038" to="2045" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LIBSVM: A Library for Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thirty years of graph matching in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="265" to="298" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast Neighborhood Subgraph Pairwise Distance Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>De Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
		<meeting>the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Tree-Based Kernel for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM International Conference on Data Mining</title>
		<meeting>the 2012 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="975" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Substructure counting graph kernels for machine learning from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K D</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="71" to="84" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structure-Activity Relationship of Mutagenic Aromatic and Heteroaromatic Nitro Compounds. Correlation with Molecular Orbital Energies and Hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinguishing Enzyme Structures from Non-enzymes Without Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">KONG: Kernels for orderedneighborhood graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Draief</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vojnovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph kernels and gaussian processes for relational reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Driessens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="91" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning molecular energies using localized graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">114107</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-Sentence Compression: Finding Shortest Paths in Word Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Characterizing Structural Relationships in Scenes Using Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal Assignment Kernels For Attributed Molecular Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sieker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Computers and intractability: a guide to NPcompleteness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>WH Freeman and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Survey of Kernels for Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On Graph Kernels: Hardness Results and Efficient Alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structural Detection of Android Malwareusing Embedded Call Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 2013 ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two New Graph Kernels and Applications to Chemoinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gaüzère</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<meeting>the 8th International Workshop on Graph-Based Representations in Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The journey of graph kernels through two decades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="88" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The pyramid quantized Weisfeiler-Lehman graph representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gkirtzou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="1495" to="1507" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recognizing Identical Events with Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>&amp;amp;šnajder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="797" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A general coefficient of similarity and some of its properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="857" to="871" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Classification on Pairwise Proximity Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bollmann-Sdorra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="438" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Pyramid Match Kernel: Efficient Learning with Sets of Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="725" to="760" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Chemoinformatics and stereoisomerism: A stereo graph kernel together with three new extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Grenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Models of Relational MDPs Using Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Halbritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Mexican International Conference on Artificial Intelligence</title>
		<meeting>the 6th Mexican International Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image Classification with Segmentation Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2007 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of California in Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Entity Disambiguation in Anonymized Graphs using Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hermansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1037" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Linear-time Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Data Mining</title>
		<meeting>the 9th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cyclic Pattern Kernels for Predictive Graph Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lovász ϑ function, SVMs and Finding Dense Subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3495" to="3536" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sub-network Based Kernels for Brain Network Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<meeting>the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="622" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Topological Graph Kernel on Multiple Thresholded Functional Connectivity Networks for Mild Cognitive Impairment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2876" to="2897" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Global graph kernels using geometric embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="694" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Predicting metamorphic relations for testing scientific software: a machine learning approach using graph kernels. Software Testing, Verification and Reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kanewala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bieman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="245" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Marginalized Kernels Between Labeled Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference in Machine Learning</title>
		<meeting>the 20th Conference in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hadamard Code Graph Kernels for Classifying Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the 5th International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Multiscale Laplacian Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Diffusion Kernels on Graphs and Other Discrete Input Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Machine Learning</title>
		<meeting>the 19th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Subgraph Matching Kernels for Attributed Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Explicit versus Implicit Graph Feature Maps:A Computational Phase Transition for Walk Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE International Conference on Data Mining</title>
		<meeting>the 2014 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On Valid Optimal Assignment Kernels and Applications to Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A Property Testing Framework for the Theoretical Expressivity of Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2348" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A graph kernel approach for alignment-free domain-peptide interaction prediction with an application to human sh3 domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Backofen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A note on the derivation of maximal common subgraphs of two directed or undirected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calcolo</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">341</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Nested Subtree Hash Kernels for Large-scale Graph Classification over Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Data Mining</title>
		<meeting>the 12th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">3d human motion retrieval using graph kernels based on adaptive graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="104" to="112" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schäf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Software Reuse</title>
		<meeting>the 15th International Conference on Software Reuse</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="315" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Graph Kernels for RDF Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lösch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bloehdorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Extended Semantic Web Conference</title>
		<meeting>the 9th Extended Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="134" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the shannon capacity of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Object Classification Based On Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahboubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-X</forename><surname>Dupé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 International Conference on High Performance Computing &amp; Simulation</title>
		<meeting>the 2010 International Conference on High Performance Computing &amp; Simulation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Extensions of marginalized graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mahé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Graph Kernels for Molecular Structure-Activity Relationship Analysis with SupportVector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mahé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="939" to="951" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Graph kernels based on tree patterns for molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mahé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="3" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Hyper-Parameter Tuning for Graph Kernels via Multiple Kernel Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Massimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Neural Information Processing</title>
		<meeting>the 23rd International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Smallest-last Ordering and Clustering and Graph Coloring Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="427" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Decoding brain states using backward edge elimination and graph kernels in fMRI connectivity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Hossein-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Conference on Data Mining</title>
		<meeting>the 2017 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Faster Kernels for Graphs with Continuous Attributes via Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE International Conference on Data Mining</title>
		<meeting>the 16th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Contextual Weisfeiler-Lehman Graph Kernel For Malware Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Joint Conference on Neural Networks</title>
		<meeting>the 2016 International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4701" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Graph Kernels for Object Category Predictionin Task-Dependent Robot Grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Mining and Learning with Graphs</title>
		<meeting>the 11th Workshop on Mining and Learning with Graphs</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A Degeneracy Framework for Graph Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2595" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Shortest-path Graph Kernels for Document Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stavrakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Matching Node Embeddings for Graph Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Enhancing Graph Kernels via Successive Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1583" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Measuring the expressivity of graph kernels through statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Graph Invariant Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Graph Kernels for Chemical Informatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1093" to="1110" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Expressivity versus Efficiency of Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Mining Graphs, Trees and Sequences</title>
		<meeting>the 1st International Workshop on Mining Graphs, Trees and Sequences</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</title>
		<meeting>the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Graph Kernels for Molecular Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="266" to="273" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Directed acyclic graph kernels for structural RNA analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mituyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">318</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Transitive Assignment Kernels for Structural Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schiavinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasparetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Similarity-Based Pattern Recognition</title>
		<meeting>the 3rd International Workshop on Similarity-Based Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="146" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Predicting Protein Function and Protein-Ligand Interaction with the 3D Neighborhood Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Discovery Science</title>
		<meeting>the 18th International Conference on Discovery Science</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A Generalized Representer Theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Structure and Minimum Degree. Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A Method Based on the Granger Causality and Graph Kernels for Discriminating Resting State from Attentional Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shahnazian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Hossein-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on Biomedical Engineering</title>
		<meeting>the 2012 International Conference on Biomedical Engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Efficient Graphlet Kernels for Large Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Giatsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">GraKeL: A Graph Kernel Library in Python</title>
		<idno type="arXiv">arXiv:1806.02193</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Graph wavelet alignment kernels for drug virtual screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lushington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bioinformatics and Computational Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="473" to="497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Kernels and Regularization on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th Conference on Learning Theory</title>
		<meeting>16th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Automatic architectural style detection using one-class support vector machines and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Automation in Construction</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Robust Visual Place Recognition with Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4535" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A Fast Kernel for Attributed Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Harang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
		<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Evolution, Detection and Analysis of Malware forSmart Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Suarez-Tangil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Tapiador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peris-Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribagorda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="961" to="987" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Halting in Random Walk Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1639" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Spline-Fitting with a Genetic Algorithm: A Method for Developing ClassificationS tructure-Activity Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>O&amp;apos;brien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1906" to="1915" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Color Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Graph-Based Inter-Subject Pattern Analysis of fMRI Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takerkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Auzias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">104586</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the Predictive Toxicology Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Brain Decoding via Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Workshop on Pattern Recognition in Neuroimaging</title>
		<meeting>the 2013 International Workshop on Pattern Recognition in Neuroimaging</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="136" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Classification of inter-subject fMRI data based on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Workshop on Pattern Recognition in Neuroimaging</title>
		<meeting>the 2014 International Workshop on Pattern Recognition in Neuroimaging</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">The optimal assignment kernel is not positive definite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno>abs/0801.4061</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Malware analysis with graph kernels and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wagener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Malicious and Unwanted Software</title>
		<meeting>the 4th International Conference on Malicious and Unwanted Software</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Directed Acyclic Graph Kernels for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Inequalities for the l 1 deviation of the empirical distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Using the Nyström Method to Speed Up Kernel Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Human Action Recognition Based on Context-Dependent Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2609" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A Structural Smoothing Framework For Robust Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2125" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Fast multi-view segment graph kernel for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">RetGK: Graph Kernels based on Return Probabilities of Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3968" to="3978" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
