<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Count-Based Exploration in Feature Space for Reinforcement Learning *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarryd</forename><surname>Martin</surname></persName>
							<email>jarrydmartinx@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Narayanan</surname></persName>
							<email>surajx@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Everitt</surname></persName>
							<email>tom.everitt@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
							<email>marcus.hutter@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Count-Based Exploration in Feature Space for Reinforcement Learning *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new count-based optimistic exploration algorithm for reinforcement learning (RL) that is feasible in environments with highdimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our φ-pseudocount achieves generalisation by exploiting the same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The φ-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on highdimensional RL benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) methods have recently enjoyed widely publicised success in domains that once seemed far beyond their reach <ref type="bibr" target="#b1">[Mnih et al., 2015]</ref>. Much of this progress is due to the application of modern function approximation techniques to the problem of policy evaluation for Markov Decision Processes (MDPs) <ref type="bibr" target="#b5">[Sutton and Barto, 1998</ref>]. These techniques address a key shortcoming of tabular MDP solution methods: their inability to generalise what is learnt from one context to another. This sort of generalisation is crucial if the state-action space of the MDP is large, because the agent * This work was supported in part by ARC DP150104590. typically only visits a small subset of that space during training.</p><p>Comparatively little progress has been made on the problem of efficient exploration in large domains. Even algorithms that use sophisticated nonlinear methods for policy evaluation tend to use very old, inefficient exploration techniques, such as the -greedy strategy <ref type="bibr" target="#b5">[van Hasselt et al., 2016b;</ref><ref type="bibr" target="#b1">Mnih et al., 2016;</ref><ref type="bibr" target="#b1">Nair et al., 2015]</ref>. There are more efficient tabular count-based exploration algorithms for finite MDPs, which drive the agent to reduce its uncertainty by visiting states that have low visit-counts <ref type="bibr" target="#b5">[Strehl and Littman, 2008]</ref>. However, these algorithms are often ineffective in MDPs with high-dimensional state-action spaces, because most states are never visited during training, and the visitcount remains at zero nearly everywhere.</p><p>Count-based exploration algorithms have only very recently been successfully adapted for these large problems <ref type="bibr">Tang et al., 2016]</ref>. Just as function approximation techniques achieve generalisation across the state space regarding value, these algorithms achieve generalisation regarding uncertainty. The breakthrough has been the development of generalised state visit-counts, which are larger for states that are more similar to visited states, and which can be nonzero for unvisited states. The key challenge is to compute an appropriate similarity measure in an efficient way, such that these exploration methods can be combined with scalable RL algorithms. It soon becomes infeasible, for example, to do so by storing the entire history of visited states and comparing each new state to those in the history. The most promising proposals instead compute generalised counts from a compressed representation of the history of visited states -for example, by constructing a visitdensity model over the state space and deriving a "pseudocount" <ref type="bibr" target="#b4">Ostrovski et al., 2017]</ref>, or by using locality-sensitive hashing to cluster states and counting the occurrences in each cluster <ref type="bibr">[Tang et al., 2016]</ref>.</p><p>This paper presents a new count-based exploration algorithm that is feasible in environments with large state-action spaces. It can be combined with any value-based RL algorithm that uses linear function approximation (LFA). Our principal contribution is a new method for computing generalised visit-counts. Following , we construct a visit-density model in order to measure the similarity between states. Our approach departs from theirs in that we do not construct our density model over the raw state space. Instead, we exploit the feature map that is used for value function approximation, and construct a density model over the transformed feature space. This model assigns higher probability to state feature vectors that share features with visited states. Generalised visit-counts are then computed from these probabilities; states with frequently observed features are assigned higher counts. These counts serve as a measure of the uncertainty associated with a state. Exploration bonuses are then computed from these counts in order to encourage the agent to visit regions of the state-space with less familiar features.</p><p>Our density model can be trivially derived from any feature map used for LFA, regardless of the application domain, and requires little or no additional design. In contrast to existing algorithms, there is no need to perform a special dimensionality reduction of the state space in order to compute our generalised visit-counts. Our method uses the same lower-dimensional feature representation to estimate value and to estimate uncertainty. This makes it simpler to implement and less computationally expensive than some existing proposals. Our evaluation demonstrates that this simple approach achieves near state-of-the-art performance on highdimensional RL benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning</head><p>The reinforcement learning (RL) problem formalises the task of learning from interaction to achieve a goal <ref type="bibr" target="#b5">[Sutton and Barto, 1998</ref>]. It is usually formulated as an MDP S, A, P, R, γ , where S is the set of states of the environment, A is the set of available actions, P : (S × A) × S → [0, 1] is the state transition distribution, R : (S ×A)×S → R is the reward function, and γ is the discount factor. The agent is formally a policy π : S → A that maps a state to an action. At timestep t, the agent is in a state s t ∈ S, receives a reward r t , and takes an action a t ∈ A. We seek a policy π that maximises the expected sum of future rewards, or value. The action-value Q π (s, a) of a state-action pair (s, a) under a policy π is the expected discounted sum of future rewards, given that the agent takes action a from state s, and follows π thereafter: Q π (s, a) = E π ∞ k=0 γ k r t+k+1 | s t = s, a t = a . RL methods that compute a value function are called valuebased methods. Tabular methods store the value function as a table having one entry for each state(-action). This representation of the state space does not have sufficient structure to permit generalisation based on the similarity between states. Function approximation methods achieve generalisation by approximating the value function by a parameterised functional form. In LFA the approximate action-value function Q π t (s, a) = θ t φ(s, a) is a linear combination of state-action features, where φ : S × A → T ⊆ R M is an M -dimensional feature map and θ t ∈ R M is a parameter vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Count-Based Exploration and Optimism</head><p>Since the true transition and reward distributions P and R are unknown to the agent, it must explore the environment to gather more information and reduce its uncertainty. At the same time, it must exploit its current information to maximise expected cumulative reward. This tradeoff between exploration and exploitation is a fundamental problem in RL.</p><p>Many of the exploration algorithms that enjoy strong theoretical guarantees implement the 'optimism in the face of uncertainty' (OFU) heuristic <ref type="bibr" target="#b5">[Strehl et al., 2009]</ref>. Most are tabular and count-based in that they compute exploration bonuses from a table of state(-action) visit counts. These bonuses are added to the estimated state/action value. Lower counts entail higher bonuses, so the agent is effectively optimistic about the value of less frequently visited regions of the environment. OFU algorithms are more efficient than random strategies like -greedy because the agent avoids actions that yield neither large rewards nor large reductions in uncertainty <ref type="bibr" target="#b3">[Osband et al., 2016b]</ref>.</p><p>One of the best known is the UCB1 bandit algorithm, which selects an action a that maximises an upper confidence boundQ t (a) + 2 log t N (a) , whereQ t (a) is the estimated mean reward and N (a) is the visit-count <ref type="bibr">[Lai and Robbins, 1985]</ref>. The dependence of the bonus term on the inverse squareroot of the visit-count is justified using Chernoff bounds. In the MDP setting, the tabular OFU algorithm most closely resembling our method is Model-Based Interval Estimation with Exploration Bonuses </p><formula xml:id="formula_0">+ γ s P (s | s, a) max a ∈AQ π (s , a ).</formula><p>Here the dependence of the bonus on the inverse square-root of the visit-count is provably optimal <ref type="bibr" target="#b0">[Kolter and Ng, 2009</ref>]. This equation can be solved using any MDP solution method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exploration in Large MDPs</head><p>While tabular OFU algorithms perform well in practice on small MDPs <ref type="bibr" target="#b5">[Strehl and Littman, 2004]</ref>, their sample complexity becomes prohibitive for larger problems <ref type="bibr">[Kakade, 2003]</ref>. MBIE-EB, for example, has a sample complexity bound ofÕ |S| 2 |A| 3 (1−γ) 6 . In the high-dimensional settingwhere the agent cannot hope to visit every state during training -this bound offers no guarantee that the trained agent will perform well.</p><p>Several very recent extensions of count-based exploration methods have produced impressive results on highdimensional RL benchmarks. These algorithms closely resemble MBIE-EB, but they substitute the state-action visitcount for a generalised count which quantifies the similarity of a state to previously visited states. Bellemare et. al. construct a Context Tree Switching (CTS) density model over the state space such that higher probability is assigned to states that are more similar to visited states <ref type="bibr" target="#b5">Veness et al., 2012]</ref>. A state pseudocount is then derived from this density. A subsequent extension of this work replaces the CTS density model with a neural network <ref type="bibr" target="#b4">[Ostrovski et al., 2017]</ref>. Another recent proposal uses locality sensitive hashing (LSH) to cluster similar states, and the number of visited states in a cluster serves as a generalised visit-count <ref type="bibr">[Tang et al., 2016]</ref>. As in the MBIE-EB algorithm, these counts are used to compute exploration bonuses. These three algorithms outperform random strategies, and are currently the leading exploration methods in large discrete domains where exploration is hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Here we introduce the φ-Exploration Bonus (φ-EB) algorithm, which drives the agent to visit states about which it is uncertain. Following other optimistic count-based exploration algorithms, we use a (generalised) state visit-count in order to estimate the uncertainty associated with a state. A generalised count is a novelty measure that quantifies how dissimilar a state is from those already visited. Measuring novelty therefore involves choosing a similarity measure for states. Of course, states can be similar in myriad ways, but not all of these are relevant to solving the MDP. If the solution method used is value-based, then states should only be considered similar if they share the features that are determinative of value. This motivates us to construct a similarity measure that exploits the feature representation that is used for value function approximation. These features are explicitly designed to be relevant for estimating value. If they were not, they would not permit a good approximation to the true value function. This sets our method apart from the approaches described in section 2.3. They measure novelty with respect to a separate, exploration-specific representation of the state space, one that bears no relation to the value function or the reward structure of the MDP. We argue that measuring novelty in feature space is a simpler and more principled approach, and hypothesise that more efficient exploration will result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Visit-Density over Feature Space</head><p>Our exploration method is designed for use with LFA, and measures novelty with respect to a fixed feature representation of the state space. The challenge is to measure novelty without computing the distance between each new feature vector and those in the history. That approach becomes infeasible because the cost of computing these distances grows with the size of the history.</p><p>Our method constructs a density model over feature space that assigns higher probability to states that share more features with more frequently observed states. Let φ : S → T ⊆ R M be the feature mapping from the state space into an Mdimensional feature space T . Let φ t = φ(s t ) denote the state feature vector observed at time t. We denote the sequence of observed feature vectors after t timesteps by φ 1:t ∈ T t , and denote the set of all finite sequences of feature vectors by T * . Let φ 1:t φ denote the sequence where φ 1:t is followed by φ.</p><p>The i-th element of φ is denoted by φ i , and the i-th element of φ t is φ t,i .</p><p>Definition 1 (Feature Visit-Density). Let ρ : T * × T → [0, 1] be a density model that maps a finite sequence of feature vectors φ 1:t ∈ T * to a probability distribution over T . The feature visit-density ρ t (φ) at time t is the distribution over T that is returned by ρ after observing φ 1:t .</p><p>We construct our feature visit-density as a product of independent factor distributions ρ</p><formula xml:id="formula_1">i t (φ i ) over individual features φ i ∈ U ⊆ R: ρ t (φ) = M i=1 ρ i t (φ i )</formula><p>If U is countable we can use a count-based estimator for the factor models ρ i t (φ i ), such as the empirical estimator</p><formula xml:id="formula_2">ρ i t (φ i ) = Nt(φi) t , where N t (φ i )</formula><p>is the number of times φ i has occurred. In our implementation we use the Krichevsky-</p><formula xml:id="formula_3">Trofimov (KT) estimator ρ i t (φ i ) = Nt(φi)+ 1 2 t+1</formula><p>. This density model induces a similarity measure on the feature space. Loosely speaking, feature vectors that share component features are deemed similar. This enables us to use ρ t (φ) as a novelty measure for states, by comparing the features of newly observed states to those in the history. If φ(s) has more novel component features, ρ t (φ) will be lower. By modelling the features as independent, and using count-based estimators as factor models, our method learns reasonable novelty estimates from very little data.</p><p>Example. Suppose we use a 3-D binary feature map and that after 3 timesteps the history of observed feature vectors is φ 1:3 = (0, 1, 0), (0, 1, 0), (0, 1, 0). Let us estimate the feature visit densities of two unobserved feature vectors φ = (1, 1, 0), and φ = (1, 0, 1). Using the KT estimator for the factor models, we have ρ 3 (φ ) = ρ 1 3 (1)·ρ 2 3 (1)·ρ 3 3 (0) = 1 8 · 7 8 · 7 8 ≈ 0.1, and ρ 3 (φ ) = ρ 1 3 (1)·ρ 2 3 (0)·ρ 3 3 (1) = ( 1 8 ) 3 ≈ 0.002. Note that ρ 3 (φ ) &gt; ρ 3 (φ ) because the component features of φ are more similar to those in the history. As desired, our novelty measure generalises across the state space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The φ-pseudocount</head><p>Here we adopt a recently proposed method for computing generalised visit-counts from density models <ref type="bibr" target="#b1">[Bellemare et al., 2016;</ref><ref type="bibr" target="#b4">Ostrovski et al., 2017]</ref>. By analogy with these pseudocounts, we derive two φ-pseudocounts from our feature visit-density.</p><p>Definition 2 (φ-pseudocount and Naive φ-pseudocount). Let ρ t (φ) be the feature visit-density after observing φ 1:t . Let ρ t (φ) denote the same density model after φ 1:t φ has been observed.</p><formula xml:id="formula_4">• The naive φ-pseudocountÑ φ t (s) for a state s ∈ S at time t isÑ φ t (s) = t · ρ t (φ(s)) • The φ-pseudocountN φ t (s) for a state s ∈ S at time t iŝ N φ t (s) = ρ t (φ(s))(1 − ρ t (φ(s))) ρ t (φ(s)) − ρ t (φ(s))</formula><p>Empirically,N φ t (s) is usually larger thanÑ φ t (s) and leads to better performance. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reinforcement Learning with φ-EB</head><p>Algorithm 1 Reinforcement Learning with LFA and φ-EB.</p><formula xml:id="formula_5">Require: β, t end while t &lt; t end do Observe φ(s), r t Compute ρ t (φ) = M i ρ i t (φ i ) for i in {1,. . . ,M} do Update ρ i t+1 with observed φ i end for Compute ρ t+1 (φ) = M i ρ i t+1 (φ i ) ComputeN φ t (s) = ρt(φ)(1−ρt+1(φ)) ρt+1(φ)−ρt(φ) Compute R φ t (s, a) = β √N φ t (s) Set r + t = r t + R φ t (s, a) Pass φ(s), r + t to RL algorithm to update θ t end while return θ tend</formula><p>Following traditional count-based exploration algorithms, we drive optimistic exploration by computing a bonus from the φ-pseudocount. Definition 3 (φ-Exploration Bonus). Let β ∈ R be a free parameter. The φ-exploration bonus for a state-action pair (s, a) ∈ S × A at time t is</p><formula xml:id="formula_6">R φ t (s, a) = β N φ t (s)</formula><p>As in the MBIE-EB algorithm, this bonus is added to the reward r t . The agent is trained on the augmented reward r + t = r t + R φ t (s, a) using any value-based RL algorithm with LFA. At each timestep our algorithm performs updates for at most M estimators, one for each feature. The cost of our method is therefore independent of the size of the stateaction space, and scales only in the number of features. If the feature vectors are sparse, we can maintain a single prototype estimator for all the features that have not yet been observed. Under these conditions our method scales only in the number of observed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Results</head><p>Here we formalise the comments made in section 3.1 by proving a bound that relates our pseudocount to an appropriate similarity measure. To simplify the analysis, we prove results for the naive φ-exploration bonusÑ φ t (s), though we expect analogous results to hold forN φ t (s) as well. We use the empirical estimator for the factor models in the visit-density.</p><formula xml:id="formula_7">1 − |φ i − φ k,i |</formula><p>The φ i = 1 case follows by an almost identical argument.</p><p>The following theorem and its corollary are the major results of this section. These connect the Hamming similarity (to previously observed feature vectors) with both the feature visit-density and the φ-pseudocount. We show that a state which shares few features with those in the history will be assigned low probability by our density model, and will therefore have a low φ-pseudocount.</p><p>Theorem 1 (Feature Visit-Density and Average Similarity). Let s ∈ S be a state with binary feature representation φ = φ(s) ∈ {0, 1} M , and let ρ t (φ) =</p><formula xml:id="formula_8">1 − 1 M φ − φ k 1 (c) = 1 t t k=1 Sim(φ, φ k )</formula><p>where (a) follows from Lemma 1, (b) from Lemma 2, and (c) from Definition 4.</p><p>We immediately get a similar bound for the naive φ-pseudocountÑ φ t (s). Corollary 1 (φ-pseudocount and Total Similarity).</p><formula xml:id="formula_9">N φ t (s) ≤ t k=1 Sim(φ, φ k )</formula><p>Proof. Immediate from Theorem 1 and Definition 2.</p><p>N φ t (s) therefore captures an intuitive relation between novelty and similarity to visited states. By visiting a state that minimises the φ-pseudocount, an agent also minimises a lower bound on its Hamming similarity to previously visited states. As desired, we have a novelty measure that is closely related to the distances between states in feature space, but which obviates the cost of computing those distances directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>Our evaluation is designed to answer the following research questions:</p><p>• Is a novelty measure derived from the features used for LFA a good way to generalise state visit-counts? • Does φ-EB produce improvement across a range of environments, or only if rewards are sparse? • Can φ-EB with LFA compete with the state-of-the-art in exploration and deep RL?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We evaluate our algorithm on five games from the Arcade Learning Environment (ALE), which has recently become a standard high-dimensional benchmark for RL <ref type="bibr" target="#b0">[Bellemare et al., 2013]</ref>. The reward signal is computed from the game score. The raw state is a frame of video (a 160×210 array of 7-bit pixels). There are 18 available actions. The ALE is a particularly interesting testbed in our context, because the difficulty of exploration varies greatly between games. Random strategies often work well, and it is in these games that Deep Q-Networks (DQN) with -greedy is able to achieve so-called human-level performance <ref type="bibr" target="#b1">[Mnih et al., 2015]</ref>. In others, however, DQN with -greedy does not improve upon a random policy, and its inability to explore efficiently is one of the key determinants of this failure <ref type="bibr" target="#b2">[Osband et al., 2016a]</ref>. We chose five of these games where exploration is hard. Three of the chosen games have sparse rewards (Montezuma's Revenge, Venture, Freeway) and two have dense rewards (Frostbite, Q*bert). <ref type="bibr">3</ref> Evaluating agents in the ALE is computationally demanding. We chose to focus more resources on Montezuma's Revenge and Venture, for two reasons: (1) we hypothesise that φ-EB will produce more improvement in sparse reward games, and (2) leading algorithms with which we seek to compare φ-EB have also focused on these games. We conducted five independent learning trials for Montezuma and Venture, and two trials for the remaining three games. All agents were trained for 100 million frames on the no-op metric <ref type="bibr" target="#b0">[Bellemare et al., 2013]</ref>. Trained agents were then evaluated for 500 episodes; <ref type="table" target="#tab_0">Table 1</ref> reports the average evaluation score.</p><p>We implement Algorithm 1 using Sarsa(λ) with replacing traces and LFA as our RL method, because it is less likely to diverge than Q-learning [Sutton and <ref type="bibr" target="#b5">Barto, 1998</ref>]. To implement LFA in the ALE we use the Blob-PROST feature set presented in <ref type="bibr" target="#b1">[Liang et al., 2016]</ref>. To date this is the best performing feature set for LFA in the ALE. The parameters for the Sarsa(λ) algorithm are set to the same values as in <ref type="bibr" target="#b1">[Liang et al., 2016]</ref>. Hereafter we refer to our algorithm as Sarsaφ-EB. To conduct a controlled investigation of the effectiveness of φ-EB, we also evaluate a baseline implementation of Sarsa(λ) with the same features but with -greedy exploration (which we denote Sarsa-). The same training and evaluation regime is used for both; learning curves are reported in <ref type="figure" target="#fig_1">Figure  1</ref>.</p><p>The β coefficient in the φ-exploration bonus was set to 0.05 for all games, after a coarse parameter search. This search was performed once, across a range of ALE games, and a value was chosen for which the agent achieved good scores in most games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Comparison with -greedy Baseline In Montezuma's Revenge, Sarsa-rarely leaves the first room. Its policy converges after an average of 20 million frames. Sarsa-φ-EB continues to improve throughout training, visiting up to 14 rooms. The largest improvement over the baseline occurs in Venture. Sarsa-fails to score, while Sarsa-φ-EB continues to improve throughout training. In Q*bert and Frostbite, the difference is less dramatic. These games have dense, well-shaped rewards that guide the agent's path through state space and elide -greedy's inefficiency. Nonetheless, Sarsa-φ-EB consistently outperforms Sarsathroughout training so its cumulative reward is much higher. In Freeway, Sarsa-φ-EB with β = 0.05 fails to match the performance of the baseline algorithm, but with β = 0.035 it performs better ( <ref type="figure" target="#fig_1">Figure 1</ref> shows the learning curve for the latter). This sensitivity to the β parameter likely results from the large number of unique Blob-PROST features that are active in Freeway, many of which are not relevant for finding the optimal policy. If β is too high the agent is content to stand still and receive exploration bonuses for observing new configurations of traffic. This accords with our hypothesis that efficient optimistic exploration should involve measuring novelty with respect to task-relevant features.</p><p>In summary, Sarsa-φ-EB with β = 0.05 outperforms Sarsa-on all tested games except Freeway. Since both use the same feature set and RL algorithm, and differ only in their exploration policies, this is strong evidence that φ-EB produces improvement over random exploration across a range of environments. This also supports our conjecture that using the same features for value function approximation and novelty estimation is an appropriate way to generalise visitcounts to the high-dimensional setting.  <ref type="bibr" target="#b5">[Schulman et al., 2015]</ref>, and TRPO-AE-SimHash (TRPO-Hash) <ref type="bibr">[Tang et al., 2016]</ref>. The most interesting comparisons for our purposes are with TRPO-Hash, DDQN-PC, A3C+, and MP-EB, because these algorithms all use exploration strategies that drive the agent to reduce its uncertainty. TRPO-Hash, DDQN-PC, and A3C+ are count-based methods, MP-EB seeks high model prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Leading Algorithms</head><p>Our Sarsa-φ-EB algorithm achieves an average score of 2745.4 on Montezuma: the second highest reported score. On this game it far outperforms every algorithm apart from DDQN-PC, despite only having trained for half the number of frames. Note that neither A3C+ nor TRPO-Hash achieves more than 200 points, despite their exploration strategies.</p><p>On Venture Sarsa-φ-EB also achieves state-of-the-art performance. It achieves the third highest reported score despite its short training regime, and far outperforms A3C+ and TRPO-Hash. DDQN-PC evaluation scores are not given for Venture, but reported learning curves suggest Sarsa-φ-EB performs much better here . The performance of Sarsa-φ-EB in Frostbite also seems competitive given the shorter training regime. Nonlinear algorithms perform better in Q*bert. In Freeway Sarsa-φ-EB fails to score any points, for reasons already discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced the φ-Exploration Bonus method, a count-based optimistic exploration strategy that scales to high-dimensional environments. It is simpler to implement and less computationally demanding than some other proposals. Our evaluation shows that it improves upon -greedy exploration on a variety of games, and that it is even competitive with leading exploration techniques developed for deep RL. Unlike other methods, it does not require the design of an exploration-specific state representation, but rather exploits the features used in the approximate value function. We have  argued that computing novelty with respect to these taskrelevant features is an efficient and principled way to generalise visit-counts for exploration. We conclude by noting that this reliance on the feature representation used for LFA is also a limitation. It is not obvious how a method like ours could be combined with the nonlinear function approximation techniques that have driven recent progress in RL. We hope the success of our simple method will inspire future work in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(MBIE-EB) [Strehl and Littman, 2008]. 1 Empirical estimatesP andR of the transition and reward functions are maintained, andR(s, a) is augmented with a bonus term β √ N (s,a) , where N (s, a) is the state-action visit-count, and β ∈ R is a theoretically derived constant. The Bellman optimality equation for the augmented action-value function isQ π (s, a) =R(s, a) + β √ N (s,a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Average training scores for Sarsa-φ-EB and the baseline Sarsa-. Dashed lines are min/max scores. Shaded regions describe one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>compares our evaluation scores to Double DQN (DDQN) [van Hasselt et al., 2016b], Double DQN with pseudocount (DDQN-PC) [Bellemare et al., 2016], A3C+ [Bellemare et al., 2016], DQN Pop-Art (DQN-PA) [van Hasselt et al., 2016a], Dueling Network (Dueling) [Wang et al., 2016], Gorila [Nair et al., 2015], DQN with Model Prediction Exploration Bonuses (MP-EB) [Stadie et al., 2015], Trust Region Policy Optimisation (TRPO)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Venture Montezuma's Revenge Freeway Frostbite Q*bert</figDesc><table><row><cell>Sarsa-φ-EB</cell><cell>1169.2</cell><cell>2745.4</cell><cell>0.0</cell><cell>2770.1</cell><cell>4111.8</cell></row><row><cell>Sarsa-</cell><cell>0.0</cell><cell>399.5</cell><cell>29.9</cell><cell>1394.3</cell><cell>3895.3</cell></row><row><cell>DDQN-PC</cell><cell>N/A</cell><cell>3459</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>A3C+</cell><cell>0</cell><cell>142</cell><cell>27</cell><cell>507</cell><cell>15805</cell></row><row><cell>TRPO-Hash</cell><cell>445</cell><cell>75</cell><cell>34</cell><cell>5214</cell><cell>N/A</cell></row><row><cell>MP-EB</cell><cell>N/A</cell><cell>0</cell><cell>12</cell><cell>380</cell><cell>N/A</cell></row><row><cell>DDQN</cell><cell>98</cell><cell>0</cell><cell>33</cell><cell>1683</cell><cell>15088</cell></row><row><cell>DQN-PA</cell><cell>1172</cell><cell>0</cell><cell>33</cell><cell>3469</cell><cell>5237</cell></row><row><cell>Gorila</cell><cell>1245</cell><cell>4</cell><cell>12</cell><cell>605</cell><cell>10816</cell></row><row><cell>TRPO</cell><cell>121</cell><cell>0</cell><cell>16</cell><cell>2869</cell><cell>7733</cell></row><row><cell>Dueling</cell><cell>497</cell><cell>0</cell><cell>0</cell><cell>4672</cell><cell>19220</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Average evaluation score for leading algorithms. Sarsa-φ-EB and Sarsa-were evaluated after 100M training frames on all games except Q*bert, for which they trained for 80M frames. DDQN-PC scores reflect evaluation after 100M training frames. The MP-EB agent was only trained for 20M frames. All other algorithms were evaluated after 200M frames. Leading scores are highlighted in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To the best of our knowledge, the first work to use exploration bonuses in the MDP setting was the Dyna-Q+ algorithm, in which the bonus is a function of the recency of visits to a state, rather than the visit-count<ref type="bibr" target="#b5">[Sutton, 1990]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The expression forN φ t (s) is derived by letting it depend on an implicit total pseudocountn that can be much larger than t, and assuming ρt(φ) =Nφ t (s) n· , and ρ t (φ) =N φ t (s)+1 n+1 [Bellemare et al., 2016].Since the feature set we use in our implementation is binary, our analysis assumes φ ∈ {0, 1} M . We begin by defining a similarity measure for binary feature vectors, and prove two lemmas.Definition 4 (Hamming Similarity for Binary Vectors). Let φ, φ ∈ {0, 1} M be M -length binary vectors. The Hamming similarity between φ and φ is Sim(φ, φ ) = 1 − 1 M φ − φ 1 . Note that Sim(φ, φ ) ∈ [0, 1] for all φ, φ ∈ {0, 1} M . TheHamming similarity is large if φ and φ share features (i.e. if the l 1 -distance between them is small). We now prove a lemma relating the joint probability of a feature vector to the sum of the probabilities of its factors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (φ i ) = 1 t N t (φ i ). Then for all φ i , φ k,i ∈ {0, 1}, ρ i t (φ i ) = 1 t t k=1 1 − |φ i − φ k,i |.Proof. Suppose φ i = 0:ρ i t (0) = 1 − ρ i t (1) = 1 − 1 t t k=1 φ k,i = 1 t t k=1 1 − |0 − φ k,i | = 1 t t k=1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">M i=1 ρ i t (φ i ) be its feature visit-density at time t. Then ρ t (φ) ≤ 1 t t k=1Sim(φ, φ k )</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that our experimental evaluation uses the stochastic version of the ALE [Bellemare et al., 2013].</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation. CoRR, abs/1606.01868</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Near-Bayesian exploration in polynomial time</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu</editor>
		<meeting><address><addrLine>Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria</addrLine></address></meeting>
		<imprint>
			<publisher>Praveen Srinivasan</publisher>
			<date type="published" when="1985" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
	<note>Autonomous Agents and Multi-Agent Systems. and David Silver. Massively parallel methods for deep reinforcement learning. CoRR, abs/1507.04296</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep exploration via bootstrapped DQN. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Osband</surname></persName>
		</author>
		<idno>abs/1602.04621</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Osband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bellemare, Aäron van den Oord, and Rémi Munos. Countbased exploration with neural density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ostrovski</surname></persName>
		</author>
		<idno>abs/1703.01310</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrated architecture for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1602.07714</idno>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Tools with Artificial Intelligence</title>
		<editor>Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel.</editor>
		<meeting><address><addrLine>Tom Schaul, Matteo Hessel</addrLine></address></meeting>
		<imprint>
			<publisher>Nando de Freitas</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
		<respStmt>
			<orgName>Sutton and Barto</orgName>
		</respStmt>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
