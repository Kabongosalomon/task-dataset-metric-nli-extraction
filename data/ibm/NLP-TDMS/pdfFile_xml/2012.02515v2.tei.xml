<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AuthNet: A Deep Learning based Authentication Mechanism using Temporal Facial Feature Movements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Raghavendra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravan</forename><surname>Omprakash</surname></persName>
							<email>pravanop@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Metallurgical Engineering</orgName>
								<orgName type="institution">National Institute of Technology Karnataka</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Kamath</surname></persName>
							<email>sowmyakamath@nitk.edu.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AuthNet: A Deep Learning based Authentication Mechanism using Temporal Facial Feature Movements</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biometric systems based on Machine learning and Deep learning are being extensively used as authentication mechanisms in resource-constrained environments like smartphones and other small computing devices. These AI-powered facial recognition mechanisms have gained enormous popularity in recent years due to their transparent, contact-less and non-invasive nature. While they are effective to a large extent, there are ways to gain unauthorized access using photographs, masks, glasses, etc. In this paper, we propose an alternative authentication mechanism that uses both facial recognition and the unique movements of that particular face while uttering a password, that is, the temporal facial feature movements. The proposed model is not inhibited by language barriers because a user can set a password in any language. When evaluated on the standard MIRACL-VC1 dataset, the proposed model achieved an accuracy of 98.1%, underscoring its effectiveness as an effective and robust system. The proposed method is also data efficient, since the model gave good results even when trained with only 10 positive video samples. The competence of the training of the network is also demonstrated by benchmarking the proposed system against various compounded Facial recognition and Lip reading models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Biometric authentication mechanisms have been long in use in digital systems for identifying and facilitating access to restricted systems or information meant only for authorized users. Facial authentication systems have gained widespread usage in the past few years because of its contact-less noninvasive nature and ease of use in verifying a person's identity from a user's image. Early image based facial recognition systems were built on hand-engineered features such as SIFT, LBP, and Fisher vectors. Recently, highly accurate deep learning methods such as FaceNet <ref type="bibr" target="#b11">(Schroff, Kalenichenko, and Philbin 2015)</ref>, Baidu  and DeepID models <ref type="bibr" target="#b8">(Ouyang et al. 2014;</ref><ref type="bibr" target="#b14">Sun, Wang, and Tang 2014;</ref><ref type="bibr" target="#b13">Sun et al. 2015)</ref> have surpassed human performance. Nevertheless, facial authentication systems have of- <ref type="figure">Figure 1</ref>: Data preprocessing Pipeline ten been bypassed by imposters using a photograph of the authorized user to gain access. To combat this vulnerability, models that track lip movement patterns of the user uttering a word are being explored. Deep learning models for lip reading such as LipNet <ref type="bibr">(Shillingford et al. 2018</ref>) have achieved a sentence-level classification accuracy of 95%. Various other lip reading models have been built on Hidden Markov models (HMM), Long Short Term Memory (LSTM) Networks, Convolutional Neural Networks (CNN), etc. However, they perform poorly in different lighting conditions and rely solely on lip movement and lip features. Deep face recognition models combined with images without the need for audio present a unique approach that are safer than basic face recognition systems and are immune to noisy environments, hence addressing the security and privacy concerns. This paper proposes an authentication model that captures the temporal facial movement patterns of a person uttering a particular word. A VGGNet model  called VGGFace <ref type="bibr" target="#b9">(Parkhi, Vedaldi, and Zisserman 2015)</ref> pre-trained on faces was employed to capture the facial features and then the sequence of features were passed through several LSTM layers, and the model learns to predict whether a valid user is speaking their chosen password or not. The model is not provided with the actual word of the chosen password, but only the videos of its utterance. Hence, it is inherently language invariant and can be any combination of characters and numbers if spoken continuously. The proposed model was benchmarked on the publicly available standard dataset, MIRACL-VC1 <ref type="bibr" target="#b10">(Rekik, Ben-Hamadou, and Mahdi 2014)</ref>, and to test the system's performance on real-Figure 2: VGGFace Architecture (Adapted from <ref type="bibr" target="#b9">(Parkhi, Vedaldi, and Zisserman 2015)</ref>) world data and requirements, a separate dataset that consists of videos from smartphone cameras taken under varying conditions was compiled. The system was tested on this collated dataset as well.</p><p>The key contribution of this paper is a strong video based facial authentication system, which uses a video of a person uttering a password, while correctly identifying the as imposter cases, which are 1) same person uttering a different word (Same Person case) or, 2) another person trying to gain access, and prevent them from gaining access ( Different person case).Another key contribution and salient point of this model is that it is language independent and domain agnostic. To our knowledge such a system has never been implemented.</p><p>The rest of this paper has been structured as follows. Section 2, discusses relevant works in the field of lip reading and authentication and the observed gaps that invite further improvements. Section 3 details the proposed approach, Section 4 describes the process by which the model was evaluated and validated, followed by conclusion and references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Several deep learning based face recognition models have been proposed and are popular currently <ref type="bibr" target="#b1">(Balaban 2015)</ref>. FaceNet <ref type="bibr" target="#b11">(Schroff, Kalenichenko, and Philbin 2015)</ref>, Baidu  and the DeepID models <ref type="bibr" target="#b8">(Ouyang et al. 2014;</ref><ref type="bibr" target="#b14">Sun, Wang, and Tang 2014;</ref><ref type="bibr" target="#b13">Sun et al. 2015)</ref> have shown near perfect results on benchmark datasets. These face recognition systems have been trained on millions of images and have surpassed human-level performance. However, they are still susceptible to attacks if used as authentication systems. In this section, the state-of-the-art works in this field is discussed and few observed gaps for further improvement in the performance of these systems are presented.</p><p>Mathulaprangsan et al. <ref type="bibr" target="#b7">(Mathulaprangsan et al. 2015</ref>) studied the various techniques used in lip reading for word classification as well as lip password authentication. Various localization, segmentation and classification approaches have been used, including conventional algorithms like FCM and LCACM that are used for lip localization in an image. They reported that the most common classifiers used for visual speech recognition at the time were, namely, Gaus-sian Mixture Models (GMMs) and Hidden Markov Models (HMMs). <ref type="bibr" target="#b6">Liu et al. (Liu and Cheung 2014)</ref> proposed a lip password verification model using Multi-boosted HMMs, focusing solely on the behavioral biometrics of lip movements. They extracted visual sequences of the mouth area region and ran them through an algorithm that segments lips in order to split the password into sub units, on which Multiboosted HMMs were applied to formulate a decision boundary. They reported an equal error rate (EER) of 4.06%, however, the passwords that could be used were constrained to only digits. Noda et al. (Özcan and Basturk 2019) proposed a deep learning based visual speech recognition system that uses a CNN that is trained on mouth area images to predict phenomes. The outputs of the CNN were regarded as sequences and a HMM+GMM observation model was used for the word recognition task. Their work demonstrated that visual features acquired with a CNN generalized to different domains outperformed conventional approaches.</p><p>Wand et al. <ref type="bibr" target="#b16">(Wand, Koutnik, and Schmidhuber 2016)</ref> propose a different approach for lip reading in the form of LSTM based neural networks. Sequences of mouth area images, whose features were extracted with a Histogram of Oriented Gradients (HOG) were fed into the LSTM, which was used for word classification. Their approach performed significantly better than the conventional SVM and HMM classifiers. Garg et al. (amit, jnoyola, and sameepb 2016) designed a new approach using a hybrid CNN and LSTM model to classify words, which was trained on the same dataset that is used in this paper, MIRACL-VC1 <ref type="bibr" target="#b10">(Rekik, Ben-Hamadou, and Mahdi 2014)</ref>. They compared different approaches that included CNN+LSTM, purely CNN and SVM classifiers. They concluded that the purely CNN approach worked best for visual speech recognition attaining a 61.2% and also noted that with more training on the CNN+LSTM model the performance might improve.</p><p>Shillingford et al. <ref type="bibr">(Shillingford et al. 2018</ref>) introduced a novel approach called LipNet that performs lip reading classification on sentence level with an accuracy of 95%. They made use of a combination of spatio-temporal convolutions and recurrent neural networks to outperform human lip readers becoming the state of the art model.</p><p>From the discussion presented on several previous works and the state-of-the-art models in the field of facial recog-nition based biometric systems, several vulnerabilities that need to be addressed for further improvements were observed. Facial recognition systems are not immune to attacks and vulnerabilities in the real world. Existing models for lip password verification have not achieved high accuracy, and the passwords are constrained to specific characters, for instance, only digits can be used. Moreover, most models are restricted to a certain language that they are trained on. In view of these limitations, an attempt has been made to solve these issues in this work, by designing an model that is inherently language and domain agnostic. The details of the proposed model are presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach</head><p>The proposed model uses a 2 step CNN-RNN architecture depicted in <ref type="figure" target="#fig_0">Fig. 3</ref>. A pre-trained VGGFace model <ref type="bibr" target="#b9">(Parkhi, Vedaldi, and Zisserman 2015)</ref> is used to extract the essential facial features at each timestep. To identify and learn the movement patterns of these features, a small network of stacked LSTM layers is employed. After the training is complete, this network can determine if a particular test case's person-word combination matches the trained person-word combination.</p><p>The MIRACL-VC1 Words dataset consists of 10 speakers speaking 10 different words, 10 times each. This has been split into different sections for different forms of testing and validation. It was randomly chosen to set aside 5 speakers i.e 5 X 10 X 10 = 500 utterances as unseen data for the model to exhibit its robustness against the Different Person Case (mentioned above). Among the remaining 5 speakers, we again set aside 3 words each, i.e 5 X 3 X 10 = 150 utterances to test the effectiveness of the model for the Same Person Case. Thus, a total of 650 examples were set aside. The remaining set has 5 X 7 X 10 = 350 utterances, in which 10 examples are positive(from the right speaker saying right word), 60 are negative samples because of the right speaker saying wrong word and 280 examples of wrong speakers. The 10 positive samples are oversampled to generate 100 new positive samples, giving a total of 350 + 100 = 450 examples. This is split into a 70 : 30 train-test split, and the the 650 examples that were set aside before are then added to this test set as negative samples,to cover all the aforementioned imposter cases.This approach is adopted to ensure that we are dealing with completely new data that model can possibly have never seen before.</p><p>In summary, for any given person-word combination, there are 10 samples of correct person correct word (oversampled to 110), 90 samples of correct person saying wrong word, 90 samples of wrong person saying correct word and 810 samples of wrong person saying wrong word, making the total of 1000 words i.e 10 speakers uttering 10 words, 10 times each.</p><p>To provide a comprehensive analysis of the performance and demonstrate the generality and robustness of the system, the model was trained in an iterative manner on all such possible combinations and tested against unseen examples. So, the training is repeated for the 5 randomly chosen speakers, each speaking 7 different words, generating 35 possible person-word combinations.</p><p>Data preprocessing. Each training and testing sample in the MIRACL-VC1 dataset is a sequence of images of the person uttering a password, typically consisting of 5 to 15 images, as depicted in <ref type="figure">Fig. 1</ref>. Only the color images for the words have been utilised in training. The images are first passed through a Haar cascade face detector <ref type="bibr" target="#b15">(Viola and Jones 2004)</ref> to crop out the region containing faces from the images. The Haar cascade detector is commonly used to detect faces and face features like eyes, lips, nose, etc., and is available in the OpenCV library. It can be used to detect faces or objects in general. The algorithm works in four steps; starting with selecting Haar features, creating a set of images for easier processing, training with Adaboost and passing them through a set of cascading classifiers. To ensure uniformity in number of timesteps, the images are padded with white images. Each image is then resized into 224 X 224 pixels to feed it into the pre-trained VGGNet model. For the manually compiled dataset, a similar approach was followed. This dataset represents a subset of possible real world conditions. They are first segmented into images using a constant frame rate, padded to ensure uniformity in number of timesteps and resized into 224 X 224 pixels to be fed into the VGGFace model.</p><p>VGGFace. Each image is passed through the pre-trained VGGFace model, which consists of a long sequence of convolutional layers that are trained on hundreds of thousands of images of celebrity faces. We obtain a 2,622 dimensional feature vector output for each image. This process is repeated for each word and for each person (as illustrated in <ref type="figure">Fig. 4)</ref>, and reshaped to make it suitable for feeding into the LSTM layer. Thus each sample has 20 timesteps and 2622 features. The samples are labelled 1 for the correct personword combination, and 0 for all other combinations, making this a binary classification problem. The resulting class imbalance was solved by using the technique of oversampling.</p><p>LSTM network. LSTM networks have reported astounding results across a wide variety of application domains where data is sequential. So, a network of 4 LSTM layers is employed, each consisting of 20 timesteps and an output sigmoid layer for predicting probabilities. Once trained, this model can determine if the input test video contains the correct person-word combination or not, so that an authenticated user can be granted access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Evaluation &amp; Metrics</head><p>Several experiments were performed to validate the proposed approach. The model was trained on a system with an i7-8640U CPU and 8GB of RAM. The model was bench marked on the standard MIRACL-VC1 dataset and also on a collated dataset, which consists of videos, each of 2 seconds duration. The videos were obtained from a OnePlus 6 smartphone with 1080p resolution and 16MP camera, a OnePlus 7 smartphone with 1080p resolution and 16MP camera, and an Apple iPhone XR smartphone with 720p resolution and  The proposed pipeline has an binary classification task as the objective, and is designed to take a series of images of size 640×480, ordered according to timesteps, as input. The images were then passed through the pretrained VGGFace model as described in <ref type="figure">Figure 2</ref>. The resultant feature vectors were then used for training and evaluating the LSTM network as described in <ref type="figure" target="#fig_0">Figure 3</ref>. The proposed model was trained using the binary crossentropy (BCE) loss function (calculated as per Eq. (1)) and the Adam optimizer, with an initial learning rate of 0.001 for 60 epochs and a batch size of 75 .</p><formula xml:id="formula_0">BCE = −(y log(p) + (1 − y) log(1 − p))</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>Various standard metrics were employed for assessing the performance of the proposed model, and to demonstrate its effectiveness. Sensitivity (measure as per Eq. (2)) evaluates the acceptance ratio of the system i.e., the probability that the authorised user gains rightful access. Specificity measures the rejection ratio of the system i.e., probability that an unauthorised attacker is thwarted from gaining access, given by Eq. <ref type="figure" target="#fig_0">(3)</ref>. Accuracy is used to measure the overall performance of the model (given by Eq. (4)) and it demonstrates the efficacy of the authentication mechanism as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity = T P T P + F N</head><p>(2)</p><formula xml:id="formula_1">Specificity = T N T N + F P<label>(3)</label></formula><formula xml:id="formula_2">Accuracy = T P + T N T P + T N + F P + F P<label>(4)</label></formula><p>TP and TN stand for True Positive samples, whereas FP and FN stand for False Positive and False Negative samples. Two other metrics were also used to evaluate the other aspects of the proposed model. The Equal Error Rate (EER) metric is defined as a point where the the probability of an intruder gaining access is equal to the probability that an authorised user is refused access. Hence, it is also the point where False Acceptance Ratio (FAR) is equal to the False Rejection Rate (FRR). FAR is calculated as per Eq. 5 and FRR is calculated as per Eq. 6. EER is a standard metric used for comparison across biometric systems. The EER was calculated from the Receiver Operating Characteristic curve (ROC) as this is less dependent on scaling. The ROC Curve is plotted and depicted in <ref type="figure" target="#fig_1">Fig. 5</ref>.</p><formula xml:id="formula_3">FAR = F P T P + T N + F P + F N (5) FRR = F N T P + T N + F P + F N<label>(6)</label></formula><p>The ROC curve is a curve of True positive rate (TPR) against False positive rate (FPR or FAR) as the threshold is varied. The ROC curve acts as a cost-benefit analysis while making decisions. It is also used to determine the threshold to ensure most true positives are gained for each false positives that the pipeline incurs.This ensures that allow as much authorised accesses are allowed while limiting the amount of unauthorised access that's allowed by the system. From the ROC curve intersecting with the line x + y = 1 , point of equal error is found out. The Area Under the Receiver Operating Characteristic curve (AUC) is calculated using the ROC curve. The probability that the model scores a random positive example higher than a random negative example is called AUC. AUC is a threshold-independent metric and hence measures the confidence that the model has in its decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>To quantify AuthNet's performance, a cross validation dataset was prepared, that accounted for the various imposter cases available for the person-word combination, namely, the same person saying a different word, a different person saying the same word and a different person saying a different word. The different speakers chosen for testing were that of new faces completely unseen by the model during the training phase. The different words spoken by the same person were also chosen such that they are completely new and hence the capability of the model to generalize to new person-word combinations is thoroughly tested. The performance of the proposed model with other metrics methods are shown in <ref type="table" target="#tab_2">Table 3</ref> and the confusion matrix is shown in <ref type="figure">Fig. 6</ref>.</p><formula xml:id="formula_4">cos dif f (t, e) = 1 − n i=1 t i e i n i=1 (t i ) 2 n i=1 (e i ) 2<label>(7)</label></formula><p>Cosine Difference, measured as per Eq. <ref type="formula" target="#formula_4">(7)</ref>, was used to compute the differences between images. The purpose of measuring cosine difference between the feature vectors across speakers and words is to show that there is a significant difference between the feature vectors obtained through the VGGFace model, among different people and even different words spoken by the same person as time progresses. This difference in feature vectors between different people and different words provides an empirical justification as to how facial features differ when people utter their passwords.Hence, when the pre-trained VGGFace model extracts a feature vector for each image, the considerable differences across images can be then be captured by the LSTM network. This is visualised in <ref type="figure">Fig. 4)</ref>, This underscores the fact that the proposed model is capable of handling imposters uttering the same password, thus allowing it to be an open password system.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, the high sensitivity value of 0.998 indicates that the system incorrectly classifies an input user-password combination 2 times out of 1000 only, to recognize the authorised user-password combination accurately. It is a good  <ref type="bibr" target="#b2">(Chung et al. 2017)</ref> 0.652 FaceNet + Gergen et al. <ref type="bibr" target="#b3">(Gergen et al. 2016)</ref> 0.864 FaceNet + LipNet <ref type="bibr" target="#b16">(Wand, Koutnik, and Schmidhuber 2016)</ref> 0.951 measure of AuthNet's ability to capture true positives. The high specificity value demonstrates the model's ability to correctly classify imposter attacks. The individual specificity errors for the different imposter cases are shown in <ref type="table" target="#tab_3">Table 4</ref>. The values show that the system denies the attacks from a different person around 98% of the time, while it correctly identifies a wrong password being spoken 94% of the time. This indicates the model's ability to act as an excellent defense mechanism against attacks by malicious entities, thereby ensuring a high level of privacy. From the ROC curve in <ref type="figure" target="#fig_1">Fig. 5</ref> the point where the TPR is equal to 0.977 and FPR to be 0.023 is calculated, thus the EER value is 0.023. A low equal error rate signifies the pipeline's ability to reduce both false acceptances and false rejections to a minimum. The AUC was found to be 0.990, which signifies the pipeline's confidence in its prediction. On the collated dataset obtained from a capturing smartphone camera videos, we find that the performance of the model is on par with the performance on the MIRACL-VC1 dataset developed in a controlled lab setting. This signifies that the system generalises well to real world cases, even in the presence of image variance and varied lighting conditions. So, the system has an inherent capability to capture patterns unique to the user, and the results are not just a product of overfitting to the data from a simulated environment with identical background and lighting conditions. To demonstrate the effectiveness of the training, AuthNet model is benchmarked against a two-level system -several state-of-the-art models in face recognition combined with lip reading. Since FaceNet <ref type="bibr" target="#b11">(Schroff, Kalenichenko, and Philbin 2015)</ref> is the current best-performing face recognition model reporting an accuracy of 99.76%, it was used for comparison. <ref type="bibr">Garg et al. (amit, jnoyola, and sameepb 2016)</ref> proposed a lip reading model that achieved an accuracy of 61.2%. The models of <ref type="bibr" target="#b2">Chung &amp; Zisserman (Chung et al. 2017</ref>) and Gergen et al. <ref type="bibr" target="#b3">(Gergen et al. 2016)</ref> were the previous state-of-theart models in lip reading on word level with their reported accuracy values of 65.4% and 86.6% respectively. Lipnet <ref type="bibr">(Shillingford et al. 2018)</ref> has been included to benchmark the performance of the proposed AuthNet model. The scores and comparisons are reported in <ref type="table" target="#tab_1">Table 2</ref>, which shows that the performance of the trained model matches the combined performance of state-of-the-art models while overcoming errors or imposter attacks in these models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, AuthNet, a temporal facial movement based authentication mechanism using 2-level trained CNN-RNN deep neural models was presented. When evaluated on the standard MIRACL-VC1 dataset, AuthNet achieved an accuracy of 98.1%, underscoring its effectiveness. The results obtained from cross-validation on a collated dataset proved its inherent language and domain agnostic nature. It also demonstrated the real-world application of such a system in mobile phones and other resource constrained smart devices, involving varying backgrounds, different lighting contents, varying video resolutions, etc. AuthNet can function as an open password system, as the model effectively classified the same password being spoken by a different person. Hence, disclosing the password does not cause any security concerns. It also has no language barrier, since the model is not dependent on knowledge of the language in which the password/phrase is being spoken, but only the temporal facial movements of the speaker extracted from the videos of its utterance. The proposed method is also data efficient, since the model gave good results even when trained with only 10 positive video samples. As part of future work, the proposed model could be optimised in its time complexity, as it takes approximately 2 seconds for testing per sample currently. Since fast processing is a critical requirement for deployment as an authentication system for smartphones and computers, optimising the testing time will be a primary concern to be addressed, which will be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Proposed Architecture built on LSTM Network Figure 4: Variations in Features Extracted using VGGFace 7MP camera. They were captured under different lighting conditions and also against varying backgrounds. The detailed statistics of each dataset used is described in Table 1. The proposed model took 154.2 seconds per person-word combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>ROC Curve for the MIRACL-V1 dataset Figure 6: Confusion Matrix of proposed AuthNet model for the MIRACL-VC1 dataset Figure 7: Performance analysis of the Proposed Approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell>Dataset</cell><cell cols="2">MIRACL-VC1 Collated Dataset</cell></row><row><cell># Speakers</cell><cell>10</cell><cell>3</cell></row><row><cell># Words</cell><cell>10</cell><cell>3</cell></row><row><cell># Utterances</cell><cell>10</cell><cell>6</cell></row><row><cell>Image Resolution</cell><cell>640 × 480 × 3</cell><cell>350 × 640 × 3</cell></row><row><cell>Language used</cell><cell>English</cell><cell>English and Hindi</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Benchmarking the proposed AuthNet model against State-of-the-art models</figDesc><table><row><cell>Models</cell><cell>Test Accuracy</cell></row><row><cell>AuthNet (Proposed)</cell><cell>0.980</cell></row><row><cell>FaceNet + Garg et al.</cell><cell></cell></row><row><cell>(amit, jnoyola, and sameepb 2016)</cell><cell>0.611</cell></row><row><cell>FaceNet + Chung &amp; Zisserman</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of proposed AuthNet model w.r.t different datasets</figDesc><table><row><cell>Metric</cell><cell cols="2">MIRACL-VC1 Collated Dataset</cell></row><row><cell>Sensitivity</cell><cell>0.998</cell><cell>0.987</cell></row><row><cell>Specificity</cell><cell>0.969</cell><cell>0.976</cell></row><row><cell>Accuracy</cell><cell>0.980</cell><cell>0.981</cell></row><row><cell>AUC Score</cell><cell>0.990</cell><cell>0.989</cell></row><row><cell>Equal Error Rate</cell><cell>0.023</cell><cell>0.037</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Specificity for different types of errors (MIRACL-VC1 dataset)</figDesc><table><row><cell>Imposter types/Error cases</cell><cell cols="2">Samples Specificity</cell></row><row><cell>Same Person-Different Words</cell><cell>210</cell><cell>0.9420</cell></row><row><cell>Different Person-Same Words</cell><cell>150</cell><cell>0.9893</cell></row><row><cell>Different Person-Different Words</cell><cell>100</cell><cell>0.9730</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Jnoyola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename></persName>
		</author>
		<title level="m">Lip reading using CNN and LSTM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning and face recognition: the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balaban</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2181526</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">94570</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lip Reading Sentences in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.367</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussen Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-166</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2135" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Targeting Ultimate Accuracy: Face Recognition via Deep Embedding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Very deep convolutional neural network based image classification using small training sample size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACPR.2015.7486599</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Multi-Boosted HMMs for Lip-Password Based Speaker Verification. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIFS.2013.2293025</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A survey of visual lip reading and lip-password verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathulaprangsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICOT.2015</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298854</idno>
		<title level="m">DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.29.41</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A New Visual Speech Recognition Approach for RGB-D Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rekik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Hamadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mahdi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11755-</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mulville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coppin</surname></persName>
		</author>
		<title level="m">Senior, A.; and Freitas, N. 2018. Large-Scale Visual Speech Recognition</title>
		<meeting><address><addrLine>Laurie, B</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">DeepID3: Face Recognition with Very Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Learning Face Representation by Joint Identification-Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 27</title>
		<meeting>NIPS 27</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust Real-Time Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000013087.49260.fb</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lip Reading Using Convolutional Neural Networks with and without Pre-Trained Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>; Ö Zcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basturk</surname></persName>
		</author>
		<idno type="DOI">10.17694/bajece.479891</idno>
	</analytic>
	<monogr>
		<title level="j">Balkan Journal of Electrical and Computer Engi</title>
		<imprint>
			<biblScope unit="page" from="6115" to="6119" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Lipreading with long short-term memory</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
