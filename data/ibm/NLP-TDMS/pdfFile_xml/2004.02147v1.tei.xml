<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
							<email>changqianyu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multi-spectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technol-ogy</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
							<email>cgao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multi-spectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technol-ogy</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong 4 Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Gang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on Multi-spectral Information Processing</orgName>
								<orgName type="institution">Huazhong University of Science and Technol-ogy</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) * Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic Segmentation · Real-time Processing · Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The low-level details and high-level semantics are both essential to the semantic segmentation task. However, to speed up the model inference, current approaches almost always sacrifice the low-level details, which leads to a considerable accuracy decrease. We propose to treat these spatial details and categorical semantics separately to achieve high accuracy and high efficiency for real-time semantic segmentation. To this end, we propose an efficient and effective architecture with a good trade-off between speed and accuracy, termed Bilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a Detail Branch, with wide channels and shallow layers to capture low-level details and generate high-resolution feature representation; (ii) a Semantic Branch, with narrow channels and deep layers to obtain high-level semantic context. The Semantic Branch is lightweight due to reducing the channel capacity and a fast-downsampling strategy. Furthermore, we design a Guided Aggregation Layer to enhance mutual connections and fuse both types of feature representation. Besides, a booster training strategy is designed to improve the segmentation performance without any extra inference cost. Extensive quantitative and qualitative evaluations demonstrate that the pro-posed architecture performs favourably against a few state-of-the-art real-time semantic segmentation approaches. Specifically, for a 2,048×1,024 input, we achieve 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than existing methods, yet we achieve better segmentation accuracy. Code and trained models will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction Semantic segmentation is the task of assigning semantic labels to each pixel. It is a fundamental problem in computer vision with extensive applications, including scene understanding <ref type="bibr" target="#b76">(Zhou et al., 2019)</ref>, autonomous driving <ref type="bibr" target="#b15">(Cordts et al., 2016;</ref><ref type="bibr" target="#b17">Geiger et al., 2012)</ref>, human-arXiv:2004.02147v1 [cs.CV] 5 Apr 2020 machine interaction and video surveillance, just to name a few. In recent years, with the advance of convolutional neural network <ref type="bibr" target="#b29">(Krizhevsky et al., 2012)</ref>, a series of semantic segmentation methods <ref type="bibr" target="#b72">(Zhao et al., 2017;</ref><ref type="bibr" target="#b11">Chen et al., 2017;</ref><ref type="bibr" target="#b66">Yu et al., 2018b;</ref><ref type="bibr" target="#b12">Chen et al., 2018;</ref><ref type="bibr" target="#b70">Zhang et al., 2018a)</ref> based on fully convolutional network (FCN) <ref type="bibr" target="#b36">(Long et al., 2015)</ref> have constantly advanced the state-of-the-art performance. The high accuracy of these methods depends on their backbone networks. There are two main architectures as the backbone networks: (i) Dilation Backbone, removing the downsampling operations and upsampling the corresponding filter kernels to maintain high-resolution feature representation <ref type="bibr" target="#b9">(Chen et al., 2015</ref><ref type="bibr" target="#b72">Zhao et al., 2017</ref><ref type="bibr" target="#b74">Zhao et al., , 2018b</ref><ref type="bibr" target="#b16">Fu et al., 2019;</ref><ref type="bibr" target="#b67">Yu et al., 2020)</ref>, as shown in <ref type="figure">Figure 2</ref> (a). (ii) Encoder-Decoder Backbone, with top-down and skip connections to recover the high-resolution feature representation in the decoder part <ref type="bibr" target="#b33">(Lin et al., 2017;</ref><ref type="bibr" target="#b44">Peng et al., 2017;</ref><ref type="bibr" target="#b66">Yu et al., 2018b)</ref>, as illustrated in <ref type="figure">Figure 2</ref> (b). However, both architectures are designed for general semantic segmentation tasks with less care about the inference speed and computational cost. In the dilation backbone, the dilation convolution is time-consuming and removing down-sampling operation brings heavy computation complexity and memory footprint. Numerous connections in the encoder-decoder architecture are less friendly to the memory access cost <ref type="bibr" target="#b37">(Ma et al., 2018)</ref>. However, the real-time semantic segmentation applications demand for an efficient inference speed.</p><p>Facing this demand, based on both backbone networks, existing methods <ref type="bibr" target="#b1">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b43">Paszke et al., 2016;</ref><ref type="bibr" target="#b73">Zhao et al., 2018a;</ref><ref type="bibr" target="#b48">Romera et al., 2018;</ref><ref type="bibr" target="#b38">Mazzini, 2018)</ref> mainly employ two appraches to accelerate the model: (i) Input Restricting. Smaller input resolution results in less computation cost with the same network architecture. To achieve real-time inference speed, many algorithms <ref type="bibr" target="#b73">(Zhao et al., 2018a;</ref><ref type="bibr" target="#b48">Romera et al., 2018;</ref><ref type="bibr" target="#b38">Mazzini, 2018;</ref><ref type="bibr" target="#b48">Romera et al., 2018)</ref> attempt to restrict the input size to reduce the whole computation complexity; (ii)Channel Pruning. It is a straight-forward acceleration method, especially pruning channels in early stages to boost inference speed <ref type="bibr" target="#b1">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b43">Paszke et al., 2016;</ref><ref type="bibr" target="#b14">Chollet, 2017)</ref>. Although both manners can improve the inference speed to some extent, they sacrifice the low-level details and spatial capacity leading to a dramatic accuracy decrease. Therefore, to achieve high efficiency and high accuracy simultaneously, it is challenging and of great importance to exploit a specific architecture for the real-time semantic segmentation task.</p><p>We observe that both of the low-level details and high-level semantics are crucial to the semantic seg-mentation task. In the general semantic segmentation task, the deep and wide networks encode both information simultaneously. However, in the real-time semantic segmentation task, we can treat spatial details and categorical semantics separately to achieve the trade-off between the accuracy and inference speed.</p><p>To this end, we propose a two-pathway architecture, termed Bilateral Segmentation Network (BiSeNet V2), for real-time semantic segmentation. One pathway is designed to capture the spatial details with wide channels and shallow layers, called Detail Branch. In contrast, the other pathway is introduced to extract the categorical semantics with narrow channels and deep layers, called Semantic Branch. The Semantic Branch simply requires a large receptive field to capture semantic context, while the detail information can be supplied by the Detail Branch. Therefore, the Semantic Branch can be made very lightweight with fewer channels and a fast-downsampling strategy. Both types of feature representation are merged to construct a stronger and more comprehensive feature representation. This conceptual design leads to an efficient and effective architecture for real-time semantic segmentation, as illustrated in <ref type="figure">Figure 2</ref> (c).</p><p>Specifically, in this study, we design a Guided Aggregation Layer to merge both types of features effectively. To further improve the performance without increasing the inference complexity, we present a booster training strategy with a series of auxiliary prediction heads, which can be discarded in the inference phase. Extensive quantitative and qualitative evaluations demonstrate that the proposed architecture performs favourably against state-of-the-art real-time semantic segmentation approaches, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The main contributions are summarized as follows:</p><p>-We propose an efficient and effective two-pathway architecture, termed Bilateral Segmentation Network, for real-time semantic segmentation, which treats the spatial details and categorical semantics separately. -For the Semantic Branch, we design a new lightweight network based on depth-wise convolutions to enhance the receptive field and capture rich contextual information. -A booster training strategy is introduced to further improve the segmentation performance without increasing the inference cost. -Our architecture achieves impressive results on different benchmarks of Cityscapes <ref type="bibr" target="#b15">(Cordts et al., 2016)</ref>, CamVid <ref type="bibr" target="#b5">(Brostow et al., 2008a)</ref>, and COCO-Stuff <ref type="bibr" target="#b7">(Caesar et al., 2018)</ref>. More specifically, we obtain the results of 72.6% mean IoU on the Cityscapes test set with the speed of 156 FPS on one NVIDIA GeForce GTX 1080Ti card. A preliminary version of this work was published in <ref type="bibr" target="#b65">(Yu et al., 2018a)</ref>. We have extended our conference version as follows. (i) We simplify the original structure to present an efficient and effective architecture for real-time semantic segmentation. We remove the timeconsuming cross-layer connections in the original version to obtain a more clear and simpler architecture. (ii) We re-design the overall architecture with more compact network structures and well-designed components. Specifically, we deepen the Detail Path to encode more details. We design light-weight components based on the depth-wise convolutions for the Semantic Path. Meanwhile, we propose an efficient aggregation layer to enhance the mutual connections between both paths. (iii) We conduct comprehensive ablative experiments to elaborate on the effectiveness and efficiency of the proposed method. (iv) We have significantly improved the accuracy and speed of the method in our previous work, i.e., for a 2048 × 1024 input, achieving 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080Ti card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have witnessed significant advances in image semantic segmentation. In this section, our discussion mainly focuses on three groups of methods most relevant to our work, i.e., generic semantic segmentation methods, real-time semantic segmentation methods, and light-weight architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generic Semantic Segmentation</head><p>Traditional segmentation methods based on the threshold selection <ref type="bibr" target="#b42">(Otsu, 1979)</ref>, the region growing (Vincent and Soille, 1991), the super-pixel <ref type="bibr" target="#b47">(Ren and Malik, 2003;</ref><ref type="bibr" target="#b0">Achanta et al., 2012;</ref><ref type="bibr" target="#b2">Van den Bergh et al., 2012)</ref> and the graph <ref type="bibr" target="#b4">(Boykov and Jolly, 2001;</ref><ref type="bibr" target="#b50">Rother et al., 2004)</ref> algorithms adopt the hand-crafted features to solve this problem. Recently, a new generation of algorithms based on FCN <ref type="bibr" target="#b36">(Long et al., 2015;</ref><ref type="bibr" target="#b53">Shelhamer et al., 2017)</ref> keep improving state-of-the-art performance on different benchmarks. Various methods are based on two types of backbone network: (i) dilation backbone network; (ii) encoder-decoder backbone network.</p><p>On one hand, the dilation backbone removes the downsampling operations and upsamples the convolution filter to preserve high-resolution feature representations. Due to the simplicity of the dilation convolution, various methods <ref type="bibr" target="#b9">(Chen et al., 2015</ref><ref type="bibr" target="#b72">Zhao et al., 2017;</ref><ref type="bibr" target="#b62">Wang et al., 2018a;</ref><ref type="bibr" target="#b70">Zhang et al., 2018a;</ref><ref type="bibr" target="#b67">Yu et al., 2020)</ref> develop different novel and effective components on it. The Deeplabv3  devises an atrous spatial pyramid pooling to capture multiscale context, while the PSPNet <ref type="bibr" target="#b72">(Zhao et al., 2017)</ref> adopts a pyramid pooling module on the dilation backbone. Meanwhile, some methods introduce the attention mechanisms, e.g., self-attention <ref type="bibr" target="#b69">(Yuan and Wang, 2018;</ref><ref type="bibr" target="#b16">Fu et al., 2019;</ref><ref type="bibr" target="#b67">Yu et al., 2020)</ref>, spatial attention <ref type="bibr" target="#b74">(Zhao et al., 2018b)</ref> and channel attention <ref type="bibr" target="#b70">(Zhang et al., 2018a)</ref>, to capture long-range context based on the dilation backbone.</p><p>On the other hand, the encoder-decoder backbone network adds extra top-down and lateral connections to recover the high-resolution feature maps in the decoder part. FCN and Hypercolumns <ref type="bibr" target="#b20">(Hariharan et al., 2015)</ref> adopt the skip connection to integrate the low-level feature. Meanwhile, U-net <ref type="bibr" target="#b49">(Ronneberger et al., 2015)</ref>, Seg-Net with saved pooling indices <ref type="bibr" target="#b1">(Badrinarayanan et al., 2017)</ref>, RefineNet with multi-path refinement <ref type="bibr" target="#b33">(Lin et al., 2017)</ref>, LRR with step-wise reconstruction <ref type="bibr" target="#b18">(Ghiasi and Fowlkes, 2016)</ref>, GCN with "large kernel" convolution <ref type="bibr" target="#b44">(Peng et al., 2017)</ref> and DFN with channel attention module <ref type="bibr" target="#b66">(Yu et al., 2018b)</ref> incorporate this backbone network to recover the detailed information. HRNet  adopts multi-branches to maintain the high resolution.</p><p>Both types of backbone network encode the lowlevel details and high-level semantics simultaneously with the wide and deep networks. Although both types of backbone network achieve state-of-the-art performance, most methods run at a slow inference speed. In this study, we propose a novel and efficient architecture to treat the spatial details and categorical semantics separately to achieve a good trad-off between segmentation accuracy and inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Real-time Semantic Segmentation</head><p>Real-time semantic segmentation algorithms attract increasing attention when a growing practical applications require fast interaction and response. SegNet (Badrinarayanan et al., 2017) uses a small network structure and the skip connection to achieve a fast speed. E-Net <ref type="bibr" target="#b43">(Paszke et al., 2016)</ref> devises a lightweight network from scratch and delivers extremely high speed. ICNet <ref type="bibr" target="#b73">(Zhao et al., 2018a)</ref> uses the image cascade to speed up the algorithm, while DLC ) employs a cascade network structure to reduce the computation in "easy regions". ERFNet <ref type="bibr" target="#b48">(Romera et al., 2018)</ref> adopts the residual connection and factorized convolutions to remain efficient and retain accuracy. Meanwhile, ESP-Net <ref type="bibr" target="#b39">(Mehta et al., 2018</ref><ref type="bibr" target="#b40">(Mehta et al., , 2019</ref> devises an efficient spatial pyramid dilated convolution for real-time semantic segmentation. GUN <ref type="bibr" target="#b38">(Mazzini, 2018</ref>) employs a guided upsampling module to fuse the information of the multiresolution input. DFANet <ref type="bibr" target="#b31">(Li et al., 2019b)</ref> reuses the feature to enhance the feature representation and reduces the complexity.</p><p>Although these methods can achieve a real-time inference speed, they dramatically sacrifice the accuracy to the efficiency with the loss of the low-level details. In this work, we take both of the low-level details and high-level semantics into consideration to achieve high accuracy and high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Light-weight Architecture</head><p>Following the pioneering work of group/depth-wise convolution and separable convolution, light-weight architecture design has achieved rapid development, including Xception <ref type="bibr" target="#b14">(Chollet, 2017)</ref>, MobileNet <ref type="bibr" target="#b24">(Howard et al., 2017;</ref><ref type="bibr" target="#b51">Sandler et al., 2018)</ref>, ShuffleNet <ref type="bibr" target="#b37">Ma et al., 2018)</ref>, to name a few. These methods achieve a valuable trade-off between speed and accuracy for the classification task. In this study, we design a light-weight network given computation complexity, memory access cost and real inference speed for the real-time semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Core Concepts of BiSeNetV2</head><p>Our architecture consists of a Detail Branch (Section 3.1) and a Semantic Branch (Section 3.2), which are merged by an Aggregation Layer (Section 3.3). In this section, we demonstrate the core concepts of our architecture, as illustrated in <ref type="figure">Figure 2</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Detail Branch</head><p>The Detail Branch is responsible for the spatial details, which is low-level information. Therefore, this branch requires a rich channel capacity to encode affluent spatial detailed information. Meanwhile, because the Detail Branch simply focuses on the low-level details, we can design a shallow structure with a small stride for this branch. Overall, the key concept of the Detail Branch is to use wide channels and shallow layers for the spatial details. Besides, the feature representation in this branch has a large spatial size and wide channels. Therefore, it is better not to adopt the residual connections, which increases the memory access cost and reduce the speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Branch</head><p>In parallel to the Detail Branch, the Semantic Branch is designed to capture high-level semantics. This branch has low channel capacity, while the spatial details can be provided by the Detail Branch. In contrast, in our experiments, the Semantic Branch has a ratio of λ(λ &lt; 1) channels of the Detail Branch, which makes this branch lightweight. Actually, the Semantic Branch can be any lightweight convolutional model (e.g., <ref type="bibr" target="#b14">(Chollet, 2017;</ref><ref type="bibr" target="#b27">Iandola et al., 2016;</ref><ref type="bibr" target="#b24">Howard et al., 2017;</ref><ref type="bibr" target="#b51">Sandler et al., 2018;</ref><ref type="bibr" target="#b71">Zhang et al., 2018b;</ref><ref type="bibr" target="#b37">Ma et al., 2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3 Overview of the Bilateral Segmentation Network.</head><p>There are mainly three components: two-pathway backbone in the purple dashed box, the aggregation layer in the orange dashed box, and the booster part in the yellow dashed box. The two-pathway backbone has a Detail Branch (the blue cubes) and a Semantic Branch (the green cubes). The three stages in Detail Branch have C1, C2, C3 channels respectively. The channels of corresponding stages in Semantic Branch can be made lightweight by the factor λ(λ &lt; 1). The last stage of the Semantic Branch is the output of the Context Embedding Block. Meanwhile, numbers in the cubes are the feature map size ratios to the resolution of the input. In the Aggregation Layer part, we adopt the bilateral aggregation layer. Down indicates the downsampling operation, U p represents the upsampling operation, ϕ is the Sigmoid function, and means element-wise product. Besides, in the booster part, we design some auxiliary segmentation heads to improve the segmentation performance without any extra inference cost.  <ref type="bibr">GE, CE)</ref>. Each operation has a kernels size k, stride s and output channels c, repeated r times. The expansion factor e is applied to expand the channel number of the operation. Here the channel ratio is λ = 1/4. The green colors mark fewer channels of Semantic Branch in the corresponding stage of the Detail Branch. Notation: Conv2d means the convolutional layer, followed by one batch normalization layer and relu activation function. Stem indicates the stem block. GE represents the gather-and-expansion layer. CE is the context embedding block. fast-downsampling strategy to promote the level of the feature representation and enlarge the receptive field quickly. High-level semantics require large receptive field. Therefore, the Semantic Branch employs the global average pooling <ref type="bibr" target="#b35">(Liu et al., 2016)</ref> to embed the global contextual response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregation Layer</head><p>The feature representation of the Detail Branch and the Semantic Branch is complementary, one of which is unaware of the information of the other one. Thus, an Aggregation Layer is designed to merge both types of feature representation. Due to the fast-downsampling strategy, the spatial dimensions of the Semantic Branch's output are smaller than the Detail Branch. We need to upsample the output feature map of the Semantic Branch to match the output of the Detail Branch.</p><p>There are a few manners to fuse information, e.g., simple summation, concatenation and some well-designed operations. We have experimented different fusion methods with consideration of accuracy and efficiency. At last, we adopt the bidirectional aggregation method, as shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bilateral Segmentation Network</head><p>The concept of our BiSeNet is generic, which can be implemented with different convolutional models <ref type="bibr" target="#b22">(He et al., 2016;</ref><ref type="bibr" target="#b25">Huang et al., 2017;</ref><ref type="bibr" target="#b14">Chollet, 2017;</ref><ref type="bibr" target="#b27">Iandola et al., 2016;</ref><ref type="bibr" target="#b24">Howard et al., 2017;</ref><ref type="bibr" target="#b51">Sandler et al., 2018;</ref><ref type="bibr" target="#b71">Zhang et al., 2018b;</ref><ref type="bibr" target="#b37">Ma et al., 2018)</ref> and any specific designs. There are mainly three key concepts: (i) The Detail Branch has high channel capacity and shallow layers with small receptive field for the spatial details; (ii)The Semantic Branch has low channel capacity and deep layers with large receptive field for the categorical semantics. (iii)An efficient Aggregation Layer is designed to fuse both types of representation.</p><p>In this subsection, according to the proposed conceptual design, we demonstrate our instantiations of the overall architecture and some other specific designs, as illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Detail Branch</head><p>The instantiation of the Detail Branch in <ref type="table" target="#tab_2">Table 1</ref> contains three stages, each layer of which is a convolution layer followed by batch normalization <ref type="bibr" target="#b28">(Ioffe and Szegedy, 2015)</ref> and activation function <ref type="bibr" target="#b19">(Glorot et al., 2011)</ref>. The first layer of each stage has a stride s = 2, while the other layers in the same stage have the same number of filters and output feature map size. Therefore, this branch extracts the output feature maps that are 1/8 of the original input. This Detail Branch encodes rich spatial details due to the high channel capacity. Because of the high channel capacity and the large spatial dimension, the residual structure <ref type="bibr" target="#b22">(He et al., 2016)</ref> will increases the memory access cost <ref type="bibr" target="#b37">(Ma et al., 2018)</ref>. Therefore, this branch mainly obeys the philosophy of VGG nets <ref type="bibr" target="#b55">(Simonyan and Zisserman, 2015)</ref> to stack the layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Branch</head><p>In consideration of the large receptive field and efficient computation simultaneously, we design the Semantic Conv, Branch, inspired by the philosophy of the lightweight recognition model, e.g., Xception <ref type="bibr" target="#b14">(Chollet, 2017)</ref>, Mo-bileNet <ref type="bibr" target="#b24">(Howard et al., 2017;</ref><ref type="bibr" target="#b51">Sandler et al., 2018;</ref><ref type="bibr" target="#b23">Howard et al., 2019)</ref>, ShuffleNet <ref type="bibr" target="#b37">Ma et al., 2018)</ref>. Some of the key features of the Semantic Branch are as follows.</p><formula xml:id="formula_0">3 × 3 ( = 2 ) MPooling 3 × 3 ( = 2 ) C Conv 1 × 1 Conv 3 × 3 ( = 2 ) Conv 3 × 3 BN ReLu ( / 2 × / 2 × ) BN ReLu ( / 2 × / 2 × / 2 ) BN ReLu ( / 4 × / 4 × ) Input ( × × 3 ) BN ReLu ( / 4 × / 4 × ) ( / 4 × / 4 × ) BN ( 1 × 1 × ) BN ReLu Broadcast ( 1 × 1 × ) ( × × ) GAPooling 3 × 3 Conv 1 × 1 ( × × ) + Conv 3 × 3 (a) Stem Block (b) Context</formula><p>Stem Block Inspired from <ref type="bibr" target="#b57">(Szegedy et al., 2017;</ref><ref type="bibr" target="#b54">Shen et al., 2017;</ref><ref type="bibr" target="#b63">Wang et al., 2018b)</ref>, we adopt the Stem Block as the first stage of the Semantic Branch, as illustrated in <ref type="figure">Figure 4</ref>. It uses two different downsampling manners to shrink the feature representation. And then the output feature of both branches are concatenated as the output. This structure has efficient computation cost and effective feature expression ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Embedding Block</head><p>As discussed in Section 3.2, the Semantic Branch requires large receptive field to capture high-level semantics. Inspired from <ref type="bibr" target="#b66">(Yu et al., 2018b;</ref><ref type="bibr" target="#b35">Liu et al., 2016;</ref><ref type="bibr" target="#b72">Zhao et al., 2017;</ref><ref type="bibr" target="#b11">Chen et al., 2017)</ref>, we design the Context Embedding Block. This block uses the global average pooling and residual connection <ref type="bibr" target="#b22">(He et al., 2016)</ref> to embed the global contextual information efficiently, as showed in <ref type="figure">Figure 4</ref>. </p><formula xml:id="formula_1">BN ReLu6 ( × × 6 ) BN ReLu6 ( × × 6 ) BN ( × × ) Conv 1 × 1 DWConv 3 × 3 Conv 1 × 1 + ( × × ) DWConv 3 × 3 Conv 1 × 1 + Conv 3 × 3 ( × × ) BN ReLu ( × × ) BN ( × × 6 ) BN ( × × ) ReLu DWConv 3 × 3 ( = 2 ) DWConv 3 × 3 Conv 1 × 1 + Conv 3 × 3 ( × × ) BN ReLu ( × × ) BN ( / 2 × / 2 × 6 ) BN ( / 2 × / 2 × ) ReLu DWConv 3 × 3 ( = 2 ) Conv 1 × 1 BN BN BN ( / 2 × / 2 × 6 ) (a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gather-and-Expansion Layer</head><p>Taking advantage of the benefit of depth-wise convolution, we propose the Gather-and-Expansion Layer, as illustrated in <ref type="figure" target="#fig_1">Figure 5</ref>. The Gather-and-Expansion Layer consists of: (i) a 3×3 convolution to efficiently aggregate feature responses and expand to a higher-dimensional space; (iii) a 3 × 3 depth-wise convolution performed independently over each individual output channel of the expansion layer; (iv) a 1×1 convolution as the projection layer to project the output of depth-wise convolution into a low channel capacity space. When stide = 2, we adopt two 3 × 3 depth-wise convolution, which further enlarges the receptive field, and one 3 × 3 separable convolution as the shortcut. Recent works <ref type="bibr" target="#b23">Howard et al., 2019)</ref> adopt 5 × 5 separable convolution heavily to enlarge the receptive field, which has fewer FLOPS than two 3 × 3 separable convolution in some conditions. In this layer, we replace the 5 × 5 depth-wise convolution in the separable convolution with two 3 × 3 depth-wise convolution, which has fewer FLOPS and the same receptive field. In contrast to the inverted bottleneck in MobileNetv2, the GE Layer has one more 3 × 3 convolution. However, this layer is also friendly to the computation cost and memory access cost <ref type="bibr" target="#b37">(Ma et al., 2018;</ref><ref type="bibr" target="#b51">Sandler et al., 2018)</ref>, because the 3 × 3 convolution is specially optimized in the CUDNN library <ref type="bibr" target="#b13">(Chetlur et al., 2014;</ref><ref type="bibr" target="#b37">Ma et al., 2018)</ref>. Meanwhile, because of this layer, the GE Layer has higher feature expression ability than the inverted bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bilateral Guided Aggregation</head><p>There are some different manners to merge two types of feature response, i.e., element-wise summation and concatenation. However, the outputs of both branches have different levels of feature representation. The Detail Branch is for the low-level, while the Semantic Branch is for the high-level. Therefore, simple combination ignores the diversity of both types of information, leading to worse performance and hard optimization. Based on the observation, we propose the Bilateral Guided Aggregation Layer to fuse the complementary information from both branches, as illustrated in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Booster Training Strategy</head><p>To further improve the segmentation accuracy, we propose a booster training strategy. As the name implies, it is similar to the rocket booster: it can enhance the feature representation in the training phase and can be discarded in the inference phase. Therefore, it increases little computation complexity in the inference phase. As illustrated in <ref type="figure">Figure 3</ref>, we can insert the auxiliary segmentation head to different positions of the Semantic Branch. In Section 5.1, we analyze the effect of different positions to insert. <ref type="figure">Figure 7</ref> illustrates the details of the segmentation head. We can adjust the computational complexity of auxiliary segmentation head and main segmentation head by controlling the channel dimension C t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we first introduce the datasets and the implementation details. Next, we investigate the effects of each component of our proposed approach on Cityscapes validation set. Finally, we report our final accuracy and speed results on different benchmarks compared with other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>Cityscapes <ref type="bibr" target="#b15">(Cordts et al., 2016)</ref> focuses on semantic understanding of urban street scenes from a car perspective. The dataset is split into training, validation and test sets, with 2, 975, 500 and 1, 525 images respectively. In our experiments, we only use the fine annotated images to validate the effectiveness of our proposed method. The annotation includes 30 classes, 19 of which are used for semantic segmentation task. This dataset is challenging for the real-time semantic segmentation because of its high resolution of 2, 048 × 1, 024. Cambridge-driving Labeled Video Database (Cam-Vid) <ref type="bibr" target="#b5">(Brostow et al., 2008a</ref>) is a road scene dataset from the perspective of a driving automobile. It contains 701 images with 960 × 720 resolution extracted from the video sequences. Following the pioneering work <ref type="bibr" target="#b6">(Brostow et al., 2008b;</ref><ref type="bibr" target="#b56">Sturgess et al., 2009;</ref><ref type="bibr" target="#b1">Badrinarayanan et al., 2017)</ref>, the images are split into 367 for training, 101 for validation and 233 for testing. We use the subset of 11 classes of the provided 32 candidate categories for the fair comparison with other methods. The pixels do not belong to one of these classes are ignored.</p><p>COCO-Stuff <ref type="bibr" target="#b7">(Caesar et al., 2018)</ref> augments 10K complex images of the popular COCO <ref type="bibr" target="#b34">(Lin et al., 2014)</ref> dataset with dense stuff annotations. This is also a challenging dataset for the real-time semantic segmentation because it has more complex categories, including 91 thing and 91 stuff classes for evaluation. For a fair comparison, we follow the split in <ref type="bibr" target="#b7">(Caesar et al., 2018)</ref>: 9K images for training and 1K images for testing.</p><p>Training. Our models are trained from scratch with the "kaiming normal" initialization manner <ref type="bibr" target="#b21">(He et al., 2015)</ref>. We use the stochastic gradient descent (SGD) algorithm with 0.9 momentum to train our model. For all datasets, we adopt 16 batch size. For the Cityscapes and CamVid datasets, the weight decay is 0.0005 weight decay while the weight decay is 0.0001 for the COCO-Stuff dataset. We note that the weight decay regularization is only employed on the parameters of the convolution layers. The initial rate is set to 5e −2 with a "poly" learning rate strategy in which the initial rate is multiplied by (1 − iter itersmax ) power each iteration with power 0.9. Besides, we train the model for 150K, 10K, 20K iterations for the Cityscapes dataset, CamVid dataset, and COCO-Stuff datasets respectively.</p><p>For the augmentation, we randomly horizontally flip, randomly scale, and randomly crop the input images to a fixed size for training. The random scales contain { 0.75, 1, 1.25, 1.5, 1.75, 2.0}. And the cropped resolutions are 2048 × 1024 for Cityscapes, 960 × 720 for CamVid, 640 × 640 for COCO-Stuff respectively. Besides, the augmented input of Cityscapes will be resized into 1024 × 512 resolution to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference.</head><p>We do not adopt any evaluation tricks, e.g., sliding-window evaluation and multi-scale testing, which can improve accuracy but are time-consuming. With the input of 2048 × 1024 resolution, we first resize it to 1024 × 512 resolution to inference and then resize the prediction to the original size of the input. We measure the inference time with only one GPU card and repeat 5000 iterations to eliminate the error fluctuation. We note that the time of resizing is included in the inference time measurement. In other words, when measuring the inference time, the practical input size is 2048×1024. Meanwhile, we adopt the standard metric of the mean intersection of union (mIoU) We choose the = 6 to make the trade-off between accuracy and computation complexity.</p><p>for the Cityscapes dataset and CamVid dataset, while the mIoU and pixel accuracy (pixAcc) for the COCO-Stuff dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup.</head><p>We conduct experiments based on PyTorch 1.0. The measurement of inference time is executed on one NVIDIA GeForce GTX 1080Ti with the CUDA 9.0, CUDNN 7.0 and TensorRT v5.1.5 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablative Evaluation on Cityscapes</head><p>This section introduces the ablation experiments to validate the effectiveness of each component in our method. In the following experiments, we train our models on Cityscapes <ref type="bibr" target="#b15">(Cordts et al., 2016)</ref> training set and evaluate on the Cityscapes validation set.</p><p>Individual pathways. We first explore the effect of individual pathways specifically. The first two rows in <ref type="table" target="#tab_7">Table 2</ref> illustrates the segmentation accuracy and computational complexity of using only one pathway alone. The Detail Branch lacks sufficient high-level semantics, <ref type="bibr">1</ref> We use FP32 data precision.</p><p>while the Semantic Branch suffers from a lack of lowlevel spatial details, which leads to unsatisfactory results. <ref type="figure" target="#fig_2">Figure 8</ref> illustrates the gradual attention on the spatial details of Detail Branch. The second group in <ref type="table" target="#tab_7">Table 2</ref> shows that the different combinations of both branches are all better than the only one pathway models. Both branches can provide a complementary representation to achieve better segmentation performance. The Semantic Branch and Detail Branch alone only achieve 64.68% and 62.35% mean IoU. However, with the simple summation, the Semantic Branch can bring in over 6% improvement to the Detail Branch, while the Detail Branch can acquire 4% gain for the Semantic Branch. This observation shows that the representations of both branches are complementary.</p><p>Aggregation methods. We also investigate the aggregation methods of two branches, as illustrated in <ref type="table" target="#tab_7">Table 2</ref>. For an effective and efficient aggregation, we design the Bilateral Guided Aggregation Layer, which adopts the high-level semantics as the guidance to aggregate the multi-scale low-level details. We also show two variants without Bilateral Guided Aggregation Layer as the naive aggregation baseline: summation and concatenation of the outputs of both branches. For a fair comparison, the inputs of the summation and concatenation are through one separable layer respectively. <ref type="figure" target="#fig_3">Figure 9</ref> demonstrates the visualization outputs of Detail Branch, Semantic Branch and the aggregation of both branches. This illustrates that Detail Branch can provide sufficient spatial details, while Semantic Branch captures the semantic context. <ref type="table">Table 3</ref> illustrates a series of analysis experiments on the Semantic Branch design.</p><p>Channel capacity of Semantic Branch. As discussed in Section 3.2 and Section 4.2, the Semantic Branch is responsible for the high-level semantics, without caring the spatial details. Therefore, the Semantic Branch can be made very lightweight with low channel capacity, which is adapted by the channel capacity ratio of λ. <ref type="table">Table 4a</ref> illustrates the detailed comparison experiments of varying λ.</p><p>Different λ brings in different extents of improvement to the Detail-only baseline. Even when λ = 1/16, the first layer of the Semantic Branch has only 4 channel dimension, which also brings in 6% (62.35% → 68.27%) <ref type="table">Table 4</ref> Booster position. We can add the auxiliary segmentation head to different positions as the booster of the Semantic Branch. Here, stages represents the auxiliary segmentation head can be added after s stage. stage5 4 and stage5 5 means the position before or after the Context Embedding Block respectively. OHEM represents the online bootstrapping strategy. improvement to the baseline. Here, we employ λ = 1/4 as our default.</p><p>Block design of of Semantic Branch. Following the pioneer work <ref type="bibr" target="#b51">(Sandler et al., 2018;</ref><ref type="bibr" target="#b23">Howard et al., 2019)</ref>, we design a Gather-and-Expansion Layer, as discussed in Section 4.2 and illustrated in <ref type="figure" target="#fig_1">Figure 5</ref>. The main improvements consist of two-fold: (i) we adopt one 3 × 3 convolution as the Gather Layer instead of one   point-wise convolution in the inverted bottleneck of Mo-bileNetV2 <ref type="bibr" target="#b51">(Sandler et al., 2018)</ref>; (ii) when stride = 2, we employs two 3 × 3 depth-wise convolution to substitute a 5 × 5 depth-wise convolution. <ref type="table">Table 4b</ref> shows the improvement of our block design. The Gather-and-Expansion Layer can enlarge the receptive field to capture high-level semantics efficiently.</p><p>Expansion ratio of GE layer. The first 3 × 3 convolution layer in GE Layer is also an expansion layer, which can project the input to a high-dimensional space.</p><p>It has an advantage in memory access cost <ref type="bibr" target="#b51">(Sandler et al., 2018;</ref><ref type="bibr" target="#b23">Howard et al., 2019)</ref>. The expansion ratio of can control the output dimension of this layer. <ref type="table">Table 4c</ref> investigates the effect of varying . It is surprising to see that even with = 1, the Semantic Branch can also improve the baseline by 4% (62.35% → 67.48%) mean IoU, which validates the lightweight Semantic Branch is efficient and effective.</p><p>Booster training strategy. We propose a booster training strategy to further improve segmentation accuracy, as discussed in Section 4.4. We insert the segmentation head illustrated in <ref type="figure">Figure 7</ref> to different positions of Semantic Branch in the training phase, which are discarded in the inference phase. Therefore, they increase little computation complexity in the inference phase, which is similar to the booster of the rocket. Table 4 shows the effect of different positions to insert segmentation head. As we can see, the booster training strategy can obviously improve segmentation accuracy. We choose the configuration of the third row of Table 4, which further improves the mean IoU by over 3% (69.67% → 73.19%), without sacrificing the inference speed. Based on this configuration, we adopt the online bootstrapping strategy <ref type="bibr" target="#b64">(Wu et al., 2016)</ref> to improve the performance further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generalization Capability</head><p>In this section, we mainly explore the generalization capability of our proposed architecture. First, we investigate the performance of a wider model and deeper  <ref type="table" target="#tab_9">Table 5</ref>. Next, we replace the Semantic Branch with some other general light-weight models to explore the compatibility ability in <ref type="table" target="#tab_10">Table 6</ref>.</p><p>Generalization to large models. Although our architecture is designed mainly for the light-weight task, e.g., real-time semantic segmentation, BiSeNet V2 can be also generalized to large models. We mainly enlarge the architecture from two aspects: (i) wider models, controlled by the width multiplier α; (ii) deeper models, controlled by the depth multiplier d. <ref type="table" target="#tab_9">Table 5</ref> shows the segmentation accuracy and computational complexity of wider models with the different width multiplier Compatibility with other models. The BiSeNetV2 is a generic architecture with two branches. In this work, we design some specific blocks for the Semantic Branch. The Semantic Branch can be any lightweight convolutional models <ref type="bibr" target="#b22">(He et al., 2016;</ref><ref type="bibr" target="#b24">Howard et al., 2017)</ref>. Therefore, to explore the compatibility ability of our architecture, we conduct a series of experiments with different general light-weight models. <ref type="table" target="#tab_10">Table 6</ref> shows the results of the combination with different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Evaluation</head><p>In this section, we compare our best model (BiSeNetV2 and BiSeNetV2-Large) with other state-of-the-art methods on three benchmark datasets: Cityscapes, CamVid and COCO-Stuff.</p><p>Cityscapes. We present the segmentation accuracy and inference speed of the proposed BiSeNetV2 on Cityscapes test set. We use the training set and validation set with 2048× 1024 input to train our models, which is resized into 1024 × 512 resolution at first in the models. Then we evaluate the segmentation accuracy on the test set. The measurement of inference time is conducted on one NVIDIA GeForce 1080Ti card. <ref type="table" target="#tab_11">Table 7</ref> reports the comparison results of our method and state-of-theart methods. The first group is non-real-time methods, containing CRF-RNN <ref type="bibr" target="#b75">(Zheng et al., 2015)</ref>, Deeplab-CRF <ref type="bibr" target="#b9">(Chen et al., 2015)</ref>, FCN-8S <ref type="bibr" target="#b36">(Long et al., 2015)</ref>, Dilation10 <ref type="bibr" target="#b68">(Yu and Koltun, 2016)</ref>, LRR <ref type="bibr" target="#b18">(Ghiasi and Fowlkes, 2016)</ref>, Deeplabv2-CRF <ref type="bibr" target="#b10">(Chen et al., 2016)</ref>, FRRN <ref type="bibr" target="#b45">(Pohlen et al., 2017)</ref>, RefineNet <ref type="bibr" target="#b33">(Lin et al., 2017)</ref>, DUC , PSPNet <ref type="bibr" target="#b72">(Zhao et al., 2017)</ref>. The real-time semantic segmentation algorithms are listed in the second group, including ENet <ref type="bibr" target="#b43">(Paszke et al., 2016)</ref>, SQ <ref type="bibr" target="#b59">(Treml et al., 2016)</ref>, ESPNet <ref type="bibr" target="#b39">(Mehta et al., 2018)</ref>, ESPNetV2 <ref type="bibr" target="#b40">(Mehta et al., 2019)</ref>, ERFNet <ref type="bibr" target="#b48">(Romera et al., 2018)</ref>, Fast-SCNN <ref type="bibr" target="#b46">(Poudel et al., 2019)</ref>, ICNet <ref type="bibr" target="#b73">(Zhao et al., 2018a)</ref>, DABNet <ref type="bibr" target="#b30">(Li et al., 2019a)</ref>, DFANet <ref type="bibr" target="#b31">(Li et al., 2019b)</ref>, GUN <ref type="bibr" target="#b38">(Mazzini, 2018)</ref>, SwiftNet (Orsic et al., 2019), BiSeNetV1 <ref type="bibr" target="#b65">(Yu et al., 2018a)</ref>. The third group is our methods with different levels of complexities. As shown in <ref type="table" target="#tab_11">Table 7</ref>, our method achieves 72.6% mean IoU with 156 FPS and yields 75.3% mean IoU with 47.3 FPS, which are state-of-the-art tradeoff between accuracy and speed. These results are even better than several non-real-time algorithms in the second group of <ref type="table" target="#tab_11">Table 7</ref>. We note that many non-real-time methods may adopt some evaluation tricks, e.g., multiscale testing and multi-crop evaluation, which can improve the accuracy but are time-consuming. Therefore, we do not adopt this strategy with the consideration of speed. To better view, we illustrate the trade-off between performance and speed in <ref type="figure" target="#fig_0">Figure 1</ref>. To highlight the effectiveness of our method, we also present some visual examples of BiSeNetV2 on Cityscapes in <ref type="figure" target="#fig_0">Figure 10</ref>.</p><p>CamVid. <ref type="table" target="#tab_12">Table 8</ref> shows the statistic accuracy and speed results on the CamVid dataset. In the inference phase, we use the training dataset and validation dataset to train our model with 960 × 720 resolution input. Our models are compared to some non-real-time algorithms, including SegNet <ref type="bibr" target="#b1">(Badrinarayanan et al., 2017)</ref>, Deeplab <ref type="bibr" target="#b9">(Chen et al., 2015)</ref>, RTA <ref type="bibr" target="#b26">(Huang et al., 2018)</ref>, Dilate8 <ref type="bibr" target="#b68">(Yu and Koltun, 2016)</ref>, PSPNet <ref type="bibr" target="#b72">(Zhao et al., 2017)</ref>, VideoGCRF <ref type="bibr" target="#b8">(Chandra et al., 2018)</ref>, and DenseDecoder <ref type="bibr" target="#b3">(Bilinski and Prisacariu, 2018)</ref>, and real-time algorithms, containing ENet <ref type="bibr" target="#b43">(Paszke et al., 2016)</ref>, IC-Net <ref type="bibr" target="#b73">(Zhao et al., 2018a)</ref>, DABNet <ref type="bibr" target="#b30">(Li et al., 2019a)</ref>, DFANet <ref type="bibr" target="#b31">(Li et al., 2019b)</ref>, SwiftNet <ref type="bibr" target="#b41">(Orsic et al., 2019)</ref>, BiSeNetV1 <ref type="bibr" target="#b65">(Yu et al., 2018a)</ref>. BiSeNetV2 achieves much faster inference speed than other methods. Apart from the efficiency, our accuracy results also outperform these work. Besides, we investigate the effect of the pre-training datasets on CamVid. The last two rows of <ref type="table" target="#tab_12">Table 8</ref> show that pre-training on Cityscapes can greatly improve the mean IoU over 6% on the CamVid test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-Stuff.</head><p>We also report our accuracy and speed results on COCO-Stuff validation dataset in <ref type="table" target="#tab_13">Table 9</ref>.</p><p>In the inference phase, we pad the input into 640 × 640 resolution. For a fair comparison <ref type="bibr" target="#b36">(Long et al., 2015;</ref><ref type="bibr" target="#b10">Chen et al., 2016;</ref><ref type="bibr" target="#b72">Zhao et al., 2017</ref><ref type="bibr" target="#b73">Zhao et al., , 2018a</ref>, we do not adopt any time-consuming testing tricks, such as multiscale and flipping testing. Even With more complex categories in this dataset, compared to pioneer work, our BiSeNetV2 still performs more efficient and achieve comparable accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>We observe that the semantic segmentation task requires both low-level details and high-level semantics. We propose a new architecture to treat the spatial details and categorical semantics separately, termed Bilateral Segmentation Network (BiSeNetV2). The BiSeNetV2 framework is a generic architecture, which can be implemented by most convolutional models. Our instantiations of BiSeNetV2 achieve a good trade-off between segmentation accuracy and inference speed. We hope that this generic architecture BiSeNetV2 will foster further research in semantic segmentation. The first row shows that our architecture can focus on the details, e.g., fence. The bus in the third row demonstrates the architecture can capture the large object. The bus in the last row illustrates the architecture can encode the spatial context to reason it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Fig. 1</head><label>1</label><figDesc>National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China 2 The University of Adelaide, Australia 3 The Chinese University of Hong Kong 4 Tencent * Corresponding author Speed-accuracy trade-off comparison on the Cityscapes test set. Red dots indicate our methods, while grey dots means other methods. The red line represents the real-time speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5</head><label>5</label><figDesc>Illustration of Inverted Bottleneck and Gatherand-Expansion Layer. (a) is the mobile inverted bottleneckConv proposed in MobileNetv2. The dashed shortcut path and summation circle do not exist with the stride = 2. (b)(c) are the proposed Gather-and-Expansion Layer. The bottleneck structure adopts: (i) a 3 × 3 convolution to gather local feature response and expand to higher-dimensional space;(ii) a 3 × 3 depth-wise convolution performed independently over each individual output channel of the expansion layer; (iii) a 1 × 1 convolution as the projection layer to project the output of depthwise convolution into a low channel capacity space. When the stride = 2, we adopt two kernel size = 3 depth-wise convolutions on the main path and a 3 × 3 separable convolution as the shortcut. Notation: Conv is convolutional operation. BN is the batch normalization. ReLu is the ReLu activation function. Meanwhile, 1×1, 3×3 denote the kernel size, H ×W ×C means the tensor shape (height, width, depth).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8</head><label>8</label><figDesc>Examples showing visual explanations for the different stages of the Detail Branch. Following the Grad-CAM<ref type="bibr" target="#b52">(Selvaraju et al., 2017)</ref>, we visualize the Grad-CAMs of Detail Branch. The visualization shows that Detail Branch can focus on the spatial details, e.g., boundary, gradually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9</head><label>9</label><figDesc>Visual improvement of the Bilateral Guided Aggregation layer on the Cityscapes val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10</head><label>10</label><figDesc>Visualization examples on the Cityscapes val set produced from BiSeNetV2 and BiSeNetV2-Large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Meanwhile, the Semantic Branch adopts the</figDesc><table><row><cell>Backbone</cell><cell>*</cell><cell>+</cell><cell>,</cell><cell></cell><cell>Aggregation Layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>×</cell></row><row><cell cols="2">1 2 ⁄</cell><cell>1 4 ⁄</cell><cell>1 8 ⁄</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seg Head</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Context</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Embedding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>1 2 ⁄</cell><cell>1 4 ⁄</cell><cell>1 8 ⁄</cell><cell>1 16 ⁄</cell><cell>1 32 ⁄</cell><cell>1 32 ⁄</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train &amp; Test</cell></row><row><cell>*</cell><cell>+</cell><cell>,</cell><cell></cell><cell></cell><cell></cell><cell>Train only</cell></row><row><cell>Booster</cell><cell>Seg Head</cell><cell>Seg Head</cell><cell>Seg Head</cell><cell>Seg Head</cell><cell></cell><cell>Test only Detail Branch</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell>Semantic Branch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 Instantiation of the Detail Branch and Semantic Branch</head><label>1</label><figDesc></figDesc><table /><note>. Each stage S contains one or more operations opr (e.g., Conv2d, Stem,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Conv is convolutional operation. BN denotes the batch normalization. Upsample means bilinear interpolation. Meanwhile, 1 × 1, 3 × 3 denote the kernel size, H × W × C means the tensor shape (height, width, depth), C represents the channel dimension, S denotes the scale ratio of upsampling, and N is the final output dimension.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>×</cell><cell>3</cell><cell></cell><cell></cell><cell>1</cell><cell>×</cell><cell>1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Conv</cell><cell></cell><cell></cell><cell cols="3">Conv</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">BN ReLu</cell><cell></cell><cell></cell><cell cols="2">Upsample</cell></row><row><cell>(</cell><cell>×</cell><cell>×</cell><cell>)</cell><cell></cell><cell>(</cell><cell>×</cell><cell>×</cell><cell>)</cell><cell></cell><cell>(</cell><cell>×</cell><cell>×</cell><cell>)</cell></row><row><cell cols="13">Fig. 7 Detailed design of Segmentation Head in</cell></row><row><cell cols="5">Booster. Notation:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ure 6. This layer employs the contextual information of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Semantic Branch to guide the feature response of De-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tail Branch. With different scale guidance, we can cap-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ture different scale feature representation, which inher-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ently encodes the multi-scale information. Meanwhile,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>this guidance manner enables efficient communication</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>between both branches compared to the simple combi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 Table 3 Ablations on the Semantic Branch design on Cityscapes.</head><label>2</label><figDesc>Ablations on Cityscapes. We validate the effectiveness of each component step by step. We show segmentation accuracy (mIoU%), and computational complexity measured in GFLOPs with the input of spatial size 2048 × 1024. Notation: Detail is the Detail Branch. Semantic is the Semantic Branch. BGA represents the Bilateral Guided Aggregation Layer. Booster means the booster training strategy. OHEM is the online hard example mining. We conduct experiments about the channel capacity, the block design, and the expansion ratio of the Semantic Branch. Notation: GLayer indicates the Gather Layer, the first 3 × 3 convolution in GE Layer. DDWConv is double depth-wise convolution layer.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Detail Semantic</cell><cell>Aggregation Sum Concate BGA</cell><cell cols="3">Booster OHEM mIoU(%) GFLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.35</cell><cell>15.26</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.68</cell><cell>7.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.60</cell><cell>20.77</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.93</cell><cell>21.98</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.67</cell><cell>21.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73.19</cell><cell>21.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73.36</cell><cell>21.15</cell></row><row><cell cols="3">mIoU(%) GFLOPs</cell><cell cols="4">GLayer DDWConv Context mIoU(%) GFLOPs</cell><cell></cell><cell>mIoU(%) GFLOPs</cell></row><row><cell>Detail-only</cell><cell>62.35</cell><cell>15.26</cell><cell></cell><cell></cell><cell>69.67</cell><cell>21.15</cell><cell>Detail-only</cell><cell>62.35</cell><cell>15.26</cell></row><row><cell>λ = 1/2</cell><cell>69.66</cell><cell>25.84</cell><cell></cell><cell></cell><cell>69.01</cell><cell>21.07</cell><cell>= 1</cell><cell>67.48</cell><cell>17.78</cell></row><row><cell cols="2">1/4 69.67</cell><cell>21.15</cell><cell></cell><cell></cell><cell>68.98</cell><cell>21.15</cell><cell>2</cell><cell>68.41</cell><cell>18.45</cell></row><row><cell>1/8</cell><cell>69.26</cell><cell>19.93</cell><cell></cell><cell></cell><cell>66.62</cell><cell>15.78</cell><cell>4</cell><cell>68.78</cell><cell>19.8</cell></row><row><cell>1/16</cell><cell>68.27</cell><cell>19.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">6 69.67</cell><cell>21.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>68.99</cell><cell>22.49</cell></row><row><cell cols="3">(a) Channel capacity ratio:</cell><cell cols="4">(b) Block Analysis: We specifically design the</cell><cell cols="2">(c) Expansion ratio: Varying val-</cell></row><row><cell cols="3">Varying values of λ can control the</cell><cell cols="4">GE Layer and adopt double depth-wise convolutions</cell><cell cols="2">ues of can affect the representa-</cell></row><row><cell cols="3">channel capacity of the first two</cell><cell cols="4">when stride = 2. The second row means we use one</cell><cell cols="2">tive ability of the Semantic Branch.</cell></row><row><cell cols="3">stages in the Semantic Branch. The</cell><cell cols="4">5 × 5 depth-wise convolution instead of two 3 × 3</cell><cell></cell></row><row><cell cols="3">channel dimensions of the last two</cell><cell cols="4">depth-wise convolution. The third row represents we</cell><cell></cell></row><row><cell cols="3">stages are still 64 and 128. Here, we</cell><cell cols="4">replace the first 3 × 3 convolution layer of GE Layer</cell><cell></cell></row><row><cell>choose λ = 1/4.</cell><cell></cell><cell></cell><cell cols="2">with the 1 × 1 convolution.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Generalization to large models. We enlarge our</cell></row><row><cell cols="5">models from two aspects: (i) wider models; (ii) deeper models.</cell></row><row><cell cols="3">Wider mIoU(%) GFLOPs</cell><cell cols="2">Deeper mIoU(%) GFLOPs</cell></row><row><cell>α = 1.0</cell><cell>73.36</cell><cell>21.15</cell><cell>d = 1.0 73.36</cell><cell>21.15</cell></row><row><cell>1.25</cell><cell>73.61</cell><cell>34.98</cell><cell>2.0 74.10</cell><cell>25.26</cell></row><row><cell>1.50</cell><cell>74.67</cell><cell>49.46</cell><cell>3.0 74.28</cell><cell>29.38</cell></row><row><cell>1.75</cell><cell>74.04</cell><cell>66.45</cell><cell>4.0 74.02</cell><cell>33.5</cell></row><row><cell cols="2">2.0 75.11</cell><cell>85.94</cell><cell></cell><cell></cell></row><row><cell cols="3">(a) Wider models: Varying</cell><cell cols="2">(b) Deeper models: Varying</cell></row><row><cell cols="3">values of α can control the</cell><cell cols="2">values of d represents the layer</cell></row><row><cell cols="3">channel capacity of our archi-</cell><cell>number of the model.</cell><cell></cell></row><row><cell>tecture.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 Compatibility with other models. We</head><label>6</label><figDesc></figDesc><table><row><cell>empoly</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc>We train and evaluate our models with 2048 × 1024 resolution input, which is resized into 1024 × 512 in the model. The inference time is measured on one NVIDIA GeForce 1080Ti card. Notation: γ is the downsampling ratio corresponding to the original 2048 × 1024 resolution. backbone indicates the backbone models pre-trained on the ImageNet dataset. "-" represents that the methods do not report the corresponding result. The DFANet A and DFANet B adopt the 1024 × 1024 input size and use the optimized depth-wise convolutions to accelerate speed.</figDesc><table><row><cell cols="2">Comparison</cell><cell cols="2">with</cell><cell cols="2">state-of-the-art</cell><cell>on</cell></row><row><cell>Cityscapes. method</cell><cell>ref.</cell><cell>γ</cell><cell cols="2">backbone</cell><cell>mIoU(%) FPS val test</cell></row><row><cell>large models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CRF-RNN  *</cell><cell cols="2">ICCV2015 0.5</cell><cell></cell><cell>VGG16</cell><cell>-62.5 1.4</cell></row><row><cell>DeepLab  *</cell><cell cols="2">ICLR2015 0.5</cell><cell></cell><cell>VGG16</cell><cell>-63.1 0.25</cell></row><row><cell>FCN-8S  *</cell><cell cols="2">CVPR2015 1.0</cell><cell></cell><cell>VGG16</cell><cell>-65.3</cell><cell>2</cell></row><row><cell>Dilation10</cell><cell cols="2">ICLR2016 1.0</cell><cell></cell><cell cols="2">VGG16 68.7 67.1 0.25</cell></row><row><cell>LRR</cell><cell cols="2">ECCV2016 1.0</cell><cell></cell><cell cols="2">VGG16 70.0 69.7</cell><cell>-</cell></row><row><cell>Deeplabv2</cell><cell cols="5">ICLR2016 1.0 ResNet101 71.4 70.4</cell><cell>-</cell></row><row><cell>FRRN</cell><cell cols="2">CVPR2017 0.5</cell><cell></cell><cell>no</cell><cell>-71.8 2.1</cell></row><row><cell>RefineNet</cell><cell cols="4">CVPR2017 1.0 ResNet101</cell><cell>-73.6</cell><cell>-</cell></row><row><cell>DUC</cell><cell cols="5">WACV2018 1.0 ResNet101 76.7 76.1</cell><cell>-</cell></row><row><cell>PSPNet</cell><cell cols="4">CVPR2017 1.0 ResNet101</cell><cell>-78.4 0.78</cell></row><row><cell>small models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ENet</cell><cell cols="2">arXiv2016 0.5</cell><cell></cell><cell>no</cell><cell>-58.3 76.9</cell></row><row><cell>SQ</cell><cell cols="5">NIPSW2016 1.0 SqueezeNet -59.8 16.7</cell></row><row><cell>ESPNet</cell><cell cols="4">ECCV2018 0.5 ESPNet</cell><cell>-60.3 112.9</cell></row><row><cell>ESPNetV2</cell><cell cols="5">CVPR2019 0.5 ESPNetV2 66.4 66.2</cell><cell>-</cell></row><row><cell>ERFNet</cell><cell cols="2">TITS2018 0.5</cell><cell></cell><cell>no</cell><cell>70.0 68.0 41.7</cell></row><row><cell cols="3">Fast-SCNN BMVC2019 1.0</cell><cell></cell><cell>no</cell><cell>68.6 68.0 123.5</cell></row><row><cell>ICNet</cell><cell cols="4">ECCV2018 1.0 PSPNet50</cell><cell>-69.5 30.3</cell></row><row><cell>DABNet</cell><cell cols="2">BMVC2019 1.0</cell><cell></cell><cell>no</cell><cell>-70.1 27.7</cell></row><row><cell>DFANet B</cell><cell cols="5">CVPR2019 0.5  *  Xception B -67.1 120</cell></row><row><cell>DFANet A</cell><cell cols="5">CVPR2019 0.5 Xception A -70.3 160</cell></row><row><cell>DFANet A</cell><cell cols="5">CVPR2019 0.5  *  Xception A -71.3 100</cell></row><row><cell>GUN</cell><cell cols="5">BMVC2018 0.5 DRN-D-22 69.6 70.4 33.3</cell></row><row><cell>SwiftNet</cell><cell cols="5">CVPR2019 1.0 ResNet18 75.4 75.5 39.9</cell></row><row><cell>BiSeNetV1</cell><cell cols="5">ECCV2018 0.75 Xception39 69.0 68.4 105.8</cell></row><row><cell>BiSeNetV1</cell><cell cols="5">ECCV2018 0.75 ResNet18 74.8 74.7 65.5</cell></row><row><cell>BiSeNetV2</cell><cell>-</cell><cell>0.5</cell><cell></cell><cell>no</cell><cell>73.4 72.6 156</cell></row><row><cell>BiSeNetV2-L</cell><cell>-</cell><cell>0.5</cell><cell></cell><cell>no</cell><cell>75.8 75.3 47.3</cell></row><row><cell>model in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 Comparison with state-of-the-art on CamVid.</head><label>8</label><figDesc>With 960 × 720 input, we evaluate the segmentation accuracy and corresponding inference speed. Notation: backbone means the backbone models pre-trained on extra datasets, e.g., Im-ageNet dataset and Cityscapes dataset. * indicates the models are pre-trained on Cityscapes. † represents the models are trained from scratch.</figDesc><table><row><cell>method</cell><cell>ref.</cell><cell cols="3">backbone mIoU(%) FPS</cell></row><row><cell>large models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SegNet</cell><cell>TPAMI2017</cell><cell>VGG16</cell><cell>60.1</cell><cell>4.6</cell></row><row><cell>DPN</cell><cell>ICCV2015</cell><cell>VGG16</cell><cell>60.1</cell><cell>1.2</cell></row><row><cell>Deeplab</cell><cell>ICLR2015</cell><cell>VGG16</cell><cell>61.6</cell><cell>4.9</cell></row><row><cell>RTA</cell><cell>ECCV2018</cell><cell>VGG16</cell><cell>62.5</cell><cell>0.2</cell></row><row><cell>Dilation8</cell><cell>ICLR2016</cell><cell>VGG16</cell><cell>65.3</cell><cell>4.4</cell></row><row><cell>PSPNet</cell><cell cols="2">CVPR2017 ResNet50</cell><cell>69.1</cell><cell>5.4</cell></row><row><cell cols="3">DenseDecoder CVPR2018 ResNeXt101</cell><cell>70.9</cell><cell>-</cell></row><row><cell cols="3">VideoGCRF  *  CVPR2018 ResNet101</cell><cell>75.2</cell><cell>-</cell></row><row><cell>small models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ENet</cell><cell>arXiv2016</cell><cell>no</cell><cell>51.3</cell><cell>61.2</cell></row><row><cell>DFANet B</cell><cell cols="2">CVPR2019 Xception B</cell><cell>59.3</cell><cell>160</cell></row><row><cell>DFANet A</cell><cell cols="2">CVPR2019 Xception A</cell><cell>64.7</cell><cell>120</cell></row><row><cell>ICNet</cell><cell cols="2">ECCV2018 PSPNet50</cell><cell>67.1</cell><cell>27.8</cell></row><row><cell>SwiftNet</cell><cell cols="2">CVPR2019 ResNet18  †</cell><cell>63.33</cell><cell>-</cell></row><row><cell>SwiftNet</cell><cell cols="2">CVPR2019 ResNet18</cell><cell>72.58</cell><cell>-</cell></row><row><cell>BiSeNetV1</cell><cell cols="2">ECCV2018 Xception 39</cell><cell>65.6</cell><cell>175</cell></row><row><cell>BiSeNetV1</cell><cell cols="2">ECCV2018 ResNet18</cell><cell>68.7</cell><cell>116.25</cell></row><row><cell>BiSeNetV2</cell><cell>-</cell><cell>no</cell><cell>72.4</cell><cell>124.5</cell></row><row><cell>BiSeNetV2-L</cell><cell>-</cell><cell>no</cell><cell>73.2</cell><cell>32.7</cell></row><row><cell>BiSeNetV2  *</cell><cell>-</cell><cell>no</cell><cell>76.7</cell><cell>124.5</cell></row><row><cell>BiSeNetV2-L  *</cell><cell>-</cell><cell>no</cell><cell>78.5</cell><cell>32.7</cell></row><row><cell cols="5">α and different depth multiplier d. According to the</cell></row><row><cell cols="5">experiments, we choose α = 2.0 and d = 3.0 to build</cell></row><row><cell cols="5">our large architecture, termed BiSeNetV2-Large, which</cell></row><row><cell cols="3">achieves 75.8% mIoU and GFLOPs.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 Comparison with state-of-the-art on COCO- Stuff.</head><label>9</label><figDesc>Our models are trained and evaluated with the input of 640×640 resolution. Notation: backbone is the backbone models pre-trained on ImageNet dataset.</figDesc><table><row><cell>method</cell><cell>ref.</cell><cell cols="4">backbone mIoU(%) pixAcc(%) FPS</cell></row><row><cell>large models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN-16s</cell><cell cols="2">CVPR2017 VGG16</cell><cell>22.7</cell><cell>52.0</cell><cell>5.9</cell></row><row><cell>Deeplab</cell><cell cols="2">ICLR2015 VGG16</cell><cell>26.9</cell><cell>57.8</cell><cell>8.1</cell></row><row><cell>FCN-8S</cell><cell cols="2">CVPR2015 VGG16</cell><cell>27.2</cell><cell>60.4</cell><cell>-</cell></row><row><cell>PSPNet50</cell><cell cols="2">CVPR2017 ResNet50</cell><cell>32.6</cell><cell>-</cell><cell>6.6</cell></row><row><cell>small models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICNet</cell><cell cols="2">ECCV2018 PSPNet50</cell><cell>29.1</cell><cell>-</cell><cell>35.7</cell></row><row><cell>BiSeNetV2</cell><cell>-</cell><cell>no</cell><cell>25.2</cell><cell>60.5</cell><cell>87.9</cell></row><row><cell>BiSeNetV2-L</cell><cell>-</cell><cell>no</cell><cell>28.7</cell><cell>63.5</cell><cell>42.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by the National Natural Science Foundation of China (No. 61433007 and 61876210).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeds: Superpixels extracted via energy-driven sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Capitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense decoder shortcut connections for single-pass semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 1, 2</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 1, 2</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International conference on artificial intelligence and statistics</title>
		<meeting>International conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV) 6</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV) 6</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient uncertainty estimation for semantic segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>arXiv abs/1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡1mb model size</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Esp-netv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">In defense of pretrained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fast-scnn: fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Combining Appearance and Structure from Motion Features for Road Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Thirty-first AAAI conference on artificial intelligence</title>
		<meeting>Thirty-first AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems Workshops 13</title>
		<meeting>Neural Information essing Systems Workshops 13</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Watersheds in digital spaces: an efficient algorithm based on immersion simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="598" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pelee: A real-time object detection system on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengel</forename><surname>Avd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2, 3</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2, 3</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In: Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
