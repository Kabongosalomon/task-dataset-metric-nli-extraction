<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqing</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhijun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-weakly-supervised learning</term>
					<term>temporal action localization</term>
					<term>action-context modeling network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised temporal action localization aims to localize action instances temporal boundary and identify the corresponding action category with only video-level labels. Traditional methods mainly focus on foreground and background frames separation with only a single attention branch and class activation sequence. However, we argue that apart from the distinctive foreground and background frames there are plenty of semantically ambiguous action context frames. It does not make sense to group those context frames to the same background class since they are semantically related to a specific action category. Consequently, it is challenging to suppress action context frames with only a single class activation sequence. To address this issue, in this paper, we propose an action-context modeling network termed ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point being action instance, context, or non-action background, simultaneously. Then based on the obtained three-branch attention values, we construct three-branch class activation sequences to represent the action instances, contexts, and non-action backgrounds, individually. To evaluate the effectiveness of our ACM-Net, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method can outperform current state-of-the-art methods, and even achieve comparable performance with fully-supervised methods. Code can be found at https://github.com/ispc-lab/ACM-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the explosive growth of video contents, understanding and learning from videos has attracted great interest in computer vision community. As one of the fundamental but challenging tasks of video understanding, temporal action localization or detection that aims to localize and classifying action instances in untrimmed videos has drawn lots of attention, due to its great potential for video retrieval <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, summarization <ref type="bibr" target="#b2">[3]</ref>, surveillance <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, anomaly detection <ref type="bibr" target="#b5">[6]</ref> and more. Thanks to the rapid development of deep learning, recently, a plenty of methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> have been proposed and achieved remarkable performance under the fully supervised definition. However, these methods require precise temporal annotation of each action instance during training, which is time-consuming, error-prone and extremely costly to collect. In contrast, weakly supervised temporal action localization (W-TAL), which requires only video-level action category labels, is a more reasonable choice and has attracted a great deal of attention. Compared with precise temporal boundary annotations of action instances, the videolevel action category label is easier to collect and beneficial to avoid localization bias introduced by human annotators.</p><p>Existing weakly supervised temporal action localization approaches can be divided into two main categories. One kind of approaches <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, inspired by the weakly-supervised image semantic segmentation task <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, formulate the weakly-supervised temporal action localization as a video recognition problem and introduce a foreground-background separation attention mechanism to construct video-level features, then apply an action classifier to recognize videos. While the other approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> formulate this problem as a multi-instance learning task <ref type="bibr" target="#b22">[23]</ref> and treat the entire untrimmed video as a bag containing both positive and negative instances, i.e. foreground action instances frames and background non-action frames. These methods first apply a classifier to obtain temporal-point class activation sequence (CAS) and then employ a top-k mechanism to aggregate the video-level classification scores.</p><p>As can be observed from the above discussion, both types of methods aim at learning the effective classification functions to identify action instances from bags of action instances and non-action frames. However, there is still a huge performance gap between the weakly supervised and supervised methods. We argue that the reason may lie in the fact that the untrimmed video contains a number of semantically ambiguous action context frames in addition to the discriminative foreground action instances and static non-action background frames. As we illustrated in <ref type="figure">Fig. 1</ref>, it is challenging to distinguish action instances and action contexts based on a simply foregroundbackground separation CAS since it does not make sense to assign these context frames directly to the background class due to that they are action-related and do not share the same semantic information as the contexts of other action categories.</p><p>To realize action contexts suppression under video-level supervision, in this paper, we propose an action context modeling network (ACM-Net). Specifically, we first introduce a classification branch to obtain the initial class activation sequence (CAS). But as we mentioned before, this initial CAS is not capable of suppressing ambiguous action context frames, since they are action semantically related. To address this issue, we propose a three-branch class-agnostic attention module to discriminate action instances, action contexts, and non-action backgrounds, individually. Then based on these three-branch attention values, we construct new three-branch  <ref type="figure">Fig. 1</ref>. Apart from distinctive action instances and non-action background frames, there are plenty of semantically ambiguous action context frames. Traditional weakly-supervised temporal localization methods mainly apply foreground-background attention mechanism to separate action instances frames and non-action frames (non-action background and semantically ambiguous action context frames). However, these methods are not capable of suppressing those context frames well, since they are semantically related to specific action instances, which makes no sense to directly assign these frames to the background class. To address this issue, we propose a three-branch attention module to measure the likelihood of each temporal point being action instances, action contexts, or non-action background. Based on the obtained attention values, we then construct three-branch class activation sequence for action instances, contexts, and non-action background, respectively. As we can see in the above figure, this mechanism is greatly beneficial for us to suppress those semantically ambiguous context frames. class activation sequence, i.e. CAS , CAS , CAS , which denotes attention values weighted action instances CAS, action contexts CAS, and the background CAS, respectively. Thereafter, we apply the multi-instance learning mechanism to compute video-level classification scores to realize separation among action instances, action contexts, and nonaction background. The detailed framework is presented in <ref type="figure">Fig. 2</ref>. To validate the effectiveness of our ACM-Net, we conduct extensive experiments on the THUMOS-14 <ref type="bibr" target="#b23">[24]</ref> and the ActivityNet-1.3 <ref type="bibr" target="#b24">[25]</ref> datasets. The results show that our ACM-Net can successfully realize the separation of action instances and action contexts, and achieve new state-of-theart performance on both benchmarks.</p><p>The main contribution are summarized as three-fold:</p><p>• Different from the previous methods that divide the frames of a video into the only foreground and background frames, we argue that there are some semantically ambiguous action context frames. In this paper, we investigate how action context modeling will affect the weaklysupervised temporal action localization and propose an action context modeling network (ACM-Net) to realize the separation of action contexts and action instances. • The proposed ACM-Net integrates a class agnostic threebranch attention module to measure the likelihood of each temporal point containing action instance, action context, and non-action background frames simultaneously. Based on the obtained attention values, we then construct threebranch class activation sequences to achieve the distinguishing of action instances, action contexts, and nonaction backgrounds. <ref type="table" target="#tab_1">• We conduct extensive experiments on the THUMOS-14</ref> and ActivityNet-1.3 datasets. The qualitative visualization results demonstrate the effectiveness of our ACM-Net in distinguishing between ambiguous action instances and action instances. And the quantitative results show that our ACM-Net outperforms current state-of-the-art methods and even can achieve comparable performance with recently fully-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action Recognition</head><p>As one of the fundamental tasks of video understanding, action recognition aimed at identifying actions in trimmed video clips has been extensively studied. Early approaches <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> mainly focused on design effective hand-crafted descriptors that incorporate spatial-temporal features. In recent years, with the development of deep learning, there are plenty of networks have been proposed. These methods are mainly constructed based on image-level backbone networks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Early approaches <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> directly apply these image backbone networks to RGB and optical flow images to model the spatial temporal information. To further improve recognition performance, researchers extend 2D convolution operation to 3D by extending the temporal dimension, and 3D CNN based models <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> (including 2D spatial convolution plus 1D temporal convolution) become the mainstream methods. However, although these methods achieve significant performance on trimmed video clips, in practice, long untrimmed videos are more often encountered, which makes these methods unable to achieve accurate semantic information modeling and limits the practical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fully-Supervised Temporal Action Localization</head><p>Different from action recognition, which focuses only on trimmed video clips to identify action categories, the temporal action localization task aims not only to classify action instances but also to localize the start and end temporal boundary of action instances in long untrimmed videos. Temporal action localization with full supervision requires manual annotated temporal boundary and category of each action instance in videos during training. Inspired by 2D object detection, a plenty of methods <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b9">[10]</ref> adopt a two-stage paradigm, i.e. proposal generation and classification. Given full action instances annotations, twostage methods usually filter out the non-action proposals at the proposal generation stage by introducing a binary classifier and then introduce temporal feature modeling to realize action proposals classification and boundary refinement. Currently, there are two main categories of proposal generation methods, namely top-down framework <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and bottom-up framework <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The former methods usually generate proposals with pre-defined regular distributed segments, e.g. sliding windows based methods, which is not flexible and often causes extensive false positive proposals. To address the above issues, the latter methods train a detector to search specific action points, e.g, action boundary or center points, and then combine these points to generate action proposals. However, since all methods require action instance labels during the proposal generation and classification stage, they are inevitably causing heavy annotation costs and not capable of widely employed in reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weakly-Supervised Temporal Action Localization</head><p>The weakly-supervised temporal action localization methods are proposed to reduce the expensive annotation costs, compared with the supervised temporal action localization that needs precise annotation of each action instance, during training, the weakly-supervised temporal action localization methods require only video-level action category labels. Existing weakly-supervised temporal action localization approaches can be divided into two branches. Inspired by the weaklysupervised image semantic segmentation task <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the first framework methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> formulate this task as an action recognition problem and introduce a foreground-background separation attention branch to construct video-level features, then apply an action classifier to recognize videos. While the latter framework approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> formulate this problem as a multi-instance learning task <ref type="bibr" target="#b22">[23]</ref> and treat the entire untrimmed video as a bag containing both positive and negative instances. They first obtain frame-level action recognition scores, i.e. the class activation sequence CAS, and then introduce a top-k mechanism to construct video-level classification scores. However, though these methods have achieved significant performance, there is still a performance gap between fully-supervised methods, we attribute this to that there are plenty of ambiguous action context frames apart from distinctive action instances and non-action background frames. It's challenging based on only a single foreground-background separation mechanism to suppress those frames as they are semantically related to specific actions. To address this issue, we introduce an action context modeling network, namely ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point containing action contexts. And then we build three-branch class activation sequences CAS based on the obtained attention values to realize action instances, contexts, and non-action background frames separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we first define the formulation of weaklysupervised temporal action localization (W-TAL), then present our action-context modeling network (ACM-Net) in detail, and thereafter, introduce the training and inference details. The overview architecture of our ACM-Net is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Assume we are given an untrimmed video , which contains multiple action instances { = ( , , )} =1 , where is the number of action instances, and denotes the start and end time of action instance , and ∈ R represents the class category. The goal of temporal action localization is to detect all action instances {ˆ= (ˆ,ˆ,ˆ,ˆ)} =1 , whereˆdenotes the confidence score of action instanceˆ.</p><p>Different from the fully-supervised temporal action localization task, during training, the action instance annotations are available. For the W-TAL task, we can only access the one-hot video-level category label = {0, 1} ∈ R +1 , where is the number of action classes and + 1 represents non-action background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Context Modeling Network</head><p>1) Feature Extraction: Following recent W-TAL methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b11">[12]</ref>, for a given untrimmed video , we first divide it into non-overlapping snippets based on a predefined sampling ratio, and then apply pre-trained networks to extract snippet-level features. Since different videos vary in temporal length, during training, we utilize interpolation operation to keep all training video have the same time dimension , i.e, for each video we keep the video snippets as = { ( )} =0 . As for snippet ( ) feature extraction, we utilize the spatial stream (RGB) and the temporal stream (optical flow) to encode the static scenes feature ( ) ∈ R and the motion features ( ) ∈ R , respectively. Thereafter, we concatenate the static scenes feature ( ) and the motion features ( ) together to form the snippet feature ( ) = [ ( ), ( )] ∈ R 2 . Afterwards, we stack all snippets feature to form the video pre-trained feature ∈ R ×2 .</p><p>2) Feature Embedding: Since the extracted features are not trained from scratch for the W-TAL task, in order to map the extracted video feature to task-specific feature space, we introduce a feature embedding module. Concretely, we apply a set of convolution layers and non-linear activation functions to map the original video feature ∈ R ×2 to task-specific video feature ∈ R ×2 . Formally, we can denote the feature embedding module as following: </p><formula xml:id="formula_0">= ReLU(Conv( , )) (1) untrimmed video V sampled snippet segments S I3D model extracted features F a</formula><formula xml:id="formula_1">snippet 1 snippet 2 snippet 3 snippet T … feature 1 feature 2 feature 3 feature T … feature 1 feature 2 feature 3 feature T … feature 1 feature 2 feature 3 feature T … Fig. 2.</formula><p>The framework of our proposed ACM-Net, which consists of three parts, i.e. pre-trained feature extraction, video feature embedding, and weaklysupervised temporal action localization guided by action context modeling. We first apply the pre-trained model to extract video snippets level spatial and temporal features, and then employ the feature embedding module to map the pre-trained feature to task-specific feature space. Therefore, to suppress the ambiguous action context frames, we propose a three-branch attention module and multiply the obtained attention values by the raw class activation sequences CAS to obtain the weighted CAS of the corresponding branch, and then employ a multi-instance learning mechanism to learn and model action instance features under supervision with only video-level labels.</p><p>where denotes the trainable convolution parameters of feature embedding layer and ReLU is the non-linear activation function we applied in this module.</p><p>3) Action Class Activation Modeling: To localize action instances in the untrimmed video , based on the embedded video feature , we first apply a snippet-level action classification branch to obtain the Class Activation Sequence (CAS). Even though this CAS can not suppress those ambiguous semantics action contexts well, it is capable of suppressing action-related and non-action-related frames. We set this CAS as an initial indicator for action instances. Concretely, we apply a multi-layer inception to project the embedded feature to action class category space. The output is Φ ∈ R ×( +1) , which denotes the classification logit of each action class over time. Formally, we can express the action class activation branch as follows:</p><formula xml:id="formula_2">Φ = MLP( , )<label>(2)</label></formula><p>where denotes the trainable operation parameters of the action class activation branch. 4) Action Context Attention Modeling: As we presented in <ref type="figure">Fig. 1</ref>, in addition to highly discriminative action instances frames and non-action background frames, there are many ambiguous frames such as ambiguous action-related background scenes frames or ambiguous incomplete action frames. For simplicity, in this paper, we denote all those ambiguous actionrelated context frames as action-contexts.</p><p>To realize the separation of action instances and contexts from the initial CAS, we first introduce a three-branch snippetlevel action attention module to detect class-agnostic action instances, semantically ambiguous contexts, and non-action background frames. Specifically, we apply a single convolution layer and softmax function to measure the likelihood of each snippet containing action instance, action context or non-action background. The output of the three-branch attention mod-</p><formula xml:id="formula_3">ule is = {( ( ), ( ), ( ))} =0 ∈ R ×3 , where ( ),</formula><p>( ), ( ) indicates the likelihood of snippet ( ) being action instance, action context or background scenes, respectively. Formally, we denote the three-branch snippet-level action attention module as following:</p><formula xml:id="formula_4">= Softmax(Conv( , ))<label>(3)</label></formula><p>where denotes the trainable convolution layer parameters of the three-branch action-context attention branch.</p><p>Then based on the obtained attention values, in order to discriminate action instances, contexts and action background frames, we build new three-branch class activation sequences CAS , CAS and CAS , respectively. For simplicity, we present the expression of CAS as:</p><formula xml:id="formula_5">CAS ( ) = ( ) * CAS( )<label>(4)</label></formula><p>where CAS ∈ R ×( +1) still presents the class activation scores for each snippet, but it can suppress those ambiguous action context snippets activation scores and still keep high values for action instance snippets. Similarly, for the , it can ignore those action-instance frames and focus on actionrelated context snippets. And for the , the weighted class activation sequence will also pay more attention to those non-action background snippets. 5) Multiple-Instance Learning: As we presented in section III-A, for the W-TAL task, we can only access the video-level action class label during training. Following recent works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we apply the Multiple-Instance Learning (MIL) mechanism [23] to obtain the video-level classification scores. Specifically, in MIL, there are two bags for individual samples, namely positive and negative bags. A positive bag contains at least one positive instance and a negative bag contains no positive instance. The goal of MIL is to distinguish each instance to be positive or negative, besides classifying a bag.</p><p>In this case, we consider the untrimmed video as a bag of video snippets, and each snippet instance is represented by the corresponding class activation score. In order to measure the loss of each CAS, we aggregate the top-action classification scores along with all video snippets for each action category and then average them to build the video-level class activation score. Formally,</p><formula xml:id="formula_6">( ) = 1 max Φ ; ⊂Φ[:, ] |Φ ; |= ∑︁ ∀ ∈Φ ; Φ<label>(5)</label></formula><p>whereΦ ; is the subset containing snippets action classification scores for class and is a hyper-parameter proportional to the video snippets length , i.e., = max(1, // ), and is a pre-defined parameter. Thereafter, we apply a softmax function to the aggregated averaged top-scores to obtain the video-level action probability for each action class:</p><formula xml:id="formula_7">( ) = exp( ( )) +1 =1 exp( ( ))<label>(6)</label></formula><p>where ( ) ∈ R +1 denotes the probability of video contains action class .</p><p>As we presented in section III-B4, to separate action instances, action context and background snippets, we have build three new class activation sequences CAS , CAS , CAS based on the initial CAS and three-branch attention values. Therefore, for the action-instance attention weighted CAS , by applying above MIL mechanism, we can obtain the video-level action probability distribution ( ). Similarly, for the CAS and CAS , we can acquire ( ) and ( ), respectively. Following previous work <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[45]</ref>, we apply the cross-entropy loss function between the predicted video-level action probability distribution ( ) and the ground truth video action probability distribution ( ) to classify different action classes in a video. Specifically, for the CAS , we can formulate the classification cross-entropy loss as:</p><formula xml:id="formula_8">L = +1 ∑︁ =1 − ( ) log( ( ))<label>(7)</label></formula><p>where is the normalized video-level label for the -th class of the video . We set the video-level label = [ ( ) = 1, ( + 1) = 0], since the with the action-instance attention weighting, in CAS , non-action background and ambiguous action context snippets have been suppressed. With similar manner, we can obtain the cross-entrophy loss L and L , respectively. Note that, we set the video-level label = [ ( ) = 1, ( + 1) = 1], = [ ( ) = 0, ( + 1) = 1], since with the attention values weighting, CAS and CAS pays more attention on action contexts and background scenes instead of action instance snippets.</p><p>After we obtained three video-level label based classification loss, L , L , L , we can compose the overall classification loss L as:</p><formula xml:id="formula_9">L = L + L + L (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization Objects</head><p>In addition to the regular classification loss L , we apply three additional losses to make the network achieve better performance, 1) Attention-Guide Loss L , which is used to constrain the action instance attention weighted CAS to follow the action instance attention. 2) Action Feature Separation Loss L for separating the action instances, action contexts, and background snippets features in feature norm space. And 3) Sparse Attention Loss L for constraining the action-instance and action-context branch pays more attention to those action-related frames. The overall loss function is formulated as follows:</p><formula xml:id="formula_10">L = L + L (9) L = 1 L + 2 L + 3 L<label>(10)</label></formula><p>where 1 , 2 and 3 are three hyper-parameters used to balancing the overall loss items. 1) Attention Guide Loss: Although we have introduced the MIL learning mechanism to build video-level classification loss to make the network classify action instances contained in a video, this manner does not optimize the action classification results at snippet-level, which is not conducive to subsequent precise action temporal localization. To make the action classification branch distinguish action instance snippets from those ambiguous action context frames at snippet-level, in addition to the applied video-level cross-entropy classification loss L , we introduce the attention guide loss. We set the action instance attention sequence as a binary indicator for each video snippet, and use it to guide the weighted CAS suppress action context and background snippets at snippetlevel. Specifically, we compose the attention guide loss L as:</p><formula xml:id="formula_11">L = 1 ∑︁ =0 |1 − +1 ( ) − ( )|<label>(11)</label></formula><p>where ( ) is the predicted snippet-level action probability distribution with softmax function applied on the weighted CAS , +1 ( ) denotes the likelihood of snippet ( ) not containing action instances, and ( ) is the action instance attention branch values at snippet ( ). By minimizing L , we can guide the network optimize the class activation sequence at snippet-level.</p><p>2) Action Feature Separation Loss: To make the embedded video snippets features more distinguishable from action instance, action context, and background features, we introduce the action feature separation loss L at feature norm space. Specifically, the L is defined as:</p><formula xml:id="formula_12">L = max(0, − || || + || ||) L = max(0, − || || + || ||) L = || || L = (L + L + L ) 2<label>(12)</label></formula><p>where || · || is the feature norm function, is a pre-defined feature norm separation margin hyper-parameter. And , ,</p><p>are the video-level action instance, action context, and background features, which are built based on the aforementioned top-mechanism. For simplicity, we present the formulation as follows:</p><formula xml:id="formula_13">= 1 idxs=argsort( ) [: ] ∑︁ ∈idxs ( )<label>(13)</label></formula><p>where argsort is a function that returns the indices that would sort an array with descending order, is a pre-defined hyper-parameter proportional to the video snippets length like the aforementioned . In a similar manner, we can obtain the video level action context features and background features , individually. 3) Sparse Attention Loss: Following <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b44">[45]</ref>, we also introduce the sparsity attention loss to constrain the network optimization process, which is based on one assumption that an action can be recognized with a sparse subset of key-snippets in a video. Formally, the is defined as:</p><formula xml:id="formula_14">L = 1 ∑︁ =1 ( ) + ( )<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Temporal Action Localization</head><p>During the inference, given a test video, we first apply the action instance attention based video-level action probability distribution ( ) to classify the test video based on a predefined classification threshold¯. And then we apply threshold strategy on the action instance attention weighted classification sequence CAS and the action instance attention sequence to localize actions. Let {(ˆ,ˆ,ˆ,ˆ)} denotes the detected action instances, like the previous works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b44">[45]</ref>, we apply the Outer-Inner-Contrastive function proposed in <ref type="bibr" target="#b43">[44]</ref> to obtain each detected action instance confidence scorê . Concretely, the confidence score is defined as:</p><formula xml:id="formula_15">= (1 − ) · CAS + · = ∫ˆˆ( ) −ˆ− ∫ˆ−ˆˆ( ) + ∫ˆ+ˆˆ( ) 2ˆ<label>( 15)</label></formula><p>where is a hyper-parameter coefficient used to combine the CAS and attention values ,ˆandˆare the temporal boundary of the detected action instance,ˆ= (ˆ−ˆ)/5 represents the inflated contrast area, andˆdenotes the corresponding action instance category. Note that to increase proposals pool, we apply multiple thresholds on the CAS and , and then we perform non-maximum-suppression (NMS) to remove overlapped action instances proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce the datasets and our implementation details about our network, and then compare our methods with the state-of-the-art methods. At last, we apply a set of ablation studies to evaluate the effectiveness of each module component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments Setting 1) Datasets:</head><p>We perform extensive experiments on two large-scale temporal action localization datasets THUMOS-14 <ref type="bibr" target="#b23">[24]</ref> and ActivityNet1.3 <ref type="bibr" target="#b24">[25]</ref>. THUMOS-14 <ref type="bibr" target="#b23">[24]</ref>, which contains 200 untrimmed validation videos and 213 untrimmed test videos with precise temporal action boundary annotations belonging to 20 action class categories. On average, each video contains 15.4 action instances and more than 70% frames are ambiguous action contexts or non-action background scenes. In addition, the video length varies from a few seconds to more than one hour, which makes it very challenging especially for weaklysupervised temporal action localization. Following previous work, we apply the validation videos for training and the test videos for testing. ActivityNet1.3 <ref type="bibr" target="#b24">[25]</ref>, which contains 10024 untrimmed training videos, 4926 untrimmed validation untrimmed videos, and 5044 videos for testing whose action instance labels are withheld. The action instance class categories involved in this dataset are 200. On average, each video contains 1.6 action instances and about 36% frames are ambiguous action contexts or non-action background scenes. For a fair comparison, same as the previous work, we also utilize training videos for training and report experiments results on the validation videos.</p><p>2) Evaluation Metrics: We use the mean Average Precision (mAP) with different temporal Intersection over Union (t-IoU) thresholds to evaluate our weakly-supervised temporal action localization performance, which denotes as mAP@t-IoU. Specifically, the t-IoU thresholds for THUMOS-14 is [0.1:0.1:0.7] and for ActivityNet is [0.5:0.05:0.95].</p><p>3) Implementation Details: For the feature extraction, we first sample RGB frames at 25 fps for each video and apply the TV-L1 algorithm <ref type="bibr" target="#b51">[52]</ref> to generate optical flow frames. Then, we divide each video into non-overlapping snippets with consecutive 16 frames. Thereafter, we perform the I3D networks <ref type="bibr" target="#b34">[35]</ref> pre-trained on the Kinetics dataset <ref type="bibr" target="#b52">[53]</ref> to obtain the video feature . Note that, for a fair comparison, we do not introduce any other feature fine-tuning operations to the pre-trained I3D model.</p><p>For the training process on the THUMOS-14 dataset, we set the training video batch size to 16, and apply the Adam optimizer <ref type="bibr" target="#b53">[54]</ref> with learning rate 10 −4 and weight decay 5 × 10 −4 . We set the video snippets length = 750, and the topnumber for action instance branch = // , for action context branch = // , for action background branch = // . According to parameter fine-tuning, we set = 8, = = 3, 1 = 2 × 10 −3 , 2 = 5 × 10 −5 , 3 = 2 × 10 −4 , = 0. For the action instance proposals generation, we set the thresholds from 0.15 to 0.25 with step 0.05, to remove overlap proposals, we perform NMS with an t-IoU threshold of 0.50.</p><p>For the training process on the ActivityNet-1.3 dataset, we set the training video batch size to 64, the optimizer <ref type="bibr" target="#b53">[54]</ref> learning rate to 10 −4 and weight decay 0.001. Considering that most video length varies from a few seconds to several minutes, much shorter than the THUMOS-14 dataset, we set the video snippets length = 75. According to parameter fine-tuning, we set = 2, = = 10, 1 = 5 × 10 −3 , 2 = 1 × 10 −5 , 3 = 0, = 0.5. For the action instance proposals generation, we set the thresholds from 0.01 to 0.02 with step 0.005, to remove overlap proposals, we apply NMS with an t-IoU threshold of 0.90.</p><p>All the experiments are evaluated with PyTorch-1.7 <ref type="bibr" target="#b54">[55]</ref> on the RTX-3090 platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with the State-of-the-arts Methods</head><p>We compare our proposed network with current existing fully-supervised and weakly-supervised temporal action localization methods. <ref type="table">Table.</ref> I compares our method with current fully and weaklysupervised based temporal action localization methods on THUMOS-14 dataset. As shown, by introducing the action context modeling mechanism, our method can achieve new state-of-the-art performances under weak video label constraints. Even compared with SF-Net <ref type="bibr" target="#b45">[46]</ref> that introduces more strong supervision (for each action instance, the SF-Net introduces a temporal point annotations), we can still achieve better performance. Furthermore, it can be observed that our methods even can achieve comparable performance with recently fully-supervised methods when t-IoU ≤ 0.5, even though we do not have access to more detailed and specific action instances annotations during training. <ref type="table">Table.</ref> II presents the performance comparison on the ActivityNet-1.3 dataset. As shown, our method can also achieve new state-of-the-art performance under the weaklysupervised assumption. However, the performance improvement is not as significant as the THUMOS-14 dataset. This may be because the ground truth results of the action instances in ActivityNet-1.3 are not as precise as the THUMOS-14 dataset, we found that the action instances annotations on ActivityNet-1.3 are more prone to contain some ambiguous action context frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study and Analysis</head><p>1) Effectiveness of action context modeling: To demonstrate the effectiveness of our proposed action context branch modeling mechanism for weakly-supervised temporal action localization, we apply extensive ablation experiments on THUMOS-14 dataset. The results are summarized in <ref type="table">Table.</ref> III.  For simplicity, we denote the additional loss constraints introduced in Section III-C as . As shown in the table, compared with baseline methods not introducing action context modeling mechanism (Exp 1, 2), our proposed action context modeling mechanism is greatly beneficial for temporal action localization (Exp 3, 4). Specifically, without bells and whistles, we can achieve more than 14% and 6% average performance gain. We attribute this huge performance gain to the fact that it is unreasonable to force the network to group all the action instances, contexts, and non-action background snippets into one category since they do not share any common semantics. It is noteworthy that the experiment 3 only based L and L constrains can achieve better performance compared to the experiment 4 introduced the L . This may be attributable to the fact that the network cannot make a clearer distinction between action instances, contexts, and background fragments without introducing additional constraints , whose effectiveness will be evaluated in the next subsection.</p><p>Besides, we can observe that on the THUMOS-14 dataset only based on the L and L constrains, without introducing any other tricks or losses, we can achieve a performance comparable to current SOTA methods, which further proves the effectiveness of our action context modeling mechanism.</p><p>We can also draw the same conclusion on the ActivityNet-13 dataset that we can achieve the current SOTA performance only by introducing the action context modeling mechanism, the results of the ablation study on ActivityNet-1.3 are presented in <ref type="table">Table.</ref> IV.</p><p>2) Effectiveness of additional loss: Following previous works, we have introduced additional constraints L to further improve the temporal action detection performance. To analysis the impact of each additional loss constraint on the final temporal action detection performance, we make extensive ablation experiments on THUMOS-14 dataset. The results are collated in the <ref type="table">Table.</ref> V. As the table presents, both the L and L are beneficial for boosting the temporal action localization performance. For the L , we can obtain 1.7% average mAP performance gain, since this constrain can facilitate the network to minimize the difference between CAS and at snippet-level. As regards the L , since it is beneficial to separate action instances, action contexts, and non-action background snippets, we can arrive 1.1% average mAP performance boost compared to the baseline experiment. However, we found that the L can not stand alone for improving the performance, possibly because directly minimizing the temporal attention values may cause the network to pay more attention to those action salient snippets and ignore those less discriminative action snippets. When we combine the L , L and L together as L , we can derive 3.6% performance improvement and boost our method to a new state-of-the-art.</p><p>3) Analysis on video snippets number : As we mentioned before, natural videos always vary in temporal length, however, under the weakly supervised formulation of the temporal action localization problem, we can only access video-level labels during the training process. Therefore, to achieve parallel optimization of the proposed network, we employ a linear interpolation-based sampling strategy such that all training videos have the same temporal dimension . To analysis the influence of the video sampling snippets number on the final performance, we carried out a series of experiments, the results are presented in the <ref type="table">Table.</ref> VI.</p><p>From the results in the table, we can conclude that is not linearly related to the growth in detection performance. When is small, increasing can lead to significant performance gains (e.g. we can obtain 5.1% average mAP boost from = 250 to = 500), but when exceeds a certain value ( = 750 for the THUMOS-14 dataset), detection performance starts to drop again. The probable reason for this is that when is small, for most videos we cannot achieve full sampling, while when exceeds a certain value, it leads to oversampling, resulting in the introduction of more ambiguous context snippets during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>To further demonstrate and exemplify the effectiveness of our action context modeling mechanism, we present some qualitative results in <ref type="figure">Fig. 3</ref>. In this figure, the CAS denotes the class activation sequence originally obtained from the classification branch.</p><p>denotes the action instance attention . Qualitative result visualization on the THUMOS-14 dataset. From the above qualitative results, we can conclude that our proposed action context modeling mechanism is greatly beneficial to suppress ambiguous action context frames and help us achieve more precise temporal action localization results. However, we can note that this mechanism is not perfect and that it may sometimes suppress the true action instance frames. The possible reason for this is that those action instance frames may not be distinctive, but rather ambiguous, similar to action context frames. values which are applied to suppress those ambiguous action context frames. While CAS represents the class activation sequence calculated from the action instance attention values weighted CAS. From those qualitative visualization results, we can conclude that our proposed action context modeling mechanism is of great benefit in suppressing ambiguous action context frames, helping to filter those false-positive errors, and obtaining more accurate temporal action localization results. However, this mechanism is not perfect under the problem definition of weak supervision, since we do not have the access to accurate action annotation information during the training process, therefore the mechanism may sometimes suppress and filter out some of the not so significant and discriminative action frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose an action context modeling network termed ACM-Net to achieve the separation among action instances, semantically ambiguous action context, and nonaction background frames. The proposed ACM-Net integrates a three-branch attention module which is used to measure the likelihood of each temporal point containing instance, context, or background frames. Based on the three-branch attention values, three-branch class activation sequences (CAS) are then introduced to represent the action instances, action contexts, or background activation scores at each temporal point. We conduct extensive experiments on two popular benchmark datasets the THUMOS-14 and the ActivityNet-1.3 to demonstrate the effectiveness of our ACM-Net. The results show that our ACM-Net can outperform current weakly-supervised stateof-the-art methods and beat those with stronger supervision, and can even achieve performance comparable to those with full supervision. For the future work, we believe the context modeling will be a promising direction for various weakly supervised learning tasks, and explore such mechanism in other related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. An example of "LongJump"actionFig. 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>). Feature ExtractionWeakly-supervised Temporal Action Localization Learning</head><label></label><figDesc></figDesc><table><row><cell cols="2">b). Feature Embedding</cell><cell></cell><cell>attention branch</cell><cell>action context attention non-action background attention bak att c). action instance attention att con</cell><cell>action-instance attention weighted action-context attention weighted background attention weighted CAS bak CAS con</cell><cell>MIL Learning MIL Learning MIL Learning</cell><cell>Video Level Prediction Video Level Prediction Video Level Prediction [ ( ) 1, ( 1) 1] y n y C    [ ( ) 1, ( 1) 0] y n y C    [ ( ) 0, ( 1) 1] y n y C   </cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ins att</cell><cell>CAS ins</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Temporal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Thresholding</cell><cell>Action</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Localization</cell></row><row><cell>extracted features F</cell><cell>feature embedding</cell><cell>embedded features X</cell><cell>classification branch</cell><cell>class activation sequence CAS</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I TEMPORAL</head><label>I</label><figDesc>ACTION LOCALIZATION PERFORMANCE COMPARISON ON THE THUMOS-14 DATASET. AVG MEANS AVERAGE MAP FROM IOU 0.1 TO 0.7 WITH 0.1 INCREMENTS. RECENT WORKS IN BOTH FULLY-SUPERVISED AND WEAKLY-SUPERVISED SETTINGS ARE REPORTED. OUR METHOD OUTPERFORMS THE STATE-OF-THE-ART WEAKLY-SUPERVISED METHODS AND EVEN CAN ACHIEVE COMPARABLE PERFORMANCE WITH RECENT FULLY-SUPERVISED METHODS WHEN T-IOU ≤ 0.5. EVEN THOUGH SF-NET [46] INTRODUCES STRONGER SUPERVISION OF ACTION INSTANCES, WE CAN STILL ACHIEVE BETTER PERFORMANCE. * DENOTES THE REPORTED PERFORMANCE NOT CONTAINING POST-PROCESSING. DATASET. AVG MEANS AVERAGE MAP FROM IOU 0.50 TO 0.95 WITH 0.05 INCREMENTS. OUR METHOD OUTPERFORMS THE STATE-OF-THE-ART WEAKLY-SUPERVISED METHODS, AND ESPECIALLY AT T-IOU= 0.5, WE CAN OBTAIN MORE THAN 3% IMPROVEMENT.</figDesc><table><row><cell>Supervision</cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP@t-IoU(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.10 0.20 0.30 0.40 0.50 0.60 0.70 Avg[0.1-0.5] Avg[0.3-0.7]</cell><cell>Avg</cell></row><row><cell></cell><cell></cell><cell>SSN (ICCV 2017) [7]</cell><cell></cell><cell></cell><cell cols="4">66.0 59.4 51.9 41.0 29.8</cell><cell>-</cell><cell>-</cell><cell>49.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">BSN (ECCV 2018) [8]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">53.5 45.0 36.9 28.4 20.0</cell><cell>-</cell><cell>36.8</cell><cell>-</cell></row><row><cell cols="2">Full action label</cell><cell cols="2">BMN (ICCV 2019) [9] G-TAD  *  (CVPR 2020) [43]</cell><cell></cell><cell cols="2">-66.1 64.2 -</cell><cell cols="2">56.0 47.4 38.8 29.7 20.5 54.5 47.6 40.2 30.8 23.4</cell><cell>-54.5</cell><cell>38.5 39.3</cell><cell>-46.7</cell></row><row><cell></cell><cell></cell><cell cols="2">BSN++ (AAAI 2021) [10]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">59.9 49.5 41.3 31.9 22.8</cell><cell>-</cell><cell>41.1</cell><cell>-</cell></row><row><cell cols="4">Weak action label SF-Net (ECCV 2020) [46]</cell><cell></cell><cell cols="4">71.0 63.4 53.2 40.7 29.3 18.4</cell><cell>9.6</cell><cell>51.5</cell><cell>30.2</cell><cell>40.8</cell></row><row><cell></cell><cell></cell><cell cols="3">Hide-and-Seek (ICCV 2017) [47]</cell><cell cols="3">36.4 27.8 19.5 12.7</cell><cell>6.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.6</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="7">UntrimmedNet (CVPR 2017) [48] 44.4 37.7 28.2 21.2 13.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">STPN (CVPR 2018) [11]</cell><cell></cell><cell cols="4">52.0 44.7 35.5 25.8 16.9</cell><cell>9.9</cell><cell>4.3</cell><cell>35.0</cell><cell>18.5</cell><cell>26.4</cell></row><row><cell></cell><cell></cell><cell cols="2">AutoLoc (ECCV 2018) [44]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">35.8 29.0 21.2 13.4</cell><cell>5.8</cell><cell>-</cell><cell>21.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">W-TALC (ECCV 2018) [19]</cell><cell></cell><cell cols="4">55.2 49.6 40.1 31.1 22.8</cell><cell>-</cell><cell>7.6</cell><cell>39.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">IWO-Net (TIP 2019) [15]</cell><cell></cell><cell cols="4">57.6 48.9 38.9 29.3 20.5</cell><cell>-</cell><cell>-</cell><cell>39.0</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Weak video label</cell><cell cols="2">MAAN (ICLR 2019) [20] TSM (ICCV 2019) [13]</cell><cell></cell><cell cols="4">59.8 50.8 41.1 30.6 20.3 12.0 --39.5 -24.5 -</cell><cell>6.9 7.1</cell><cell>40.5 -</cell><cell>22.2 -</cell><cell>31.6 -</cell></row><row><cell></cell><cell></cell><cell cols="2">BasNet (AAAI 2020) [21]</cell><cell></cell><cell cols="4">58.2 52.3 44.6 36.0 27.0 18.6 10.4</cell><cell>43.6</cell><cell>27.3</cell><cell>35.3</cell></row><row><cell></cell><cell></cell><cell cols="2">DGAM (CVPR 2020) [49]</cell><cell></cell><cell cols="4">60.0 54.2 46.8 38.2 28.8 19.8 11.4</cell><cell>45.6</cell><cell>29.0</cell><cell>37.0</cell></row><row><cell></cell><cell></cell><cell cols="2">EM-MIL (ECCV 2020) [22]</cell><cell></cell><cell cols="4">59.1 52.7 45.5 36.8 30.5 22.7 16.4</cell><cell>45.0</cell><cell>30.4</cell><cell>37.7</cell></row><row><cell></cell><cell></cell><cell cols="2">A2CL-PT (ECCV 2020) [12]</cell><cell></cell><cell cols="4">61.2 56.1 48.1 39.0 30.1 19.2 10.6</cell><cell>46.9</cell><cell>29.4</cell><cell>37.8</cell></row><row><cell></cell><cell></cell><cell cols="2">ACSNet (AAAI 2021) [45]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">51.4 42.7 32.4 22.0 11.7</cell><cell>-</cell><cell>32.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">CoLA (CVPR 2021) [50]</cell><cell></cell><cell cols="4">66.2 59.5 51.5 41.9 32.2 22.0 13.1</cell><cell>50.3</cell><cell>32.1</cell><cell>40.9</cell></row><row><cell></cell><cell></cell><cell>ACM-Net(Ours)</cell><cell></cell><cell></cell><cell cols="4">68.9 62.7 55.0 44.6 34.6 21.8 10.8</cell><cell>53.2</cell><cell>33.4</cell><cell>42.6</cell></row><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">TEMPORAL ACTION LOCALIZATION PERFORMANCE COMPARISON ON THE</cell><cell></cell></row><row><cell cols="3">ACTIVITYNET-1.3 Supervision Method</cell><cell cols="3">mAP@t-IoU(%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.50 0.75 0.95</cell><cell>Avg</cell><cell></cell></row><row><cell></cell><cell cols="2">SSN (ICCV 2017) [7]</cell><cell cols="2">39.1 23.5</cell><cell>5.5</cell><cell>24.0</cell><cell></cell></row><row><cell></cell><cell cols="2">BSN (ECCV 2018) [8]</cell><cell cols="2">46.5 30.0</cell><cell>8.0</cell><cell>30.0</cell><cell></cell></row><row><cell>Full</cell><cell cols="2">BMN (ICCV 2019) [9] P-GCN (CVPR 2019) [51]</cell><cell cols="2">50.1 34.8 48.3 33.2</cell><cell>8.3 3.3</cell><cell>33.9 31.1</cell><cell></cell></row><row><cell></cell><cell cols="2">G-TAD (CVPR 2020) [43]</cell><cell cols="2">50.4 34.6</cell><cell>9.0</cell><cell>34.1</cell><cell></cell></row><row><cell></cell><cell cols="2">BSN++ (AAAI 2021) [10]</cell><cell cols="2">51.3 35.7</cell><cell>8.3</cell><cell>34.9</cell><cell></cell></row><row><cell></cell><cell cols="2">STPN (CVPR 2018) [11]</cell><cell cols="2">29.3 16.9</cell><cell>2.6</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell cols="2">MAAN (ICLR 2019) [20]</cell><cell cols="2">33.7 21.9</cell><cell>5.5</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell cols="2">TSM (ICCV 2019) [13]</cell><cell cols="2">30.3 19.0</cell><cell>4.5</cell><cell>-</cell><cell></cell></row><row><cell>Weak</cell><cell cols="2">BasNet (AAAI 2020) [21]</cell><cell cols="2">34.5 22.5</cell><cell>4.9</cell><cell>22.2</cell><cell></cell></row><row><cell></cell><cell cols="2">A2CL-PT (ECCV 2020) [12]</cell><cell cols="2">36.8 22.5</cell><cell>5.2</cell><cell>22.5</cell><cell></cell></row><row><cell></cell><cell cols="2">ACSNet (AAAI 2021) [45]</cell><cell>36.3</cell><cell>24.2</cell><cell>5.8</cell><cell>23.9</cell><cell></cell></row><row><cell></cell><cell cols="2">ACM-Net(Ours)</cell><cell cols="2">40.1 24.2</cell><cell>6.2</cell><cell>24.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY OF THE EFFECTIVENESS OF OUR PROPOSED ACTION CONTEXT MODELING MECHANISM ON THE THUMOS-14 DATASET. AVG MEANS AVERAGE MAP FROM T-IOU 0.1 TO 0.7 WITH 0.1 INCREMENTS.</figDesc><table><row><cell>Exp</cell><cell>L</cell><cell>L</cell><cell cols="2">mAP@t-IoU(%)</cell></row><row><cell>L</cell><cell>L</cell><cell>L</cell><cell cols="2">0.10 0.30 0.50 0.70</cell><cell>Avg</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>49.9 32.9 16.6</cell><cell>5.3</cell><cell>26.1</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>55.9 41.9 23.0</cell><cell>7.1</cell><cell>32.0</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell cols="3">67.4 50.8 31.5 10.8 40.3</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell cols="3">65.6 49.4 29.6 10.0 38.8</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell cols="3">68.9 55.0 34.6 10.8 42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="6">ABLATION STUDY OF THE EFFECTIVENESS OF OUR PROPOSED ACTION</cell></row><row><cell cols="6">CONTEXT MODELING MECHANISM ON THE ACTIVITYNET-1.3 DATASET.</cell></row><row><cell cols="6">AVG MEANS AVERAGE MAP FROM T-IOU 0.50 TO 0.95 WITH 0.05</cell></row><row><cell></cell><cell></cell><cell>INCREMENTS.</cell><cell></cell><cell></cell></row><row><cell>Exp</cell><cell>L</cell><cell>L</cell><cell cols="2">mAP@t-IoU(%)</cell></row><row><cell>L</cell><cell>L</cell><cell>L</cell><cell cols="2">0.50 0.75 0.95</cell><cell>Avg</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>34.5 21.0</cell><cell>4.9</cell><cell>21.5</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>39.1 23.4</cell><cell>6.0</cell><cell>23.9</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>39.1 23.1</cell><cell>5.8</cell><cell>23.7</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>40.1 24.2</cell><cell>6.2</cell><cell>24.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V EVALUATION</head><label>V</label><figDesc>THE EFFECTIVENESS OF ADDITIONAL LOSS FUNCTIONS ON THE THUMOS-14 DATASET.</figDesc><table><row><cell>Exp</cell><cell>L</cell><cell>L</cell><cell cols="2">mAP@t-IoU(%)</cell><cell></cell></row><row><cell>L</cell><cell>L</cell><cell>L</cell><cell>0.10 0.30 0.50</cell><cell>0.70</cell><cell>Avg</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>65.6 49.4 29.6</cell><cell>10.0</cell><cell>38.8</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>66.4 51.5 32.2</cell><cell>11.0</cell><cell>40.5</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>66.4 51.2 31.1</cell><cell>10.3</cell><cell>39.9</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>64.8 48.9 30.3</cell><cell>10.7</cell><cell>38.7</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell>67.4 53.1 33.8</cell><cell>11.2</cell><cell>41.8</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell>68.9 55.0 34.6</cell><cell>10.8</cell><cell>42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI EVALUATION</head><label>VI</label><figDesc>THE INFLUENCE OF VIDEO SNIPPETS SAMPLE NUMBER ON THE THUMOS-14 DATASET. .20 0.30 0.40 0.50 0.60 0.70 Avg 1 250 64.4 56.3 47.0 36.6 26.4 15.8 7.4 36.3 2 500 67.2 61.2 53.5 43.4 32.5 20.8 10.9 41.4 3 750 68.9 62.7 55.0 44.6 34.6 21.8 10.8 42.6 4 900 67.0 61.9 54.0 43.8 33.6 21.3 10.8 41.8 5 1000 67.7 61.7 53.3 43.5 32.4 20.7</figDesc><table><row><cell>Exp</cell><cell>T</cell><cell>mAP@t-IoU(%)</cell></row><row><cell></cell><cell></cell><cell>0.10 011.0</cell><cell>41.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Movement pattern histogram for action recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review on human action analysis in videos for retrieval applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramezani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yaghmaee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="514" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on activity recognition and behavior understanding in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwakarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="983" to="1009" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bsn++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial background-aware loss for weaklysupervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5797" to="5808" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with expectation-maximization multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-instance learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science &amp; Technology, Nanjing University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">G-tad: Subgraph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Acsnet: Action-context separation network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sf-net: Single-frame supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cola: Weaklysupervised temporal action localization with snippet contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
