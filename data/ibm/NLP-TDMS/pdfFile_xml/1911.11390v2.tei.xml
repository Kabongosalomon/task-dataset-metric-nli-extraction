<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Attention Mechanism for Visual Dialog that can Handle All the Interactions between Multiple Inputs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van-Quang</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Grad School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
							<email>suganuma@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Grad School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
							<email>okatani@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Grad School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Attention Mechanism for Visual Dialog that can Handle All the Interactions between Multiple Inputs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Dialog</term>
					<term>Attention</term>
					<term>Multimodality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It has been a primary concern in recent studies of vision and language tasks to design an effective attention mechanism dealing with interactions between the two modalities. The Transformer has recently been extended and applied to several bi-modal tasks, yielding promising results. For visual dialog, it becomes necessary to consider interactions between three or more inputs, i.e., an image, a question, and a dialog history, or even its individual dialog components. In this paper, we present a neural architecture named Light-weight Transformer for Many Inputs (LTMI) that can efficiently deal with all the interactions between multiple such inputs in visual dialog. It has a block structure similar to the Transformer and employs the same design of attention computation, whereas it has only a small number of parameters, yet has sufficient representational power for the purpose. Assuming a standard setting of visual dialog, a layer built upon the proposed attention block has less than one-tenth of parameters as compared with its counterpart, a natural Transformer extension. The experimental results on the VisDial datasets validate the effectiveness of the proposed approach, showing improvements of the best NDCG score on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from 64.47 to 66.53 with ensemble models, and even to 74.88 with additional finetuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction from a dialog history to know what 'they' refers to and look at the relevant image region to find out a color.</p><p>In recent studies of vision-language tasks, a primary concern has been to design an attention mechanism that can effectively deal with interactions between the two modalities. In the case of visual dialog, it becomes further necessary to consider interactions between an image, a question, and a dialog history or additionally multiple question-answer pairs in the history. Thus, the key to success will be how to deal with such interactions between three and more entities. Following a recent study <ref type="bibr">[39]</ref>, we will use the term utility to represent each of these input entities for clarity, since the term modality is inconvenient to distinguish between the question and the dialog history.</p><p>Existing studies have considered attention from one utility to another based on different hypotheses, such as "question → history → image" path in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">30]</ref>, and "question → image → history → question" path in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">46]</ref>, etc. These methods cannot take all the interactions between utilities into account, although the missing interactions could be crucial. Motivated by this, a recent study tries to capture all the possible interactions by using a factor graph <ref type="bibr">[39]</ref>. However, building the factor graph is computationally inefficient, which seemingly hinders the method from unleashing the full potential of modeling all the interactions, especially when the dialog history grows long.</p><p>The Transformer <ref type="bibr">[44]</ref> has become a standard neural architecture for various tasks in the field of natural language processing, especially since the huge success of its pretrained model, BERT <ref type="bibr" target="#b11">[12]</ref>. Its basic mechanism has recently been extended to the bi-modal problems of vision and language, yielding promising results <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">28,</ref><ref type="bibr">29,</ref><ref type="bibr">50]</ref>. Then, it appears to be natural to extend it further to deal with many-to-many utility interactions. However, it is not easy due to several reasons. As its basic structure is designed to be deal with self-attention, even in the simplest case of bi-modality, letting X and Y be the two utilities, there are four patterns of attention, X → Y , Y → X, X → X, and Y → Y ; we need an independent Transformer block for each of these four. When extending this to deal with many-to-many utility interactions, the number of the blocks and thus of their total parameters increases proportionally with the square of the number of utilities, making it computationally expensive. Moreover, it is not apparent how to aggregate the results from all the interactions.</p><p>To cope with this, we propose a neural architecture named Light-weight Transformer for Many Inputs (LTMI) that can deal with all the interactions between many utilities. While it has a block structure similar to the Transformer and shares the core design of attention computation, it differs in the following two aspects. One is the difference in the implementation of multi-head attention. Multi-head attention in the Transformer projects the input feature space linearly to multiple lower-dimensional spaces, enabling to handle multiple attention maps, where the linear mappings are represented with learnable parameters. In the proposed model, we instead split the input feature space to subspaces mechanically according to its indexes, removing all the learnable parameters from the attention computation.</p><p>The other difference from the Transformer is that LTMI is designed to receive multiple utilities and compute all the interactions to one utility from all the others including itself. This yields the same number of attended features as the input utilities, which are then concatenated in the direction of the feature space dimensions and then linearly projected back to the original feature space. We treat the parameters of the last linear projection as only learnable parameters in LTMI. This design makes it possible to retain sufficient representational power with a much fewer number of parameters, as compared with a natural extension of the Transformer block to many utilities. By using the same number of blocks in parallel as the number of utilities, we can deal with all the interactions between the utilities; see <ref type="figure" target="#fig_0">Fig. 2</ref> for example. Assuming three utilities and the feature space dimensionality of 512, a layer consisting of LTMI has 2.38M parameters, whereas its counterpart based on naive Transformer extension has 28.4M parameters.</p><p>The contribution of this study is stated as follows. i) A novel attention mechanism for visual dialog that can deal with all interactions between multiple utilities is proposed. It is lightweight and yet has a sufficient representational power. ii) A series of experiments and ablative studies are conducted, achieving the new state-of-the-art results on the VisDial datasets, e.g., high NDCG scores on the VisDial v1.0 dataset. These validate the effectiveness of the proposed method.</p><p>iii) The visualization of the inference conducted by our method is given, providing interpretation of how the proposed mechanism works. Our method achieves the third place with only a small gap to the first place in the Visual Dialog 2019 leaderboard; the result is achieved without using external training data and with a fewer number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention Mechanisms for Vision-Language Tasks</head><p>Attention mechanisms are currently indispensable to build neural architectures for vision-language tasks, such as VQA <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">31,</ref><ref type="bibr">33,</ref><ref type="bibr">48,</ref><ref type="bibr">51,</ref><ref type="bibr">52]</ref> and visual grounding <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">49,</ref><ref type="bibr">55]</ref>, etc. Inspired by the recent success of the Transformer for language tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">44]</ref>, several studies have proposed its extensions to bi-modal visionlanguage tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">28,</ref><ref type="bibr">29,</ref><ref type="bibr">43,</ref><ref type="bibr">50]</ref>. Specifically, for VQA, it is proposed to use intra-modal and inter-modal attention blocks and stack them alternately to fuse question and image features <ref type="bibr" target="#b13">[14]</ref>; it is also proposed to use a cascade of modular co-attention layers that compute the self-attention and guided-attention of question and image features <ref type="bibr">[50]</ref>. The method of pretraining a Transformer model used in BERT <ref type="bibr" target="#b11">[12]</ref> is employed along with Transformer extension to bi-modal tasks for several vision-language tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">28,</ref><ref type="bibr">29]</ref>. They first pretrain the models on external datasets, such as COCO Captions <ref type="bibr" target="#b5">[6]</ref> or Conceptual Captions dataset <ref type="bibr">[41]</ref>, and then fine-tune them on several target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Dialog</head><p>The task of visual dialog has recently been proposed by two groups of researchers concurrently <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. De Vries et al. introduced the GuessWhat?! dataset, which is built upon goal-oriented dialogs held by two agents to identify unknown objects in an image through a set of yes/no questions <ref type="bibr" target="#b9">[10]</ref>. Das et al. released the VisDial dataset, which is built upon dialogs consisting of pairs of a question and an answer about an image that are provided in the form of natural language texts <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr">Kottur et al.</ref> recently introduced CLEVR-Dialog as the diagnostic dataset for visual dialog <ref type="bibr" target="#b24">[25]</ref>.</p><p>Most of the existing approaches employ an encoder-decoder architecture <ref type="bibr">[42]</ref>. They can be categorized into the following three groups by the design of the encoder: i) fusion-based methods, e.g., LF <ref type="bibr" target="#b7">[8]</ref> and HRE <ref type="bibr" target="#b7">[8]</ref>, which fuses the inputs by their concatenation followed by the application of a feed-forward or recurrent network, and Synergistic <ref type="bibr" target="#b14">[15]</ref>, which fuses the inputs at multiple stages; ii) attention-based methods that compute attended features of the input image, question, and history utilities, e.g., MN <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr">CoAtt [46]</ref>, HCIAE [30], Synergistic <ref type="bibr" target="#b14">[15]</ref>, ReDAN <ref type="bibr" target="#b12">[13]</ref>, FGA [39], and CDF <ref type="bibr" target="#b20">[21]</ref>; ReDAN compute the attention over several reasoning steps, FGA models all the interactions over many utilities via a factor graph; iii) methods that attempt to resolve visual co-reference, e.g., <ref type="bibr">RvA [34]</ref> and CorefNMN <ref type="bibr" target="#b23">[24]</ref>, which use neural modules to form an attention mechanism, DAN <ref type="bibr" target="#b19">[20]</ref>, which employs a network having two attention modules, and AMEM <ref type="bibr">[40]</ref>, which utilizes a memory mechanism for attention. As for the decoder, there are two designs: i) discriminative decoders that rank the candidate answers using the cross-entropy loss <ref type="bibr" target="#b7">[8]</ref> or the n-pair loss [30]; and ii) generative decoders that yield an answer by using a MLE loss <ref type="bibr" target="#b7">[8]</ref>, weighted likelihood estimation [53], or a combination with adversarial learning <ref type="bibr">[30,</ref><ref type="bibr">46]</ref>, which trains a discriminator on both positive and negative answers, then transferring it to the generator with auxiliary adversarial learning.</p><p>Other approaches include GNN [54], which models relations in a dialog by an unknown graph structure; the employment of reinforcement learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>; and HACAN [47] which adopts policy gradient to learn the impact of history by intentionally imposing the wrong answer into dialog history. In <ref type="bibr">[45,</ref><ref type="bibr">32]</ref>, pretrained vision-language models are adopted, which consist of many Transformer blocks with hundreds of millions parameters, leading to some performance gain. Qi et al.</p><p>[37] present model-agnostic principles for visual dialog to maximize performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Attention Mechanism for Many Utilities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Mechanism of Transformer</head><p>As mentioned earlier, the Transformer has been applied to several bi-modal vision-language tasks, yielding promising results. The Transformer computes and uses attention from three types of inputs, Q (query), K (key), and V (value). Its computation is given by</p><formula xml:id="formula_0">A(Q, K, V ) = softmax QK √ d V,<label>(1)</label></formula><formula xml:id="formula_1">T T T T Y 1 Y 2 Y U-1 X MH-Attn MH-Attn S T MH-Attn S T MH-Attn S T Concatenate Feed Forward AddNorm T T T T X T … Y X Multi-Head Attention S T Feed Forward AddNorm X AddNorm (a) (b)F</formula><p>ig. 1: (a) Source-to-target attention for bi-modal problems implemented by the standard Transformer block; the source Y is attended by weights computed from the similarity between the target X and Y . (b) The proposed block that can deal with many utilities; the source features {Y 1 , . . . , Y U −1 } are attended by weights computed between them and the target X. Shaded boxes have learnable weights where Q, K, and V are all collection of features, each of which is represented by a d-dimensional vector. To be specific, Q = [q 1 , . . . , q M ] ∈ R M ×d is a collection of M features; similarly, K and V are each a collection of N features, i.e., K, V ∈ R N ×d . In Eq.(1), V is attended with the weights computed from the similarity between Q and K.</p><p>The above computation is usually multi-plexed in the way called multi-head attention. It enables to use a number of attention distributions in parallel, aiming at an increase in representational power. The outputs of H 'heads' are concatenated, followed by linear transformation with learnable weights</p><formula xml:id="formula_2">W O ∈ R d×d as A M (Q, K, V ) = head 1 , · · · , head H W O .<label>(2)</label></formula><p>Each head is computed as follows:</p><formula xml:id="formula_3">head h = A(QW Q h , KW K h , V W V h ), h = 1, . . . , H,<label>(3)</label></formula><p>where W Q h , W K h , W V h ∈ R d×d H each are learnable weights inducing a linear projection from the feature space of d-dimensions to a lower space of d H (= d/H)dimensions. Thus, one attentional block A M (Q, K, V ) has the following learnable weights:</p><formula xml:id="formula_4">(W Q 1 , W K 1 , W V 1 ), · · · , (W Q H , W K H , W V H ) and W O .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application to Bi-Modal Tasks</head><p>While Q, K, and V in NLP tasks are of the same modality (i.e., language), the above mechanism has been extended to bi-modality and applied to visionlanguage tasks in recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">28,</ref><ref type="bibr">29,</ref><ref type="bibr">43,</ref><ref type="bibr">50]</ref>. They follow the original idea of the Transformer, considering attention from source features Y to target features X as</p><formula xml:id="formula_5">A Y (X) = A M (X, Y, Y ).<label>(5)</label></formula><p>In MCAN [50], language feature is treated as the source and visual feature is as the target. In [28] and others <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">29,</ref><ref type="bibr">43]</ref>, co-attention, i.e., attention in the both directions, is considered. Self-attention, i.e., the attention from features to themselves, is given as a special case by</p><formula xml:id="formula_6">A X (X) = A M (X, X, X).<label>(6)</label></formula><p>In the above studies, the Transformer block with the source-to-target attention and that with the self-attention are independently treated and are stacked, e.g., alternately or sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Light-weight Transformer for Many Inputs</head><p>Now suppose we wish to extend the above attention mechanism to a greater number of utilities 3 ; we denote the number by U . If we consider every possible source-target pairs, there are U (U − 1) cases in total, as there are U targets, for each of which U − 1 sources exist. Then we need to consider attention computation A Y (X) over U − 1 sources Y 's for each target X. Thus, the straightforward extension of the above attention mechanism to U utilities needs U (U − 1) times the number of parameters listed in Eq.(4). If we stack the blocks, the total number of parameters further increases proportionally.</p><p>To cope with this, we remove all the weights from Eq.(5). To be specific, for each head h(= 1, . . . , H), we choose and freeze (W Q h , W K h , W V h ) as</p><formula xml:id="formula_7">W Q h = W K h = W V h = [O d H , · · · , O d H (h−1)d H , I d H , O d H , · · · , O d H (H−h)d H ] ,<label>(7)</label></formula><p>where O d H is a d H ×d H zero matrix and I d H is a d H ×d H identity matrix. In short, the subspace for each head is determined to be one of H subspaces obtained by splitting the d-dimensional feature space with its axis indexes. Besides, we set W O = I, which is the linear mapping applied to the concatenation of the heads' outputs. LetĀ Y (X) denote this simplified attention mechanism. Now let the utilities be denoted by {X, Y 1 , . . . , Y U −1 }, where X ∈ R M ×d is the chosen target and others Y i ∈ R Ni×d are the sources. Then, we compute all the source-to-target attention asĀ Y1 (X), · · · ,Ā Y U −1 (X). In the standard Transformer block (or rigorously its natural extensions to bi-modal problems), the attended features are simply added to the target as X + A Y (X), followed by normalization and subsequent computations. To recover some of the loss in representational power due to the simplification yieldingĀ Y (X), we propose a different approach to aggregateĀ Y1 (X), · · · ,Ā Y U −1 (X) and X. Specifically, we concatenate all the source-to-target attention plus the self-attentionĀ X (X) from X to X as</p><formula xml:id="formula_8">S 1 S 2 S U-1 T Y 1 Y 2 Y U-1 X T X … S1 S2 T S1 S2 T S1 S2 T V l-1 Q l-1 R l-1 V l Q l R l T T T (b) (a)</formula><formula xml:id="formula_9">X concat = [Ā X (X),Ā Y1 (X), · · · ,Ā Y U −1 (X)],<label>(8)</label></formula><p>where X concat ∈ R M ×U d . We then apply linear transformation to it given by W ∈ R U d×d and b ∈ R d with a single fully-connected layer, followed by the addition of the original X and layer normalization as</p><formula xml:id="formula_10">X = LayerNorm(ReLU(X concat W + 1 M · b ) + X),<label>(9)</label></formula><p>where 1 M is M -vector with all ones. With this method, we aim at recovery of representational power as well as the effective aggregation of information from all the utilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Interactions between All Utilities</head><p>We have designed a basic block ( <ref type="figure">Fig. 1(b)</ref>) that deals with attention from many sources to a single target. We wish to consider all possible interactions between all the utilities, not a single utility being the only target. To do this, we use U basic blocks to consider all the source-to-target attention. Using the basic block as a building block, we show how an architecture is designed for visual dialog having three utilities, visual features V , question features Q, and dialog history features R, in <ref type="figure" target="#fig_0">Fig. 2</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details for Visual Dialog</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Definition</head><p>The problem of Visual Dialog is stated as follows. An agent is given the image of a scene and a dialog history containing T entities, which consists of a caption and question-answer pairs at T − 1 rounds. Then, the agent is further given a new question at round T along with 100 candidate answers for it and requested to answer the question by choosing one or scoring each of the candidate answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Representation of Utilities</head><p>We first extract features from an input image, a dialog history, and a new question at round T to obtain their representations. For this, we follow the standard method employed in many recent studies. For the image utility, we use the bottom-up mechanism <ref type="bibr" target="#b0">[1]</ref>, which extracts region-level image features using the Faster-RCNN [38] pre-trained on the Visual Genome dataset <ref type="bibr" target="#b25">[26]</ref>. For each region (i.e., a bounding box = an object), we combine its CNN feature and geometry to get a d-dimensional vector v i (i = 1, . . . , K), where K is the predefined number of regions. We then define</p><formula xml:id="formula_11">V = [v 1 , v 2 , · · · , v K ] ∈ R K×d .</formula><p>For the question utility, after embedding each word using an embedding layer initialized by pretrained GloVe vectors, we use two-layer Bi-LSTM to transform them to q i (i = 1, . . . , N ), where N is the number of words in the question. We optionally use the positional embedding widely used in NLP studies. We examine its effects in an ablation test. We then define Q = [q 1 , . . . , q N ] ∈ R N ×d . For the dialog history utility, we choose to represent it as a single utility here. Thus, each of its entities represents the initial caption or the question-answer pair at one round. As with the question utility, we use the same embedding layer and a twolayer Bi-LSTM together with the positional embeddings for the order of dialog rounds to encode them with a slight difference in formation of an entity vector r i (i = 1, . . . , T ), where T is the number of Q&amp;A plus the caption. We then define R = [r 1 , . . . , r T ] ∈ R T ×d . More details are provided in the supplementary material. <ref type="figure" target="#fig_1">Figure 3</ref> shows the entire network. It consists of an encoder and a decoder. The encoder consists of L stacks of the proposed attention blocks; a single stack has U blocks in parallel, as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>. We set V 0 = V , Q 0 = Q, and R 0 = R as the inputs of the first stack. After the l-th stack, the representations of the image, question, and dialog history utilities are updated as V l , Q l , and R l , respectively. In the experiments, we apply dropout with the rate of 0.1 to the linear layer inside every block. There is a decoder(s) on top of the encoder. We consider a discriminative decoder and a generative decoder, as in previous studies. Their design is explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Network Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Design of Decoders</head><p>Decoders receive the updated utility representations, V L , Q L , and R L at their inputs. We convert them independently into d-dimensional vectors c V , c Q , and c R , respectively. This conversion is performed by a simple self-attention computation. We take c V as an example here. First, attention weights over the entities of V L are computed by a two-layer network as</p><formula xml:id="formula_12">a V = softmax(ReLU(V L W 1 + 1 K b 1 )W 2 + 1 K b 2 ),<label>(10)</label></formula><p>where</p><formula xml:id="formula_13">W 1 ∈ R d×d , W 2 ∈ R d×1 , b 1 ∈ R d , b 2 ∈ R 1 , and 1 K is K-vector with all ones.</formula><p>Then, c V is given by</p><formula xml:id="formula_14">c V = K i=1 v L,i a V,i ,<label>(11)</label></formula><p>where v L,i is the i-th row vector of V L and a V,i is the i-th attention weight (a scalar). The others, i.e., c Q and c R , can be obtained similarly. These vectors are integrated and used by the decoders. In our implementation for visual dialog, we found that c R does not contribute to better results; thus we use only c V and c Q . Note that this does not mean the dialog utility R is not necessary; it is interacted with other utilities inside the attention computation, contributing to the final prediction. The two d-vectors c V and c Q are concatenated as [c V , c Q ] , and this is projected to d-dimensional space, yielding a context vector c ∈ R d .</p><p>We design the discriminative and generative decoders following the previous studies. Receiving c and the candidate answers, the two decoders compute the score of each candidate answer in different ways. See details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multi-Task Learning</head><p>We observe in our experiments that accuracy is improved by training the entire network using the two decoders simultaneously. This is simply done by minimizing the sum of the losses, L D for the discriminative one and L G for the generative one (we do not use weights on the losses):</p><formula xml:id="formula_15">L = L D + L G .<label>(12)</label></formula><p>The increase in performance may be attributable to the synergy of learning two tasks while sharing the same encoder. Details will be given in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Dataset We use the VisDial v1.0 dataset in our experiments which consists of the train 1.0 split (123,287 images), the val 1.0 split (2,064 images), and test v1.0 split (8,000 images). Each image has a dialog composed of 10 questionanswer pairs along with a caption. For each question-answer pair, 100 candidate answers are given. The val v1.0 split and 2,000 images of the train v1.0 split are provided with dense annotations (i.e., relevance scores) for all candidate answers.</p><p>Although the test v1.0 split was also densely annotated, the information about the ground truth answers and the dense annotations are not publicly available. Additionally, we evaulate the method on the Audio Visual Scene-aware Dialog Dataset <ref type="bibr" target="#b15">[16]</ref>; the results are shown in the supplementary.</p><p>Evaluation metrics From the visual dialog challenge 2018, normalized discounted cumulative gain (NDCG) has been used as the principal metric to evaluate methods on the VisDial v1.0 dataset. Unlike other classical retrieval metrics such as R@1, R@5, R@10, mean reciprocal rank (MRR), and mean rank, which are only based on a single ground truth answer, NDCG is computed based on the relevance scores of all candidate answers for each question, which can properly handle the case where each question has more than one correct answer, such as 'yes it is' and 'yes'; such cases do occur frequently.</p><p>Other configurations We employ the standard method used by many recent studies for the determination of hyperparameters etc. For the visual features, we detect K = 100 objects from each image. For the question and history features, we first build the vocabulary composed of 11,322 words that appear at least five times in the training split. The captions, questions, and answers are truncated or padded to 40, 20, and 20 words, respectively. Thus, N = 20 for the question utility Q. T for the history utilities varies depending on the number of dialogs. We use pre-trained 300-dimensional GloVe vectors [36] to initialize the embedding layer, which is shared for all the captions, questions, and answers. For the attention blocks, we set the dimension of the feature space to d = 512 and the number of heads H in each attention block to 4. We mainly use models having two stacks of the proposed attention block. We train our models on the VisDial v0.9 and VisDial v1.0 dataset using the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> with 5 epochs and 15 epochs respectively. The learning rate is warmed up from 1×10 −5 to 1 × 10 −3 in the first epoch, then halved every 2 epochs. The batch size is set to 32 for the both datasets.  <ref type="bibr" target="#b23">[24]</ref>, DAN <ref type="bibr" target="#b19">[20]</ref>, and ReDAN <ref type="bibr" target="#b12">[13]</ref>, all of which were trained without using external datasets or data imposition. Unless noted otherwise, the results of our models are obtained from the output of discriminative decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with</head><p>Results on the val v1.0 split We first compare single-model performance on the val v1.0 split. We select here MN, CoAtt, HCIAE, and ReDAN for comparison, as their performances from the both decoders in all metrics are available in the literature. To be specific, we use the accuracy values reported in <ref type="bibr" target="#b12">[13]</ref> for a fair comparison, in which these methods are reimplemented using the bottomup-attention features. Similar to ours, all these methods employ the standard design of discriminative and generative decoders as in <ref type="bibr" target="#b7">[8]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the results. It is seen that our method outperforms all the compared methods on the NDCG metric with large margins regardless of the decoder type. Specifically, as compared with ReDAN, the current state-of-the-art on the VisDial v1.0 dataset, our model has improved NDCG from 59.32 to 62.72 and from 60.47 to 63.58 with discriminative and generative decoders, respectively.</p><p>Results on the test-standard v1.0 split We next consider performance on the test-standard v1.0 split. In our experiments, we encountered a phenomenon that accuracy values measured by NDCG and other metrics show a trade-off relation (see the supplementary material for details), depending much on the choice of metrics (i.e., NDCG or others) for judging convergence at the training time. This is observed in the results reported in <ref type="bibr" target="#b12">[13]</ref> and is attributable to the inconsistency between the two types of metrics. Thus, we show two results here, the one obtained using NDCG for judging convergence and the one using MRR for it; the latter is equivalent to performing early stopping. <ref type="table" target="#tab_2">Table 2</ref>(a) shows single-model performances on the blind test-standard v1.0 split. With the outputs from the discriminative decoder, our model gains improvement of 3.33pp in NDCG from the best model. When employing the aforementioned early stopping, our model achieves at least comparable or better performance in other metrics as well. <ref type="table" target="#tab_2">Table 2</ref>: Comparison in terms of (a) single-and (b) ensemble-model performance on the blind test-standard v1.0 split of the VisDial v1.0 dataset and in terms of (c) the number of parameters of the attention mechanism. The result obtained by early stopping on MRR metric is denoted by and those with fine-tuning on dense annotations are denoted by †. a) Performance of single models Many previous studies report the performance of an ensemble of multiple models. To make a comparison, we create an ensemble of 16 models with some differences, from initialization with different random seeds to whether to use sharing weights across attention blocks or not, the number of attention blocks (i.e. L = 2, 3), and the number of objects in the image (i.e. K = 50, 100). Aiming at achieving the best performance, we also enrich the image features by incorporating the class label and attributes of each object in an image, which are also obtained from the pretrained Faster-RCNN model. Details are given in the supplementary material. We take the average of the outputs (probability distributions) from the discriminative decoders of these models to rank the candidate answers. Furthermore, we also test fine-tuning each model with its discriminative decoder on the available dense annotations from the train v1.0 and val v1.0, where the cross-entropy loss with soft labels (i.e. relevance scores) is minimized for two epochs. With optional fine-tuning, our ensemble model further gains a large improvement in NDCG, resulting in the third place in the leaderboard. The gap in NDCG to the first place (VD-BERT) is only 0.25pp, while our model yields performance that is better in all the other metrics, i.e, by 2.14pp, 5.67pp, and 3.37pp in MRR, R@5, and R@10, respectively, and 5.36% reduction in Mean.</p><formula xml:id="formula_16">Model NDCG ↑ MRR ↑ R@1 ↑ R@5 ↑ R@10 ↑ Mean</formula><p>Table 2(c) shows the number of parameters of the multi-modal attention mechanism employed in the recent methods along with their NDCG scores on the VisDial v1.0 test-standard set. We exclude the parameters of the networks computing the input utilities and the decoders, as they are basically shared among these methods. 'Naive Transformer' consists of two stacks of transformer blocks with simple extension to three utilities as mentioned in Sec. 1. The efficiency of our models can be observed. Note also that the gap between (Q, V) and (Q, V, R) is small, contrary to the argument in [37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>To evaluate the effect of each of the components of our method, we perform the ablation study on the val v1.0 split of VisDial dataset. We evaluate here the accuracy of the discriminative decoder and the generative decoder separately. We denote the former by D-NDCG and the latter by G-NDCG, and the accuracy of their averaged model by A-NDCG (i.e., averaging the probability distributions over the candidate answers obtained by the discriminative and generative decoders). The results are shown in <ref type="table" target="#tab_3">Table 3</ref>(a-b). The first block of <ref type="table" target="#tab_3">Table 3</ref>(a) shows the effect of the number of stacks of the proposed attention blocks. We observe that the use of two to three stacks achieves good performance on all three measures. More stacks did not bring further improvement, and thus are omitted in the table.</p><p>The second block of <ref type="table" target="#tab_3">Table 3</ref>(a) shows the effect of self-attention, which computes the interaction within a utility, i.e.,Ā X (X). We examine this because it can be removed from the attention computation. It is seen that self-attention does contribute to good performance. The third block shows the effects of how to aggregate the attended features. It is seen that their concatenation yields better performance than their simple addition. The fourth block shows the impact of sharing the weights across the stacks of the attention blocks. If the weights can be shared as in <ref type="bibr" target="#b26">[27]</ref>, it contributes a further decrease in the number of parameters. We observe that the performance does drop if weight sharing is employed, but the drop is not very large.</p><p>The first block of  image is important for achieving the best performance. The second block of <ref type="table" target="#tab_3">Table 3</ref>(b) shows the effects of simultaneously training the both decoders (with the entire model). It is seen that this contributes greatly to the performance; this indicates the synergy of learning two tasks while sharing the encoder, resulting better generalization as compared with those trained with a single decoder.</p><p>We have also confirmed that the use of fewer objects leads to worse results. Besides, the positional embedding for representing the question and history utilities as well as the spatial embedding (i.e., the bounding box geometry of objects) for image utility representation have a certain amount of contribution. <ref type="figure" target="#fig_2">Figure 4</ref> shows attention weights generated in our model on two rounds of Q&amp;A on two images. We show here two types of attention. One is the self-attention weights used to compute the context vectors c V and c Q . For c V , the attention weights a V are generated over image regions (i.e., bounding boxes), as in Eq. <ref type="bibr" target="#b9">(10)</ref>. Similarly, for c Q , the attention weights are generated over question words. These two sets of attention weights are displayed by brightness of the image bounding-boxes and darkness of question words, respectively, in the center and the rightmost columns. It can be observed from these that the relevant regions and words are properly highlighted at each Q&amp;A round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualization of Generated Attention</head><p>The other attention we visualize is the source-to-target attention computed inside the proposed block. We choose here the image-to-question attentionĀ V (Q) and the history-to-question attentionĀ R (Q). For each, we compute the average of the attention weights over all the heads computed inside the block belonging to the upper stack. In <ref type="figure" target="#fig_2">Fig. 4</ref>, the former is displayed by the red boxes connected between an image region and a question word; only the region with the largest weight is shown for the target word; the word with the largest self-attention weight is chosen for the target. The history-to-question attention is displayed by the Q&amp;As highlighted in blue color connected to a selected question word that is semantically ambiguous, e.g., 'its', 'he', and 'his'. It is seen that the model performs proper visual grounding for the important words, 'hair', 'shorts', and 'tusks'. It is also observed that the model properly resolves the co-reference for the words, 'he' and 'its'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Conclusion</head><p>In this paper, we have proposed LTMI (Light-weight Transformer for Many Inputs) that can deal with all the interactions between multiple input utilities in an efficient way. As compared with other methods, the proposed architecture is much simpler in terms of the number of parameters as well as the way of handling inputs (i.e., their equal treatment), and nevertheless surpasses the previous methods in accuracy; it achieves the new state-of-the-art results on the VisDial datasets, e.g., high NDCG scores on the VisDial v1.0 dataset. Thus, we believe our method can be used as a simple yet strong baseline.</p><p>28. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019) 29. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Representations of Utilities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Image Utility</head><p>The image utility is represented by the standard method employed in many recent studies. It is based on the bottom-up mechanism <ref type="bibr" target="#b0">[1]</ref>, which extracts regionlevel image features using the Faster-RCNN pre-trained on the Visual Genome dataset <ref type="bibr" target="#b25">[26]</ref>. For each input image, we select the top K objects, and represent each of them by a visual feature v r i ∈ R 2048 and a bounding box expressed by (x i,1 , x i,2 ) and (x i,3 , x i,4 ) (the coordinates of the upper-left and lower-right corners.)</p><p>The feature vector v r i is then converted into another vector v f i ∈ R d as follows. We introduce the following notation to express a single FC layer with ReLU, to which dropout regularization is applied:</p><formula xml:id="formula_17">MLP k→d (x) ≡ Dropout(ReLU(W x + b)),<label>(13)</label></formula><p>where x ∈ R k is the input and W ∈ R k×d and b ∈ R d are the weights and biases.</p><p>Then</p><formula xml:id="formula_18">, v f i is obtained by v f i = LayerNorm( MLP 2048→d (v r i )),<label>(14)</label></formula><p>where LayerNorm is the layer normalization <ref type="bibr" target="#b2">[3]</ref> applied to the output. The bounding box geometry is converted into v b i ∈ R d in the following way. First, the image is resized to 600 × 600 pixels and the bounding box geometry is transformed accordingly. Then, representing each of the four coordinates by a one-hot vector of size 600, we convert them into the embedding vectorŝ x i,1 , . . . ,x i,4 (∈ R d ) using four different embedding layers. Then, we obtain v b i as below</p><formula xml:id="formula_19">v b i = 4 j=1</formula><p>LayerNorm(MLP d→d (x i,j )).</p><p>Finally, v f i encoding the visual feature and v b i encoding the spatial feature are aggregated by adding and normalizing as</p><formula xml:id="formula_21">v i = LayerNorm(v f i + v b i ).<label>(16)</label></formula><p>The resulting v i 's for the K objects (i = 1, . . . , K) comprise a matrix V = [v 1 , v 2 , · · · , v K ] ∈ R K×d , which gives the representation of the visual utility.</p><p>Optional Image Feature Enrichment. In the experiment of comparing ensembles on the test split of Visdial v1.0, we enrich the image features for further improvement. To be specific, for each object, we also obtain a class label with highest probability (e.g. 'cat', 'hair', and 'car') and the top 20 attributes for each class label (e.g., 'curly', 'blond', 'long', and so on, for the label 'hair'). These can be extracted from the Faster-RCNN along with the above CNN features and bounding box geometry. We incorporate these into the image utility representation in the following way. The class label for the i-th object is first encoded into an embedding vector e c i ∈ R 300 using the same embedding layer as the question. Then, we convert e c</p><formula xml:id="formula_22">i into a d-dimensional vector v c i by v c i = LayerNorm(MLP 300→d (e c i )).<label>(17)</label></formula><p>Similarly, for the top 20 attributes of each object i, we encode them into embedding vectors of size 300, i.e. e a i,1 , . . . , e a i,20 , and then convert them further into v a</p><formula xml:id="formula_23">i ∈ R d as v a i = 20 j=1 LayerNorm(MLP 300→d (e a i,j )w a i,j ,<label>(18)</label></formula><p>where w a i,j is the confidence score extracted from the Faster-RCNN for attribute j of the i-th object. Then, the visual feature v f i , the spatial feature v b i , the class feature v c i , and the attribute feature v a i are aggregated by their addition followed by normalization as</p><formula xml:id="formula_24">v i = LayerNorm(v f i + v b i + v c i + v a i ).<label>(19)</label></formula><p>We then use these vectors to form the matrix V instead of Eq.(16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Question Utility</head><p>The question utility is also obtained by the standard method but with one exception, the employment of positional embedding used in NLP studies. Note that we examine its effects in an ablation test shown in the main paper. A given question sentence is first fit into a sequence of N words; zero-padding is applied if necessary. Each word w i (i = 1, . . . , N ) is embedded into a vector e i of a fixed size using an embedding layer initialized with pretrained GloVe vectors <ref type="bibr">[36]</ref>. They are then inputted into two-layer Bi-LSTM, obtaining two d-dimensional vectors − → h i and ← − h i as their higher-layer hidden state:</p><formula xml:id="formula_25">− → h i = LSTM(e i , − − → h i−1 ), ← − h i = LSTM(e i , ← − − h i+1 ).<label>(20)</label></formula><p>Their concatenation,</p><formula xml:id="formula_26">h i = [ − → h i , ← − h i ]</formula><p>, is then projected back to a d-dimensional space using a linear transformation, yielding a vector q f i . Positional embedding q p i from the paper [44] is added to get the final representation q i ∈ R d of w i as</p><formula xml:id="formula_27">q i = LayerNorm(q f i + q p i ).<label>(21)</label></formula><p>The representation of the question utility is given as Q = [q 1 , . . . , q N ] ∈ R N ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Dialog History Utility</head><p>In this study, we choose to represent the dialog history as a single utility. Each of its entities represents the question-answer pair at one round. As with previous studies, the caption is treated as the first round of 2N -word which is padded or truncated if necessary. For each round t &gt; 1, the word sequences of the question and the answer at the round is concatenated into 2N -word sequence with zero padding if necessary. As with the question utility, after embedding each word into a GloVe vector, the resulting sequence of 2N embedded vectors is inputted to two-layer Bi-LSTM, from which only their last (higher-layer) hidden states are extracted to construct 2d-dimensional vector</p><formula xml:id="formula_28">[ − → h 0 , ← − − h 2N ] .</formula><p>We then project it with a linear transform to a d-dimensional space, yielding r f t ∈ R d . For the linear projection, we use different learnable weights from the question utility. As in Eq.(21), we add positional embedding, which represents the order of rounds, and then apply layer normalization, yielding a feature vector of the round t questionanswer pair. The history utility is then given by R = [r 1 , . . . , r T ] ∈ R T ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Design of Decoders</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Discriminative Decoder</head><p>A discriminative decoder outputs the likelihood score for each of 100 candidate answers for the current question at round T in the following way. We use a similar architecture to the one used to extract question features in Sec. A.2 to convert each candidate answer (indexed by i(= 1, . . . , 100)) to a feature vector a i ∈ R d . Specifically, it is two-layer Bi-LSTM receiving a candidate answer at its input, on top of which there is a linear projection layer followed by layer normalization. Using the resulting vectors, the score p i for i-th candidate answer is computed by p i = logsoftmax i (a 1 c, . . . , a 100 c).</p><p>In the test phase, we sort the candidate answers using these scores. In the training phase, the cross-entropy loss L D between p = [p 1 , . . . , p 100 ] and the ground truth label encoded by a one-hot vector y is minimized:</p><formula xml:id="formula_30">L D = − 100 i=1 y i p i .<label>(23)</label></formula><p>When relevance scores s = [s 1 , . . . , s 100 ] over the answer candidates are available (called dense annotation in the VisDial dataset) rather than a single ground truth answer, we can use them by setting y i = s i for all i's and minimize the above loss. We employ dropout with rate of 0.1 for the LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Generative Decoder</head><p>Following <ref type="bibr" target="#b7">[8]</ref>, we also consider a generative decoder to score the candidate answers using the log-likelihood scores. The generative decoder consists of a twolayer LSTM to generate an answer using the context vector c as the initial hidden state. In the training phase, we predict the next token based on the current token from the ground truth answer. In details, we first append the special token "SOS" at the beginning of the ground truth answer, then embedding all the sentence into the embedding vectors a gt = [w 0 , w 1 , . . . , w N ] where w 0 is the embedding vector of "SOS" token. The hidden state h n ∈ R d at the n-th timestep (extracted from the higher-layer LSTM) is computed given w n−1 and h n−1 as follows:</p><formula xml:id="formula_31">h n = LSTM(w n−1 , h n−1 ),<label>(24)</label></formula><p>where h 0 is initialized by c. Thus, we compute p n , the log-likelihood of n-th word as</p><formula xml:id="formula_32">p n = logsoftmax j (W n h n + b n ),<label>(25)</label></formula><p>where W n ∈ R d×|V | and p n ∈ R |V | , where |V | is the vocabulary size; and j is the index of n-th word in the vocabulary.</p><p>In the training phase, we minimize L G , the summation of the negative loglikelihood defined by</p><formula xml:id="formula_33">L G = − N n=1 p n .<label>(26)</label></formula><p>In the validation and test phase, for each candidate answer A T,i , we compute</p><formula xml:id="formula_34">s i = N n=1 p (A T ,i ) n where p (A T ,i ) n</formula><p>is the log-likelihood of the n-th word in the candidate answer A T,i which is computed similarly as in Eq. <ref type="bibr" target="#b24">(25)</ref>. Then, the rankings of the candidate answers are derived as softmax i (s 1 , . . . , s 100 ). We employ dropout with rate of 0.1 for the LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>When computingĀ Y (X), we perform the following form of computation</p><formula xml:id="formula_35">A(Q, K, V ) = softmax QK √ d V,</formula><p>where we compute a matrix product QK as above. In the computation of A X (Y ), we need another matrix product, but it is merely the transposed matrix KQ due to the symmetry between X and Y . For the computational efficiency, we perform computation ofĀ Y (X) andĀ X (Y ) simultaneously; see MultiHeadAttention(X, Y ) in our code. Further, following [33], we also pad X and Y with two d-dimensional vectors that are randomly initialized with He normal initialization. This implements "no-where-to-attend" features in the computation ofĀ Y (X) andĀ X (Y ). <ref type="table" target="#tab_5">Table 4</ref> shows the hyperparameters used in our experiments, which are selected following the previous studies. We perform all the experiments on a GPU server that has four Tesla V100-SXM2 of 16GB memory with CUDA version 10.0 and Driver version 410.104. It has Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz of 80 cores with the RAM of 376GB memory. We use Pytorch version 1. <ref type="bibr">2 [35]</ref> as the deep learning framework. As mentioned in the main paper, we observed the inconsistency in the performance of models evaluated by NDCG and other metrics, such as MRR. The same is also reported in recent studies. We show an analysis on this. We first recap how the Visdial v1.0 dataset was collected <ref type="bibr" target="#b7">[8]</ref>. A live chat between two workers, i.e., a questioner and an answerer, was conducted on Amazon Mechanical Turk (AMT). For an image provided with a caption, the questioner raised a question based on the caption without seeing the image. The answerer responded to the question by looking at the image, which are used as ground truth answers.</p><p>To cope with the difficulty of evaluating answers generated by a model in the form of free texts, Das et al. <ref type="bibr" target="#b7">[8]</ref> proposed a method that discriminatively evaluates the performance of visual dialog systems by using a set of 100 candidate answers, to each of which a relevance score is given. It makes a system under evaluation return the rankings of all the candidate answers and then calculates the scores of metrics, e.g. NDCG, MRR, etc. based on the returned rankings. To create a set of 100 candidate answers for each question, they collected from all the answers given by the answerers, the plausible answers of the 50 most similar questions to the ground truth answer including itself, the 30 most popular answers, and 20 random answers. Each of these candidate answers was then given a relevance score with a consensus of several AMT workers. Now we make a few observations on the dataset. First, the ground truth answers provided by the answerers are not always high-quality. As shown in <ref type="table" target="#tab_7">Table 5</ref>, 33.6% of the ground truth answers have relevance scores lower than 0.5. Assuming the AMT workers giving the relevance scores to be accurate, this implies some of the "ground truth" answers provided by the answerers are simply wrong. Second, the answerers tend to more frequently use short, general answers, such as 'no' and 'yes'. This is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref> that shows the frequencies of the most popular ones in the ground truth answers and also those having  non-zero relevance scores. It is clearly seen that the short and less informative answers (i.e., 'no' and 'yes') are less frequently considered to be relevant.</p><p>Recall that NDCG metric is measured based on the rankings of all the candidate answers, whereas MRR and other metrics are based on the ranking of the ground truth answers. Based on the above observations, we can say that the NDCG is more appropriate as an evaluation metric, following the other recent studies. This claim is also supported by <ref type="table" target="#tab_7">Table 5</ref>, the results of the experiments examining how evaluated performances vary depending on when to stop the training of the proposed model. It is seen from the table that the model at epoch 5, which is noted as 'MRR-favored' as it corresponds to early stopping based on validation on MRR, yields high MRR and low NDCG scores over all questions. It tends to give higher scores on the safe and popular answers that appear more frequently in the ground truth answers. When we continue to train the model until 12 epochs, it (noted as NDCG-favored) generates better rankings for all possible answers rather than only the ground truth answers, yielding large improvements in NDCG scores. However, it yields lower MRR scores, since the model does not give high ranks to some of the "ground truth" answers; they are indeed very likely to be bad answers. It is also seen from the table that the both models yield better scores on the both MRR and NDCG metrics for the questions having the ground truth answers with high relevance scores.   Following the previous studies, we report the performance of our method (specifically, the discriminative decoder) on the VisDial v0.9 dataset. The v0.9 dataset consists of the train v0.9 split (82,783 images) and the val v0.9 split (40,504 images). Note that all the hyperparameter settings are the same as those on the Visdial v1.0 dataset except that we train the model with only five epochs. <ref type="table" target="#tab_9">Table 7</ref> shows the results on the validation set along with performances of other methods. It shows that our model consistently outperforms all the methods across all metrics: MRR, R@1, R@5, R@10 and Mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Qualitative Results</head><p>We provide additional examples of the results obtained by our method in Figs. 6-??. They are divided into two groups, results for which the top-1 prediction coincides with the ground truth answer (Figs. <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> and those for which they do not coincide ( <ref type="figure">Figs. 9-10</ref>). For each result, we show the attention maps created on the input image and question, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Experiments on AVSD</head><p>To test the generality of the proposed method on other tasks as well as its performance on a greater number of utilities, we additionally apply it to the Audio Visual Scene-aware Dialog (AVSD) task <ref type="bibr" target="#b15">[16]</ref>. This task requires a system to generate an answer to a question about events seen in a video given with a previous dialog. AVSD provides more utilities than Visual Dialog, i.e., audio features and video features, such as VGG or I3D features (I3D RGB sequence and I3D flow sequence). We build a network by simply replacing the multimodal attention mechanism in the baseline model of <ref type="bibr" target="#b15">[16]</ref> with a simple extension of the proposed attention mechanism. Details are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Network Design</head><p>Following the baselines <ref type="bibr" target="#b15">[16]</ref>, we extract the question utility Q using a two-layer LSTM. We separate the caption from the dialog history and feed it into another two-layer LSTM to obtain the caption utility C. Similar to <ref type="bibr" target="#b15">[16]</ref>, the dialog history consisting of previous question-answer pairs is inputted into a hierarchical LSTM network; specifically, we encode each question-answer pair with one LSTM and summarize the obtained encodings with another LSTM, yielding a final vector representation c r . All LSTMs used for language encoding have d units. We convert words into vectors with a shared embedding layer initialized with GLoVe vectors. The video provides two sources of features, i.e., video features and audio features. We use the audio features extracted from the pretrained VGGish model <ref type="bibr" target="#b15">[16]</ref>, which are fed to a projection layer, providing the audio utility A; it is represented as a collection of d-dimensional vectors. For video processing, following <ref type="bibr" target="#b15">[16]</ref>, we consider two models with different features: i) VGG features extracted from four uniformly sampled frames in the video, giving the video utility V , and ii) I3D features extracted by the I3D network pretrained on an action recognition task, which are forwarded to projection layers to obtain an I3D-rgb utility and an I3D-flow utility denoted by V and F .</p><p>To compute the multimodal attention between U utilities, we add a stack of U proposed attention blocks; U = 4 for the model (i) and U = 5 for (ii). To make the designs of two models (i) and (ii) similar, we use only A utility to attend language utilities; and only Q and C are allowed to attend audio and video utilities. After obtaining the updated representations of all utilities, we summarize each utility into a single vector by the self-attention mechanism, in which the summarized vector of question utility is denoted by c q . We concatenate all these vectors together with c r , projecting it into a d-dimensional vector of context representation c.</p><p>The decoder architecture is similar to the generative decoder described in Sec. B.2 except that the input of the decoder at the i-th step is the concatenation of w i−1 , c q , and c r . At the time of inference, we use the beam search technique to efficiently find the most likely hypothesis generated by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Experimental Setup</head><p>Following <ref type="bibr" target="#b15">[16]</ref>, we perform the experiment on the AVSD prototype which is split into training, validation, and test sets with 6172, 732, and 733 videos, respectively. Each video is collected from the Charades dataset, annotated with a caption and 10 dialog rounds. The hidden size d is set to 512; the GLoVe vectors are 300-dimensional. We train the models in 15 epochs using the Adam optimizer with initial learning rate 1 × 10 −3 in all the experiments. The dropout with rate of 0.2 is applied for the LSTMs. <ref type="table" target="#tab_10">Table 8</ref> shows the results, which include evaluation on a number of metrics to measure the quality of generated answers, i.e. CIDEr, BLEU, METEOR, ROUGE L. It is seen that our models outperform the baselines presented in <ref type="bibr" target="#b15">[16]</ref> over all the metrics; specifically, it improves the CIDEr score by 22.3% (from 0.618 to 0.841) with VGG features and by 12.4% (from 0.727 to 0.851) with I3D features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Experimental Results</head><p>Q&amp;A at a round Q&amp;A at another round <ref type="figure">Fig. 6</ref>: Examples of results for which the top-1 prediction is the same as the ground truth answer on the validation split of Visdial v1.0. Each row shows selected two rounds of Q&amp;A for one image.</p><p>Q&amp;A at a round Q&amp;A at another round <ref type="figure">Fig. 7</ref>: Examples of results for which the top-1 prediction is the same as the ground truth answer on the validation split of Visdial v1.0. Each row shows selected two rounds of Q&amp;A for one image.</p><p>Q&amp;A at a round Q&amp;A at another round <ref type="figure">Fig. 8</ref>: Examples of results for which the top-1 prediction is the same as the ground truth answer on the validation split of Visdial v1.0. Each row shows selected two rounds of Q&amp;A for one image. <ref type="figure">Fig. 9</ref>: Examples of results for which the top-1 prediction is different from the ground truth answer on the validation split of Visdial v1.0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Simplified symbol of the proposed block shown inFig. 1(b). (b) Its application to Visual Dialog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The entire network built upon the proposed LTMI for Visual Dialog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Examples of visualization for the attention weights generated in our model at two Q&amp;A rounds on two images. See Sec. 5.4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, J., Kannan, A., Yang, J., Parikh, D., Batra, D.: Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model. In: Advances in Neural Information Processing Systems. pp. 314-324 (2017) 31. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention for visual question answering. In: Advances in Neural Information Processing Systems. pp. 289-297 (2016) 32. Murahari, V., Batra, D., Parikh, D., Das, A.: Large-scale pretraining for visual dialog: A simple state-of-the-art baseline. arXiv preprint arXiv:1912.02379 (2019) 33. Nguyen, D.K., Okatani, T.: Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6087-6096 (2018) 34. Niu, Y., Zhang, H., Zhang, M., Zhang, J., Lu, Z., Wen, J.R.: Recursive visual attention in visual dialog. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6679-6688 (2019) 35. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch (2017) 36. Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word representation. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 1532-1543 (2014) 37. Qi, J., Niu, Y., Huang, J., Zhang, H.: Two causal principles for improving visual dialog. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10860-10869 (2020) 38. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing Systems. pp. 91-99 (2015) 39. Schwartz, I., Yu, S., Hazan, T., Schwing, A.G.: Factor graph attention. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2039-2048 (2019) 40. Seo, P.H., Lehrmann, A., Han, B., Sigal, L.: Visual reference resolution using attention memory for visual dialog. In: Advances in Neural Information Processing Systems. pp. 3719-3729 (2017) 41. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 2556-2565 (2018) 42. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: Advances in Neural Information Processing Systems. pp. 3104-3112 (2014) 43. Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from transformers. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (2019) 44. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems. pp. 5998-6008 (2017) 45. Wang, Y., Joty, S., Lyu, M.R., King, I., Xiong, C., Hoi, S.C.: Vd-bert: A unified vision and dialog transformer with bert. arXiv preprint arXiv:2004.13278 (2020) 46. Wu, Q., Wang, P., Shen, C., Reid, I., van den Hengel, A.: Are you talking to me? reasoned visual dialog generation through adversarial learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6106-6115 (2018) 47. Yang, T., Zha, Z.J., Zhang, H.: Making history matter: History-advantage sequence training for visual dialog. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2561-2569 (2019) 48. Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: Stacked attention networks for image question answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 21-29 (2016) 49. Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.: Mattnet: Modular attention network for referring expression comprehension. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1307-1315 (2018) 50. Yu, Z., Yu, J., Cui, Y., Tao, D., Tian, Q.: Deep modular co-attention networks for visual question answering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6281-6290 (2019) 51. Yu, Z., Yu, J., Fan, J., Tao, D.: Multi-modal factorized bilinear pooling with coattention learning for visual question answering. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1821-1830 (2017) 52. Yu, Z., Yu, J., Xiang, C., Fan, J., Tao, D.: Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering. IEEE Transactions on Neural Networks and Learning Systems 29(12), 5947-5959 (2018) 53. Zhang, H., Ghosh, S., Heck, L., Walsh, S., Zhang, J., Zhang, J., Kuo, C.C.J.: Generative visual dialogue system via weighted likelihood estimation. In: Proceedings of the International Joint Conference on Artificial Intelligence. pp. 1025-1031 (2019) 54. Zheng, Z., Wang, W., Qi, S., Zhu, S.C.: Reasoning visual dialogs with structural and partial observations. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6669-6678 (2019) 55. Zhuang, B., Wu, Q., Shen, C., Reid, I., van den Hengel, A.: Parallel attention: A unified framework for visual object discovery through dialogs and queries. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4252-4261 (2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The distribution of the 30 most popular answers in the Validation v1.0 set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>State-of-the-art Methods Compared methods We compare our method with previously published methods on the VisDial v0.9 and VisDial v1.0 datasets, including LF, HRE, MN [8], LF-Att, MN-Att (with attention) [8], SAN [48], AMEM [40], SF [19], HCIAE Comparison of the performances of different methods on the validation set of VisDial v1.0 with discriminative and generative decoders. ] 55.13 60.42 46.09 78.14 88.05 4.63 56.99 47.83 38.01 57.49 64.08 18.76 CoAtt [46] 57.72 62.91 48.86 80.41 89.83 4.21 59.24 49.64 40.09 59.37 65.92 17.86 HCIAE [30] 57.75 62.96 48.94 80.5 89.66 4.24 59.70 49.07 39.72 58.23 64.73 18.43 ReDAN [13] 59.32 64.21 50.6 81.39 90.26 4.05 60.47 50.02 40.27 59.93 66.78 17.4 LTMI 62.72 62.32 48.94 78.65 87.88 4.86 63.58 50.74 40.44 61.61 69.71 14.93</figDesc><table><row><cell>Model</cell><cell>Discriminative</cell><cell>Generative</cell></row><row><cell></cell><cell cols="2">NDCG↑ MRR↑ R@1↑ R@5↑ R@10↑ Mean↓ NDCG↑ MRR↑ R@1↑ R@5↑ R@10↑ Mean↓</cell></row><row><cell cols="3">MN [8[30] and Sequential CoAttention model (CoAtt) [46], Synergistic [15], FGA [39],</cell></row><row><cell cols="2">GNN [54], RvA [34], CorefNMN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>(b) shows the results. It is observed that our ensemble model (w/o the fine-tuning) achieves the best NDCG = 66.53 in all the ensemble models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the components of our method on the val v1.0 split of VisDial dataset. ↑ indicates the higher the better.</figDesc><table><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>Component</cell><cell cols="4">Details A-NDCG ↑ D-NDCG ↑ G-NDCG ↑</cell><cell>Component</cell><cell cols="4">Details A-NDCG ↑ D-NDCG ↑ G-NDCG ↑</cell></row><row><cell>Number of</cell><cell>1</cell><cell>65.37</cell><cell>62.06</cell><cell>62.95</cell><cell>Context feature</cell><cell>[Q]</cell><cell>65.12</cell><cell>61.50</cell><cell>63.19</cell></row><row><cell>attention blocks</cell><cell>2</cell><cell>65.75</cell><cell>62.72</cell><cell>63.58</cell><cell>aggregation</cell><cell>[Q, V]</cell><cell>65.75</cell><cell>62.72</cell><cell>63.58</cell></row><row><cell></cell><cell>3</cell><cell>65.42</cell><cell>62.48</cell><cell>63.22</cell><cell></cell><cell>[Q, V, R]</cell><cell>65.53</cell><cell>62.37</cell><cell>63.38</cell></row><row><cell>Self-Attention</cell><cell>No</cell><cell>65.38</cell><cell>61.76</cell><cell>63.31</cell><cell>Decoder Type</cell><cell>Gen</cell><cell>-</cell><cell>-</cell><cell>62.35</cell></row><row><cell></cell><cell>Yes</cell><cell>65.75</cell><cell>62.72</cell><cell>63.58</cell><cell></cell><cell>Disc</cell><cell>-</cell><cell>61.80</cell><cell>-</cell></row><row><cell cols="2">Attended features Add</cell><cell>64.12</cell><cell>60.28</cell><cell>61.49</cell><cell></cell><cell>Both</cell><cell>65.75</cell><cell>62.72</cell><cell>63.58</cell></row><row><cell>aggregation</cell><cell>Concat</cell><cell>65.75</cell><cell>62.72</cell><cell>63.58</cell><cell>The number of</cell><cell>36</cell><cell>65.25</cell><cell>62.40</cell><cell>63.08</cell></row><row><cell>Shared Attention weights</cell><cell>No Yes</cell><cell>65.75 65.57</cell><cell>62.72 62.50</cell><cell>63.58 63.24</cell><cell>objects in an image</cell><cell>50 100</cell><cell>65.24 65.75</cell><cell>62.29 62.72</cell><cell>63.12 63.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Positional and</cell><cell>No</cell><cell>65.18</cell><cell>61.84</cell><cell>62.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>spatial embeddings</cell><cell>Yes</cell><cell>65.75</cell><cell>62.72</cell><cell>63.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>(b)  shows the effect of how to aggregate the context features c V , c Q , and c R in the decoder(s), which are obtained from the outputs of our encoder. As mentioned above, the context vector c R of the dialog history does not contribute to the performance. However, the context vector c v of the The young boy is playing tennis at the court Is the young boy a toddler? No</figDesc><table><row><cell>tennis at the court H0: The young boy is playing Dialog history</cell><cell>What</cell><cell>Is the young boy a toddler? No The young boy is playing tennis at the court</cell><cell>Is he</cell><cell></cell></row><row><cell>H1: Is the young boy a toddler? No</cell><cell>is</cell><cell></cell><cell>wearing</cell><cell>What color is his hair? It 's black</cell></row><row><cell>Q2: What color is his hair? (It's black) Q3: Is he wearing shorts? (Yes)</cell><cell>his hair ?</cell><cell>Q: What color is his hair? GT answer: It's black Prediction: Black</cell><cell>shorts ?</cell><cell>Q: Is he wearing shorts? GT answer: Yes Prediction: Yes</cell></row><row><cell>H2: Is the elephant an adult ? Yes H1: Is there people ? No greenery of the jungle H0: An elephant walks through the Dialog history</cell><cell>its Can see you</cell><cell>Is the elephant an adult ? Yes Is there people ? No An elephant walks through the greenery of the jungle</cell><cell>Does have he</cell><cell>Cans you see its tusks ? Yes An elephant walks through the greenery of the jungle Is there people ? No Is the elephant an adult ? Yes</cell></row><row><cell>Q3: Can you see its tusks (Yes) Q4: Does he have a saddle? (No)</cell><cell>tusks ?</cell><cell>Q: Can you see its tusks? GT answer: Yes Prediction: Yes</cell><cell>saddle ? a</cell><cell>Q: Does he have a saddle? GT answer: No Prediction: No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyperparamters used in the training procedure.</figDesc><table><row><cell></cell><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell></cell><cell>Warm-up learning rate</cell><cell>1e−5</cell></row><row><cell></cell><cell>Warm-up factor</cell><cell>0.2</cell></row><row><cell></cell><cell cols="2">Initial learning rate after the 1st epoch 1e−3</cell></row><row><cell></cell><cell>β1 in Adam</cell><cell>0.9</cell></row><row><cell></cell><cell>β2 in Adam</cell><cell>0.997</cell></row><row><cell></cell><cell>in Adam</cell><cell>1e−9</cell></row><row><cell></cell><cell>Weight decay</cell><cell>1e−5</cell></row><row><cell></cell><cell>Number of workers</cell><cell>8</cell></row><row><cell></cell><cell>Batch size</cell><cell>32</cell></row><row><cell cols="3">D Analysis on Visdial v1.0 Validation Split</cell></row><row><cell>D.1</cell><cell cols="2">Analyzing Inconsistency between NDCG and Other Metrics</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The performance of the training strategies, i.e. based on the MRR or NDCG early stopping, categorized by questions of corresponding relevance score of ground truth answers.</figDesc><table><row><cell>Rel Score</cell><cell>Percentage</cell><cell cols="2">MRR-favored</cell><cell cols="2">NDCG-favored</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell>NDCG</cell><cell>MRR</cell><cell>NDCG</cell></row><row><cell>0.0</cell><cell>9.0%</cell><cell>58.26</cell><cell>44.06</cell><cell>56.12</cell><cell>48.00</cell></row><row><cell>0.2</cell><cell>11.0%</cell><cell>58.26</cell><cell>44.06</cell><cell>57.70</cell><cell>54.46</cell></row><row><cell>0.4</cell><cell>13.6%</cell><cell>61.07</cell><cell>56.94</cell><cell>60.69</cell><cell>58.94</cell></row><row><cell>0.6</cell><cell>16.0%</cell><cell>65.38</cell><cell>59.40</cell><cell>62.35</cell><cell>62.37</cell></row><row><cell>0.8</cell><cell>19.2%</cell><cell>67.34</cell><cell>62.90</cell><cell>64.45</cell><cell>66.79</cell></row><row><cell>1.0</cell><cell>31.2%</cell><cell>67.48</cell><cell>65.13</cell><cell>65.37</cell><cell>69.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The performance comparison of discriminative and generative decoders evaluated on question types evaluated on the NDCG metric. , we perform a question-type analysis of the NDCG scores achieved by different decoders from the model mentioned in our main paper. The questions are classified into four categories: Yes/No, Number, Color, and Others. As shown inTable 6, the Yes/No questions account for the majority whereas there is only 3% of the Number questions. Therefore, the performance on the Yes/No questions translates into the overall performance of any models. Similar to ReDAN<ref type="bibr" target="#b12">[13]</ref>, the performance of ours on the Number questions is the lowest among the other question types, reflecting the hardness of the counting task. Another similarity observed on our models and ReDAN is that generative decoders show better performance on the Yes/No questions while discriminative decoders yield higher NDCG scores on the other questions. It is because generative decoders favor the short answers that are relevant more often in the Yes/No questions. It is also seen that our model consistently shows better performance over all question types, i.e. about 3pp on the Yes/No and Color questions, 5pp on the Number questions, and 6pp on the other questions.</figDesc><table><row><cell>Question Type</cell><cell cols="3">Yes/No Number Color Others</cell></row><row><cell>Percentage</cell><cell>75%</cell><cell>3%</cell><cell>11% 11%</cell></row><row><cell>Decoder</cell><cell>Model</cell><cell></cell><cell></cell></row><row><cell>Generative</cell><cell cols="3">ReDAN [13] 63.49 41.09 Ours 66.24 46.35 55.77 57.25 52.16 51.45</cell></row><row><cell>Discriminative</cell><cell cols="3">ReDAN [13] 60.89 44.47 Ours 64.08 49.86 60.95 58.16 58.13 52.68</cell></row><row><cell cols="2">D.2 Question-Type Analysis</cell><cell></cell><cell></cell></row><row><cell>Following [13]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Retrieval performance of compared methods and ours on the val v0.9 split reported with a single model.</figDesc><table><row><cell cols="2">Model MRR ↑ R@1 ↑ R@5 ↑ R@10 ↑ Mean ↓</cell></row><row><cell>SAN [48] 57.64 43.44 74.26 83.72</cell><cell>5.88</cell></row><row><cell>LF [8] 58.07 43.82 74.68 84.07</cell><cell>5.78</cell></row><row><cell>HRE [8] 58.46 44.67 74.5 84.22</cell><cell>5.72</cell></row><row><cell>HREA [8] 58.68 44.82 74.81 84.36</cell><cell>5.66</cell></row><row><cell>MN [8] 59.65 45.55 76.22 85.37</cell><cell>5.46</cell></row><row><cell>NMN [17] 61.60 48.28 77.54 86.75</cell><cell>4.98</cell></row><row><cell>HCIAE [30] 62.22 48.48 78.75 87.59</cell><cell>4.81</cell></row><row><cell>AMEM [40] 62.27 48.53 78.66 87.43</cell><cell>4.86</cell></row><row><cell>SF [19] 62.42 48.55 78.75 87.75</cell><cell>4.47</cell></row><row><cell>GNN [54] 62.85 48.95 79.65 88.36</cell><cell>4.57</cell></row><row><cell>CoAtt [46] 63.98 50.29 80.71 88.81</cell><cell>4.47</cell></row><row><cell>CoefNMN [24] 64.10 50.92 80.18 88.81</cell><cell>4.45</cell></row><row><cell>FGA [39] 65.25 51.43 82.08 89.56</cell><cell>4.35</cell></row><row><cell>RvA [34] 66.34 52.71 82.97 90.73</cell><cell>3.93</cell></row><row><cell>DAN [20] 66.38 53.33 82.42 90.38</cell><cell>4.04</cell></row><row><cell cols="2">Ours 67.94 55.05 83.98 91.58 3.69</cell></row></table><note>E Results on the Visdial v0.9 dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison of response generation evaluation results with objective measures.</figDesc><table><row><cell>Model</cell><cell cols="4">Video Feat. CIDEr BLEU1 BLEU2 BLEU3 BLEU4 METEOR ROUGE L</cell></row><row><cell>Baseline [16]</cell><cell>VGG</cell><cell>0.618 0.231 0.141 0.095 0.067</cell><cell>0.102</cell><cell>0.259</cell></row><row><cell>Ours</cell><cell>VGG</cell><cell>0.841 0.266 0.172 0.118 0.086</cell><cell>0.117</cell><cell>0.296</cell></row><row><cell>Baseline [16]</cell><cell>I3D</cell><cell>0.727 0.256 0.161 0.109 0.078</cell><cell>0.113</cell><cell>0.277</cell></row><row><cell>Ours</cell><cell>I3D</cell><cell>0.851 0.277 0.178 0.122 0.088</cell><cell>0.119</cell><cell>0.302</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">As we stated in Introduction, we use the term utility here to mean a collection of features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 10: Examples of results for which the top-1 prediction is different from the ground truth answer on the validation split of Visdial v1.0.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating visual conversational agents via cooperative human-ai games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Human Computation and Crowdsourcing</title>
		<meeting>AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abc-cnn: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05960</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2951" to="2960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5503" to="5512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7746" to="7755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-step reasoning via recurrent dual attention for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Association for Computational Linguistics</title>
		<meeting>the Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6463" to="6474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-question-answer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end audio visual scene-aware dialog using multimodal attention-based video features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2352" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two can play this game: visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual attention networks for visual reference resolution in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modality-balanced models for visual dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06354</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03166</idno>
		<title level="m">Clevr-dialog: A diagnostic dataset for multi-round reasoning in visual dialog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
