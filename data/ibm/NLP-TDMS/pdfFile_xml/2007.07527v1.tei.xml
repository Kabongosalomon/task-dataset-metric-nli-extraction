<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Parse Wireframes in Images of Man-Made Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
							<email>huangkun@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
							<email>wangyf@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
							<email>zzhou@ist.psu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Ding</surname></persName>
							<email>dingtj@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
							<email>yima@eecs.berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Parse Wireframes in Images of Man-Made Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a learning-based approach to the task of automatically extracting a "wireframe" representation for images of cluttered man-made environments. The wireframe (see <ref type="figure">Fig. 1</ref>) contains all salient straight lines and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved significantly better performance than stateof-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation. The data and source code are available at https: //github.com/huangkuns/wireframe.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>How to infer 3D geometric information of a scene from 2D images has been a fundamental problem in computer vision. Conventional approaches to build a 3D model typically rely on detecting, matching, and triangulating local image features (e.g. corners, edges, SIFT features, and patches). One great advantage of working with local features is that the system can be somewhat oblivious to the scene, as long as it contains sufficient distinguishable features. Meanwhile, modern applications of computer vision systems often require an autonomous agent (e.g., a car, a robot, or a UAV) to efficiently and effectively negotiate with a physical space in cluttered man-made (indoor or outdoor) environments. Such scenarios present significant challenges to the current local-feature based approaches: Man-made environments typically consist of large textureless surfaces (e.g. white walls or the ground); or they may be full of repetitive patterns hence local features are ambiguous to match; and the visual localization system is required to work robustly and accurately over extended routes and sometimes across very large baseline between views.</p><p>Nevertheless, the human vision system seems capable of effortlessly localizing or navigating among such environments arguably by exploiting larger-scale (global or semiglobal) structural features or regularities of the scene. For instance, many works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref> have demonstrated that prior knowledge about the scene such as a Manhattan world could significantly benefit the 3D reconstruction tasks. The Manhattan assumption can often be violated in cluttered man-made environments, but it is rather safe to assume that man-made environments are dominantly piecewise planar hence rich of visually salient lines (intersection of planes) and junctions (intersection of lines). Conceptually, such junctions or lines could just be a very small "subset" among the local corner features (or SIFTs) and edge features detected by conventional methods, but they already encode most information about larger-scale geometry of the scene. For simplicity, we refer to such a set of lines and their intersected junctions collectively as a "wireframe". <ref type="bibr" target="#b0">1</ref> The goal of this work is to study the feasibility of developing a vision system that could efficiently and effectively extract the wireframe of a man-made scene. Intuitively, we wish such a system could emulate the level of human perception of the scene geometry, even from a single image. To this end, we have collected a database of over 5,000 images of typical indoor and outdoor environments and asked human subjects to manually label out all line segments and junctions that they believe to be important for understanding shape of regular objects or global geometric layout of the scene. <ref type="bibr" target="#b1">2</ref>  <ref type="figure" target="#fig_0">Fig. 1</ref> (first row) shows some representative examples of labelled wireframes.</p><p>In the literature, several methods have been proposed to detect line segments <ref type="bibr" target="#b45">[46]</ref> or junctions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref> in the image, and to further reason about the 3D scene geometry using the detected features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52]</ref>. These methods typically take a bottom-up approach: First, line segments are detected in the image. Then, two or more segments are grouped to form candidate junctions. However, there are several inherent difficulties with this approach. First, by enumerating all pairs of line segments, a large number of intersections are created. But only a very small subset of them are true junctions in the image. To retrieve the true junctions, various heuristics as well as RANSAC-based verification techniques have been previously proposed. As result, such methods are often time consuming and even break down when the scene geometry and texture become complex. Second, detecting line segments itself is a difficult problem in computer vision. If one fails to detect certain line segments in the image, then it would be impossible for the method to find the associated junctions. Third, since all existing methods rely on low-level cues such as image gradients and edge features to detect line segments and junctions, they are generally unable to distinguish junctions and line segments that are of global geometric importance with those produced by local textures or irregular shapes.</p><p>In view of the fundamental difficulties of existing methods, we propose a complementary approach to wireframe (junctions and line segments) detection in this paper. Our method does not rely on grouping low-level features such as image gradients and edges. Instead, we directly learn detectors for junctions and lines of large spatial support from the above large-scale dataset of manually labeled junctions and lines. In particular, inspired by the recent success of convolutional neural networks in object detection, we design novel network architectures for junction and line detection, respectively. We then give a simple but effective method to <ref type="bibr" target="#b0">1</ref> In architecture design, a wireframe is often referred to a line drawing of a building or a scene on paper. Interpretation of such line drawings of 3D objects has a long history in computer vision dated back to the '70s and '80s <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26]</ref>. <ref type="bibr" target="#b1">2</ref> For simplicity, this work is limited to wireframes consisting of straight lines. But the idea and method obviously apply to wireframes with curves. establish incidence relationships among the detected junctions and lines and produce a complete wireframe for the scene. <ref type="figure" target="#fig_0">Fig. 1</ref> (second row) shows typical results of the proposed method. As one can see, our method is able to detect junctions formed by long line segments with weak gradient while significantly reducing the number of false detections. In addition, as the labelled junctions and line segments are primarily associated with salient, large-scale geometric structures of the scene, the resulting wireframe is geometrically more meaningful, emulating human perception of the scene's geometry. Contributions of this work include: (i) the establishment of a large dataset for learning-based wireframe detection of man-made environments, and (ii) the development of effective, end-to-end trainable CNNs for detecting geometrically informative junctions and line segments. Comparing with existing methods on junction and line segment detection, our learning-based method has achieved, both quantitatively and qualitatively, superior performances on both tasks, hence convincingly verified the feasibility of wireframe parsing. Furthermore, both junction and line detection achieves almost real-time performance at the testing time, thus is suitable for a wide range of real-world applications such as feature correspondence, 3D reconstruction, vision-based mapping, localization and navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Edge and line segment detection. Much work has been done to extract line segments from images. Existing methods are typically based on perceptual grouping of low-level cues (i.e., image gradients) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. A key challenge of these local approaches is the choice of some appropriate threshold to discriminate true line segments from false conjunctions. Another line of work extends Hough transform to line segment detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b50">51]</ref>. While Hough transform has the ability to accumulate information over the entire image to determine the presence of a line structure, identifying endpoints of the line segment in the image remains a challenge <ref type="bibr" target="#b0">[1]</ref>. Recently, machine learning based approaches have been shown to produce the state-of-the-art results in generating pixel-wise edge maps <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b27">28]</ref>. But these methods do not attempt to extract straight line segments from the image. Junction detection. Detecting and analyzing junctions in real-world images remains a challenging problem due to a large number of fragmented, spurious, and missing line segments. In the literature, there are typically two ways to tackle this problem. The first group of methods focuses on operators based on local image cues, such as the Harris corner detector <ref type="bibr" target="#b17">[18]</ref>. However, local junction detection is known to be difficult, even for humans <ref type="bibr" target="#b31">[32]</ref>. More recent methods detect junctions by first locating contours (in natural images) <ref type="bibr" target="#b24">[25]</ref> or straight line segments (in man-made environments) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b47">48]</ref> and then grouping them to <ref type="bibr">Figure 2</ref>. Example images of our wireframe dataset, which covers a wide range of man-made scenes with different viewpoints, lighting conditions, and styles. For each image, we show the manually labelled line segments (first row) and the ground truth junctions derived from the line segments (second row). form junctions. As we discussed before, such bottom-up methods are (i) sensitive to scene complexity, and (ii) vulnerable to imperfect line segment detection results. Line-and junction-based geometric reasoning. Knowledge about junctions and the associated line structures is known to benefit many real-world 3D vision tasks. From a single image, a series of recent work use these features to recover the 3D layout of the scene <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52]</ref>. Meanwhile, observing that the junctions impose incident constraints on the adjacent line segments, <ref type="bibr" target="#b19">[20]</ref> devises a method for 3D reconstruction of lines without explicitly matching them across views, whereas <ref type="bibr" target="#b46">[47]</ref> proposes a surface scaffold structure that consists of sets of connected edges to regularize stereo-based methods for building reconstruction. Furthermore, <ref type="bibr" target="#b9">[10]</ref> uses line segments and junctions to develop a robust and efficient method for two-view pose estimation, and <ref type="bibr" target="#b49">[50]</ref> systematically studies how knowledge about junctions can affect the complexity and number of solutions to the Perspective-n-Line (PnL) problem. Machine learning and geometry. There is a large body of work on machine learning based approach to inferring pixel-level geometric properties of the scene, such as the depth <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref>, and the surface normal <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. But few work has been done on detecting mid/high-level geometric primitives with supervised training data. Recently, <ref type="bibr" target="#b16">[17]</ref> proposes a method to recognize planes in a single image, <ref type="bibr" target="#b15">[16]</ref> uses SVM to classify indoor planes (e.g., walls and floors), and <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5]</ref> train fully convolutional networks (FCNs) to predict "informative" edges formed by the pairwise intersections of room faces. However, none of the work aims to detect highly compressive vectorized junctions or line segments in the image, let alone a complete wireframe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A New Dataset for Wireframe Detection</head><p>As part of our learning-based framework to wireframe detection, we have collected 5,462 images of man-made environments. Some examples are shown in <ref type="figure">Fig. 2</ref>. The scenes include both indoor environments such as bedroom, living room, and kitchen, and outdoor scenes, such as house and yard. For each image, we manually labelled all the line segments associated with the scene structures. Here, our focus is on the structural elements in the image, that is, elements (i.e., line segments) from which meaningful geometric information of the scene can be extracted. As a result, we do not label line segments that are associated with texture (e.g., curtains, tree leaves), irregular or curved objects (e.g., sofa, humans, plants), shadows etc.</p><p>With the labelled line segments, ground truth junction locations and their branches can be easily obtained from the intersection or incidence relationships among two or more line segments in the image <ref type="figure">(Fig. 2</ref>, second row). Note that, unlike previous works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, we do not restrict ourselves to Manhattan junctions, which are formed by line segments aligned with one of three principal and mutually orthogonal directions in the scene. In fact, many scenes in our dataset do not satisfy the Manhattan world assumption <ref type="bibr" target="#b3">[4]</ref>. For example, the scene depicted in the last column of <ref type="figure">Fig. 2</ref> has more than two horizontal directions.</p><p>In summary, our annotation in each image includes a set of junction points P = {p n } N n=1 and a set of line segments L = {l m } M m=1 . Each junction p is the intersection of several, say R, line segments, called its branches. The coordinates of p are denoted as x ∈ R 2 and its line branches are recorded by their angles {θ r } R r=1 . The number R is known as the order of the junction, and the typical "L", "Y ", and "X"-type junctions have orders R = 2, 3, and 4, respectively. Each line segment is represented by its two end points: l = (p 1 , p 2 ). Hence, the wireframe, denoted as W , records all incidence and intersection relationships between junctions in P and lines in L. It can be represented by an N × M matrix W whose nm-th entry is 1 if p n is on l m , and 0 otherwise. Notice that two line segments are intersected at some junction if and only if the corresponding entry in W T W is nonzero; and similarly W W T for connected junctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Wireframe Detection Method</head><p>Recently, deep convolutional neural networks (CNNs) such as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref> have shown impressive performance in object detection tasks. Utilizing the dataset we have, we here design new, end-to-end trainable CNNs for detecting junctions and lines, respectively, and then merge them into a complete wireframe. <ref type="figure">Fig. 3</ref> shows the overall architecture of our proposed networks and method. Note that we choose different network architectures for junctions and lines due to the nature of their geometric properties, which we will elaborate below.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Junction Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Design Rationale</head><p>Our design of the network architecture is guided by several important observations about junctions. Fully convolutional network for global detection. As we mentioned before, local junction detection is a difficult task, which often leads to spurious detections. Therefore, it is important to enable the network to reason globally when making predictions. This motivates us to choose a fully convolutional network (FCN), following its recent success in object detection <ref type="bibr" target="#b37">[38]</ref>. Unlike other popular object detection techniques that are based on sliding windows <ref type="bibr" target="#b41">[42]</ref> or region proposals <ref type="bibr" target="#b38">[39]</ref>, FCN sees the entire image so it implicitly captures the contextual information about the junctions. Similar to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, our network divides the input image into an H ×W mesh grid, see <ref type="figure">Fig. 4</ref> right. If the center of a junction falls into a grid cell, that cell is responsible for detecting it. Thus, each ij-th cell predicts a confidence score c ij reflecting how confident the model thinks there exists a junction in that cell. To further locate the junction, each ij-th cell also predicts its relative displacement x ij w.r.t. the center of the cell. Note that the behavior of the grid cells resembles the so-called "anchors", which serve as regression references in the latest object detection pipelines <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22]</ref>. Multi-bin representation for junction branches. Unlike traditional object detection tasks, each cell in our network needs to make different numbers of predictions due to the varying number of branches in a junction. To address this issue, we borrow the idea of spatial grid and propose a new multi-bin representation for the branches, as shown in <ref type="figure">Fig. 4</ref> left. We divide the circle (i.e., from 0 to 360 degrees) into K equal bins, with each bin spanning 360 K degrees. Let the center of the k-th bin be b k , we then represent an angle θ as (k, ∆ k ), if θ fall into the k-th bin, where ∆ k is the angle residual from the center b k in the clockwise direction. Thus, for each bin we regress to this local orientation ∆ k .</p><p>As a result, our network architecture consists of an encoder and two sets of decoders. The encoder takes the whole image as input and produces an H × W grid of high-level descriptors via a convolutional network. The decoders then use the feature descriptors to make junction predictions. Each junction is represented by</p><formula xml:id="formula_0">p ij = x ij , c ij , {θ ijk , c θ ijk } K k=1</formula><p>, where x ij is the coordinates of the junction center, c ij ∈ [0, 1] is the confidence score for the presence of a junction in the ij-th grid cell, θ ijk is the angle for the branch in the k-th bin, and c ijk is the confidence score for the bin. The two sets of decoders predict the junction center and the branches respectively. Each FCN decoder is simply a convolutional layer followed by a regressor, as shown in <ref type="figure">Fig. 3</ref> top.</p><p>Unlike local junctions, the junctions we aim to detect each is formed by the intersection of two or more long line segments (the branches). While the junction detection does not explicitly rely on edge/line detection as an intermediate step, the knowledge about the associated edges is indirectly learned by enforcing the network to make correct detection of the branches and their directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Loss Function</head><p>To guide the learning process towards the desired output, our loss function consists of four modules. Given a set of ground truth junctions P = {p 1 , . . . , p N } in an image, we write the loss function as follows:</p><formula xml:id="formula_1">L = λ c conf L c conf +λ c loc L c loc +λ b conf L b conf +λ b loc L b loc .</formula><p>(1) In the following, we explain each term in more detail. Junction center confidence loss L c conf . The junction center confidence decoder predicts a scoreĉ ij , indicating the probability of a junction for each grid cell. Let c ij be the ground truth binary class label, we use the cross-entropy loss:</p><formula xml:id="formula_2">L c conf = − 1 H × W i,j E(ĉij, cij).<label>(2)</label></formula><p>Junction center location loss L c loc . The junction center location decoder predicts the relative positionx ij of a junction for each grid cell. We compare the prediction with each ground truth junction using the 2 loss:</p><formula xml:id="formula_3">L c loc = − 1 N N n=1 x f (n) − x f (n) 2 2 ,<label>(3)</label></formula><p>where f (n) returns the index of the grid cell that the n-th ground truth junction falls into, and x f (n) is the relative position of the ground truth junction w.r.t. that cell center. Junction branch confidence loss L b conf . The junction branch confidence decoder predicts a scoreĉ θ ijk for each bin in each grid cell, indicating the probability of a junction branch in that bin. Similar to the junction center confidence loss above, we use the cross-entropy loss to compare the predictions with the ground truth labels. The only difference is that we only consider those grid cells in which a ground truth junction is present:</p><formula xml:id="formula_4">L b conf = − 1 N × K N n=1 K k=1 E(ĉ θ f (n),k , c θ f (n),k ).<label>(4)</label></formula><p>Junction branch location loss L b loc . Similar to the junction center location loss, we first decide, for each ground truth junction, the indices of the bins that its branches fall into, denoted as g(r), r = 1, . . . , R n , where R n is the order of p n . Then, we compare our predictions with the ground truth using the 2 loss:</p><formula xml:id="formula_5">L b loc = − 1 N N n=1 1 Rn Rn r=1 θ f (n),g(r) − θ f (n),g(r) 2 2 . (5)</formula><p>Implementation details. We construct our model to encode an image into 60×60 grid of 256-dimensional features. Each cell in the grid is responsible for predicting if a junction is present in the corresponding image region. Our encoder is based on Google's Inception-v2 model <ref type="bibr" target="#b43">[44]</ref>, which extracts multi-scale features and is well-suited for our problem. For our problem, we only use the early layers in the Inception network, i.e., the first layer to "Mixed 3b". Each decoder consists of a 3 × 3 × 256 convolutional layer, followed by a ReLU layer and a regressor. Note that the regressor is conveniently implemented as 1 × 1 × d convolutional layer, where d is the dimension of the output.</p><p>The default values for the weights in Eq. (1) are set to the following: λ c conf = λ b conf = 1, λ c loc = λ b loc = 0.1.</p><p>We choose the number of bins K = 15. Our network is trained from scratch with the Stochastic Gradient Descent (SGD) method. The momentum parameter is set to 0.9, and the batch size is set to 1. We follow the standard practice in training deep neural networks to augment the data with image domain operations including mirroring, flipping upsidedown, and cropping. The initial learning rate is set to 0.01. We decrease it by a multiple of 0.1 after every 100,000 iterations. Convergence is reached at 300,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Line Detection</head><p>Next we design and train a convolutional neural network ( <ref type="figure">Fig. 3 bottom)</ref> to infer line information from RGB images. The network predicts for each pixel p whether it falls on a (long) line l. To suppress local edges, short lines, and curves, the predicted value h(p) (of the heat map) at pixel p is set to be the length of the line it belongs to. Given an image with ground truth lines L, the target value for h(p) is defined to be:</p><formula xml:id="formula_6">h(p) = d(l) p is on a line l in L, 0 p is not on any line in L,<label>(6)</label></formula><p>where d(l) is the length of the line l. Letĥ(p) be the estimated heatmap value, then the loss function we try to minimize the 2 loss:</p><formula xml:id="formula_7">L = i,j ĥ (p ij ) − h(p ij ) 2 2 .<label>(7)</label></formula><p>where the sum is over all pixels of the image. Implementation details. The network architecture is inspired by the Stacked Hourglass network <ref type="bibr" target="#b32">[33]</ref>. It takes a 320 × 320 × 3 RGB image as input, extracts a 80 × 80 × 256 feature maps via three Pyramid Residual Modules (PRM), see <ref type="figure">Fig. 3</ref> bottom. The feature maps then go through five stacked hourglass modules, followed by two fully convolutional and ReLU layers (5 × 5 × 32 and 5 × 5 × 16) and a 5 × 5 × 1 convolutional layer to output a 320 × 320 × 1 pixel-wise heat map. The detailed pyramid residual module and stacked hourglass module can be found in <ref type="bibr" target="#b32">[33]</ref>.</p><p>During training, we adopt the Stochastic Gradient Descent (SGD) method. The momentum parameter is set to 0.9, and the batch size is set to 4. Again, we augment the data with image domain operations including mirroring and flipping upside-down. The initial learning rate is set to 0.001. We decrease it by a multiple of 0.1 after 100 epochs. Convergence is reached at 120 epochs.</p><p>Notice that we have used an Inception network for junction detection whereas an hourglass network for line detection. In junction detection, we are not interested in the entire support of the line, hence the receptive field of an Inception network is adequate for such tasks. However, we find that for accurately detecting lines with large spatial support, the Stacked Hourglass network works much better due to its large (effective) receptive field. In addition, our experiment also shows that above length-dependent 2 loss is more effective than the cross-entropy cost often used in learningbased edge detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Combine Junctions and Lines for Wireframe</head><p>The final step of the system is to combine the results from junction detection and line detection to generate a wireframe W for the image, which, as mentioned before, consists of a set of junction points P connected by a set of line segments L.</p><p>Specifically, given a set of detected junctions</p><formula xml:id="formula_8">{p i } N i=1</formula><p>and a line heat map h, we first apply a threshold w to convert h into a binary map M. Then, we construct the wireframe based on the following rules and procedure:</p><p>1. The set P is initialized with the output from the junction detector. A pair of detected junctions p and q ∈ P are connected by a line segment l = (p, q) if they are on (or close to be on) each other's branches, and we add this segment l to L. If there are multiple detected junctions on the same branch of a junction point p, we only keep the shortest segment to avoid overlap. <ref type="bibr" target="#b2">3</ref> 2. For any branch of a junction p that is not connected to any other junction, we attempt to recover additional line segment using M. We first find the farthest line pixel q M (pixel p is a line pixel if M(p) = 1) that is also on the ray starting at p along the branch. Then, we find all the intersection points {q 1 , . . . , q S } of line segment (p, q M ) with existing segments in L. Let q 0 = p i and q S+1 = q M , we calculate the line support ratio κ(q s−1 , q s ), s = {1, . . . , S, S + 1}, for each segment. Here, κ is defined as the ratio of the number of line pixels to the total length of the segment. If κ is above a threshold, say 0.6, we add the segment to L and its endpoints to P .</p><p>Notice that both the sets P and L may have two sources of candidates. For the junction set P , besides those directly detected by the junction detection, the line segments could also produce new intersections or endpoints that were missed by the junction detection. For the line segment set L, it could come from branches of the detected junctions and the line detection. We leave more detailed description of the algorithm to the supplementary material. Of course, there could be more advanced ways to merge the detected junctions and line heat map which we will explore in future work. Nevertheless, from our experiments (see next section), we find that the results from junction detection and line detection are rather complementary to each other and the above simple procedure already produces rather decent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments to evaluate the quality of junctions and final wireframes generated by our method, and compare it to the state-of-theart. All experiments are conducted on one NVIDIA Titan X GPU device. In testing phase, our method runs at about two frames per second, thus our method is potentially suitable for applications which require real-time processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>For performance evaluation, we split our wireframe dataset into a training set and a testing set. Among the 5,462 images in the dataset, 5,000 images are randomly selected for training and validation, and the remaining 462 images are used for testing. For junction detection (Section 4.2), we compare the junctions detected by any method with the ground truth junctions <ref type="figure">(Fig. 2, second row)</ref>. For wireframe construction (Section 4.3), we compare the line segments detected by any method with the ground truth line segments labeled by human subjects <ref type="figure">(Fig. 2, first row)</ref>.</p><p>For both junction detection and wireframe construction experiments, all methods are the evaluated quantitatively by means of the recall and precision as described in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48]</ref>. In the context of junction detection, recall is the fraction of true junctions that are detected, whereas precision is the fraction of junction detections that are indeed true positives. In the context of wireframe construction, recall is the fraction of line segment pixels that are detected, whereas precision is the fraction of line segment pixels that are indeed true positives.</p><p>Specifically, let G denote the set of ground truth junctions (or line segment pixels), and Q denote the set of junctions (or line segment pixels) detected by any method, the precision and recall are defined as follows:</p><p>Precision</p><formula xml:id="formula_9">. = |G ∩ Q|/|Q|, Recall . = |G ∩ Q|/|G|. (8)</formula><p>Note that, following the protocols of previous work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48]</ref>, the particular measures of recall and precision allow for some small tolerance in the localization of the junctions (or line segment pixels). In this paper, we set the tolerance to be 0.01 of the image diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Junction Detection Comparison</head><p>We compare our junction detection method with two recent methods, namely Manhattan junction detection (MJ) <ref type="bibr" target="#b35">[36]</ref> and a contrario junction detection (ACJ) <ref type="bibr" target="#b47">[48]</ref>. MJ <ref type="bibr" target="#b35">[36]</ref>: This method detects Manhattan junctions formed by line segments in three principal orthogonal directions using a simple voting-based scheme. As the authors did not release their code, we use our own implementation of the method. Line segments are first detected using LSD <ref type="bibr" target="#b45">[46]</ref>, and then clustered using J-Linkage <ref type="bibr" target="#b44">[45]</ref> to obtain the vanishing points. Note that this method only ap- plies to scenes that satisfy the Manhattan world assumption. For fair comparison, we only keep the images in which three principal vanishing points are detected. An important parameter in our implementation is the maximum distance d max between a line segment and a point p for that line segment to vote for p. We vary the value d max ∈ {10, 20, 30, 50, 100, 200, 300, 500} pixels.</p><p>ACJ <ref type="bibr" target="#b47">[48]</ref>: This method relies on statistical modeling of image gradients and an a contrario approach to detect junctions. Specifically, meaningful junctions are detected as those which are very unlikely under a null hypothesis H 0 , which is defined based on the distribution of gradients of arbitrary natural images. In the method, each candidate junction is associated with a strength value depending on the image gradients around it. Then, the candidate junction is validated with a threshold, which is derived by controlling the number of false detections, , in an image following H 0 . For the experiments, we use the implementation provided by the authors of <ref type="bibr" target="#b47">[48]</ref> and vary the value ∈ {10 −3 , 10 −2 , 10 −1 , 1, 10 1 , 10 2 , 10 3 }.</p><p>Performance comparison. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the precisionrecall curves of all methods on our new dataset. For our method, we vary the junction confidence threshold τ from 0.1 to 0.9. As one can see, our method outperforms the other methods by a large margin. <ref type="figure" target="#fig_5">Fig. 7</ref> compares qualitatively the results of all methods on our test data. Compared to the other two methods, MJ tends to miss important junctions due to the imperfect line segment detection results. Moreover, since MJ relies on local image features, it noticeably produces quite a few repetitive detections around some junctions. By directly modeling the image gradients, ACJ is able to find most junctions on the scene structures. However, as a local method, ACJ makes a lot of false predictions on textured regions (e.g., floor of the first, sky of the fourth image). In contrast, our method is able to detect most junctions intersected by salient lines, while minimizing the number of false detections. This is no surprise because our supervised framework implicitly encodes highlevel structural and semantic information of the scene as it learns from the labeled data provided by humans. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Line Segment Detection Comparison</head><p>In this section, we compare the wireframe results of our method with two state-of-the-art line segment detection methods, namely the Line Segment Detector (LSD) <ref type="bibr" target="#b45">[46]</ref> and the Markov Chain Marginal Line Segment Detector (MCMLSD) <ref type="bibr" target="#b0">[1]</ref>. We test and compare with these methods on both our new dataset and the York Urban dataset <ref type="bibr" target="#b6">[7]</ref> used in the work of MCMLSD <ref type="bibr" target="#b0">[1]</ref>. LSD <ref type="bibr" target="#b45">[46]</ref>: This method is a linear-time line segment detector that requires no parameter tuning. It also uses an a contrario approach to control the number of false detections. In this experiment, we use the code released by the authors <ref type="bibr" target="#b3">4</ref> and vary the threshold for − log(NFA) (NFA is the number of false alarms) in 0.01 × {1.75 0 , 1.75 1 , 1.75 2 , ..., 1.75 19 }. MCMLSD <ref type="bibr" target="#b0">[1]</ref>: This method proposes a two-stage algorithm to find line segments. In the first stage, it uses the probabilistic Hough transform <ref type="bibr" target="#b30">[31]</ref> to identify globally optimal lines. In the second stage, it searches each of these lines for their supports (segments) in the image, which can be modeled as labeling hidden states in a linear Markov chain. In this experiment, we use the code released by the authors. <ref type="bibr" target="#b4">5</ref> Be aware that authors of <ref type="bibr" target="#b0">[1]</ref> have introduced a different metric than ours that tends to penalize over-segmentation. Hence our metric can be unfair to their method. Nevertheless, our metric is more appropriate for wireframe detection as we prefer to interpret a long line as several segments between junctions if it intersects with other lines. Performance comparison. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the precisionrecall curves of all methods on our dataset and the York Urban dataset, respectively. As one can see, our method outperforms the other methods by a significant margin on our dataset. The margin on the York Urban dataset is decent but not so large. According to <ref type="bibr" target="#b0">[1]</ref>, the labeling of the York Urban dataset is not as complete for salient line segments, hence it is not entirely suitable for the wireframe detection task here. <ref type="figure" target="#fig_6">Fig. 8</ref> compares qualitatively the results of all methods on our test data. Since the other two methods rely on local measurements, they tend to produce many line seg-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has demonstrated the feasibility of parsing wireframes in images of man-made environments. The proposed method is based on combining junctions and lines detected from respective neural networks trained on a new large-scale dataset. Both quantitatively and qualitatively, the results of our method approximately emulate those la-belled by humans. The junctions and line segments in a wireframe and their incidence relationships encode rich and accurate large-scale geometry of the scene and shape of regular objects therein, in a highly compressive and efficient manner. Hence results of this work can significantly facilitate and benefit visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation in man-made environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wireframe Construction Algorithm Detail</head><p>Given an image, our wireframe construction algorithm takes a set of junctions</p><formula xml:id="formula_10">{p i } N i=1 , p i = x i , {θ ik } Ki k=1</formula><p>, and a line heat map h as input. Note that for the junctions and their branches predicted by our network, we only keep those with confidence scores higher than certain thresholds τ c and τ b , respectively. As a pre-processing step, we further adopt a strategy similar to non-maximum suppression to remove duplicate detections.</p><p>Our wireframe construction algorithm is presented in Alg. 1. In the algorithm, we first apply a threshold ω to convert the line heat map h(p) into a binary map M (line 2). Note that this threshold ω is varied to obtain the precisionrecall curve in our experiments on wireframe construction. The algorithm then proceeds as follows:</p><p>First, we connect all pairs of junctions which are aligned with each other's branch directions (lines <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Let r ik represent the ray starting at p i along its k-th branch. We collect all possible rays as R = {r 11 , ..., r 1K1 , ..., r i1 , ..., r iKi , ...}, and use (i, k) = π(t) to map the t-th ray in R to its junction index i and branch index k. Then, for the rays in R, we use V ∈ R Nr×Nr , N r = |R|, to record the indices of the corresponding ray/branch of the closest opposite junction. Specifically, ∀t 1 ∈ {1, . . . , N r }, we set V(t 1 , t 2 ) to 1 if and only if (i) p i is the on the ray r jk2 and p j is on the ray r ik1 , where (i, k 1 ) = π(t 1 ), (j, k 2 ) = π(t 2 ), and (ii) the distance between p i and p j is the shortest among all such aligned pairs (lines <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Then, we consider two rays are matched if V(t 1 , t 2 ) = V(t 2 , t 1 ) = 1 and add the corresponding junctions and line segments to P and L, respectively (lines <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</p><p>Second, for any ray r ik which fails to find a matching ray using the above procedure, we attempt to recover additional line segments using the line support M (lines <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. We consider the following cases:</p><p>(a) If the distance between p i and q b , the intersection of r ik and the image boundary, is smaller than certain threshold (say 0.05 × m where m is the maximum of image width and height), we add {p i , q b } and the connecting line segment to P and L, respectively (lines 24-26).</p><p>(b) For a ray exceeding the length threshold in (a), we first find the farthest line pixel q M along the ray on M.</p><p>Then, we find all the intersection points {q 1 , . . . , q S } of line segment (p i , q M ) with existing segments in L (lines <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Let q 0 = p i and q S+1 = q M , we calculate the line support ratio κ(q s−1 , q s ), s = {1, . . . , S, S + 1}, for each segment. Here, κ is defined as the ratio of line pixels (pixel p is a line pixel</p><formula xml:id="formula_11">Algorithm 1 Wireframe Construction Input: Junctions {p i } N i=1 , p i = x i , {θ ik } Ki k=1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and a line heat map h(p)</head><p>Output: Wireframe W consisting of a set of junction points P connected by a set of line segments L 1: Initialize P ← Ø, L ← Ø, V ← 0 2: Binarize h(p) with threshold ω into M 3: for t 1 ∈ {1, 2, . . . , N r } do 4:</p><formula xml:id="formula_12">(i, k 1 ) ← π(t 1 ), d min ← ∞, z ← 0 5: for t 2 ∈ {1, 2, . . . , N r } do 6: (j, k 2 ) ← π(t 2 ) 7:</formula><p>if j = i and p j on r ik1 and p i on r jk2 then <ref type="bibr">8:</ref> if x i − x j &lt; d min then 9: if V(t 1 , t 2 ) = 1 and V(t 2 , t 1 ) = 1 then 19: Find all intersections {q 1 , . . . , q S } of (p i , q M ) with segments in L 30:</p><formula xml:id="formula_13">d min ← x i − x j ,</formula><formula xml:id="formula_14">(i, k 1 ) ← π(t 1 ), (j, k 2 ) ← π(t 2 ) 20: P ← P {p i , p j }, L ← L {(p i , p j )} 21:</formula><formula xml:id="formula_15">q 0 ← p i , q S+1 ← q M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>31:</head><p>for s ∈ {1, 2, ..., S, S + 1} do <ref type="bibr">32:</ref> if κ(q s−1 , q s ) &gt; 0.6 then 33:</p><formula xml:id="formula_16">P ← P {q s−1 , q s } 34: L ← L {(q s−1 , q s )} 35:</formula><p>end if <ref type="bibr">36:</ref> end for <ref type="bibr">37:</ref> end if 38: end for if M(p) = 1) to the total length of the segment. If κ is above a threshold, say 0.6, we add the segment to L and its endpoints to P (lines 30-36). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Experiment on Junction Detection Network Parameters</head><p>In this section, we examine the choices of two important hyper-parameters in our junction detection network.</p><p>Effect of balancing positive and negative samples. In this experiment, we vary the value r max , which controls the maximum ratio between negative and positive samples at each iteration. Note that setting r max = ∞ is equivalent to using all grid cells during training. We can observe in <ref type="figure" target="#fig_8">Figure 9</ref>(a) that the precision-recall curves largely overlap. But as r max increases, the curve shifts toward the highprecision-low-recall regime, and vice versa. For example, when r max = 1, the precision and recall at τ = 0.5 are 0.19 and 0.94, respectively. And when r max = ∞, the precision and recall at τ = 0.5 are 0.70 and 0.44, respectively. Note that this has an important implication in practice, as human annotators tend to miss true junctions much more often than labelling wrong junctions. Empirically, we have found that r max = 7 yields more satisfactory results.</p><p>Going deeper. It is also interesting to investigate how the network depth of the encoder affects the performance. In this experiment, we compared two different choices based on Google Inception-v2, namely the first layer to "Mixed 3b", and the first layer to "Mixed 4b". Note that the latter has a larger depth and receptive field, at the cost of spatial resolution (30×30). As one can see in <ref type="figure" target="#fig_8">Figure 9</ref>(b), increasing the depth (i.e., predicting at the "coarser" level) results in higher precision but lower recall. This suggests possibilities to further improve the performance of our method using a "skip-net" architecture, that is, combining predictions at multiple levels. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Experiment on Line Segment Detection</head><p>In this experiment, we study the possibility of extracting line segments directly from the pixel-wise line heat map predicted by our network (i.e., without using junctions). To this end, we simply perform a probabilistic hough transform <ref type="bibr" target="#b30">[31]</ref> on the line heat map to generate line segments. We compare the results with LSD, MCMLSD, and our full wireframe construction method. <ref type="figure" target="#fig_0">Figure 10</ref> shows the precision-recall curves of all methods. We make the following observations on the results: First, the performance of our "Heatmap + Hough" approach is comparable to that of the state-of-the-art line segment detection method MCMLSD, verifying the effectiveness of the our line detection network. Second, by combining the predicted junctions with the line heat map, our full wireframe construction method performs significantly better than using the line heat map alone. This further illustrates the importance of junction detection in parsing the <ref type="bibr">Figure 11</ref>. Failure cases on our test dataset. First row: Our method. Second row: Ground truth. wireframe: By detecting the "endpoints" of the line segments, we effectively overcome the difficulties faced by traditional line segment detection methods, including the false detection problem and the inaccurate endpoint problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additional Results on Junction Detection</head><p>In <ref type="figure" target="#fig_0">Figure 12</ref>, we show additional junction detection results obtained by all methods. One can see that our method is able to detect most junctions and their branches in the image, achieving superior performance over existing methods.</p><p>From <ref type="figure" target="#fig_0">Figure 12</ref> we can also observe some limitations of our method. Specifically, there are occasionally repeated detections in our result. This may be caused by junctions located at the boundary of two adjacent grid cells used in our junction detection network. Similarly, the use of grid could also lead to missed detection if two junctions are very close to each other. But we note that such cases are rather uncommon in practice and have very small effect on the overall scene structure estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Additional Results on Wireframe Construction</head><p>In <ref type="figure" target="#fig_0">Figure 13</ref>, we show additional wireframe detection results obtained by all methods. Our method outperforms other two in most areas and produces much cleaner results as we focus on long line segments and exploit their relations (junctions). Therefore, the resulted wireframes are potentially more suitable for 3D reconstruction tasks.</p><p>In <ref type="figure" target="#fig_0">Figure 11</ref>, we further show some failure cases of our method. One challenging case corresponds to structures with relatively small scale and weak image gradients (e.g., the stairs in the first image). Also, our method sometimes has difficulty in image region of repetitive patterns (e.g., the handrails in the second image and the brick wall in the third image), generating fragment, incomplete results. This suggests opportunities for further improvement by explicitly harnessing such geometric structure in our wireframe construction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>First row: Examples of typical indoor or outdoor scenes with geometrically meaningful wireframes labelled by humans; Second row: Wireframes automatically extracted by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>PRM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .WFigure 4 .</head><label>34</label><figDesc>Architecture of the overall system. Top: junction detection network. Bottom: line detection network.H Representation of a junction with three branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The precision-recall curves of different junction detection methods on our test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The precision-recall curves of different line segment detection methods. Left: on our test dataset. Right: on the York Urban dataset<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Junction detection results. First row: MJ (dmax = 20). Second row: ACJ ( = 1). Third row: Our method (τ = 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Line/wireframe detection results. First row: LSD (-log(NFA) &gt; 0.01 × 1.75 8 ). Second row: MCMLSD (confidence top 100). Third row: Our method (line heat map h(p) &gt; 10). Fourth row: Ground truth. ments on textured regions (e.g. curtain of the first image) which do not correspond to scene structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>end if 22 :else 28 :</head><label>2228</label><figDesc>end for23:  for all r ik not matched to another ray do24:    Find the intersection of r ik and image boundary q b 25:if x i − q b ≤ 0.05 × m then 26: P ← P {p i , q b }, L ← L {(p i , q b )} 27:Find the farthest point q M along r ik on M 29:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Experiment on junction detection network parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Experiment on line segment detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Line/wireframe detection results. First row: LSD (-log(NFA) &gt; 0.01 × 1.75 8 ). Second row: MCMLSD (confidence top 100). Third row: Our method (line heat map h(p) &gt; 10). Fourth row: Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for all t 1 , t 2 ∈ {1, 2, . . . , N r }, t 1 = t 2 do</figDesc><table><row><cell></cell><cell>z ← t 2</cell></row><row><cell>10:</cell><cell>end if</cell></row><row><cell>11:</cell><cell>end if</cell></row><row><cell>12:</cell><cell>end for</cell></row><row><cell>13:</cell><cell>if z = 0 then</cell></row><row><cell>14:</cell><cell>V(t 1 , z) ← 1</cell></row><row><cell>15:</cell><cell>end if</cell></row><row><cell cols="2">16: end for</cell></row><row><cell>17: 18:</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Hence we are less interested in detecting a straight line with the longest possible support, instead, we are interested in its incidence relationship with other lines and junctions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.ipol.im/pub/art/2012/gjmr-lsd/ 5 http://www.elderlab.yorku.ca/resources/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 12. Junction detection results. First row: MJ (dmax = 20). Second row: ACJ ( = 1). Third row: Our method (τ = 0.5).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mcmlsd: A dynamic programming approach to line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generalisable framework for saliency-based line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Windridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guillemaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3993" to="4011" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On seeing things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Clowes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="116" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manhattan world: Orientation and outlier detection by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1063" to="1088" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delay: Robust spatial layout estimation for cluttered indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic single-image 3D reconstructions of indoor manhattan world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Robotics Research</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1841" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Line-based relative pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Manhattan scene understanding using monocular, stereo, and 3D features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2228" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data-driven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manhattan-world stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1422" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction by analyzing distribution around peaks in hough space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Predicting complete 3d models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>abs/1504.02437</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognising planes in a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1849" to="1861" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Alvey Vision Conference</title>
		<meeting>the Alvey Vision Conference</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Impossible objects as nonsense sentences. Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting global connectivity constraints for reconstruction of 3d line segments from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kurz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Intelligent line segment perception with cortex-like mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1522" to="1534" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cannylines: A parameter-free line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="507" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using contours to detect and localize junctions in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Interpreting line drawings of curved objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="73" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="580" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Robust detection of lines using the progressive probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="119" to="137" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Robust detection of lines using the progressive probabilistic hough transform. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Psychophysics with junctions in real images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1101" to="1127" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Line segment detection using weighted mean shift procedures on a 2d slice sampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="163" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lifting 3D manhattan lines from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Manhattan junction catalogue for spatial reasoning of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A coarse-to-fine indoor layout estimation (CFILE) method. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno>abs/1607.00598</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3-D depth reconstruction from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mathematical structures of line drawings of polyhedrons-toward man-machine communication by means of line drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="458" to="469" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-iterative approach for fast and accurate vanishing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image-based building regularization using structural linear features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1760" to="1772" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Accurate junction detection and characterization in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pose estimation from line correspondences: A complete analysis and a series of solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1209" to="1222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Accurate and robust line segment extraction using minimum entropy with hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="822" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient 3D room shape recovery from a single panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
