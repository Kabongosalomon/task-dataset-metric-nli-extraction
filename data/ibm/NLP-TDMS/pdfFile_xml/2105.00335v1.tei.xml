<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANSFORMER ARCHITECTURES FOR LARGE SCALE AUDIO UNDERSTANDING. ADIEU CONVOLUTIONS *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Verma</surname></persName>
							<email>prateekv@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berger</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>450 Jane Stanford Way</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford CA</orgName>
								<address>
									<postCode>94305</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRANSFORMER ARCHITECTURES FOR LARGE SCALE AUDIO UNDERSTANDING. ADIEU CONVOLUTIONS *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformers</term>
					<term>audio understanding</term>
					<term>wavelets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An overview of the proposed Audio Transformer architecture using front end fully connected encoder with Transformer layers and pooling layers. It takes 1s of input, and divides it into patches of size 25ms, followed by learning a front end, to feed it to Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K, comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean average precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional network designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant bandwidth filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation. 1 1 *This is not the first time an acoustic scene understanding model with-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION AND RELATED WORK</head><p>Acoustic scene analysis is a classical signal processing and machine learning problem whose goal is to predict the contents of an input signal within a brief duration, typically one second. In addition to modeling perception, computer simulation of hearing combined with models of other sensory systems will help bridge the gap between humans and computers. For the past decade, CNNs have become a de-facto architecture in learning mappings from fixed dimensional inputs to fixed dimensional outputs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. CNN architectures inspired from vision <ref type="bibr" target="#b1">[2]</ref>, adapted for acoustic scene understanding, achieve similar performance gains for audio also.</p><p>The core backbone of this work is Transformer architecture which recently have recently produced state of the art results in a variety of domains, including protein sequences <ref type="bibr" target="#b3">[4]</ref>, text <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, symbolic music <ref type="bibr" target="#b6">[7]</ref>, video <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and image understanding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. By learning transformers on the latent representations, and conditioning a wavenet generator, they were able to achieve compelling results in music generation <ref type="bibr" target="#b11">[12]</ref> and style transfer, which was impossible without the guidance of meta-data and convolutional architectures <ref type="bibr" target="#b12">[13]</ref>. They have also been used in learning latent audio out convolutions is presented, although perhaps it is a first end-to-end one, to the best of our knowledge. At the same time as viT, <ref type="bibr" target="#b0">[1]</ref> showed how in a two-step process, one can achieve the same.</p><p>representations such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> for solving pseudo-tasks such as infilling to learn time-dependent representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. As opposed to learning latent representations, the reduced time-scales of Transformers can advantageously model input representations. A major drawback of convolutional architecture is the fixed filter across the entire input. Furthermore, Transformers take advantage of attention mechanism with the output at a location dependent upon the input at some other location.</p><p>The core idea of this work is to replace traditional convolutional based architectures <ref type="bibr" target="#b2">[3]</ref>, combined convolutional and Transformer architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, and recurrent architectures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> with a purely Transformer based architecture. Our work is distinct from the method proposed in <ref type="bibr" target="#b0">[1]</ref> which was not an end-to-end approach and which required a two step approach (specifically, Learning a dictionary of latent codes, and using the discrete latent codes as an input to transformer architectures). Similar approaches were successfully used in areas such as speech recognition <ref type="bibr" target="#b15">[16]</ref> to mimic BERT <ref type="bibr" target="#b5">[6]</ref>. All these state of the art performances were possible due to the architectures' ability to model long term dependency inputs and the attention mechanism present in them enabling focus only on the part of the input that is important <ref type="bibr" target="#b19">[20]</ref>.</p><p>The organization of the paper is as follows: Section I introduces the problem and the literature survey followed by the dataset we used to benchmark the results in section II. The next section details the methodology followed by results and discussion in Section IV. We conclude the paper in Section V followed by our thoughts of future work and references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATASET</head><p>We use FSD50K <ref type="bibr" target="#b20">[21]</ref> an open dataset containing over 51k audio files to train and evaluate our architectures. FSD50K comprises over 100 hours of manually labeled audio using 200 classes drawn from the AudioSet ontology. We chose to evaluate this over popular AudioSet <ref type="bibr" target="#b2">[3]</ref>, as FSD50K is freely available under creative commons license, contains many more high quality audio annotations, and twice number of training examples in the balanced set-up. We used the already provided training and the validation splits to tune the model and tested them on the evaluation setup provided. In total there are about 51,197 clips available ranging from 0.3-30s. We downsample all the clips to 16kHz sampling rate using <ref type="bibr" target="#b21">[22]</ref>. We follow the same setup for reporting the results as done in <ref type="bibr" target="#b20">[21]</ref>. All the training was carried on 1s audio chunks with the labels inherited for all the chunks in clips greater than 1s. For samples less than 1s, the audio clip is repeated to make the duration 1s resulting in a single training example for that clip. On an average, the duration per clip is 7.6s, with 1.22 average labels per clip, uploaded by 7225 user ids thus encompassing a diverse range of sources, acoustic environments, microphones, locations to name a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Transformer Architectures</head><p>This section describes the Transformer architecture as described in <ref type="bibr" target="#b19">[20]</ref> that we used to train the system as shown in <ref type="figure">Figure 1</ref>. A detailed explanation is given in <ref type="bibr" target="#b22">[23]</ref>, but for the sake of clarity and completeness we describe it here. As a black-box, which we would describe in more detail in this section, it takes as an input a sequence of a fixed length T , and produces the same length but with a chosen dimension, which we call E, which denotes the size of the latent space. More specifically, it maps a sequence x = (x1, x2, ....xT ) to a sequence of same length T , namely z : (z1, z2, ....zT ) , where each of the dimensions of (z1, z2, ....zT ) is the chosen hyperparameter E, which in our case is 64, the size of the embedding. For the sake of brevity, we would explain only one Transformer Encoder, and for a model with layers L, each of the stack is superimposed on the other one.</p><p>Each Transformer module consists of a attention block and a feed-forward block. The output of each of them is passed through a layer norm and a residual layer. So after both the attention block and the feed-forward block, if the input to a subblock (attentionFa or feed-forwardF f f block) is a sequence x b , instead of passing the output directly to the next module/sub-block, we pass along the block layer norm and the residual output x bo as</p><formula xml:id="formula_0">x bo = LayerN orm(x b + F a/f f (x b ))</formula><p>This follows the notion that layer-norm/skip connections help in better convergence/improved performance. We now describe each of the two sub-blocks that are part of the transformer block namely, i) multi-headed causal attention ii) feed-forward architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Multi-Headed Causal Attention</head><p>A multi-headed causal attention function can be described as a weighting function that decides how to get the output of each step. It learns a probabilistic score of how important each of the embedding, is while predicting the output. A multi-headed attention consists of first learning a probabilistic score. It is then multiplied with each of the inputs to determine how important each of the input is for prediction of the embedding for a position pos belonging to 1, 2, 3....T . We use scaled-dot product attention as the type of attention mechanism. A query, key and a value vector is learned for each of the position for each of the inputs. This is done by implicitly learning matrices, WQ, WK and WV to produce a query vector q, key vector k and value vector v for each of the inputs for a single attention head. We take the dot product of query and key vectors, the result is multiplied by a normalization factor (the inverse of square root of size of the vector as done in <ref type="bibr" target="#b19">[20]</ref>), before taking a soft-max across all the inputs. Each of the value vector is multiplied by this score to get the output of the attention module. Mathematically, for a query matrix Q, key matrix K, and a value matrix V , it is defined as,</p><formula xml:id="formula_1">Attention(Q, K, V ) = sof tmax( QK T √ d k</formula><p>). We can also learn multiple such attention maps for h attention heads, defined as,</p><formula xml:id="formula_2">M utliHeadAttention(Q, K, V ) = Concat(h1, h2, ...h h )Wo,</formula><p>where each of the attention heads hi is defined as</p><formula xml:id="formula_3">Attention(Qi, Ki, Vi) = sof tmax( QiK T i √ d k )</formula><p>and Wo is a matrix learned during training. In this work, we focus on causal attention map which is made possible by multiplying with a mask of a triangular matrix so that each of the attention head only gives weightage to the previous sample at position pos and all the future entries are set to zero.</p><p>[5]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Feed Forward Architecture &amp; Positional Information</head><p>We weigh the saliency of each of the input signal via multi-headed attention for passing at a position pos. Each of the input at position pos is passed through a feed-forward architecture. We have the output of the feed-forward layers x bo for an input x b , for the dimension of feed-forward layers d f f , in case of 2-layer network is,</p><formula xml:id="formula_4">F F (x b ) = max(0, x b W1 + b1)W2 + b2.</formula><p>. We apply this function identically at each of the inputs. As described in <ref type="bibr" target="#b19">[20]</ref>, to each of the inputs, positional encoding are added. As the input is passed on as a list, the model does not take into account the relative position, and thus the positional encoding are needed. For any position pos for the dimension i of the latent space, we use sinusoidal function, i.e. to each position pos and embedding dimension i in E, we add,</p><formula xml:id="formula_5">P E pos,2i/2i+1 = sin/cos(pos/10000 (2i/E) )</formula><p>This adds positional information for each point in time, of input with dimension E , before passing thorough self-attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adapting Transformer Architecture for raw waveforms</head><p>We adapt Transformer architectures using ideas from traditional signal processing. Since the Transformer has O(n 2 ) complexity w.r.t memory and computation requirements, we choose to follow the traditional route of windowing the signal. For all the experiments, as discussed before we work with 1s of audio input sampled at 16kHz yielding 16,000 samples. <ref type="figure">Figure 2</ref>: Core idea of wavelets utilizing multi-scale learning on (left) from <ref type="bibr" target="#b23">[24]</ref>, and using them to create a layer that operates on intermediate Transformer embeddings at various scales. We show a demo signal and we retains half of them, and modify the other half using variable sized windows.</p><p>The widow length is chosen to be 25ms, choosing a nonoverlapping rectangular window. The rectangular window provides the network an optimal windowing function which, as we will see in a few of the learned filters, adapts itself to a shape of hanning/hamming window. We fix the front end to be a dense layer of size 2048 neurons followed by another layer of size 64, primarily to adapt to the size of the embedding layer of Transformer. A single dense layer of size 2048 successfully learned a filter-back to learn a neural-time frequency representation as shown in <ref type="bibr" target="#b24">[25]</ref>. This design was chosen as it produced state of the art results for an equally difficult problem of pitch estimation in polyphonic audio <ref type="bibr" target="#b24">[25]</ref>, with feed forward layers. Since Transformer layers only consists of attention + feed-forward blocks, we achieve an end-to-end architecture that does not have any convolutional operators. <ref type="bibr" target="#b0">1</ref> This yields a front end representation of 40 time steps each of size 64 dimensions, (64 being a hyper-parameter). We choose 6 layers of Transformer module with the size of latent code being 64 for each of the layers, and 8 attention head, with 128 dim 3-layer dense layer to convert to the desired feature space. For comparing with a smaller model, we choose 3-layers of Transformers with similar setup. The last layer of Transformer is reduced to a smaller dimension using average pooling across time. The output of the last dense layer of dimension 200, chosen same as number of output labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformer Architectures Inspired From CNNs: Pooling</head><p>We explored further performance enhancements to the baseline Transformer architecture proposed in the previous section. For this we draw inspiration of convolutional architectures used for the past decade to understand images <ref type="bibr" target="#b25">[26]</ref> and audio <ref type="bibr" target="#b2">[3]</ref>. The traditional models e.g. Resnet-50 <ref type="bibr" target="#b1">[2]</ref>, consists of using a combination of convolutional layers followed by pooling. The use of pooling layers has two advantages. It reduces the number of computations by reducing the size of inputs in the higher layers. More importantly it allows the higher-layer neurons to have much broader receptive field sizes, and allows the network to learn hierarchically ordered features, which is important for a particular problem. Pooled Transformers outperform the baseline Transformer architecture, while retaining the same number of parameters. In our experiments average pooling performed significantly better than max-pooling, as it retains more information on the signal. As described in <ref type="figure">Figure 1</ref>, we use pooling across time after every two layers of Transformers, with stride 1, to reduce the dimensionality of the input by a factor of 2, and shows significant performance gain as compared to the original Transformer architecture without pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning multi-scale embeddings</head><p>In this adaptation, we draw inspiration from wavelet decomposition and success of pooling layers. We explored if we can decompose the intermediate embeddings out of the Transformer, at multiple scale similar to idea of wavelet decomposition. In order to achieve it, we fix up our kernel to be average operation across all windows chosen at a particular level. Notice that we choose different widow sizes at different dimensions of embedding along the time axis. The manner of implementation is again a design choice and there are several interesting ideas possible in future, including the choice of kernel. We draw inspiration from the work carried out in <ref type="bibr" target="#b23">[24]</ref>, as seen in <ref type="figure">Figure 2</ref>. We adapt the window size, in factors of 1,2,4,8 and so on, following a geometric progression. The value is assigned to all of the elements as opposed to reducing the size, as done in pooling thus retaining the same size. This operation in fully differentiable, and can be trained in end-to-end architectures. This is different than work carried out on spectral filtering <ref type="bibr" target="#b26">[27]</ref>, as we choose to operate firstly with variable window size as opposed to fixed windows, and secondly do not take explicit hand-crafted bands of filters. Additionally, we choose to model the space of embeddings-time hierarchically with only a few large windows, and large number of smaller window, most of them being 1 to retain the embeddings at their original scale. This retains the original transformer embeddings, with half of the embeddings unchanged, and tinkers with the other half. This combination has been at the core of wavelet transforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS &amp; DISCUSSION</head><p>For all of the architectures, we only tuned learning rate to be consistent with the results shown in <ref type="bibr" target="#b20">[21]</ref>. All of the Transformers have 6 layers (3 for small transformers) with 64 dim embeddings, and 3-Layer 128 neuron feed forward layers, and 8 attention head. The front end consists of 1024/2048 dimensional layer followed by a 64 dimensional dense layer for small and large transformers. We compared the same Transformer architectures with that of using i) pool- <ref type="figure">Figure 3</ref>: Sorted filters, learned by the front end, learns a problem specific non linear, non constant bandwidth filter-bank. This is shown by comparing it to that learned by the same front end for polyphonic pitch estimation as shown in <ref type="bibr" target="#b24">[25]</ref>.</p><p>ing layers ii) multi-scale filters. We observed that even the smallest of the Transformer architectures outperform traditional convolutional architectures. This is quite significant, unlike problems in vision <ref type="bibr" target="#b9">[10]</ref>, where the margin was not as significant. Another observation is also that the performance keeps improving with more depth. All the models were trained using Tensorflow framework <ref type="bibr" target="#b27">[28]</ref>, with Huber Loss as the error criteria between the predictions and the ground truth, using Adam optimizer <ref type="bibr" target="#b28">[29]</ref>. <ref type="table">Table 1</ref>: Comparison of various proposed architecture as shown in the table below for mean average precision (mAP) metric. We see how even baseline Transformer architectures without using any convolutional layers can outperform widely used CNN architectures for acoustic scene understanding by significant margins. <ref type="bibr" target="#b20">[21]</ref> Neural Model Architecture mAP # Param CRNN <ref type="bibr" target="#b20">[21]</ref> 0.417 0.96M VGG-like <ref type="bibr" target="#b20">[21]</ref> 0.434 0.27M ResNet-18 <ref type="bibr" target="#b20">[21]</ref> 0.373 11.3M DenseNet-121 <ref type="bibr" target="#b20">[21]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">What the front end learns</head><p>We follow a strategy similar to that described in <ref type="bibr" target="#b24">[25]</ref> to understand what the filters learn. We deploy the same front end in our work which is again a feed-forward layer consisting of 2048 neuron, followed by a 64-dim dense layer. This is similar to the analogy of getting a mel-like representation which is learnable end-to-end. After training, we take the first layer and sort the filter according to the peaks of their Fourier representation. We see that it manages to learn a non-linear, non-constant bandwidth filterbank as seen in <ref type="figure">Figure 3</ref>. We also see that with using the same front end for two different applications, namely for pitch estimation and acoustic scene understanding, the shape and the resolution of the learned filter-bank is different. In addition, we can also see a step-wise pattern, which shows multiple filters assigned to the same frequency bin to account for the phase variations of the input signals. <ref type="figure">Figure 4</ref> depicts a few chosen filters for the sake of discussion here. We observe a variety of ideas that can be interpreted from signal processing perspective, and also to take into account the characteristics of the input signal i.e. frequency, timbre, and energy. We can see, in center-top row, that a filter learns a pure sinusoidal basis of a certain frequency. Furthermore, it also manages to learn a windowing function that closely resembles hanning/hamming window. The filters in the left column present at the top-bottom, are characteristic of an onset detector, which can be a slow/rapid onset respectively. Further the filter present in the second row, third column shows a slowly moving signal, which may be latching onto the overall energy envelop of a signal for certain characteristic sounds. It is exciting and interesting to see these correlations to traditional signal processing ideas present in these filters. <ref type="figure">Figure 4</ref>: Filters learned from the first layer of front end show strong correlations to signal processing, particularly learning sinusoidal signals, onset detectors, energy envelops, and windowing functions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION &amp; FUTURE WORK</head><p>We have shown here how a Transformer architecture without using any convolutional filters can be adapted for large scale audio understanding. This work shows considerable promise, as it has outperformed other convolutional architectures based bench-marks by a significant margin. We show our model can learn a time frequency front end that is adaptable to the particular problem of interest in this case, large scale audio understanding. There are several possible research directions ahead. With the advancements in Transformer architectures such as switch transformers <ref type="bibr" target="#b29">[30]</ref>, and sparse transformers <ref type="bibr" target="#b30">[31]</ref>, these results would further improve. Additionally, with the success of unsupervised representation learning architectures for audio <ref type="bibr" target="#b0">[1]</ref>, it will be interesting to do large scale pretraining for making robust audio representations. It will be also be useful to explore a wider search over hyper-parameters to increase the reported precision scores.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">October 17-20, 2021, New Paltz, NY AUDIO TRANSFORMERS:</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although not the norm and in wide circulation, a fully connected layer is also a convolution operation with no stride and receptive field size equal to the size of the input data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A framework for contrastive and generative learning of audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11459</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Progen: Language modeling for protein generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03497</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Music transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Jukebox: A generative model for music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural style transfer for audio spectograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01589</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neuralogram: A deep neural network based representation for audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chafe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05073</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Selfsupervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Conditional end-to-end audio transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-linguistic embeddings for spoken sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7355" to="7359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fsd50k: an open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00475</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scipy 1.0: fundamental algorithms for scientific computing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-4012" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Removing noise from music using local trigonometric bases and wavelet packets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Audio Engineering Society</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="808" to="818" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frequency estimation from waveforms using multi-layered neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2165" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language through a prism: A spectral approach for multiscale language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>{OSDI} 16</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2101.03961" />
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
