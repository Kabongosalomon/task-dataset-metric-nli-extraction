<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadarsh</forename><surname>Sahoo</surname></persName>
							<email>sahooaadarsh@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
							<email>rpanda@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
							<email>rsferis@us.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iit</forename><surname>Kharagpur</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability and domain invariance in the latent space. To alleviate the above issues, we develop a novel 'Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. First, we present a simple yet efficient "select" module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the "label" module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space. Finally, the "mix" module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed framework over state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks usually do not generalize well to domains that are not distributed identically to the training data. Domain adaptation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> addresses this problem by transferring knowledge from a label-rich source domain to a target domain where labels are scarce or unavailable. Despite impressive results on commonly used benchmark datasets, domain adaptation algorithms often assume that the source and target domains share the same label space <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. However, since largescale labelled datasets are readily accessible as source domain data, a more realistic scenario is partial domain adaptation (PDA), which assumes that the target label space is a subset of the source label space, that has received increasing research attention recently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Several methods have been proposed to solve partial domain adaptation by reweighting source samples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>. However, (1) most of the existing methods still suffer from negative transfer due to the presence of outlier source domain classes, which cripples domain-wise transfer with untransferable knowledge; <ref type="bibr" target="#b1">(2)</ref> in absence of labels, they often neglect the class-aware information in the target domain which fails to guarantee the discriminability of the latent space; and (3) given filtering of the outliers, samples from the source and the target domain are not alone sufficient to learn domain invariant features for such a complex problem. As a result, the domain classifier may falsely align unlabeled target samples with samples of a different class in the source domain, leading to inconsistent predictions.</p><p>To address the above mentioned challenges, we propose a novel end-to-end Select, Label, and Mix (SLM) framework for learning discriminative invariant feature representation while preventing negative transfer in partial domain adaptation. Our framework consists of three unique modules working in concert, i.e., select, label and mix, as shown in <ref type="figure">Figure 1</ref>. First, the select module facilitates the identification of relevant source samples while preventing the negative transfer of untransferrable knowledge. To be specific, our main idea is to learn a model (referred to as the selector network) that outputs the posterior probabilities of all the binary decisions for selecting or discarding each source domain sample before aligning source and target distributions using an adversarial discriminator <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. As these decision functions are discrete and non-differentiable, we rely on a recent Gumbel Softmax sampling approach <ref type="bibr" target="#b19">[20]</ref> to learn the policy jointly with the network parameters through standard back-propagation, without resorting to complex reinforcement learning settings, as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Second, we develop an efficient self-labeling strategy that iteratively trains the classifier using both the labeled source domain data and the generated soft pseudo-labels for the target domain to enhance the discriminabilty of the latent space. Finally, the mix module utilizes both intra-domain and interdomain mixup regularization <ref type="bibr" target="#b47">[48]</ref> to generate convex com-Source Domain Target Domain Adaptation: <ref type="figure">Figure 1</ref>: A conceptual overview of our approach. Our approach adopts three unique strategies namely Select, Label and Mix in a unified framework to mitigate domain shift and generalize the model to an unlabelled target domain possessing a label space which is subset of that of the labelled source domain. Our Select module discards outlier samples from the source domain to eliminate negative transfer of untransferable knowledge. On the other hand, Label and Mix modules ensure discriminability and invariance of the latent space respectively while adapting the source classifier to the target domain in partial domain adaptation. Best viewed in color.</p><p>binations of pairs of training samples and their corresponding labels in both domains. Our proposed mix strategy not only helps to explore more intrinsic structures across domains leading to an invariant latent space, but also helps to stabilize the domain discriminator while bridging the distribution shift across domains. In each mini-batch, our unified framework simultaneously eliminates negative transfer by removing outlier source samples and learns discriminative invariant features by labeling and mixing samples respectively for partial domain adaptation. Experiments on several datasets illustrate the effectiveness of our proposed framework with state-of-the-art performance (e.g., our approach outperforms DRCN <ref type="bibr" target="#b20">[21]</ref> (TPAMI'20) by 18.91% on the challenging VisDA-2017 <ref type="bibr" target="#b32">[33]</ref> benchmark). To summarize, our key contributions include:</p><p>• We propose a novel Select, Label, and Mix (SLM) framework for learning discriminative and invariant feature representation while preventing intrinsic negative transfer in partial domain adaptation. • We develop a simple and efficient source sample selection strategy where the selector network is jointly trained with the domain adaptation model using backpropagation through Gumbel Softmax sampling. • We conduct extensive experiments on several PDA benchmark datasets, including Office31 <ref type="bibr" target="#b34">[35]</ref>, Office-Home <ref type="bibr" target="#b40">[41]</ref>, ImageNet-Caltech, and VisDA-2017 <ref type="bibr" target="#b32">[33]</ref> to demonstrate the superiority of our proposed approach over state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Domain Adaptation. Unsupervised domain adaptation which aims to leverage labeled source domain data to learn to classify unlabeled target domain data has been studied from multiple perspectives (see reviews <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref>). Various strategies have been developed, including methods for reducing the cross-domain divergence <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, adding domain discriminators for adversarial training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>, and imageto-image translation techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. Despite remarkable progress, UDA methods assume that label spaces across source and target domains are identical unlike the practical problem we consider in this work.</p><p>Partial Domain Adaptation. Representative PDA methods train domain discriminators <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b48">49]</ref> with weighting, or use residual correction blocks <ref type="bibr" target="#b20">[21]</ref>, or weight source examples based on their similarities to target domain <ref type="bibr" target="#b4">[5]</ref>. Most relevant to our approach is the work in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> which uses Reinforcement Learning (RL) for source data selection in partial domain adaptation. RL policy gradients are often complex, unwieldy to train and require techniques to reduce variance during training. By contrast, our approach utilizes a gradient based optimization for relevant source sample selection which is extremely fast and computationally efficient. Moreover, while prior PDA methods try to reweigh source samples in some form or other, they often do not take class-aware information in target domain into consideration. Our approach instead, ensures discriminability and invariance of the latent space by considering both pseudolabeling and cross-domain mixup with sample selection in an unified framework for partial domain adaptation.</p><p>Self-training with Pseudo-Labels. Deep self-training methods that focus on iteratively training the model by using both labeled source data and generated target pseudolabels have been proposed for aligning both domains <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50]</ref>. While majority of the methods directly choose hard pseudo-labels with high prediction confidence, the works in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> use class-balanced confidence regularizers to generate soft pseudo-labels for unsupervised domain adaptation that share same label space across domains. Our work on the other hand iteratively utilizes soft pseudo-labels within a batch by smoothing one-hot pseudo-label to a conservative target distribution for partial domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extractor</head><p>Selector Network  <ref type="figure">Figure 2</ref>: Illustration of our proposed framework. Our framework consists of a feature extractor G which maps the images to a common latent feature space, a classifier network F to provide class-wise predictions, a domain discriminator D to reduce domain discrepancy, and a selector network H for discarding outlier source samples ("Select") to mitigate the problem of negative transfer in partial domain adaptation. Our approach also comprises of two additional modules namely "Label" and "Mix" that works in conjunction with the "Select" module to ensure the discriminability and domain invariance of the latent space. Given a mini-batch of source and target domain images, all the components are optimized jointly in an iterative manner. See Section 3 for more details. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Domain Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Domain Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-Domain MixUp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Source MixUp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Selection</head><p>Mixup Regularization. Mixup regularization <ref type="bibr" target="#b47">[48]</ref> or its variants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref> that train models on virtual examples constructed as convex combinations of pairs of inputs and labels are recently used in many computer vision tasks to improve the generalization of neural networks. A few very recent methods apply Mixup, but mainly for UDA to stabilize the domain discriminator <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> or to smoothen the predictions <ref type="bibr" target="#b27">[28]</ref>. Our proposed SLM strategy can be regarded as an extension of this line of research by introducing both intra-domain and inter-domain mixup not only to stabilize the discriminator but also to guide the classifier in enriching the intrinsic structure of the latent space to solve the more challenging partial domain adaptation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Partial domain adaptation aims to mitigate the domain shift and generalize the model to an unlabelled target domain with a label space which is a subset of that of the labelled source domain. Formally, given a set of labelled source domain samples</p><formula xml:id="formula_0">D source = {(x s i , y i )} N S i=1 and unla- belled target domain samples D target = {x t i } N T i=1</formula><p>, with label spaces L source and L target , respectively, where L source L target . Let p and q represent the probability distribution of data in source and target domain respectively. In case of PDA, we further have p = q and p Ltarget = q, where p Ltarget is the distribution of source domain data in L target . Our goal is to develop an approach with the above given data to improve the performance of a model on D target . <ref type="figure">Figure 2</ref> illustrates an overview of our proposed approach. Our framework consists of a feature extractor G, a classifier network F, a domain discriminator D and a selector network H. Our goal is to improve the classification performance of the combined network F(G(.)) on D target . While the feature extractor G maps the images to a common latent space, the task of the classifier F is to output a probability distribution over the classes for a given feature from G. Given a feature from G, the discriminator D helps in minimizing the domain discrepancy by identifying the domain (either source or target) to which it belongs. The selector network H helps in reducing negative transfer by identifying outlier source samples from D source . On the other hand, label module utilizes the predictions of F(G(.)) to obtain the soft pseudo-labels for the target domain samples. Finally, the mix module leverages both pseudo-labeled target samples and source samples to generate augmented images for achieving a domain invariance in the latent space. During training, for a mini-batch of images, all the components are trained jointly in an iterative manner and during testing, we evaluate the performance using classification accuracy of the network F(G(.)) on target domain data D target .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Approach Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Select, Label, and Mix Framework</head><p>Our approach adopts three unique modules namely Select, Label and Mix in a unified framework for learning discriminative invariant feature representation while eliminating negative transfer in partial domain adaptation. The individual modules are discussed below. Select Module. This module aims to identify and get rid of the outlier class samples in the source domain to minimize negative transfer in partial domain adaptation. Instead of using different heuristically designed criteria for weighting source samples, we develop a novel selector network H, that takes images from the source domain as input and decides the relevant source samples to align with the target samples. Specifically, the selector network H performs robust selection by providing a discrete output of either a 0 (discard) or 1 (select) for each source sample, i.e., H : D source → {0, 1}. However, the fact that the decision policy is discrete makes the network non-differentiable and therefore difficult to optimize via standard backpropagation. To resolve the non-differentiability and enable gradient descent optimization for the selector, we adopt Gumbel-Softmax distribution <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref> that uses the Gumbel-Max trick with softmax as a continuous relaxation to argmax. For a binary decision space, as in our case (select or discard), the formulation is as follows:</p><formula xml:id="formula_1">Y i = exp((log α i +G i )/τ ) j∈{0,1} exp((log α j +G j )/τ ) for i ∈ {0, 1} (1)</formula><p>where G i 's are i.i.d samples from standard Gumbel distribution Gumbel(0, 1) = −log(−log(U )), where U ∼ Uniform[0, 1], α 0 and α 1 are output probability of the selector network for a sample to be selected and discarded respectively. τ denotes temperature of the softmax. Clearly, when τ &gt; 0, the Gumbel-Softmax distribution is smooth and hence gradients can be computed with respect to logits α i 's to train the selector network using backpropagation. As τ approaches 0, the decision Y i becomes one-hot and discrete. Note that to avoid any interference from the backbone feature extractor G, we use a separate feature extractor for the select module, while making these decisions. For a given batch of source domain images D b source and target domain images D b target , of size b, the selector results in two subsets of source samples</p><formula xml:id="formula_2">D b sel = {x ∈ D b source : H(x) = 1} and D b dis = {x ∈ D b source : H(x) = 0}.</formula><p>For training the selector, we propose a triplet loss L select that tries</p><formula xml:id="formula_3">to bring D b sel &amp; D b target closer while pushing apart D b dis &amp; D b</formula><p>target in the latent feature space of G as follows:</p><formula xml:id="formula_4">d sel = d H (G(D b sel ), G(D b target )) d dis = d H (G(D b dis ), G(D b target )) L select = λ s max(d sel − d dis + margin, 0) + L reg (2)</formula><p>where d H (X, Y ) represents the average Hausdorff distance between set of features X and Y .</p><formula xml:id="formula_5">L reg = λ reg1 x∈D b source H(x) log(H(x)) + λ reg2 { p l ent (p) − l ent (p m )},</formula><p>with l ent being the entropy loss,p is the Softmax prediction of F(G(D target )) andp m is mean prediction of outputp. L reg is a regularization to restrict H from producing trivial all-0 or all-1 outputs as well as ensuring confident and diverse predictions by F(G(.)) for D target . During forward pass, we obtain discrete decisions for source images to discard the outliers, and during backward pass, we learn the parameters of selector network using Gumbel-Softmax while jointly training it with other components. Label Module. While our select module helps in removing source domain outliers, it fails to guarantee the discriminability of the latent space due to the absence of classaware information in the target domain. Specifically, given our main objective is to improve the classification performance on target domain samples, it becomes essential for the classifier to learn confident decision boundaries in the latent feature space. To this end, we propose a label module that provides additional self-supervision for target domain samples. Motivated by the effectiveness of confidence guided self-training <ref type="bibr" target="#b50">[51]</ref>, we generate soft pseudo-labels for the target domain samples that efficiently attenuates the unwanted deviations caused by false and noisy one-hot pseudo-labels. For a target domain sample x t k ∈ D target , the soft-pseudo-labelŷ t k is computed as follows:</p><formula xml:id="formula_6">y t(i) k = p(i|x t i ) 1 α |Lsource| j=1 p(j|x t i ) 1 α (3) where p(j|x t i ) = F(G(x t i )) (j)</formula><p>is the softmax probability of the classifier for class j given x t i as input, and α is a hyperparameter that controls the softness of the label. The soft pseudo-labelŷ t i is then used to compute the loss L label for a given batch of target samples D b target as follows:</p><formula xml:id="formula_7">L label = E x t i ∈D b target l ce (F(G(x t i )),ŷ t i )<label>(4)</label></formula><p>where l ce (.) represents the cross-entropy loss. Mix Module. Learning a domain-invariant latent space is crucial for effective adaptation of a classifier from source to target domain. However, with limited samples per batch and after discarding the outlier samples, it becomes even more challenging in preventing over-fitting and learning domain invariant representation in partial domain adaptation.</p><p>To mitigate this problem, we apply MixUp <ref type="bibr" target="#b47">[48]</ref> on the relevant source samples and the target samples for discovering ingrained structures in establishing domain invariance. Given D b sel from the select module and D b target with corresponding labelsŷ t from the label module, we perform convex combinations of the images belonging to these two sets on pixel-level in three different ways namely, inter-domain, intra-source domain and intra-target domain to obtain the following sets of augmented data:</p><formula xml:id="formula_8">D b inter mix = {(λx s i + (1−λ)x t j , λy i + (1−λ)ŷ t j )} D b intra mix s = {(λx s i + (1−λ)x s j , λy i + (1−λ)y j )} D b intra mix t = {(λx t i + (1−λ)x t j , λŷ t i + (1−λ)ŷ t j )} D b mix = D b inter mix ∪ D b intra mix s ∪ D b intra mix t (5) where (x s i/j , y i/j ) ∈ D b sel , while x t i/j ∈ D b target withŷ t i/j</formula><p>being the corresponding soft-pseudo-labels. λ is the mixratio randomly sampled from a beta distribution Beta(α, α) for α ∈ (0, ∞). We use α = 2.0 in all our experiments. We utilize the new augmented images in training both the classifier F and the domain discriminator D as follows:</p><formula xml:id="formula_9">L mix cls = E (xi,yi)∈D b mix l ce (F(G(x i )), y i ) L mix dom = E xi∼D b inter mix {λ log(D(G(x i ))) + (1−λ) log(1−D(G(x i )))} + E xi∼D b intra mix s log(D(G(x i ))) + E xi∼D b intra mix t log(1−D(G(x i ))) L mix = L mix cls + L mix dom<label>(6)</label></formula><p>where L mix cls and L mix dom represent loss for classifier and domain discriminator respectively. Our mix strategy with the combined loss L mix not only helps to explore more intrinsic structures across domains leading to an invariant latent space, but also helps to stabilize the domain discriminator while bridging the distribution shift across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>Besides the above three unique modules that are tailored for PDA, we use the standard supervised loss on the labeled source data and domain adversarial loss as follows:</p><formula xml:id="formula_10">L sup = E (xi,yi)∈D b sel l ce (F(G(x i )), y i ) L adv = E x s ∼D b sel w s log(D(G(x s ))) + E x t ∼D b target w t log(1−D(G(x t )))<label>(7)</label></formula><p>where L adv is standard entropy-conditioned domain adversarial loss with weights w s and w t for source and target domain respectively <ref type="bibr" target="#b23">[24]</ref>. The overall loss L total is</p><formula xml:id="formula_11">L total = L sup + L adv + L select + L label + L mix (8)</formula><p>where L select , L label , and L mix are given by Equations <ref type="formula">(2)</ref>, (4), and (6) respectively. We integrate all the modules into one framework, as shown in the <ref type="figure">Figure 2</ref> and train the network jointly for partial domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments to show that our SLM framework outperforms many competing approaches to achieve the new state-of-the-art on several PDA benchmark datasets. We also perform comprehensive ablation experiments and feature visualizations to verify the effectiveness of different components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We evaluate the performance of our approach using several standard domain adaptation datasets under PDA setting, namely Office31 <ref type="bibr" target="#b34">[35]</ref>, Office-Home <ref type="bibr" target="#b40">[41]</ref> ImageNet-Caltech and VisDA-2017 <ref type="bibr" target="#b32">[33]</ref>. Office31 contains 4,110 images of 31 classes from three distinct domains, namely Amazon (A), Webcam (W) and DSLR (D). We follow the setting in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref> and select 10 classes shared by Of-fice31 and Caltech256 <ref type="bibr" target="#b13">[14]</ref> as target categories. Office-Home is a challenging dataset that contains images of everyday objects from four domains: Artistic images (Ar), Clipart images (Cl), Product images (Pr) and Real-World images (Rw). We follow <ref type="bibr" target="#b8">[9]</ref> to select the first 25 categories (in alphabetic order) in each domain as the target classes. ImageNet-Caltech is another challenging dataset that consists of two subsets, ImageNet1K (I) <ref type="bibr" target="#b33">[34]</ref> and Caltech256 (C) <ref type="bibr" target="#b13">[14]</ref>. While source domain contains 1,000 and 256 classes for ImageNet and Caltech respectively, each target domain contains only 84 classes that are common across both domains. VisDA-2017 is a large-scale dataset with 12 categories across 2 domains: one consists photo-realistic images or real images (R) and the other comprises of synthetic 2D renderings of 3D models (S). Following <ref type="bibr" target="#b20">[21]</ref>, we select the first 6 categories (in alphabetical order) in each of the domain as the target categories. Baselines. We compare our approach with several methods that fall into two main categories: (1) popular unsupervised domain adaptation methods like DAN <ref type="bibr" target="#b22">[23]</ref>, DANN <ref type="bibr" target="#b11">[12]</ref>, and CORAL <ref type="bibr" target="#b38">[39]</ref>, (2) existing partial domain adaptation methods including PADA <ref type="bibr" target="#b3">[4]</ref>, SAN <ref type="bibr" target="#b2">[3]</ref>, ETN <ref type="bibr" target="#b4">[5]</ref>, and DRCN <ref type="bibr" target="#b20">[21]</ref>. We also compare with the recent state-of-theart method, RTNet <ref type="bibr" target="#b8">[9]</ref> (CVPR'20) that uses reinforcement learning for source dataset selection in partial domain adaptation. We directly quote the numbers reported in published papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref> and use the same backbone network in our approach to make a fair comparison with different baselines. Implementation Details. We use ResNet-50 <ref type="bibr" target="#b14">[15]</ref> as the backbone network for the feature extractor while we use ResNet-18 for the selector network, initialized with Ima-geNet <ref type="bibr" target="#b33">[34]</ref> pretrained weights. In Eqn. 2 we set λ s , λ reg1 and λ reg2 as 0.01, 10.0 and 0.1, respectively. We use gradient reversal layer (GRL) for adversarially training the discriminator. We set τ = 1.0 in Eqn. 1, α = 0.1 in Eqn. 3, and λ = 0.0 for the GRL as initial values and gradually anneal τ and α down to 0 while increase λ to 1.0 during the training, as in <ref type="bibr" target="#b19">[20]</ref>. We set a threshold of 0.3 while obtaining the soft pseudo-labels for all the datasets except for the case when ImageNet was used as the source domain where  highlight the best and second best method on each transfer task. While the upper section shows the results of some popular unsupervised domain adaptation approaches, the lower section shows results of existing partial domain adaptation methods. Our proposed framework, SLM achieves the best performance on 5 out of 6 transfer tasks including the best average performance among all compared methods.</p><formula xml:id="formula_12">Office31 Method A → W D → W W → D A → D D → A W → A</formula><p>we did not use any threshold value. Additionally, we use label-smoothing for all the losses for the feature extractor involving source domain images as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>, with = 0.2. We use SGD for optimization with momentum=0.9 while a weight decay of 1e-3 and 5e-4 for the selector network and the other networks respectively. We use an initial learning rate of 5e-3 for the selector and the classifier, while 5e-4 for the rest of the networks and decay it following a cosine annealing strategy. A batch size of 64 is used for all datasets except ImageNet-Caltech for which a batch size of 128 is used. We report average classification accuracy and standard deviation over 3 random trials. More implementation details are included in Appendix and we will make our source codes publicly available at https://github. com/CVIR/Select-Label-Mix-SLM-PDA. <ref type="table" target="#tab_2">Table 1</ref> shows the results of our method and other competing approaches on Office31 dataset. We have the following key observations. (1) As expected, the popular UDA methods fail to outperform the simple no adaptation model (ResNet-50), which implies that they suffer from negative transfer due to the presence of outlier source samples. <ref type="bibr" target="#b1">(2)</ref> Overall, our SLM framework outperforms all the existing PDA methods by achieving the best results on 5 out of 6 transfer tasks. Among PDA methods, RTNet adv <ref type="bibr" target="#b8">[9]</ref> is the most competitive. However, the gap is still significant (96.9% vs 98.3%) due to our two novel components working in concert with the removal of outliers: enhanc-ing discriminability of the latent space via iterative pseudolabeling of target domain samples and learning domaininvariance through mixup regularizations. (3) Our approach performed remarkably well on transfer tasks from a large source domain to small target domains, e.g., on A→W, SLM outperforms RTNet adv by 3.5%. Likewise, our approach obtains 3.8% improvement over RTNet adv on D → A task where the source domain is very small compared to the target domain. These results well demonstrate that our method improves the generalization ability of the source classifier in the target domain while reducing the negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analysis</head><p>On the challenging Office-Home dataset, our proposed approach significantly outperforms all the compared methods to obtain the best performance of 75.3% which is about 3% more than the previous state-of-the-art performance on this dataset <ref type="table" target="#tab_4">(Table 2</ref>). Our method obtains the best on 9 out of 12 transfer tasks. <ref type="table" target="#tab_5">Table 3</ref> summarizes the results of different baselines on ImageNet-Caltech and VisDA-2017 datasets. Our approach achieves new state-of-the-art result, outperforming the next competitive method by a margin of about 2.8% and 18.9% on ImageNet-Caltech and VisDA-2017 datasets respectively. Especially for task S → R on VisDA-2017, our approach significantly outperforms SAFN <ref type="bibr" target="#b45">[46]</ref> and DRCN <ref type="bibr" target="#b20">[21]</ref> by an increase of 24.1% and 33.5% respectively. Note that on the most challenging VisDA-2017 dataset, our approach is still able to distill more positive knowledge from the synthetic to the real domain despite significant domain gap across them. In summary, our SLM framework clearly outperforms the existing   PDA methods on all four datasets, showing the effectiveness of our approach in not only identifying the most relevant source classes but also learning more transferable features for partial domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We perform the following experiments to test the effectiveness of the proposed modules including the effect of number of target classes on different datasets.</p><p>Effectiveness of Individual Modules. We conduct experiments to investigate the importance of our three unique modules on Office-Home dataset. As seen from <ref type="table" target="#tab_7">Table 4</ref>, while the Select only module improves the vanilla performance by 8%, addition of Label and Mix modules progressively improves the result to obtain the best performance of 75.29% on Office-Home dataset. This corroborates the fact that both discriminability and invariance of the latent space plays a crucial role in partial domain adaptation in addition to the removal of source domain outlier samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Varying Number of Target Classes.</head><p>We compare different methods by varying the number of target classes. <ref type="figure" target="#fig_1">Figure 4</ref> shows that our SLM framework consistently obtains the best results indicating its advantage in alleviating negative transfer by removing outlier source samples. Moreover, our approach outperforms all the compared methods even in the case of completely shared space (A31 → W31), which shows that it does not discard relevant samples incorrectly when there are no outlier classes.</p><p>Effectiveness of Hausdorff Distance. We investigate the effect of Hausdorff distance (Eqn. 2) in training the selector network and find that by removing it from the total loss lowers down the performance from 75.29% to 73.67% on Office-Home dataset, showing its importance in guiding the selector network to discard the outlier source samples for effective reduction in negative transfer.</p><p>Distance between Domains. Following <ref type="bibr" target="#b8">[9]</ref>, we compute the Wasserstein distance between the probability distribution of the target samples (T) with that of the selected (S sel ) and discarded samples (S dis ) by the selector network. <ref type="table">Table 5</ref> shows that dist(S sel , T) is smaller than dist(S all , T), while dist(S dis , T) is greater than dist(S all , T) on two randomly sampled adaptation tasks from Office31 and Office-Home datasets. This results indicate that the samples selected by our selector network is closer to the target domain while the discarded samples are very dissimilar to the target domain.</p><p>Effectiveness of Soft Pseudo-Labels. We also test the effectiveness of soft pseudo-labels by replacing them with hard pseudo-labels for the target samples and observe that hard pseudo-labels decreases the average performance from 75.29% to 71.96% on Office-Home dataset. This confirms that soft pseudo-labels are critical for good performance as it efficiently attenuates the unwanted deviations caused by the false and noisy hard pseudo-labels.    tions working for both discriminator and classifier, the average performance on Office-Home dataset is 75.29%. By removing mixup regularization from the training of domain discriminator, the performance decreases to 73.58%. Similarly, by removing mixup regularization from the classifier training, the average performance becomes 73.85%. This corroborates the fact that our Mix strategy not only helps to explore intrinsic structures across domains, but also helps to stabilize the domain discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Different</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Feature Visualizations</head><p>We use t-SNE <ref type="bibr" target="#b25">[26]</ref> to visualize the features learned using different components of our SLM framework. We choose an UDA setup (similar to DANN <ref type="bibr" target="#b11">[12]</ref>) as a vanilla method and add different modules one-by-one to visualize the contribution of each of the modules in learning discriminative features for partial domain adaptation. As can be seen from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance</head><p>A → D W → A Cl → Pr Rw → Pr dist(S sel , T) 0.999 0.893 0.819 0.947 dist(S dis , T) 1.013 1.144 1.418 1.008 <ref type="table">Table 5</ref>: Wasserstein Distance between Domains. <ref type="table">Table shows</ref> values for two randomly sampled tasks from Office-31 and Office-Home. The values are normalized by assuming the distance for dist(S all , T) to be equal to 1.0, where S all represents all source samples for the corresponding tasks. Numbers show that samples selected by our selector network is closer to the target domain while discarded samples are very dissimilar to the target domain. <ref type="figure" target="#fig_0">Figure 3</ref>, the feature space for the vanilla setup lacks dicriminability for both source and target features. The discriminability improves for both source as well as the target features as we add "Select" and "Label" to the Vanilla setup. The best results are obtained when all the three modules "Select", "Label" and "Mix" i.e. SLM are added and trained jointly in an end-to-end manner. Additional results, discussions and more visualization are included in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an end-to-end framework for learning discriminative invariant feature representation while preventing negative transfer in partial domain adaptation. While our select module facilitates the identification of relevant source samples for adaptation, the label module enhances the discriminability of the latent space by utilizing pseudo-labels for the target domain samples. The mix module uses mixup regularizations jointly with the other two strategies to enforce domain invariance in latent space. We demonstrate the effectiveness of our approach on four standard datasets, outperforming several competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Details</head><p>We evaluate the performance of our approach on several benchmark datasets for partial domain adaptation, namely Office31 <ref type="bibr" target="#b34">[35]</ref>, Office-Home <ref type="bibr" target="#b40">[41]</ref>, ImageNet-Caltech and VisDA-2017 <ref type="bibr" target="#b32">[33]</ref>. The following are the detailed descriptions of the above datasets:</p><p>Office31. This dataset contains 4,110 images distributed among 31 different classes and collected from three different domains: Amazon (A), Webcam (W) and DSLR (D), resulting in 6 transfer tasks. The dataset is imbalanced across domains with 2,817 images belonging to Amazon, 795 images to Webcam, and 498 images to DSLR, making Amazon a larger domain as compared to Webcam and DSLR. For all our experiments, we select the 10 classes shared by Office31 and Caltech256 <ref type="bibr" target="#b13">[14]</ref> as the target categories and obtain the following label spaces: <ref type="bibr">1, 5, 10, 11, 12, 15, 16, 17, 22}</ref>. Number of Outlier Classes = 21. <ref type="figure" target="#fig_2">Figure 5</ref> shows few randomly sampled images from this dataset.</p><formula xml:id="formula_13">L source = {0, 1, 2, ..., 30}. L target = {0,</formula><p>The dataset is publicly available to download at: https://people.eecs.berkeley.edu/ jhoffman/domainadapt/#datasets_code. Office-Home. This dataset contains 15,588 images distributed among 65 different classes and collected from four different domains: Art (Ar), Clipart (Cl), Product (Pr), and RealWorld (Rw), resulting in 12 transfer tasks. The dataset is split across domains with 2427 images belonging to Art, 4365 images to Clipart, 4439 images to Product, and 4347 images to RealWorld. We select the first 25 categories (in alphabetic order) in each domain as the target classes and obtain the following label spaces: L source = {0, 1, 2, ..., 64}. L target = {0, 1, 2, ..., 24}.</p><p>Number of Outlier Classes = 40. <ref type="figure">Figure 6</ref> displays a gallery of sample images for this dataset.</p><p>The dataset is publicly available to download at: http: //hemanthdv.org/OfficeHome-Dataset/.   The dataset is publicly available to download at: http://ai.bu.edu/visda-2017/#download.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-Caltech. This large-scale dataset consists of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Following are the detailed description of the implementation we follow for various components of the framework:</p><p>Feature Extractor (G). We use ResNet-50 <ref type="bibr" target="#b14">[15]</ref> backbone for the feature extractor. The overall structure of ResNet-50 is Initial Layers, Layer-1, Layer-2, Layer-3, Layer-4, AvgPool, Fc. The model is initialized with ImageNet <ref type="bibr" target="#b33">[34]</ref> pretrained weights. Additionally, we add a bottleneck layer of width 256 just after the AvgPool layer to obtain the features and replace all the BatchNorm layers with Domain-Specific Batch-Normalization <ref type="bibr" target="#b5">[6]</ref> layers. All the layers till Layer-3 are frozen and only the rest of the layers are fine-tuned.</p><p>Selector Network (H). We use a ResNet-18 <ref type="bibr" target="#b14">[15]</ref> network with the Fc layer replaced with a binary-length fully connected layer as the selector network in our framework. The network is initialized with ImageNet pretrained weights and all the layers are trained while optimization.</p><p>Classifier (F). The final Fc layer of ResNet-50 described above is replaced with a task-specific fully-connected layer to form the classifier network of our framework.</p><p>Domain Discriminator (D). A three-layer fully-connected network is used as the domain discriminator network. It takes the 256-length features obtained from the feature extractor as input. The adversarial training is incorporated using a gradient reversal layer (GRL).</p><p>Hyperparameters. All the networks are optimised with using mini-batch stochastic gradient descent with a momentum of 0.9. A batch size of 64 is used for Office31, Office-Home and VisDA-2017, while a batch size of 128 is used for ImageNet-Caltech. For feature extractor an initial learning rate of 5e-5 for the convolutional layers while an initial learning rate of 5e-4 for all the fully-connected layers is used. For the selector network and the domain discriminator an initial learning rate of 5e-3 and 5e-4 are used respectively. The learning rates are decayed following a cosineannealing strategy as the training progresses. The best models are captured by obtaining the performance on a validation set. We do not follow the ten-crop technique <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, to improve the performance in the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results</head><p>Effectiveness on Different Backbone Networks. To show that the proposed framework is backbone-agnostic, i.e. it provides the best performance irrespective of the architecture of the feature extractor, we conduct experiments using a VGG-16 <ref type="bibr" target="#b37">[38]</ref> backbone for the feature extractor. We report the results on the transfer tasks from the Office31 dataset in <ref type="table" target="#tab_10">Table 6</ref> and compare it with the current state-ofthe-art methods. Our method outperforms the previously best results by a margin of 3.05% on average and achieves new state-of-the-art results. This confirms that our proposed framework for partial domain adaptation is robust with respect to the change of backbone network. Effectiveness of Individual Modules. In Section 4.3 of the main paper, we discussed the importance of the proposed three unique modules on Office-Home dataset. Here, we extend the experiments to Office31 and VisDA-2017 and provide the performance on the transfer tasks in <ref type="table" target="#tab_11">Table 7</ref>. Similar to the results on Office-Home dataset, our approach with all the three modules (Select, Label and Mix) working jointly, works the best on both datasets. Effectiveness of Hausdorff Distance. In Section 4.3 of the main paper, we discussed the importance of Hausdorff Distance loss in guiding the selector network to discard the outlier source samples. Here we provide the individual performance of all the transfer tasks on Office-Home dataset in <ref type="table" target="#tab_12">Table 8</ref>, which shows that our approach with Hausdorff distance loss works the best in all cases. Effectiveness of Soft Pseudo-Labels. As discussed in the Section 4.3, we confirmed the importance of soft pseudolabels for our framework as it attenuates the unwanted deviations because of noisy and false hard pseudo-labels. Here, we provide the performance on each of the transfer tasks from Office-Home in <ref type="table" target="#tab_13">Table 9</ref>. Effectiveness of Different MixUp. We examined the effect of mixup regularization on both domain discriminator and classifier separately in Section 4.3 of the main paper. We concluded that our Mix strategy not only helps to explore intrinsic structures across domains, but also helps to stabilize the domain discriminator. Here, we provide the corresponding performance on each of the transfer tasks of Office-Home in <ref type="table" target="#tab_2">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>Feature Visualizations. We provide some additional feature visualizations using t-SNE <ref type="bibr" target="#b25">[26]</ref> in <ref type="figure">Figure 9</ref>. Similar to Section 4.4 in the main paper, we choose an UDA setup as a vanilla method and add the proposed modules one-by-one to visualize the contribution of each of the modules in learning discriminative features for partial domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Office31</head><p>Method       <ref type="figure">Figure 9</ref>: Feature Visualizations using t-SNE. Plots show visualization of our approach with different modules on A→W, W→A, and D→A tasks repectively (top to down) from Office31 dataset. Blue and red dots represent source and target data respectively. As can be seen, features for both target as well as source domain become progressively discriminative and improve from left to right by adoption of our proposed modules. Best viewed in color.</p><formula xml:id="formula_14">A → W D → W W → D A → D D → A W → A</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Feature Visualizations using t-SNE. Plots show visualization of our approach with different modules on A→W task from Office-31 dataset. Blue and red dots represent source and target data respectively. As can be seen, features for both target as well as source domain become progressively discriminative and improve from left to right by adoption of our proposed modules. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Performance by varying the number of target classes on A→W task from Office31 dataset. Our SLM framework consistently obtains the best results. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Sampled Images from Office31 Dataset. Each row from top to bottom corresponds to the domains Amazon, Dslr and Webcam, respectively. The images in the same column belong to the same class. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Sampled Images from Office-Home Dataset. Each row from top to bottom corresponds to the domains Art, Clipart, Product and RealWorld, respectively. The images in the same column belong to the same class. Best viewed in color. ImageNet Caltech Sampled Images from ImageNet-Caltech Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 Figure 8 :</head><label>88</label><figDesc>displays a gallery of sample images for this dataset.Synthetic Real Sampled Images from VisDA-2017 Dataset. The top row corresponds to the Synthetic domain, while the bottom row to the Real domain. The images in the same column belong to the same class. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AverageResNet-50<ref type="bibr" target="#b14">[15]</ref> (CVPR'16)   76.5±0.3 99.2 ±0.2 97.7 ±0.1 87.5 ±0.2 87.2 ±0.1 84.1 ±0.3 88.7 DAN [23] (ICML'15) 53.6 ±0.7 62.7 ±0.5 57.8 ±0.6 47.7 ±0.5 61.2 ±0.6 69.7 ±0.5 58.8 DANN [12] (JMLR'16) 62.8 ±0.6 71.6 ±0.4 65.6 ±0.5 65.1 ±0.7 78.9 ±0.3 79.2 ±0.4 ±0.04 100.00 ±0.00 99.79 ±0.14 98.73 ±0.00 96.10 ±0.03 95.89 ±0.00 98.38</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.5</cell></row><row><cell>CORAL [39] (ECCV'16)</cell><cell>52.1 ±0.5</cell><cell>65.2 ±0.2</cell><cell>64.1 ±0.7</cell><cell>58.0 ±0.5</cell><cell>73.1 ±0.4</cell><cell>77.9 ±0.3</cell><cell>65.1</cell></row><row><cell>ADDA [40] (CVPR'16)</cell><cell>75.7 ±0.2</cell><cell>95.4 ±0.2</cell><cell>99.9 ±0.1</cell><cell>83.4 ±0.2</cell><cell>83.6 ±0.1</cell><cell>84.3 ±0.1</cell><cell>87.0</cell></row><row><cell>RTN [25] (NeurIPS'16)</cell><cell>75.3</cell><cell>97.1</cell><cell>98.3</cell><cell>66.9</cell><cell>85.6</cell><cell>85.7</cell><cell>84.8</cell></row><row><cell cols="2">CDAN+E [24] (NeurIPS'18) 80.5 ±1.2</cell><cell>99.0 ±0.0</cell><cell>98.1 ±0.0</cell><cell>77.1 ±0.9</cell><cell>93.6 ±0.1</cell><cell>91.7 ±0.0</cell><cell>90.0</cell></row><row><cell>JDDA [7] (AAAI'19)</cell><cell>73.5 ±0.6</cell><cell>93.1 ±0.3</cell><cell>89.3 ±0.2</cell><cell>76.4 ±0.4</cell><cell>77.6 ±0.1</cell><cell>82.8 ±0.2</cell><cell>82.1</cell></row><row><cell>PADA [4] (ECCV'18)</cell><cell>86.3 ±0.4</cell><cell>99.3 ±0.1</cell><cell>100 ±0.0</cell><cell>90.4 ±0.1</cell><cell>91.3 ±0.2</cell><cell>92.6 ±0.1</cell><cell>93.3</cell></row><row><cell>SAN [3] (CVPR'18)</cell><cell>93.9 ±0.5</cell><cell>99.3 ±0.5</cell><cell>99.4 ±0.1</cell><cell>94.3 ±0.3</cell><cell>94.2 ±0.4</cell><cell>88.7 ±0.4</cell><cell>95.0</cell></row><row><cell>IWAN [49] (CVPR'18)</cell><cell>89.2 ±0.4</cell><cell>99.3 ±0.3</cell><cell>99.4 ±0.2</cell><cell>90.5 ±0.4</cell><cell>95.6 ±0.3</cell><cell>94.3 ±0.3</cell><cell>94.7</cell></row><row><cell>ETN [5] (CVPR'19)</cell><cell>93.4 ±0.3</cell><cell>99.3 ±0.1</cell><cell>99.2 ±0.2</cell><cell>95.5 ±0.4</cell><cell>95.4 ±0.1</cell><cell>91.7 ±0.2</cell><cell>95.8</cell></row><row><cell>DRCN [21] (TPAMI'20)</cell><cell>88.1</cell><cell>100.0</cell><cell>100.0</cell><cell>86.0</cell><cell>95.6</cell><cell>95.8</cell><cell>94.3</cell></row><row><cell>RTNet [9] (CVPR'20)</cell><cell>95.1 ±0.3</cell><cell cols="3">100.0 ±0.0 100.0 ±0.0 97.8 ±0.1</cell><cell>93.9 ±0.1</cell><cell>94.1 ±0.1</cell><cell>96.8</cell></row><row><cell>RTNet adv [9] (CVPR'20)</cell><cell>96.2 ±0.3</cell><cell cols="3">100.0 ±0.0 100.0 ±0.0 97.6 ±0.1</cell><cell>92.3 ±0.1</cell><cell>95.4 ±0.1</cell><cell>96.9</cell></row><row><cell>SLM (Ours)</cell><cell>99.77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Performance on Office31. Numbers show the accuracy (%) of different methods on partial domain adaptation setting. We</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Cl Ar → Pr Ar → Rw Cl → Ar Cl → Pr Cl → Rw Pr → Ar Pr → Cl Pr → Rw Rw → Ar Rw → Cl Rw → Pr Average ResNet-50 [15] (CVPR'16) 47.2±0.2 66.8±0.3 76.9±0.5 57.6±0.2 58.4±0.1 62.5±0.3 59.4±0.3 40.6±0.2 75.9±0.3 65.6±0.1 49.1±0.2 75.8±0.4 61.3 DAN [23] (ICML'15) 35.7±0.2 52.9±0.4 63.7±0.2 45.0±0.3 51.7±0.3 49.3±0.1 42.4±0.2 31.5±0.4 68.7±0.1 59.7±0.3 34.6±0.4 67.8±0.1 50.3 DANN [12] (JMLR'16) 43.2±0.5 61.9±0.2 72.1±0.4 52.3±0.4 53.5±0.2 57.9±0.1 47.2±0.3 35.4±0.1 70.1±0.3 61.3±0.2 37.0±0.2 71.7±0.3 55.3 CORAL [39] (ECCV'16) 38.2±0.1 55.6±0.3 65.9±0.2 48.4±0.4 52.5±0.1 51.3±0.2 48.9±0.3 32.6±0.1 67.1±0.2 63.8±0.4 35.9±0.2 69.8±0.1 54±0.03 83.75±0.04 90.48±0.17 76.03±0.11 73.99±0.00 80.95±0.19 72.97±0.00 56.60±2.15 87.32±0.17 82.55±0.10 59.76±0.01 82.52±0.01 75.29</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Office-Home</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="13">Ar → 52.5</cell></row><row><cell>ADDA [40] (CVPR'16)</cell><cell>45.2</cell><cell>68.8</cell><cell>79.2</cell><cell>64.6</cell><cell>60.0</cell><cell>68.3</cell><cell>57.6</cell><cell>38.9</cell><cell>77.5</cell><cell>70.3</cell><cell>45.2</cell><cell>78.3</cell><cell>62.8</cell></row><row><cell>RTN [25] (NeurIPS'16)</cell><cell>49.4</cell><cell>64.3</cell><cell>76.2</cell><cell>47.6</cell><cell>51.7</cell><cell>57.7</cell><cell>50.4</cell><cell>41.5</cell><cell>75.5</cell><cell>70.2</cell><cell>51.8</cell><cell>74.8</cell><cell>59.3</cell></row><row><cell>CDAN+E [24] (NeurIPS'18)</cell><cell>47.5</cell><cell>65.9</cell><cell>75.7</cell><cell>57.1</cell><cell>54.1</cell><cell>63.4</cell><cell>59.6</cell><cell>44.3</cell><cell>72.4</cell><cell>66.0</cell><cell>49.9</cell><cell>72.8</cell><cell>60.7</cell></row><row><cell>JDDA [7] (AAAI'19)</cell><cell cols="12">45.8±0.4 63.9±0.2 74.1±0.3 51.8±0.2 55.2±0.3 60.3±0.2 53.7±0.2 38.3±0.1 72.6±0.2 62.5±0.1 43.3±0.3 71.3±0.1</cell><cell>57.7</cell></row><row><cell>PADA [4] (ECCV'18)</cell><cell cols="12">53.2±0.2 69.5±0.1 78.6±0.1 61.7±0.2 62.7±0.3 60.9±0.1 56.4±0.5 44.6±0.2 79.3±0.1 74.2±0.1 55.1±0.3 77.4±0.2</cell><cell>64.5</cell></row><row><cell>SAN [3] (CVPR'18)</cell><cell>44.4</cell><cell>68.7</cell><cell>74.6</cell><cell>67.5</cell><cell>65.0</cell><cell>77.8</cell><cell>59.8</cell><cell>44.7</cell><cell>80.1</cell><cell>72.2</cell><cell>50.2</cell><cell>78.7</cell><cell>65.3</cell></row><row><cell>IWAN [49] (CVPR'18)</cell><cell>53.9</cell><cell>54.5</cell><cell>78.1</cell><cell>61.3</cell><cell>48.0</cell><cell>63.3</cell><cell>54.2</cell><cell>52.0</cell><cell>81.3</cell><cell>76.5</cell><cell>56.8</cell><cell>82.9</cell><cell>63.6</cell></row><row><cell>ETN [5] (CVPR'19)</cell><cell cols="12">60.4±0.3 76.5±0.2 77.2±0.3 64.3±0.1 67.5±0.3 75.8±0.2 69.3±0.1 54.2±0.1 83.7±0.2 75.6±0.3 56.7±0.2 84.5±0.3</cell><cell>70.5</cell></row><row><cell>SAFN [46] (ICCV'19)</cell><cell cols="12">58.9±0.5 76.3±0.3 81.4±0.3 70.4±0.5 73.0±1.4 77.8±0.5 72.4±0.3 55.3±0.5 80.4±0.8 75.8±0.4 60.4±0.8 79.9±0.2</cell><cell>71.8</cell></row><row><cell>DRCN [21] (TPAMI'20)</cell><cell>54.0</cell><cell>76.4</cell><cell>83.0</cell><cell>62.1</cell><cell>64.5</cell><cell>71.0</cell><cell>70.8</cell><cell>49.8</cell><cell>80.5</cell><cell>77.5</cell><cell>59.1</cell><cell>79.9</cell><cell>69.0</cell></row><row><cell>RTNet [9] (CVPR'20)</cell><cell cols="12">62.7±0.1 79.3±0.2 81.2±0.1 65.1±0.1 68.4±0.3 76.5±0.1 70.8±0.2 55.3±0.1 85.2±0.3 76.9±0.2 59.1±0.2 83.4±0.3</cell><cell>72.0</cell></row><row><cell>RTNetadv [9] (CVPR'20)</cell><cell cols="12">63.2±0.1 80.1±0.2 80.7±0.1 66.7±0.1 69.3±0.2 77.2±0.2 71.6±0.3 53.9±0.3 84.6±0.1 77.4±0.2 57.9±0.3 85.5±0.1</cell><cell>72.3</cell></row><row><cell>SLM (Ours)</cell><cell>56.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance on Office-Home. We highlight the best and second best method on each task. While the upper section shows the results of unsupervised domain adaptation approaches, the lower section shows results of existing partial domain adaptation methods. Our SLM framework achieves the best performance on 9 out of 12 tasks including the best average performance among all compared methods. 31±0.02 81.41±0.40 81.86 77.48±0.74 91.74±0.68 84.61</figDesc><table><row><cell></cell><cell cols="2">ImageNet-Caltech</cell><cell></cell><cell></cell><cell>VisDA-2017</cell><cell></cell></row><row><cell>Method</cell><cell>I → C</cell><cell cols="3">C → I Average R → S</cell><cell cols="2">S → R Average</cell></row><row><cell cols="4">ResNet-50 [15] (CVPR'16) 69.69±0.78 71.29±0.74 70.49</cell><cell>64.28</cell><cell>45.26</cell><cell>54.77</cell></row><row><cell>DAN [23] (ICML'15)</cell><cell>71.57</cell><cell>66.48</cell><cell>69.03</cell><cell>68.35</cell><cell>47.60</cell><cell>57.98</cell></row><row><cell>DANN [12] (JMLR'16)</cell><cell>68.67</cell><cell>52.97</cell><cell>60.82</cell><cell>73.84</cell><cell>51.01</cell><cell>62.43</cell></row><row><cell>ADDA [40] (CVPR'16)</cell><cell cols="3">71.82±0.45 69.32±0.41 70.57</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RTN [25] (NeurIPS'16)</cell><cell>72.24</cell><cell>68.33</cell><cell>70.29</cell><cell>72.93</cell><cell>50.04</cell><cell>61.49</cell></row><row><cell>CDAN+E [24]</cell><cell cols="3">72.45±0.07 72.02±0.13 72.24</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PADA [4] (ECCV'18)</cell><cell cols="3">75.03±0.36 70.48±0.44 72.76</cell><cell>76.50</cell><cell>53.53</cell><cell>65.01</cell></row><row><cell>SAN [3] (CVPR'18)</cell><cell cols="3">77.75±0.36 75.26±0.42 76.51</cell><cell>69.70</cell><cell>49.90</cell><cell></cell></row><row><cell>IWAN [49] (CVPR'18)</cell><cell cols="3">78.06±0.40 73.33±0.46 75.70</cell><cell>71.30</cell><cell>48.60</cell><cell></cell></row><row><cell>ETN [5] (CVPR'19)</cell><cell cols="3">83.23±0.24 74.93±0.44 79.08</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAFN [46] (ICCV'19)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.65±0.51</cell><cell>-</cell></row><row><cell>DRCN [21] (TPAMI'20)</cell><cell>75.30</cell><cell>78.90</cell><cell>77.10</cell><cell>73.20</cell><cell>58.20</cell><cell>65.70</cell></row><row><cell>SLM (Ours)</cell><cell>82.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance on ImageNet-Caltech and VisDA-2017.</figDesc><table /><note>Our SLM framework achieves new state-of-the-art performance on both datasets by significantly outperforming existing methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Mixup. We examine the effect of mixup regularization on both domain discriminator and classifier on Office-Home dataset. With mixup regulariza-Cl Ar → Pr Ar → Rw Cl → Ar Cl → Pr Cl → Rw Pr → Ar Pr → Cl Pr → Rw Rw → Ar Pr → Cl Pr → Rw Average</figDesc><table><row><cell>Modules</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Office-Home</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Select Label</cell><cell>Mix Ar → 44.16</cell><cell>61.64</cell><cell>75.94</cell><cell>54.58</cell><cell>55.18</cell><cell>65.03</cell><cell>50.99</cell><cell>37.25</cell><cell>69.59</cell><cell>64.80</cell><cell>42.37</cell><cell>71.37</cell><cell>57.74</cell></row><row><cell></cell><cell>50.55</cell><cell>72.87</cell><cell>79.16</cell><cell>65.44</cell><cell>67.21</cell><cell>71.71</cell><cell>60.76</cell><cell>46.69</cell><cell>77.05</cell><cell>71.90</cell><cell>49.39</cell><cell>76.97</cell><cell>65.81</cell></row><row><cell></cell><cell>56.14</cell><cell>82.37</cell><cell>89.82</cell><cell>74.20</cell><cell>72.96</cell><cell>81.56</cell><cell>70.83</cell><cell>48.40</cell><cell>87.04</cell><cell>80.10</cell><cell>53.11</cell><cell>81.70</cell><cell>73.19</cell></row><row><cell></cell><cell>56.54</cell><cell>83.75</cell><cell>90.48</cell><cell>76.03</cell><cell>73.99</cell><cell>80.95</cell><cell>72.97</cell><cell>56.60</cell><cell>87.32</cell><cell>82.55</cell><cell>59.76</cell><cell>82.52</cell><cell>75.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of Different Modules on Office-Home Dataset. Our proposed approach achieves the best performance with all the modules working jointly for learning discriminative invariant features in partial domain adaptation.</figDesc><table><row><cell>Vanilla</cell><cell>Select</cell><cell>Select + Label</cell><cell>Select + Label + Mix (SLM)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The top row corresponds to the ImageNet domain, while the bottom row to the Caltech domain. The images in the same column belong to the same class. Best viewed in color.</figDesc><table><row><cell>two datasets (ImageNet1K [34] (I) &amp; Caltech256 [14] (C))</cell></row><row><cell>as two separate domains and consist of over 14 million</cell></row><row><cell>images combined. 2 transfer tasks are formed for this</cell></row><row><cell>dataset. While source domain contains 1,000 and 256</cell></row><row><cell>classes for ImageNet and Caltech respectively, each target</cell></row><row><cell>domain contains only 84 classes that are common across</cell></row><row><cell>both domains. As it is a general practice to use ImageNet</cell></row><row><cell>pretrained weights for network initialization, we use the</cell></row><row><cell>validation set images when using ImageNet as the target</cell></row><row><cell>domain. Number of Outlier Classes = 172 for C→I, 916 for</cell></row><row><cell>I→C. Figure 7 displays a gallery of sample images for this</cell></row><row><cell>dataset. The datasets are publicly available to download at:</cell></row><row><cell>http://www.image-net.org/</cell></row><row><cell>http://www.vision.caltech.edu/Image_</cell></row><row><cell>Datasets/Caltech256/.</cell></row><row><cell>VisDA-2017. This dataset contains 280,157 images dis-</cell></row><row><cell>tributed among 12 different classes and two domains. The</cell></row><row><cell>dataset contains three sets of images: training, validation</cell></row><row><cell>and testing. The training set contains 152,397 synthetic (S)</cell></row><row><cell>images, the validation set contains 55,388 real-world (R)</cell></row><row><cell>images, while the test set contains 72,372 real-world im-</cell></row><row><cell>ages. For the experiments, the training set is considered</cell></row><row><cell>as the Synthetic (S) domain, while the validation set as the</cell></row><row><cell>Real (R) domain, following [21]. This results in 2 transfer</cell></row><row><cell>tasks. The first 6 categories (in alphabetical order) are se-</cell></row><row><cell>lected in each of the domains as the target classes, and the</cell></row><row><cell>following label spaces are obtained:</cell></row><row><cell>L</cell></row></table><note>source = {0, 1, 2, ..., 11}. L target = {0, 1, 2, ..., 5}. Number of Outlier Classes = 6.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Average VGG-16<ref type="bibr" target="#b37">[38]</ref> (ICLR'15)  60.34 ±0.84 97.97 ±0.63 99.36 ±0.36 76.43 ±0.48 72.96 ±0.56 79.12 ±0.54 81.03 PADA [4] (ECCV'18) 86.05 ±0.36 100.0 ±0.00 100.0 ±0.00 81.73 ±0.34 93.00 ±0.24 95.26 ±0.27 92.54 SAN [3] (CVPR'18) 83.39 ±0.36 99.32 ±0.45 100.0 ±0.00 90.70 ±0.20 87.16 ±0.23 91.85 ±0.35 92.07 IWAN [49] (CVPR'18) 82.90 ±0.31 79.75 ±0.26 88.53 ±0.16 90.95 ±0.33 89.57 ±0.24 93.36 ±0.22 87.51 ETN [5] (CVPR'19) 85.66 ±0.16 100.0 ±0.00 100.0 ±0.00 89.43 ±0.17 95.93 ±0.23 92.28 ±0.20 93.88 SLM (Ours) 91.98 ±0.04 99.77 ±0.15 99.58 ±0.54 98.09 ±0.00 96.14 ±0.03 96.03 ±0.02 96.93</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Performance on Office31 with VGG-16 backbone. Numbers show the accuracy (%) of different methods on partial domain adaptation setting. We highlight the best and second best method on each transfer task. Our proposed framework, SLM achieves the best performance on 4 out of 6 transfer tasks including the best average performance among all compared methods.→ W D → W W → D A → D D → A W → A Average R → S S → RAverage 88.04 98.31 95.75 88.75 84.48 80.20 89.25 57.68 56.40 57.04 91.75 99.32 96.60 93.84 94.22 93.46 94.87 69.04 68.40 68.72 92.43 99.89 99.15 94.90 95.51 93.84 95.95 77.24 84.84 81.04 99.77 100.00 99.79 98.73 96.10 95.89 98.38 77.48 91.74 84.61</figDesc><table><row><cell>Modules</cell><cell>Office31</cell><cell>VisDA-2017</cell></row><row><cell>Select Label</cell><cell>Mix A</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Effectiveness of Different Modules on Office31 and VisDA-2017 Datasets. Our proposed approach achieves the best performance with all the modules working jointly for learning discriminative invariant features in partial domain adaptation.</figDesc><table><row><cell cols="2">W/o Hausdorff Loss 56.22</cell><cell>83.14</cell><cell>90.26</cell><cell>72.60</cell><cell>71.45</cell><cell>80.78</cell><cell>71.44</cell><cell>51.64</cell><cell>84.80</cell><cell>82.49</cell><cell>57.51</cell><cell>81.66</cell><cell>73.67</cell></row><row><cell>Ours (SLM)</cell><cell>56.54</cell><cell>83.75</cell><cell>90.48</cell><cell>76.03</cell><cell>73.99</cell><cell>80.95</cell><cell>72.97</cell><cell>56.60</cell><cell>87.32</cell><cell>82.55</cell><cell>59.76</cell><cell>82.52</cell><cell>75.29</cell></row></table><note>Ar → Cl Ar → Pr Ar → Rw Cl → Ar Cl → Pr Cl → Rw Pr → Ar Pr → Cl Pr → Rw Rw → Ar Pr → Cl Pr → Rw Average</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Effectiveness of Hausdorff Triplet Loss on Office-Home Dataset. The table shows the performance of the framework without (top-row) and with (bottom-row) the inclusion of the Hausdorff distance triplet loss. The results highlight the importance of the Hausdorff distance loss in our proposed framework.</figDesc><table><row><cell cols="2">W/ Hard Pseudo-labels 52.52</cell><cell>79.89</cell><cell>90.17</cell><cell>73.46</cell><cell>72.61</cell><cell>78.17</cell><cell>69.88</cell><cell>47.54</cell><cell>87.50</cell><cell>78.57</cell><cell>50.59</cell><cell>82.67</cell><cell>71.96</cell></row><row><cell>Ours (SLM)</cell><cell>56.54</cell><cell>83.75</cell><cell>90.48</cell><cell>76.03</cell><cell>73.99</cell><cell>80.95</cell><cell>72.97</cell><cell>56.60</cell><cell>87.32</cell><cell>82.55</cell><cell>59.76</cell><cell>82.52</cell><cell>75.29</cell></row></table><note>Ar → Cl Ar → Pr Ar → Rw Cl → Ar Cl → Pr Cl → Rw Pr → Ar Pr → Cl Pr → Rw Rw → Ar Pr → Cl Pr → Rw Average</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Effectiveness of Soft Pseudo-labels on Office-Home Dataset. Table shows the performance of the framework when we replace the soft pseudo-labels with hard pseudo-labels (top-row) for the target samples. The results justify that the soft pseudo-labels are critical for our framework and attenuate unwanted deviations caused by hard pseudo-labels. → Cl Ar → Pr Ar → Rw Cl → Ar Cl → Pr Cl → Rw Pr → Ar Pr → Cl Pr → Rw Rw → Ar Pr → Cl Pr → Rw Average</figDesc><table><row><cell cols="2">Ar No Domain Discriminator MixUp 56.18</cell><cell>81.49</cell><cell>90.02</cell><cell>74.01</cell><cell>71.76</cell><cell>80.31</cell><cell>72.18</cell><cell>50.93</cell><cell>86.25</cell><cell>79.80</cell><cell>57.99</cell><cell>82.00</cell><cell>73.58</cell></row><row><cell>No Classifier MixUp</cell><cell>57.81</cell><cell>82.88</cell><cell>88.53</cell><cell>75.05</cell><cell>73.63</cell><cell>79.26</cell><cell>69.02</cell><cell>54.85</cell><cell>86.64</cell><cell>79.77</cell><cell>57.57</cell><cell>81.21</cell><cell>73.85</cell></row><row><cell>Ours (SLM)</cell><cell>56.54</cell><cell>83.75</cell><cell>90.48</cell><cell>76.03</cell><cell>73.99</cell><cell>80.95</cell><cell>72.97</cell><cell>56.60</cell><cell>87.32</cell><cell>82.55</cell><cell>59.76</cell><cell>82.52</cell><cell>75.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Effectiveness of Different MixUp on Office-Home Dataset. The table shows the performance of the framework with the exclusion of mixup regularization from the domain discriminator (top-row) and the classsifier (middle-row). The final row shows the results of the proposed SLM framework, which provides the best performance confirming the importance of our Mix strategy.</figDesc><table><row><cell>Vanilla</cell><cell>Select</cell><cell>Select + Label</cell><cell>Select + Label + Mix (SLM)</cell></row><row><cell>Vanilla</cell><cell>Select</cell><cell>Select + Label</cell><cell>Select + Label + Mix (SLM)</cell></row><row><cell>Vanilla</cell><cell>Select</cell><cell>Select + Label</cell><cell>Select + Label + Mix (SLM)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partially supported by the SERB Grant SRG/2019/001205.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tackling partial domain adaptation with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="70" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to transfer examples for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woong-Gi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tackgeun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Domain adversarial reinforcement learning for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04094</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Selective transfer with reinforced transfer network for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-weight partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongya</forename><surname>Tuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongliang</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Duplex generative adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual correction network for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08546</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Virtual mixup training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangbin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12197</idno>
		<title level="m">stance adaptive self-training for unsupervised domain adaptation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02176</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Multi-adversarial domain adaptation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01217</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dual mixup regularized learning for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Roby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1426" to="1435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improve unsupervised domain adaptation with mixup training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00677,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Label propagation with augmented anchors: A simple semi-supervised learning baseline for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="781" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
