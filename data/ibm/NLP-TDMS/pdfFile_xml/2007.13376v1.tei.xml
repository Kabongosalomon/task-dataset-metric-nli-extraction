<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlong</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><forename type="middle">Sun</forename><surname>Xiaowei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Feiyue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Tencent</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtu</forename><surname>Lab</surname></persName>
						</author>
						<title level="a" type="main">NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS • Computing methodologies → Neural networks; Object detec- tion KEYWORDS Pedestrian Detection, Non-maximum Suppression</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Greedy-NMS inherently raises a dilemma, where a lower NMS threshold will potentially lead to a lower recall rate and a higher threshold introduces more false positives. This problem is more severe in pedestrian detection because the instance density varies more intensively. However, previous works on NMS don't consider or vaguely consider the factor of the existent of nearby pedestrians. Thus, we propose Nearby Objects Hallucinator (NOH), which pinpoints the objects nearby each proposal with a Gaussian distribution, together with NOH-NMS, which dynamically eases the suppression for the space that might contain other objects with a high likelihood. Compared to Greedy-NMS, our method, as the state-of-the-art, improves by 3.9% AP, 5.1% Recall, and 0.8% MR -2 on CrowdHuman to 89.0% AP and 92.9% Recall, and 43.9% MR -2 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Non-maximum Suppression (NMS) is widely used in proposal-based object detectors <ref type="bibr">[2, 5, 6, 8-11, 15, 16, 20, 23-26]</ref>, as the post processing step to eliminate the redundant detections. Ideally, the proposal with the maximum score should suppress and only suppress all the other proposals of the same object. However, NMS distinguishes objects solely by a universal Intersection over Union (IoU) threshold. That is, if two proposals have an IoU above the pre-defined threshold, they will be considered as detecting the same object and one of them will be eliminated as the duplicate.</p><p>This scheme works fine in generic object detection task. However, it raises a dilemma in pedestrian detection task where the object density varies a lot, making it infeasible to find a perfect universal IoU threshold as a higher threshold fits for the regions with higher density and the less crowded regions desire a lower threshold (See <ref type="figure">Fig. 1</ref>).</p><p>Previous work tries to address this issue of the rigid NMS threshold. Soft-NMS <ref type="bibr" target="#b0">[1]</ref> proposes to degrade the score of nearby highly overlapped proposals instead of eliminating them, but just like Greedy-NMS, it still blindly penalizes the highly overlapped boxes. Adaptive-NMS <ref type="bibr" target="#b17">[18]</ref> suggests directly predicting a proper NMS threshold for each proposal. However, even though the proposal Emails: {penghaozhou,chongzhou,popeyepeng,jeffdu,winfredsun,scorpioguo,garyhuang} @tencent.com  <ref type="figure">Figure 1</ref>: Comparison among various NMS methods The blue dotted box in (b) shows the mistakenly suppressed detection, which is caused by the NMS threshold dilemma in Greedy-NMS. The red dotted box in (c) highlights the false positive introduced by Adaptive-NMS as it is unable to pinpoint the overlapping areas. In order to recall the detection of the boy and suppress the red box, adapting the IoU threshold is not enough since iou(box r ed , box дr een ) &lt; iou(box blue , box дr een ), and NOH is designed for filling this gap.</p><p>can sense the density of the nearby objects, it is not aware of the locations and spread of the crowded regions, which results in a new dilemma, as shown in <ref type="figure">Fig. 1</ref>, where the left to the proposal is not dense at all and the right is rather crowded. Thus, to tackle this problem, we propose Nearby Objects Hallucinator (NOH) and NOH-NMS. Our key observation is, in a crowded scene, the visual information inside a bounding box of one pedestrian will mostly contain the cues of the locations and sizes of other pedestrians. Therefore, we design NOH, which hallucinates the objects nearby each proposal based on the Region-of-Interest (RoI) feature and represents the hallucination with a Gaussian distribution. Furthermore, we propose NOH-NMS to perform a novel NMS strategy leveraging the Gaussian distribution.</p><p>The proposed NOH and NOH-NMS can be integrated naturally into both one-stage and two-stage object detectors with marginal computation cost and acquire no more extra annotations other than the full-body bounding boxes during training.</p><p>To evaluate the effectiveness of our method, we have conducted quantitative and qualitative experiments on CityPersons <ref type="bibr" target="#b33">[34]</ref> and CrowdHuman <ref type="bibr" target="#b27">[28]</ref> datasets (see Sec. 4). As a result, we achieve state-of-the-art performance with 89.0% AP, 92.9% Recall, 43.9% MR -2 on CrowdHuman, and 10.8% MR -2 on CityPersons.</p><p>Our contributions can be summarized as follow:</p><p>• We propose NOH-NMS, which is aware of the existence of other nearby objects when performing the suppression, to address the rigid NMS threshold problem in pedestrian detection. • We design NOH to pinpoint the objects nearby each proposal with a Gaussian distribution. • Our method achieves state-of-the-art performance on CityPersons and Crowdhuman with negligible overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Over the past decade, deep convolutional neural networks (CNNs) have made great strides in image recognition <ref type="bibr" target="#b12">[13]</ref>. To adapt an image classifier into an object detector, the current common practice, called proposal-based object detector, leverages sliding window to densely predict, for each proposal, a set of category confidence scores and proposal refinement coefficients. These refined proposals are then fed into the NMS algorithm to get rid of the redundant detections. According to different strategies to generate the proposals, proposal-based object detectors can be classified into one-stage, where proposals are pre-defined anchors, and two-stage, where Region Proposal Networks (RPNs) are used for proposal generation. In addition, great progress has been made in multiple scaling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>, learnable anchors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, deformable feature sampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>, etc. Even though state-of-the-art generic object detectors show promising performance on benchmark datasets, such as COCO <ref type="bibr" target="#b16">[17]</ref> and Pascal VOC <ref type="bibr" target="#b6">[7]</ref>, it is non-trivial to adapt them into the pedestrian detection task, because the occlusion is much more severe and frequent in pedestrian detection datasets.</p><p>Occlusion can be divided into two categories, namely inter-class occlusion and intra-class occlusion. In intra-class occlusion scenarios, the pedestrian is occluded by other pedestrians. And the inter-class occlusion results in the partially visible feature of pedestrians mixed with the feature of background objects.</p><p>To address the problem of inter-class occlusion, some algorithms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> seek to leverage the annotated visible bounding box (VBB). <ref type="bibr" target="#b35">[36]</ref> introduces a visible part estimation branch and a new training sample selecting strategy assisted by VBB. OR-CNN <ref type="bibr" target="#b34">[35]</ref> exploits the topological structure of the pedestrian with visibility prediction for occluded pedestrian detection. To emphasize on visible pedestrian regions during feature extraction, MGAN <ref type="bibr" target="#b21">[22]</ref> proposes an attention module supervised by VBB.</p><p>In intra-class occlusion scenarios, the pedestrian is occluded by other pedestrians, which occurs frequently in the crowd scene. The heavily occluded between pedestrians confuses the models as it's hard to distinguish instance boundaries. To alleviate this problem, OR-CNN <ref type="bibr" target="#b34">[35]</ref> designs aggregation loss to enforce generating more compact bounding boxes. In addition, RepLoss <ref type="bibr" target="#b30">[31]</ref> proposes a novel repulsion loss to prevent the proposal from shifting to surrounding objects.</p><p>Though OR-CNN <ref type="bibr" target="#b34">[35]</ref> and RepLoss <ref type="bibr" target="#b30">[31]</ref> successfully ease the localization problem in the crowded scenes, there still exists an even worse issue during the post-processing stage. In the post-processing stage, Non-maximum Suppression (NMS) is wildly used to suppress false positive proposals (i.e., the redundant pedestrian proposals belong to the same identity). However, NMS may also suppress true positive proposals (i.e., the highly overlapped pedestrian proposals belong to different identities). Therefore, a lower threshold leads to a lower Recall while a higher threshold results in lower precision.</p><p>To address this dilemma, <ref type="bibr" target="#b0">[1]</ref> proposes Soft-NMS to replace the elimination operation with decaying the detection scores according to the IoU. And <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> suggest using additional annotated head bounding boxes to solve the problem of NMS in a crowd, as the head parts usually suffer less from occlusion. More recently, Adaptive-NMS <ref type="bibr" target="#b17">[18]</ref> proposes to predict the adaptive IoU threshold in NMS for each proposal. It aims at predicting a higher NMS threshold if the objects gather together and occlude each other, and predicting lower NMS threshold if the objects are sparse. However, even though Adaptive-NMS could predict accurate density for each proposal, a density scalar is not enough to precisely express the spatial locations of the crowded areas. In other words, the proposal is capable of sensing how crowded its surrounding is, but cannot tell if the area to its left is more crowded than the area to its right. As a result, Adaptive-NMS gets stuck into a new dilemma when different spatial locations to one object desire different IoU thresholds, as shown in <ref type="figure">Fig.1</ref>.</p><p>We observe this inflexibility in Adaptive-NMS and thus propose NOH and NOH-NMS to address this problem. Specifically, for NOH, we design a mini 2-fc branch to predict, for each proposal, not only a density scalar but also a Gaussian distribution which highlights the surrounding objects. In addition, our NOH-NMS leverages the output from NOH as the auxiliary information, together with the normal NMS input (detection boxes with class confidence), to perform a nearby-objects-aware NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD</head><p>In this section, we first briefly recap the previous NMS algorithms (Sec. 3.1). Then we propose our NOH-NMS which integrates the nearby-objects distribution into the NMS pipeline (Sec. 3.2). In addition, we illustrate how our NOH module learns to predict the nearby-objects distribution just from the box-level supervision (Sec. 3.3). Finally, we compare our method with the state-of-the-art NMS counterparts in with visualization (Sec. 3.4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><formula xml:id="formula_0">Input : B = b 1 , . . . , b N , S = s 1 , . . . , s N , D = d 1 , . . . , d N , P = p 1 , . . . , p N , N t B</formula><p>is the list of initial detection boxes S contains corresponding detection scores D contains corresponding detection densities P contains the parameters of nearby-objects distribution of corresponding detection proposals are usually densely arranged and there is no punishment if two or more detections are detecting the same object. Thus, prior to stage 5, it is rather common that one object area is occupied with multiple detections whereas only one of them counts towards true positive, and the rest are considered as false positive.</p><formula xml:id="formula_1">N t is the NMS threshold begin F ← {} while B empty do m ← argmax S M ← b m F ← F M; B ← B − M for b i ∈ B do if iou(M, b i ) ≥ N t then B ← B − b i ; S ← S − s i ; Greedy-NMS s i ← s i · f (M, b i , d M , p M ); NOH-NMS end end end return F , S end</formula><p>To avoid the aforementioned problem, Greedy-NMS selects the detection with the maximum score M and eliminates its surrounding inferior detections whose IoU with M is above certain threshold N t , and then repeats this pruning process with the next best detection, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The pruning step, as the core of the NMS algorithm, can be formulated into a re-scoring function as follow:</p><formula xml:id="formula_2">s i = s i , iou(M, b i ) &lt; N t 0, iou(M, b i ) ≥ N t ,<label>(1)</label></formula><p>where s i and b i denote the confidence score and bounding box coefficients of the inferior detections. b i will be either left unmodified or completely removed depending solely on its IoU with M. This introduces two problems. (1) The consequence is too extreme and IoU, as the only metric, is not robust enough, which makes the performance very sensitive to the choice of the NMS threshold. E.g., when N t is set to 0.5, detection b i will be eliminated if iou(M, b i ) equals to 0.51, however, with a slight perturbation, iou(M, b i ) could become 0.49, which makes b i survive. <ref type="bibr" target="#b1">(2)</ref> There is no such NMS threshold that makes everyone happy. E.g., an image occupied with 100 objects might desire 0.3 as the threshold, while it is not suitable for the image with a single object.</p><p>In response to the first problem, Soft-NMS softens the consequence by gradually decaying the score of the overlapped detections instead of eliminating them. Below shows its re-scoring function:</p><formula xml:id="formula_3">s i = s i , iou(M, b i ) &lt; N t s i · f (M, b i ), iou(M, b i ) ≥ N t ,<label>(2)</label></formula><p>where decaying function f is chosen to be:</p><formula xml:id="formula_4">f (M, b i ) = 1 − iou(M, b i ) or exp(−iou(M, b i ) 2 /σ )<label>(3)</label></formula><p>For the second problem, Adaptive-NMS customizes an NMS IoU threshold for each proposal and follows the design of Greedy-NMS except now the IoU threshold N M varies with the current best detection M. Their strategy can be formulated as:</p><formula xml:id="formula_5">N M := max(N t , d M ),<label>(4)</label></formula><formula xml:id="formula_6">s i = s i , iou(M, b i ) &lt; N M 0, iou(M, b i ) ≥ N M ,<label>(5)</label></formula><p>where d M is the density prediction of proposal M.</p><p>As we carefully re-visit Adaptive-NMS, we find that due to the maximum function, Adaptive-NMS can be re-written into a super case of Soft-NMS:</p><formula xml:id="formula_7">s i = s i , iou(M, b i ) &lt; N t s i · f (M, b i , d M ), iou(M, b i ) ≥ N t ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">f (M, b i , d M ) = 1, iou(M, b i ) &lt; d M 0, iou(M, b i ) ≥ d M<label>(7)</label></formula><p>As shown in Eq. 2 and Eq. 6, compared to Greedy-NMS, Soft-NMS adds the location of b i into consideration when suppressing b i and Adaptive-NMS further considers the density of M. However, both of them cannot accurately distinguish whether b i is detecting a nearby object or b i is a false positive. Although equipped with density prediction, Adaptive-NMS still cannot tell where the objects around M are, let alone Soft-NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NOH-NMS</head><p>The key idea of our NOH-NMS is to introduce the nearby-objects distribution P p M into the NMS pipeline, where p M denotes its parameters. The nearby-objects distribution could be obtained by any probability distribution functions (PDFs), and we will cover our choice of generating P p M in Sec. 3.3. Note that, in the pedestrian detection task, the only object category we care is human, therefore the nearby objects refer to nearby pedestrians mostly in this paper. However, our method can also be used in other tasks where the nearby objects won't be limited to humans only.</p><p>NOH-NMS consists of two components, namely overlap detector and NOH-Suppressor.</p><p>Overlap Detector Since our assumption is that the bounding box area of one pedestrian will mostly contain the cues of other pedestrians, we need to first rule out the cases where the cues are not abundant (e.g. a pedestrian is by its alone). Thus, we propose a simple overlap detector, which predicts the IoU between the M and the object overlapped with M the most. If the predicted IoU is less than a threshold d t , which we empirically set to 0.3, then NOH-Suppressor won't be triggered because of insufficient cues,   <ref type="bibr" target="#b25">[26]</ref>. Note that our NOH can fit in single-stage object detectors as well by placing the NOH branch in parallel with the detection head. In this example, the lady at the front left is highly overlapped with the lady behind her, and our NOH pinpoints the location and shape of the lady behind so that the detection of her won't be mistakenly suppressed whereas other false positives will be eliminated.</p><p>and we will follow the design of Greedy-NMS (s i := 0) or Soft-NMS (s i := s i f (iou(M, b i ))). NOH-Suppressor If the cues are predicted to be sufficient, we will perform NOH-Suppression, which re-scores the s i by multiplying the probability of b i being a nearby object. In this way, when a neighboring box meets the attributes of being a nearby object, the suppression on it will be dynamically eased, whereas if it is very unlikely to be a nearby object, then we treat it as detecting the same object of M, which should be degraded. We formally describe the difference between NOH-NMS and Greedy-NMS in <ref type="figure" target="#fig_2">Fig. 2</ref>. As we only replace the re-scoring function with a Gaussian function runs at O(1), we haven't introduced computational complexity into the NMS pipeline. In addition, since we leverage the mini 2-fc branch to predict both distribution parameters and density directly from RoI feature, the overhead is negligible (See <ref type="figure" target="#fig_4">Fig. 3</ref>).</p><p>In summary, the strategy we adopt can be described as follow:</p><formula xml:id="formula_9">s i = s i , iou(M, b i ) &lt; N t s i · f (M, b i , d M , p M ), iou(M, b i ) ≥ N t ,<label>(8)</label></formula><formula xml:id="formula_10">f (M, b i , d M , p M ) = P p M (M, b i ), d M ≥ d t 0, d M &lt; d t<label>(9)</label></formula><p>Note that, if the step function in Eq. 7 is used as the PDF, then our NOH-NMS degenerates to Adaptive-NMS. However, the step function is rarely used for modeling the natural distributions because (1) it is not continuous, and (2) it is oversimplified. Thus, we propose NOH (Sec. 3.3) to better capture the true nearby-objects distribution using the Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Nearby Objects Hallucinator (NOH)</head><p>NOH is responsible for generating the nearby-objects distribution for each M. We achieve this by hallucinating the locations and shapes of the nearby objects from the cues in region M, and expressing the hallucination with a Gaussian distribution. We term this process as hallucination because different from proposal-based instance recognition, which predicts box coefficients from the proper RoI feature, our NOH could only rely on partially visible cues.</p><p>Essentially, based on the features extracted from region M, multiple hallucination objects could be proposed. However, for simplicity, we only capture one nearby object which overlaps with M the most. We represent the hallucinated object with its relative center location, width, height with M, denoted as µ M . Since the hallucinated object is predicted by partially visible cues, the prediction is expected to be imprecise. Thus, we decay the nearby-objects likelihood with a Gaussian distribution which centers at µ M and spreads with a hyper-parameter σ .</p><p>With all the definition above, our NOH applies the following strategy:</p><formula xml:id="formula_11">P µ M (M, b i ) = exp(− b i |M − µ M 2 /2σ 2 )<label>(10)</label></formula><formula xml:id="formula_12">b i |M = { x b i − x M w M , y b i − y M h M , log w b i w M , log h b i h M }<label>(11)</label></formula><p>We implement NOH with a prediction head in parallel with the classification and regression head of Faster-RCNN. The training target of NOH is derived from the relative box coefficients of the most nearby object with M, and we impose Smooth-L1 loss as the training loss. Note that, the Gaussian function is not represented during the training. However, we could convert the training target from the relative box coefficients into a Dirac delta function, and supervise it with KL Loss <ref type="bibr" target="#b13">[14]</ref>. In this paper, we keep the training process simple, as we find it works up to the expectation, and stick with the Smooth-L1 loss. To visualize it in 2-d space, we unify the shapes of all the boxes so that each box can be represented by its center point. The detection score is attached to the corner of the box. The color map shows to what extent the detection with the maximum score (blue box) suppresses its surrounding inferior detections. For instance, the center point of the green box in (d) lies in the red area (keeping area), meaning it is very likely to survive the suppression, whereas the red box will be penalized harshly as its center sits in the blue area (suppressing area).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparisons with other NMS Strategies</head><p>To better understand the difference among NMS strategies that we and other methods propose, we visualize the suppression effect of M on overlapped other detections in <ref type="figure" target="#fig_5">Fig. 4</ref>. According to the figure, Greedy-NMS harshly eliminates the detections around M, and Soft-NMS gradually adds keeping area. Adaptive-NMS, on the other hand, adds a more harsh keeping area, as the result of the usage of the step function, but the proportion of such area is adaptive to the pedestrian density. Note that when combining Soft-NMS and Adaptive-NMS together, the keeping area will be both continuous and adaptive. However, all the aforementioned methods cannot shift the center of the keeping area because they don't explicitly predict the distribution of the nearby pedestrians, whereas our method places the keeping area more accurate thanks to the NOH module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first cover the datasets and metrics that we use for all the experiments (Sec. 4.1). We then reveal our implementation details in Sec. 4.2 and show quantitative results of NOH-NMS compared to various NMS methods (Sec. 4.3). We also conduct sensitivity analysis (Sec. 4.4) to prove the robustness of our method. Qualitative results are also prepared in Sec. 4.5 for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>CityPersons CityPersons <ref type="bibr" target="#b33">[34]</ref> is a currently wildly used benchmark dataset in the pedestrian detection task. Based on the 5000 images in the Cityscapes <ref type="bibr" target="#b3">[4]</ref> dataset, CityPersons creates more finegrained bounding box annotations which dedicate to pedestrian detection. In total, CityPersons covers ∼ 35k person and ∼ 13k ignore region (fake humans like statues) annotations. In addition, CityPersons aims at including persons with heavy occlusion and small scale, yielding an average density of ∼ 7 persons per image.</p><p>CrowdHuman CrowdHuman <ref type="bibr" target="#b27">[28]</ref> was released more recently, which further emphasizes the crowd issue. It contains 15, 000 images, with ∼ 340k person and ∼ 99k ignore region annotations. The person density is significantly higher than CityPersons and reaches ∼ 22.6 persons per image with 2.4 pairwise overlapping instances (IoU larger than 0.5).</p><p>Evaluation metrics We follow the evaluation metrics used in CityPersons and CrowdHuman, denotes as MR -2 , AP, and Recall:</p><p>• MR -2 , or log-average Miss Rate on False Positive Per Image (FPPI) in [10 −2 , 10 0 ], is commonly used to evaluate detectors whose applications have an upper limit on the acceptable FPPI rate independent of object density. Thus, MR -2 is particularly sensitive to false positives. • Average Precision (AP) is the most popular metric in generic object detection, which summarizes the precision-recall curve of the detection results. In the following experiments, we follow the AP metric in PASCAL VOC <ref type="bibr" target="#b6">[7]</ref>, where a prediction is positive if IoU ≥ 0.5. • Recall is short for the maximum recall given a fixed number of detections. As both Soft-NMS, Adaptive-NMS, and NOH-NMS aim at recalling the mis-eliminated true positives, as shown in <ref type="figure" target="#fig_5">Fig .4</ref>, this metric reflects the effectiveness of this intention. For fair comparisons, we set the allowed number of detections to be 100 for all NMS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For all the experiments, we adapt the Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> with FPN <ref type="bibr" target="#b14">[15]</ref> as our baseline and build various NMS methods upon the same baseline for fair comparisons. In specific, we choose the standard ResNet-50 <ref type="bibr" target="#b12">[13]</ref> as the backbone and replace the ROIPooling operation in the original Faster-RCNN with the RoIAlign <ref type="bibr" target="#b10">[11]</ref>. We also change the aspect ratios of the anchors to H /W = {1, 2, 3} for CrowdHuman and H /W = {2.44} for CityPersons, as the original anchor settings are optimized towards COCO <ref type="bibr" target="#b16">[17]</ref>. Following the choice of input size in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b27">[28]</ref>, we enlarge the input height and width of CityPersons by 1.3 times and resize the input of Crowd-Human so that the shorter edge of input equals to 800 pixels while keeping the longer edge no longer than 1,400 pixels.  During training, we randomly initialize all the parameters of the model by Kaiming initialization <ref type="bibr" target="#b11">[12]</ref>, except the ResNet-50 backbone, whose initial parameters are loaded from ImageNet <ref type="bibr" target="#b26">[27]</ref> pre-train. We use SGD with 0.9 momentum and 0.0001 weight decay as the optimizer and train the model with 5, 600 and 28, 125 iterations in total for CityPersons and CrowdHuman respectively. The initial learning rate is 0.02(0.04) and decreases by a factor of 10 after 3, 400(18, 750) and 4, 600 <ref type="bibr" target="#b23">(24,</ref><ref type="bibr">375)</ref> iterations for CityPersons (CrowdHuman). The batch size is set to be 16 for both datasets. Note that we train on 8 GPUs without Synchronized BN.</p><p>For CityPersons, a sample will be assigned as positive if its IoU with ground-truth is greater than 0.7, and as negative if the IoU is less than 0.5, otherwise the sample will be ignored and won't contribute to the loss. For CrowdHuman, samples with IoU greater than 0.5 qualify as the positive and otherwise are considered as negative. In addition, we clip the ground-truth bounding boxes at the image boundary for CityPersons, while don't apply this operation in CrowdHuman.</p><p>During inference, we set the NMS IoU threshold to 0.5 for all NMS methods and allow at most 100 detections per image. We also follow the same input resizing operation as mentioned in the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>CityPersons We report the results of NOH-NMS and other state-ofthe-art pedestrian detectors on CityPersons validation set in Tab. <ref type="bibr" target="#b0">1</ref>  the group of the methods which don't use extra annotation, NOH-NMS achieves the best performance on Reasonable, which is the most valued, and Partial splits. Moreover, our performance is comparable to that of the methods using additional annotations (e.g. head bounding boxes, visible bounding boxes). CrowdHuman Tab. 2 shows the performance on CrowdHuman validation set. To have a comprehensive evaluation, three evaluation metrics are chosen to evaluate our method, which are AP, Recall, and MR -2 . We re-implement a strong FPN <ref type="bibr" target="#b14">[15]</ref> baseline. Our baseline achieves 85.1% AP, 87.8% Recall and 44.7% MR -2 , which outperforms the baseline in Adaptive-NMS <ref type="bibr" target="#b17">[18]</ref> by 0.4% AP and 5.0% MR -2 . Although compared to our strong Greedy-NMS baseline, NOH-NMS still significantly improves the AP, Recall, and MR -2 by 3.9%, 5.1%, and 0.8%. Moreover, compared to other state-of-the-art methods, superior performance demonstrates the effectiveness of our method.</p><p>To better demonstrate that our performance gain is not from the strong baseline, and show more clearly the advantage of NOH-NMS compared with its counterparts, we re-implement Soft-NMS and Adaptive-NMS on our strong baseline. The results are shown in Tab. 3. According to the results, NOH-NMS still delivers the best performance across all the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitivity Analysis</head><p>Although NOH-NMS introduces two more hyper-parameters (density threshold d t and Gaussian standard deviation σ ) than the other NMS methods, as we analyze later, it is not only robust to the choice of d t and σ , but also less sensitive to the common hyper-parameter N t than other NMS.</p><p>IoU threshold As shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, we plot the precision vs. recall curves on various NMS IoU thresholds for both Greedy-NMS and NOH-NMS. We conclude two points from the <ref type="figure">figure.</ref> (1)  Even though both methods degrade with sub-optimal IoU threshold hyper-parameter, NOH-NMS is less sensitive as it outperforms the Greedy-NMS in all recall levels across all the choice of N t . (2) Simply flexing the IoU threshold for Greedy-NMS does recall more true positives but also introduces even more false positives that overwhelm the overall performance.</p><p>Density threshold As one of the additional hyper-parameters we introduce, the density threshold d t determines what it takes to be considered as having abundant cues to support the existence of other nearby pedestrians. As shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, the performance for AP, recall, and MR -2 jitters slightly with a wild range of d t (from 0.2 to 0.5 with an interval of 0.05), which proves the robustness of NOH-NMS.</p><p>Gaussian standard deviation σ controls the spread of the Gaussian distribution we use in NOH. Even though we empirically set it to 0.2 in our previous experiments, it is proven to be not very sensitive as illustrated in <ref type="figure" target="#fig_7">Fig. 6</ref>. Note that, if using KL Loss during training, the σ can be trained end-to-end, and will no longer be a hyper-parameter. However, we leave this as future work since it is not the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>Qualitative results are given in two aspects: (1) detections visualization compared with Greedy-NMS and Adaptive-NMS ( <ref type="figure">Fig. 7)</ref>; (2) illustration of the effectiveness of the nearby objects hallucination <ref type="figure">(Fig. 8</ref>).</p><p>As shown in <ref type="figure">Fig. 7</ref>, our NOH-NMS successfully recalls the highly overlapped detections that other methods fail to do so. Moreover, in <ref type="figure">Fig. 8</ref>, the Nearby Objects Hallucinator works as expect, pinpointing the nearby persons with a reasonable Gaussian distribution, which contributes significantly to helping NOH-NMS ease the suppression on the highly overlapped areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present a novel NOH-NMS algorithm that improves the performance of pedestrian detection by taking into account the distribution of nearby objects. As the core part of our algorithm, Nearby Objects Hallucinator learns to predict the Gaussian distribution of nearby objects from only full-body box annotations and introduces marginal overhead. Comprehensive experiments and analyses are done on CityPersons <ref type="bibr" target="#b33">[34]</ref> and CrowdHuman <ref type="bibr" target="#b27">[28]</ref> to show the strength of NOH-NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy-NMS</head><p>Adaptive-NMS NOH-NMS ! ours " <ref type="figure">Figure 7</ref>: Qualitative results Evaluation results on the CrowdHuman validation set. The NMS IoU threshold is set to 0.5 for all the methods. The dotted boxes show the missing detections. <ref type="figure">Figure 8</ref>: The visualization of the nearby objects hallucination results NOH models the distribution of nearby objects with a 4-d Gaussian whose mean µ M represents the expectation of the location and shape of the nearest object (shown in the dotted blue box). The variance of the 2-d transition of the center points is illustrated in red (we don't show the shape variance). The green boxes show the prediction for M.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Original image with GTs (b) Greedy-NMS (c) Adaptive-NMS (d) NOH-NMS (ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>proposal-based object detection framework consists of the following five stages: (1) extracting full-image-level feature; (2) generating bounding box proposals; (3) extracting proposal-region-level feature; (4) performing classification and box regression for each proposal; (5) removing redundant detections. In this pipeline, the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Algorithm pseudo code NOH-NMS replaces the pruning step (highlighted in red) in Greedy-NMS with a nearby-objectsaware re-scoring function (marked with green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Architecture The illustration of integrating Nearby Objects Hallucinator (NOH) into the two-stage object detector, such as Faster-RCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the suppression degree The suppression degree is a function of the relative center location and relative shape of two boxes, resulting in 4-d freedom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Precision vs. Recall at multiple NMS IoU thresholds N t Experiments are conducted on the CrowdHuman validation set and all the NMS methods are implemented by us based on the same baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Sensitivity to hyper-parameters We show the effect of the different choices of σ and d t on NOH-NMS. All the experiments are done on the CrowdHuman validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on the CityPersons validation set. MR -2 is used as the metric (lower is better). Scale is short for input scale.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="6">Extra Anno. Backbone Scale Reasonable Bare Partial Heavy</cell></row><row><cell cols="2">OR-CNN [35]</cell><cell></cell><cell></cell><cell>✓</cell><cell>VGG-16</cell><cell>×1.3</cell><cell>11.0</cell><cell>5.9</cell><cell>13.7</cell><cell>51.3</cell></row><row><cell></cell><cell>MGAN [22]</cell><cell></cell><cell></cell><cell>✓</cell><cell>VGG-16</cell><cell>×1.3</cell><cell>10.5</cell><cell>-</cell><cell>-</cell><cell>47.2</cell></row><row><cell></cell><cell>JointDet [3]</cell><cell></cell><cell></cell><cell>✓</cell><cell cols="2">ResNet-50 ×1.3</cell><cell>10.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TLL (MRF) [29]</cell><cell></cell><cell></cell><cell></cell><cell>ResNet-50</cell><cell>-</cell><cell>14.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Adapted Faster RCNN [34]</cell><cell></cell><cell></cell><cell>VGG-16</cell><cell>×1.3</cell><cell>13.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ALFNet [21]</cell><cell></cell><cell></cell><cell></cell><cell>VGG-16</cell><cell>×1</cell><cell>12.0</cell><cell>8.4</cell><cell>11.4</cell><cell>51.9</cell></row><row><cell></cell><cell>RepLoss [31]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-50 ×1.3</cell><cell>11.6</cell><cell>7.0</cell><cell>14.8</cell><cell>55.3</cell></row><row><cell cols="3">Adaptive-NMS w/ AggLoss [18]</cell><cell></cell><cell></cell><cell>VGG-16</cell><cell>×1.3</cell><cell>10.8</cell><cell>6.2</cell><cell>11.4</cell><cell>54.0</cell></row><row><cell></cell><cell>Our baseline</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-50 ×1.3</cell><cell>11.9</cell><cell>7.4</cell><cell>12.3</cell><cell>53.0</cell></row><row><cell></cell><cell>NOH-NMS</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-50 ×1.3</cell><cell>10.8</cell><cell>6.6</cell><cell>11.2</cell><cell>53.0</cell></row><row><cell>Methods</cell><cell cols="4">Backbone AP Recall MR -2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Repulsion Loss [31]</cell><cell>R50</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>JointDet* [3]</cell><cell>R50</cell><cell>-</cell><cell>-</cell><cell>46.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline in [18]</cell><cell>R50</cell><cell>83.0</cell><cell>90.6</cell><cell>52.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adaptive-NMS [18]</cell><cell>R50</cell><cell>84.7</cell><cell>91.3</cell><cell>49.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our Baseline</cell><cell>R50</cell><cell>85.1</cell><cell>87.8</cell><cell>44.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NOH-NMS</cell><cell>R50</cell><cell cols="2">89.0 92.9</cell><cell>43.9</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on the CrowdHuman validation set. R50 denotes ResNet-50. * marks the methods which leverage extra annotations (e.g. head box) during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different NMS methods on the CrowdHuman validation set. All the methods are implemented by us, and for fair comparisons, we show the best results from multiple runs.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-NMS-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10674</idno>
		<title level="m">Relational Learning for Joint Head and Human Detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Jian Sun</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Path Aggregation Network for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Small-scale pedestrian detection based on topological line localization and temporal feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leiyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09998</idno>
		<title level="m">Double Anchor R-CNN for Human Detection in a Crowd</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Occlusionaware R-CNN: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
