<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Fuse Things and Stuff</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<email>jie.li@tri.global</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Raventos</surname></persName>
							<email>allan.raventos@tri.global</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Bhargava</surname></persName>
							<email>arjun.bhargava@tri.global</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Tagawa</surname></persName>
							<email>takaaki.tagawa@tri.global</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<email>adrien.gaidon@tri.global</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Fuse Things and Stuff</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end learning approach for panoptic segmentation, a novel task unifying instance (things) and semantic (stuff) segmentation. Our model, TASCNet, uses feature maps from a shared backbone network to predict in a single feed-forward pass both things and stuff segmentations. We explicitly constrain these two output distributions through a global things and stuff binary mask to enforce cross-task consistency. Our proposed unified network is competitive with the state of the art on several benchmark datasets for panoptic segmentation as well as on the individual semantic and instance segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Panoptic segmentation is a computer vision task recently proposed by Kirillov et al. <ref type="bibr" target="#b14">[15]</ref> that aims to unify the tasks of semantic segmentation (assign a semantic class label to each pixel) and instance segmentation (detect and segment each object instance). This task has drawn attention from the computer vision community as a key next step in dense scene understanding <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>, and several publicly available benchmark datasets have started to provide labels supporting this task, including Cityscapes <ref type="bibr" target="#b7">[8]</ref>, Mapillary Vistas <ref type="bibr" target="#b26">[27]</ref>, ADE20k <ref type="bibr" target="#b42">[44]</ref>, and COCO <ref type="bibr" target="#b21">[22]</ref>.</p><p>To date, state-of-the-art techniques for semantic and instance segmentation have evolved in different directions that do not seem directly compatible. On one hand, the best semantic segmentation networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b5">6]</ref> focus on dense tensor-to-tensor classification architectures that excel at recognizing stuff categories like roads, buildings, or sky <ref type="bibr" target="#b0">[1]</ref>. These networks leverage discriminative texture and contextual features, achieving impressive results in a wide variety of scenes. On the other hand, the best performing instance segmentation methods rely on the recent progress in object detection -they first detect 2D bounding boxes of objects and then perform foreground segmentation on regions of interest <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>. This approach uses the key insight that things have a well-defined spatial extent and discriminative appearance features. The fundamental differences in approaches between handling stuff and things yields a strong natural baseline for panoptic segmentation <ref type="bibr" target="#b14">[15]</ref>: using two independent networks for semantic and instance segmentation followed by heuristic post-processing and late fusion of the two outputs.</p><p>In contrast, we postulate that addressing the two tasks together will result in increased performance for the joint panoptic task as well as for the separate semantic and instance segmentation tasks.</p><p>The basis for this hypothesis is the explicit relation between the tasks at the two ends of the modeling pipeline: i) early on at the feature level (capturing general appearance properties), and ii) at the output space level (mutual exclusion, overlap constraints, and contextual relations).</p><p>Therefore, the main challenge we address is how to formulate a unified model and optimization scheme where the sub-task commonalities reinforce the learning, while preventing the aforementioned fundamental differences from leading to training instabilities or worse combined performance, a common problem in multi-task learning <ref type="bibr" target="#b40">[42]</ref>.</p><p>Our main contribution is a deep network and end-to-end learning method for panoptic segmentation that is able to optimally fuse things and stuff. Most parameters are shared in a ResNet backbone <ref type="bibr" target="#b12">[13]</ref> and a 4-stage Feature Pyramid Network (FPN) <ref type="bibr" target="#b20">[21]</ref> that is able to learn representations useful for subsequent semantic and instance segmentation heads. In addition, we propose a new differentiable Things and Stuff Consistency (TASC) to maintain alignment between the output distributions of the two sub-tasks during training. This additional objective encourages separation between the outputs of our semantic and instance segmentation heads to be minimal, while simultaneously enabling mask-guided fusion (cf. <ref type="figure" target="#fig_0">Figure 1</ref>). Our unified architecture, TASCNet, maintains or improves the performance of individually trained models and is competitive with panoptic quality benchmarks on the Mapillary Vistas <ref type="bibr" target="#b26">[27]</ref>, Cityscapes datasets <ref type="bibr" target="#b8">[9]</ref> and COCO datasets <ref type="bibr" target="#b22">[23]</ref>. We conduct a detailed ablative analysis, experimentally confirming that our cross-task constraint is key to improving training stability and accuracy. Finally, we show that using a single network has the benefit of simplifying training and inference procedures, while improving efficiency by greatly reducing the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Tackling dense scene understanding and individual object recognition simultaneously has a long and rich history in computer vision. Tu et al. <ref type="bibr" target="#b36">[38]</ref> proposed a hierarchical probabilistic graphical model for scene parsing, disentangling objects, faces, textures, segments, and shapes. This seminal paper inspired a fertile research direction, including contributions on how to explicitly model the relations between things and stuff categories <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b25">26]</ref>. For instance, Sun et al. <ref type="bibr" target="#b32">[34]</ref> use a CRF over image segments to model geometric and semantic relations. Yao et al. <ref type="bibr" target="#b39">[41]</ref> incorporate segmentation unary potentials and object reasoning ones (co-occurrence and detection compatibility) in a holistic structured loss. Tighe et al. <ref type="bibr" target="#b34">[36]</ref> combined semantic segmentation with per-exemplar sliding window. These approaches rely on handcrafting specific unary and pairwise potentials acting as constraining priors on scene components and their expected relations.</p><p>In contrast, deep neural networks can learn powerful shared representations from data, leading to the state of the art in both semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b5">6]</ref> and object detection <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>. As the corresponding architectures share fundamental similarities inherited from the seminal AlexNet model <ref type="bibr" target="#b16">[17]</ref>, several works have naturally leveraged the commonalities to propose multi-task models that can simultaneously address semantic segmentation, object detection, and more <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">35]</ref>. These networks typically follow an encoder-decoder architecture, sharing initial layers followed by separate task-specific branches. In the case of tasks partially competing with each other (e.g., disagreeing on specific image regions), this can result in worse performance (globally and for each task), training instabilities, or outputs not consistent across tasks as noted in <ref type="bibr" target="#b14">[15]</ref>. In order to better leverage task affinities and reduce the need for supervision, Zamir et al. <ref type="bibr" target="#b40">[42]</ref> build a "taskonomy" by learning general task transfer functions. Other works have proposed simple methods tackling the issue of loss weighting. Kendall et al. <ref type="bibr" target="#b13">[14]</ref> propose to use task-dependent uncertainty to weigh the different loss components. Chen et al. <ref type="bibr" target="#b6">[7]</ref> propose another weighting based on gradient norms. Alternatively, Sener et al.</p><p>[33] formulate multi-task learning as a multi-objective optimization problem. These approaches avoid complicated hyper-parameter tuning and reduce training times, but only marginally improve joint performance, under-performing larger individual per-task models.</p><p>In contrast to these general multi-task learning approaches, we focus explicitly on the relations between stuff and thing categories, with the goal of improving individual performance and addressing the unified panoptic prediction task. Dai et al. <ref type="bibr" target="#b9">[10]</ref> predict things and stuff segmentation with a shared feature extractor and convolutional feature masking of region proposals designed initially for object detection. Those are sampled and combined to provide sufficient coverage of the stuff regions, but the relation between things and stuff is not explicitly leveraged. Chen et al. <ref type="bibr" target="#b4">[5]</ref> leverage semantic segmentation logits to refine instance segmentation masks, but not vice-versa.</p><p>Formalizing stuff and things segmentation as a single task, Kirillov et al. <ref type="bibr" target="#b14">[15]</ref> propose a unified metric called Panoptic Quality (PQ) and a strong late fusion baseline combining separate state-of-the-art networks for instance and semantic segmentation. This method uses a simple nonmaximum suppression (NMS) heuristic to overlay instance segmentation predictions on top of a "background" of dense semantic segmentation predictions. Saleh et. al <ref type="bibr" target="#b31">[32]</ref> show that this heuristic is particularly effective for sim2real transfer of semantic segmentation by first decoupling things and stuff before late fusion. Indeed, stuff classes can have photo-realistic synthetic textures (ensuring stuff segmentation transfer), while objects typically have realistic shapes (ensuring detection-based instance segmentation generalization). This approach leverages the specificity of things and stuff, but not their relation and does not tackle the joint panoptic task. Li et al. <ref type="bibr" target="#b17">[18]</ref> propose an end-to-end approach that tackles the unified panoptic problem by reducing it to a semantic segmentation partitioning problem using a fixed object detector and "dummy detections" to capture stuff categories. Their work focuses on the flexibility to handle weak supervision, at the expense of accuracy, yielding significantly worse performance than the panoptic baseline of <ref type="bibr" target="#b14">[15]</ref>, even when fully supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>TASCNet: Our unified architecture jointly predicts things, stuff, and a fusion mask. The proposed heads are built on top of a ResNet + FPN backbone. The Stuff Head uses fully convolutional layers to densely predict all stuff classes and an additional things mask. The Things Head uses region-based CNN layers for instance detection and segmentation. In between these two prediction heads, we propose Things and Stuff Consistency loss to ensure alignment between the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">End-to-end Panoptic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TASCNet Architecture</head><p>High-performance models for instance and semantic segmentation share similar structures, typically employing deep backbones that generate rich feature representations on top of which task-specific heads are attached <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b11">[12]</ref>. Our TASCNet architecture follows this general motif, as is depicted in <ref type="figure">Figure 2</ref>. We use a ResNet50 <ref type="bibr" target="#b12">[13]</ref> with an FPN <ref type="bibr" target="#b20">[21]</ref> as our backbone, with two task specific heads that share feature maps from the FPN. While the ResNet alone has a large receptive field due to aggressive downsampling, this comes at the expense of spatial resolution and the ability to accurately localize small and large objects. Using an FPN enables us to capture low-level features from deeper within the backbone network to recognize a broader range of object scales with far fewer parameters than dilated convolutions. This is a crucial design choice when considering hardware constraints for the already memory-intensive semantic and instance segmentation tasks, let alone the joint learning task <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Things Head</head><p>To regress instances, we use Region-based CNN heads on top of the FPN, similarly to Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>. We augment the FPN with two additional high level context feature maps, similarly to <ref type="bibr" target="#b19">[20]</ref>. Improving the instance segmentation architecture was not the primary focus of this work, and we use the same head structures as in <ref type="bibr" target="#b11">[12]</ref>. We train the bounding box regression head, class prediction head, and mask head in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Stuff Head</head><p>Taking inspiration from Kirillov et al <ref type="bibr" target="#b1">[2]</ref>, we leverage the multi-scale features from the FPN with minimal additional parameters to make dense semantic predictions. From each feature map level of the FPN, we:</p><p>1. apply a set of 3x3 convolutions, reducing the number of channels from 256 to 128;</p><p>2. normalize the layer using a group normalization with 16 groups <ref type="bibr" target="#b38">[40]</ref>;</p><p>3. apply an additional set of 3x3 convolutions, maintaining the number of channels;</p><p>4. normalize and upsample to the largest FPN feature map size (4x downsampled from the input resolution).</p><p>Each output layer is then stacked and one final convolution is applied to predict the class per pixel.</p><p>In order to provide sufficient information for a full ontology panoptic segmentation in the end, the minimum number of classes to be predicted in the stuff head is N + 1, which N stuff classes and 1 class for all the things. However, we found that treating all the classes (N + M ) in the stuff head can help improve the final model performance. More analysis will be provided in Section 4.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Things and Stuff Consistency (TASC)</head><p>Although the sub-task heads are trained using shared features, the output distributions of the two heads can still drift apart. There are several potential causes of this drift, such as minor differences in annotations for instance vs. semantic segmentations, sub-optimal loss functions that capture separate objectives, and local minima for the sub-tasks that do not optimize the joint criterion.</p><p>For panoptic segmentation, however, we aim to train towards a global minimum in which the things and stuff segmentations from the two tasks are identical. We seek to enforce such a shared representation through an intermediate confidence mask reflecting which pixels each task considers to be things vs. stuff.</p><p>This mask can be constructed in a differentiable manner from both instance and semantic segmentation outputs. Doing so from dense semantic predictions is trivial. First, we apply a threshold of 0.5 to the logits of the Stuff head. Then, all remaining pixels predicting things classes are assigned to their logit values, and all pixels predicting stuff classes are assigned to 0.</p><p>For the Things head, constructing the confidence mask is slightly more involved. At train time, an Region Proposal Network (RPN) proposes regions of interest (RoI), which are pooled to a fixed size using an RoI-Align operation. The mask head then produces a per-class foreground/background confidence mask for each positive proposal from the RPN. We can then re-assemble the global binary mask using an operation we dub "RoI-Flatten":</p><p>1. For each image, we construct an empty tensor of equivalent size to the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A threshold of 0.5 is applied to each mask. The thresholded mask is then added to the RoI's original position in the empty tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>To obtain our final confidence mask, we normalize by the instance count at each pixel post-threshold.</p><p>To encourage our instance and semantic segmentation heads to agree on which pixels are things and which are stuff, we minimize the residual between these two masks using an L 2 loss. This residual is visualized in <ref type="figure" target="#fig_2">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mask-Guided Fusion</head><p>Our learning objective encourages the two masks to agree. Therefore, in a converged TASCNet we can use the semantic segmentation mask to select which pixels are obtained from the instance segmentation output and which pixels are obtained from the semantic segmentation output.</p><p>We consequently define a simple post-processing procedure: we add regressed instances into the final panoptic output in decreasing order of confidence, only adding an instance to the output if it has an IoU of under 0.4 with instances that have already been added and an IoU of greater than 0.7 with the mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our proposed approach using different benchmark datasets, Cityscapes <ref type="bibr" target="#b8">[9]</ref>, Mapillary Vistas <ref type="bibr" target="#b26">[27]</ref> and COCO <ref type="bibr" target="#b22">[23]</ref>. These datasets have large gaps in both complexity and image content.</p><p>Cityscapes is comprised of street imagery from Europe and has a total of 5000 densely annotated images with an ontology of 19 classes, 8 thing classes and 11 stuff classes. All the images are at 1024 x 2048 resolution, and are split into separate training, validation and test sets. We train our model on the "fine" annotations in the training set and test on the provided validation set.</p><p>The Mapillary Vistas dataset, on the otherhand, while still a street scene dataset, is far more challenging. It consists of a wide variety of geographic settings, camera types, weather conditions, image aspect ratios, and object frequencies. The average resolution of the images is around 9 megapixels, which causes considerable complications for training deep nets with limited memory. The dataset consists of 18000 training images, 2000 validation images, and 5000 testing images. Annotations for pixel-wise semantic segmentation and instance segmentation are available for the training and validation sets. The labels are defined on an ontology of 65 semantic classes, including 37 thing classes and 28 stuff classes.</p><p>COCO is a large scale object detection and segmentation dataset. We use its 2017 edition with 118k training images, 5k validation images and 2k testing images. The labels consist of 133 classes. 80 classes have instance level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We evaluate our model's performance on Panoptic Segmentation task using the PQ metric proposed by <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_0">P Q = Σ (p,g)∈T P IoU (p,g) |T P | + 1 2 |F P | + 1 2 |F N |<label>(1)</label></formula><p>where p and g are matched predicted and ground truth segments exceeding an IoU threshold of 0.5, and TP, FP, FN denote true positives, false positives, and false negatives, respectively.</p><p>To better explore the capability of our proposed approach, we also evaluate our model performance on the task of instance segmentation and semantic segmentation. For semantic segmentation, we use the standard metric, PAS-CAL VOC Intersection over Union (IoU).</p><p>For instance segmentation, following recent literature, we average over the AP r <ref type="bibr" target="#b10">[11]</ref> with acceptance IoU from 0.5 to 0.95 in increments of 0.05. We call this metric AP in the rest of the paper without ambiguity. For minor scores, we also report the AP 50 = AP r (IoU &gt; 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We conduct most of our experiments on P3.16xlarge instances on Amazon Web Services (AWS), each of them features 8 V100 GPUs.</p><p>For Cityscapes experiments, we train our models for 20000 iterations with 1 image per GPU, 0.01 base learning rate, 0.0001 weight decay. We divide the learning rate by a factor of 10 at iteration 14000 and 18000. For Mapillary Vistas experiments, we follow the same settings for Cityscapes except for a longer schedule to account for the size of dataset. We train the models for 180000 iterations and decrease the learning rate at 120000 and 16000. In this set of experiments, due to the limitation of GPU memory, we collapes the prediction ontology in stuff head from N + M to N + 1 merging all the M things classes as a single "thing" class. Therefore, a full ontology mIoU is not available. For COCO experiments, we follows the 1x training schedule used in <ref type="bibr" target="#b11">[12]</ref>. We train our models for 90000 iterations with 2 images per GPU, 0.02 base learning rate and 0.0001 weight decay.</p><p>For data augmentation, we apply aspect ratio-preserving scale with jitter and randomized left-right flip.</p><p>In Cityscapes and Vistas experiments, we randomly sample a shortest side from 800 1400. We apply a longest side maximum of 2500 to Vistas dataset due to its wide range of image sizes. In COCO experiments, we randomly sample a shortest side from [640, 800] with a maximum longest side of 1333.</p><p>During inference, we apply per-class NMS with a cutoff threshold of 0.3 for bounding boxes proposals and another run of mask level NMS with the same threshold over all the proposals. For test time augmentation, we conduct inference at multiple scales, each with a horizontal flip.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head><p>We compare our approach with other state-of-the-art panoptic segmentation models and state-of-the-art models for instance and semantic segmentation on the 3 datasets. In <ref type="table" target="#tab_1">Table 1, Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>, we present this comparison over the relevant metrics for each task. All experiments are carried out using a ResNet-50 (R50) backbone.  We compare to the challenging baseline proposed in <ref type="bibr" target="#b14">[15]</ref>, which combines the state-of-the-art models from single task semantic segmentation (PSPNet <ref type="bibr" target="#b41">[43]</ref>) and instance segmentation (Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>). The panoptic segmentation performance of our proposed TASCNet, using only ResNet-50 backbone and basic test time augmentation, matches the P Q reported in <ref type="bibr" target="#b14">[15]</ref>. We also compared to Li et. al. <ref type="bibr" target="#b17">[18]</ref>, who also presented their model performance in fully supervised scenario. The panoptic segmentation performance of our unified TASCNet outperforms their fully supervised joint solution <ref type="bibr" target="#b17">[18]</ref> by a large margin. It is also worth noting that our single task semantic segmentation outputs generated from panoptic segmentation results is comparable to the state-of-the-art segmentation only methods using very large backbones such as <ref type="bibr" target="#b2">[3]</ref>.</p><p>Limited previous panoptic segmentation performance has been reported on the Mapillary Vistas dataset. <ref type="bibr" target="#b14">[15]</ref> reported a reference P Q of 38.3 on a subset of the Vistas test set, combining the winning entries from the in LSUN'17 Segmentation Challenge <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b23">24]</ref>. Megvii report their ECCV'18 Panoptic Segmentation Challenge winning entry in <ref type="bibr" target="#b3">[4]</ref>. However, it is difficult to make a fair comparison to their models without additional technical details. Instead, we treat our own separate modules as baselines, generating the panoptic output as in <ref type="bibr" target="#b14">[15]</ref>. We also present our network performance under different settings.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, Our TASCNet outperform other unifiedmodel entries on COCO Panoptic Segmentation Leaderboard. In our prediction, no test-time augmentation or extra heuristic post-processing for COCO, which typically used in challenge entries <ref type="bibr" target="#b3">[4]</ref>, e.g. threshold small stuff area, heuristically not suppress ties on top of human beings, and etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablative Analysis</head><p>In this section, we present a thorough ablation study of our proposed method on the Cityscapes dataset. We compare key hyper-parameters as well as variations to important components of the network. We also compare different training strategies on the model.</p><p>In <ref type="table" target="#tab_6">Table 4</ref>, we compare different variation to the proposed model on key components. Let λ denotes the weight to TASC loss. We compare the model performance under   <ref type="table">Table 5</ref>: TASCNet training strategies on Cityscapes. We compare our approach given different pretrained backbones. We also compare different training strategy (Single pass joined training vs. stage-wise training).</p><p>different λ values as well as λ = 0, which indicates a model without the propose TASC loss. The weight for all the other losses are 1.0 everywhere else. The experimental results indicates that the use of TASC loss improves the model performance in PQ with a small variation base on the weight. We choose λ = 1 as our final configuration in all of our other experiments. We also make a comparison on the prediction ontology of stuff head. As discussed Section 3.1.2, the minimum number of classes to be predicted in stuff head is N + 1 with N stuff classes and a collapsed 'things' class. We find that predicting a full ontology (N stuff classes and M thing classes) in the stuff head help improve the final performance.</p><p>For the second part of the analysis, we explore the training protocol for our joint network. We pretrain on various datasets and also examine stage-wise training, as shown in <ref type="table">Table 5</ref>. It is obvious that a backbone pretrained from COCO datasets in general helps to improve the final panop-tic segmentation on Cityscapes. In the exploration of stagewise training, we start with training our model with only one head attached, things head or stuff head. When the single-head model converged, we attached the other head and the TASC, then retrain the model. The experimental results indicates that joint training without any fully trained head tends to converge to better minima.</p><p>Please refer to the appendix for more detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed an end-to-end network, TASCNet, to jointly predict stuff and things. We demonstrate that a novel cross-task constraint can boost performance on instance, semantic, and panoptic segmentation performance. Our proposed approach is competitive with state-of-the-art models on both panoptic segmentation task and single modal segmentation tasks on benchmark datasets, while using far fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vistas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Image</head><p>TASCNet Panoptic Segmentation Mismatched Segments </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Detailed Experimental Results</head><p>In this appendix, we provide PQ performance in detail on both Cityscapes <ref type="table" target="#tab_8">(Table 6</ref>) and Mapillary Vistas <ref type="table" target="#tab_9">(Table 7)</ref> as well as more qualitative results ( <ref type="figure">Figure 6 and Figure 7)</ref>. These results were obtained using the test-time augmentation described in the main paper (multi-scale and flip).</p><p>We note that, in Cityscapes, some of the stuff classes that achieve the lowest PQ scores (e.g. wall and fence) actually perform respectably on IoU. This is because PQ treats all pixels from each stuff class as a single segment.</p><p>This effectively penalizes stuff segmentation more harshly than things segmentation (where there are potentially multiple segments per class). We believe it's important to improve on the PQ metric to strike a better balance between how thing and stuff classes are evaluated.</p><p>We can tell from <ref type="table" target="#tab_9">Table 7</ref> that our main challenge on Mapillary Vistas is poor performance on rare classes, even though we used a weighted, bootstrapped loss <ref type="bibr" target="#b29">[30]</ref> for training. Similar issues have been observed by existing approaches to semantic segmentation on this dataset <ref type="bibr" target="#b23">[24]</ref>. In future work we hope to explore other techniques for balanced batch sampling and rare class bootstrapping to improve TASCNet performance. The examples in <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref> further depict how our proposed method is able to achieve high quality panoptic segmentation using a unified architecture with a small backbone.</p><p>Some examples also further illustrate our discussion of stuff bias in PQ. In the third sample in <ref type="figure">Figure 7</ref>, although we correctly classify a substantial fraction of lane marking pixels, the lane marking PQ for this image is 0 as the fraction is under 50%.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Image</head><p>TASCNet Panoptic Segmentation Mismatched Segments <ref type="figure">Figure 6</ref>: More Panoptic Segmentation Examples from Cityscapes. In panoptic segmentation results, different instances are color-coded with different colors with small variations from the base color of their semantic class. In mismatched segments, segments belongs to true positives are marked as white, while false positive and false negative segments are marked as black.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Image</head><p>TASCNet Panoptic Segmentation Mismatched Segments <ref type="figure">Figure 7</ref>: More Panoptic Segmentation Examples from Mapillary Vistas. In panoptic segmentation results, different instances are color-coded with different colors with small variations from the base color of their semantic class. In mismatched segments, segments belongs to true positives are marked as white, while false positive and false negative segments are marked as black.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We propose an end-to-end architecture for panoptic segmentation. Our model predicts things and stuff with a shared backbone and an internal mask enforcing Things and Stuff Consistency (TASC) that can be used to guide fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>RoI-Flatten. We proposed a differentiable operation to merge individual proposal masks into a binary mask to provide global constrain across tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Residual Example. Example image of residuals from a model trained without TASC (left) and a model trained with TASC (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Panoptic segmentation examples from Cityscapes and Mapillary Vistas. In panoptic segmentation results, different instances are color-coded with different colors with small variation from the base color of their semantic class. In matched segments, segments belongs to true positives are marked as white, while false positive and false negative segments are marked as black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Cityscapes Panoptic Segmentation Results.</figDesc><table><row><cell>Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Vistas Panoptic Segmentation Results. We compare our segmentation performance of the unified network to separate networks for instance and semantic segmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method Spec. PQ SQ RQ PQ th. SQ th. RQ th. PQ st. SQ st. RQ st.</figDesc><table><row><cell>MMAP-seg</cell><cell>32.2 76.0 40.8 39.0</cell><cell>78.2</cell><cell>49.1</cell><cell>22.0</cell><cell>72.8</cell><cell>28.4</cell></row><row><cell>MPS-TU</cell><cell>27.2 71.9 35.9 29.6</cell><cell>71.6</cell><cell>39.4</cell><cell>23.4</cell><cell>72.3</cell><cell>30.6</cell></row><row><cell>LeChen</cell><cell>26.2 74.2 33.2 31.3</cell><cell>76.2</cell><cell>39.3</cell><cell>18.7</cell><cell>71.2</cell><cell>24.1</cell></row><row><cell>TASCNet</cell><cell>40.7 78.5 50.1 47.0</cell><cell>80.6</cell><cell>57.1</cell><cell>31.0</cell><cell>75.3</cell><cell>39.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>COCO (Test-dev) Panoptic Segmentation Results. We report TASCNet single model performance on COCO testdev server without any test-time augmentation. We compare our model to the challenge entries on the panoptic segmentation leaderboard that also uses unified networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation analysis on important network components. We conduct ablation analysis on different components of the proposed method. M and N indicates the number of things classes and stuff classes respectively. λ indicates the weight of TASC loss. 'M.' indicates the use of test time augmentation. The final TASCNet configuration is included in the last row.</figDesc><table><row><cell cols="2">Pretrain Data Training Strategy</cell><cell>PQ</cell><cell cols="2">PQ th. PQ st.</cell></row><row><cell>ImageNet</cell><cell>Joined</cell><cell cols="2">55.9 50.5</cell><cell>59.8</cell></row><row><cell>COCO</cell><cell>Stuff Head + Joined</cell><cell cols="2">57.3 51.6</cell><cell>61.5</cell></row><row><cell>COCO</cell><cell cols="3">Things Head + Joined 58.5 55.0</cell><cell>61.0</cell></row><row><cell>COCO</cell><cell>Joined</cell><cell cols="2">59.2 56.0</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Panoptic Quality (PQ) on Cityscapes. IoUs are also included. Note that PQ, particularly for stuff classes, is a very stringent metric.</figDesc><table><row><cell>Class</cell><cell cols="3">PQ SQ RQ</cell><cell>Class</cell><cell cols="2">PQ SQ RQ</cell></row><row><cell>mean</cell><cell cols="3">34.3 74 43.5</cell><cell>object-banner</cell><cell cols="2">30.6 82.3 37.1</cell></row><row><cell>animal-bird</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>object-bench</cell><cell cols="2">26.8 73.8 36.4</cell></row><row><cell>animal-ground-animal</cell><cell cols="3">39.1 80.5 48.5</cell><cell>object-bike-rack</cell><cell cols="2">6.7 71.2 9.4</cell></row><row><cell>construction-barrier-curb</cell><cell cols="3">42.9 70.5 60.9</cell><cell>object-billboard</cell><cell cols="2">40.1 82.1 48.8</cell></row><row><cell>construction-barrier-fence</cell><cell cols="3">32.0 72.2 44.3</cell><cell>object-catch-basin</cell><cell cols="2">35.7 75.4 47.4</cell></row><row><cell>construction-barrier-guard-rail</cell><cell cols="3">29.5 73.2 40.3</cell><cell>object-cctv-camera</cell><cell cols="2">21.0 69.5 30.2</cell></row><row><cell cols="4">construction-barrier-other-barrier 24.3 77.4 31.3</cell><cell>object-fire-hydrant</cell><cell cols="2">56.8 80.8 70.2</cell></row><row><cell>construction-barrier-wall</cell><cell cols="3">19.4 72.7 26.7</cell><cell>object-junction-box</cell><cell cols="2">41.1 85.4 48.1</cell></row><row><cell>construction-flat-bike-lane</cell><cell cols="3">13.0 69.9 18.7</cell><cell>object-mailbox</cell><cell cols="2">23.0 79.0 29.2</cell></row><row><cell cols="4">construction-flat-crosswalk-plain 32.9 75.3 43.7</cell><cell>object-manhole</cell><cell cols="2">47.7 81.2 58.8</cell></row><row><cell>construction-flat-curb-cut</cell><cell cols="3">3.3 62.0 5.4</cell><cell>object-phone-booth</cell><cell cols="2">15.1 84.7 17.9</cell></row><row><cell>construction-flat-parking</cell><cell cols="3">5.9 64.2 9.2</cell><cell>object-pothole</cell><cell cols="2">0.6 63.0 1.0</cell></row><row><cell cols="4">construction-flat-pedestrian-area 19.1 85.2 22.4</cell><cell>object-street-light</cell><cell cols="2">46.0 74.0 62.1</cell></row><row><cell>construction-flat-rail-track</cell><cell cols="3">11.9 70.5 16.9</cell><cell>object-support-pole</cell><cell cols="2">33.8 70.3 48.0</cell></row><row><cell>construction-flat-road</cell><cell cols="3">83.1 89.1 93.3</cell><cell cols="3">object-support-traffic-sign-frame 18.8 64.4 29.2</cell></row><row><cell>construction-flat-service-lane</cell><cell cols="3">28.6 80.2 35.7</cell><cell>object-support-utility-pole</cell><cell cols="2">36.9 68.4 53.9</cell></row><row><cell>construction-flat-sidewalk</cell><cell cols="3">52.4 77.6 67.6</cell><cell>object-traffic-light</cell><cell cols="2">59.0 79.4 74.4</cell></row><row><cell>construction-structure-bridge</cell><cell cols="3">33.1 76.6 43.2</cell><cell>object-traffic-sign-back</cell><cell cols="2">39.0 74.9 52.2</cell></row><row><cell>construction-structure-building</cell><cell cols="3">69.2 83.0 83.5</cell><cell>object-traffic-sign-front</cell><cell cols="2">58.8 83.8 70.2</cell></row><row><cell>construction-structure-tunnel</cell><cell cols="3">8.9 56.8 15.7</cell><cell>object-trash-can</cell><cell cols="2">48.5 83.7 57.9</cell></row><row><cell>human-person</cell><cell cols="3">54.7 78.7 69.5</cell><cell>object-vehicle-bicycle</cell><cell cols="2">37.7 72.1 52.2</cell></row><row><cell>human-rider-bicyclist</cell><cell cols="3">43.3 73.6 58.8</cell><cell>object-vehicle-boat</cell><cell cols="2">17.1 67.8 25.3</cell></row><row><cell>human-rider-motorcyclist</cell><cell cols="3">38.2 70.3 54.3</cell><cell>object-vehicle-bus</cell><cell cols="2">54.6 87.6 62.3</cell></row><row><cell>human-rider-other-rider</cell><cell cols="3">7.8 52.8 14.8</cell><cell>object-vehicle-car</cell><cell cols="2">68.7 85.8 80.1</cell></row><row><cell>marking-crosswalk-zebra</cell><cell cols="3">46.1 76.1 60.5</cell><cell>object-vehicle-caravan</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>marking-general</cell><cell cols="3">41.1 68.6 59.9</cell><cell>object-vehicle-motorcycle</cell><cell cols="2">45.0 73.1 61.6</cell></row><row><cell>nature-mountain</cell><cell cols="3">22.1 71.8 30.8</cell><cell>object-vehicle-on-rails</cell><cell cols="2">6.9 79.4 8.7</cell></row><row><cell>nature-sand</cell><cell cols="3">4.2 81.5 5.1</cell><cell>object-vehicle-other-vehicle</cell><cell cols="2">22.2 74.6 29.7</cell></row><row><cell>nature-sky</cell><cell cols="3">96.0 96.8 99.2</cell><cell>object-vehicle-trailer</cell><cell cols="2">13.6 79.3 17.1</cell></row><row><cell>nature-snow</cell><cell cols="3">33.8 82.3 41.1</cell><cell>object-vehicle-truck</cell><cell cols="2">51.6 86.3 59.8</cell></row><row><cell>nature-terrain</cell><cell cols="3">37.8 76.9 49.2</cell><cell>object-vehicle-wheeled-slow</cell><cell cols="2">27.8 70.3 39.5</cell></row><row><cell>nature-vegetation</cell><cell cols="3">81.3 86.0 94.5</cell><cell>void-car-mount</cell><cell cols="2">51.7 85.2 60.7</cell></row><row><cell>nature-water</cell><cell cols="3">17.7 75.4 23.4</cell><cell>void-ego-vehicle</cell><cell cols="2">71.5 90.4 79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Panoptic Quality (PQ) on Mapillary Vistas.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. For each RoI, we only consider the foreground/background mask for the class of the ground truth instance the RoI was assigned to regress, 3. We interpolate each of these single-instance masks, (M 1 , ..., M K ), to the size of its corresponding RoI in the input image.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On seeing stuff: the perception of materials by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4299</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>Human vision and electronic imaging VI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Unified Architecture for Instance and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<ptr target="http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf,2017.Online.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<idno>abs/1712.02616</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MSCOCO and Mapillary Panoptic Segmentation Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jingbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Changqian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huanyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yueqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<ptr target="http://www.skicyyu.org/Presentation/panoptic.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04837</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gradnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02257</idno>
		<title level="m">Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-and semisupervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weakly-and semisupervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03575</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lsun17: insatnce segmentation task, ucenter winner team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast scene understanding for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Vehicle Perception, workshop at the IEEE Symposium on Intelligent Vehicles</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06132</idno>
		<idno>arXiv:1810.04650</idno>
	</analytic>
	<monogr>
		<title level="m">Sener and V. Koltun. Multi-task learning as multi-objective optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relating things and stuff via objectproperty interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1370" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Finding things: Image parsing with regions and per-exemplar detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3001" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3748" to="3755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixellevel encoding and depth layering for instance-level semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Group normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1803.08494</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
