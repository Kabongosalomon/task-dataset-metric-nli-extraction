<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
							<email>yuhangzang@foxmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China 4 Megvii (Face++) Technology Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff3">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text detection, an important step of scene text reading systems, has witnessed rapid development with convolutional neural networks. Nonetheless, two main challenges still exist and hamper its deployment to real-world applications. The first problem is the trade-off between speed and accuracy. The second one is to model the arbitrary-shaped text instance. Recently, some methods have been proposed to tackle arbitrary-shaped text detection, but they rarely take the speed of the entire pipeline into consideration, which may fall short in practical applications. In this paper, we propose an efficient and accurate arbitrary-shaped text detector, termed Pixel Aggregation Network (PAN), which is equipped with a low computational-cost segmentation head and a learnable post-processing. More specifically, the segmentation head is made up of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a cascadable U-shaped module, which can introduce multi-level information to guide the better segmentation. FFM can gather the features given by the FPEMs of different depths into a final feature for segmentation. The learnable post-processing is implemented by Pixel Aggregation (PA), which can precisely aggregate text pixels by predicted similarity vectors. Experiments on several standard benchmarks validate the superiority of the proposed PAN. It is worth noting that our method can achieve a competitive F-measure of 79.9% at 84.2 FPS on CTW1500. * Authors contributed equally. † Corresponding author.</p><p>10.7% better 4 × faster <ref type="figure">Figure 1</ref>. The performance and speed on curved text dataset CTW1500. PAN-640 is 10.7% better than CTD+TLOC, and PAN-320 is 4 times faster than EAST. * indicates the results from <ref type="bibr" target="#b30">[31]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection is a fundamental and critical task in computer vision, as it is a key step in many text-related applications, such as text recognition, text retrieval, license plate recognition and text visual question answering. In virtue of recent development of object detection <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b56">57]</ref> and segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> based on CNN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b24">25]</ref>, scene text detection has witnessed great progress <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b23">24]</ref>. Arbitrary-shaped text detection, one of the most challenging tasks in text detection, is receiving more and more research attention Some new methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b23">24]</ref> have been put forward to detect curve text instance. However, many of these methods suffer from low inference speed, because of their heavy models or complicated post-processing steps, which limits their deployment in the real-world environment. On the other hand, previous text detectors <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b31">32]</ref> with high efficiency are mostly designed for quadrangular text instances, which have flaws when detecting curved text. Therefore, "how to design an efficient and accurate arbitrary-shaped text detector" remains largely unsolved.</p><p>To solve these problems, here we propose an arbitraryshaped text detector, namely Pixel Aggregation Network (PAN), which can achieve a good balance between speed and performance. PAN makes arbitrary-shaped text detection following the simple pipeline as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, which only contains two steps: i) Predicting the text regions, kernels and similarity vectors by segmentation network. ii) Rebuilding complete text instances from the predicted kernels. For high efficiency, we need to reduce the time cost of these two steps. First and foremost, a lightweight backbone is required for segmentation. In this paper, we use ResNet18 <ref type="bibr" target="#b13">[14]</ref> as the default backbone of PAN. However, the lightweight backbone is relatively weak in feature extraction, and thus its features typically have small receptive fields and weak representation capabilities. To remedy this defect, we propose a low computation-cost segmentation head, which is composed of two modules: Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a U-shaped module built by separable convolutions (see <ref type="figure">Fig. 4</ref>), and therefore FPEM is able to enhance the features of different scales by fusing the lowlevel and high-level information with minimal computation overhead. Moreover, FPEM is cascadable, which allows us to compensate for the depth of lightweight backbone by appending FPEMs after it (see <ref type="figure" target="#fig_2">Fig. 3</ref> (d)(e)). To gather low-level and high-level semantic information, before final segmentation, we introduce FFM to fuse the features generated by the FPEMs of different depths. In addition, to reconstruct complete text instances accurately, we propose a learnable post-processing method, namely Pixel Aggregation (PA), which can guide the text pixels to correct kernels through the predicted similarity vectors.</p><p>To show the effectiveness of our proposed PAN, we conduct extensive experiments on four challenging benchmark datasets including CTW1500 <ref type="bibr" target="#b30">[31]</ref>, Total-Text <ref type="bibr" target="#b1">[2]</ref>, ICDAR 2015 <ref type="bibr" target="#b21">[22]</ref> and MSRA-TD500 <ref type="bibr" target="#b51">[52]</ref>. Among these datasets, CTW1500 and Total-Text are new datasets designed for curve text detection. As shown in <ref type="figure">Fig. 1</ref>, on CTW1500, the F-measure of PAN-640 is 83.7% which is 10.7% better than CTD+TLOC <ref type="bibr" target="#b30">[31]</ref>, and the FPS of PAN-320 is 84.2 which is 4 times faster than EAST <ref type="bibr" target="#b57">[58]</ref>. Meanwhile, PAN also has promising performance on multi-oriented and long text datasets.</p><p>In summary, our contributions are three-fold. Firstly, we propose a lightweight segmentation neck consisting of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM) which are two high-efficiency modules that can improve the feature representation of the network. Secondly, we propose Pixel Aggregation (PA), in which the text similarity vector can be learned by the network and be used to selectively aggregate pixels nearby the text kernels. Finally, the proposed method achieves state-ofthe-art performance on two curved text benchmarks while still keeping the inference speed of 58 FPS. To our knowledge, ours is the first algorithm which can detect curved text precisely in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, text detectors based on deep learning have achieved remarkable results. Most of these methods can be roughly divided into two categories: anchor-based methods and anchor-free methods. Among these methods, some use a heavy framework or complicated pipeline for high accuracy, while others adopt a simple structure to maintain a good balance between speed and accuracy.</p><p>Anchor-based text detectors are usually inspired by object detectors such as Faster R-CNN <ref type="bibr" target="#b40">[41]</ref> and SSD <ref type="bibr" target="#b40">[41]</ref>. TextBoxes <ref type="bibr" target="#b26">[27]</ref> directly modifies the anchor scales and shape of convolution kernels of SSD to handle text with extreme aspect ratios. TextBoxes++ <ref type="bibr" target="#b25">[26]</ref> further regresses quadrangles instead of horizontal bounding boxes for multioriented text detection. RRD <ref type="bibr" target="#b27">[28]</ref> applies rotation-invariant and sensitive features for text classification and regression from two separate branches for better long text detection. SSTD <ref type="bibr" target="#b15">[16]</ref> generates text attention map to enhance the text region of the feature map and suppress background information, which is beneficial for tiny texts. Based on Faster R-CNN, RRPN <ref type="bibr" target="#b37">[38]</ref> develops rotated region proposals to detect titled text. Mask Text Spotter <ref type="bibr" target="#b35">[36]</ref> and SPCNet <ref type="bibr" target="#b49">[50]</ref> regard text detection as an instance segmentation problem and use Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> for arbitrary text detection. The above-mentioned methods achieve remarkable results on several benchmarks. Nonetheless, most of them rely on complex anchor setting, which makes these approaches heavy-footed and prevent them from applying to real-world problems.</p><p>Anchor-free text detectors formulate text detection as a text segmentation problem, which are often built upon fully convolutional networks (FCN) <ref type="bibr" target="#b33">[34]</ref>. Zhang et al. <ref type="bibr" target="#b54">[55]</ref> first estimatetext blocks with FCNs and detectcharacter candidates from those text blocks with MSER. Yao et al. <ref type="bibr" target="#b52">[53]</ref> use FCN to predict three parts of a text instance, text/non-text, character classes, and character linking orientations, then  apply a group process for text detection. To separate adjacent text instances, PixelLink <ref type="bibr" target="#b2">[3]</ref> performs text/non-text and links prediction in pixel level, then applies post-processing to obtain text boxes and excludes noises. EAST <ref type="bibr" target="#b57">[58]</ref> and DeepReg <ref type="bibr" target="#b16">[17]</ref> adopt FCNs to predict shrinkable text score maps and perform per-pixel regression, followed by a postprocessing NMS. TextSnake <ref type="bibr" target="#b34">[35]</ref> models text instances with ordered disks and text center lines, which is able to represent text in arbitrary shapes. PSENet <ref type="bibr" target="#b23">[24]</ref> uses FCN to predict text instances with multiple scales, then adopts progressive scale expansion algorithm to reconstruct the whole text instance. Briefly speaking, the main differences among anchor-free methods are the way of text label generation and post-processing. Nevertheless, among these methods, only TextSnake and PSENet are designed for detecting curved text instances which also widely appear in natural scenes. However, they suffer from a heavy framework or a complicated pipeline, which usually slows down their inference speed.</p><p>Real time text detection requires a fast way to generate high-quality text prediction. EAST <ref type="bibr" target="#b57">[58]</ref> directly employs FCNs to predict the score map and corresponding coordinates and, followed by a simple NMS. The whole pipeline of EAST is concise so that it can maintain a relatively high speed. MCN <ref type="bibr" target="#b31">[32]</ref> formulates text detection problem as a graph-based clustering problem and generates bounding boxes without using NMS, which can be fully parallelized on GPUs. However, these methods are designed for quadrangular text detection and fail to locate the curve text instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>PAN follows a segmentation-based pipeline (see <ref type="figure" target="#fig_0">Fig. 2</ref>) to detect arbitrary-shaped text instances. For high efficiency, the backbone of the segmentation network must be lightweight. However, the features offered by a lightweight backbone often have small receptive fields and weak representation capabilities. For this reason, we propose segmentation head that is computationally efficient to refine the features. The segmentation head contains two key modules, namely Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). As shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (d)(e) and <ref type="figure">Fig. 4</ref>, FPEM is cascadable and has low computational cost, which can be attached behind the backbone to make features of different scales deeper and more expressive. After that, we employ the Feature Fusion Module (FFM) to fuse the features produced by the FPEMs of different depths into a final feature for segmentation. PAN predicts text regions (see <ref type="figure" target="#fig_2">Fig. 3</ref> (g)) to describe the complete shapes of text instances, and predicts kernels (see <ref type="figure" target="#fig_2">Fig. 3</ref> (h)) to distinguish different text instances. The network also predicts similarity vector (see <ref type="figure" target="#fig_2">Fig. 3</ref> (i)) for each text pixel, so that the distance between the similarity vectors of pixel and kernel from the same text instance is small. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the overall architecture of PAN. We employ a lightweight model (ResNet-18 <ref type="bibr" target="#b13">[14]</ref>) as the backbone network of PAN. There are 4 feature maps (see <ref type="figure" target="#fig_2">Fig. 3</ref> (b)) generated by conv2, conv3, conv4, and conv5 stages of backbone, and note that they have strides of 4, 8, 16, 32 pixels with respect to the input image. We use 1×1 convolution to reduce the channel number of each feature map to 128, and get a thin feature pyramid F r . The feature pyra-</p><formula xml:id="formula_0">2× 3×3 DWConv 1×1 Conv BN &amp; ReLU + + 3×3 DWConv (stride=2) 2× 1×1 Conv BN &amp; ReLU Input Output</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Up-Scale Enhancement</head><p>Down-Scale Enhancement <ref type="figure">Figure 4</ref>. The details of FPEM. "+", "2×", "DWConv", "Conv" and "BN" represent element-wise addition, 2× linear upsampling, depthwise convolution <ref type="bibr" target="#b17">[18]</ref>, regular convolution <ref type="bibr" target="#b22">[23]</ref> and Batch Normalization <ref type="bibr" target="#b20">[21]</ref> respectively. mid is enhanced by n c cascaded FPEMs. Each FPEM produces an enhanced feature pyramid, and thus there are n c enhanced feature pyramids F 1 , F 2 ,..., F nc . FFM fuses the n c enhanced feature pyramids into a feature map F f , whose stride is 4 pixels and the channel number is 512. F f is used to predict text regions, kernels and similarity vectors. Finally, we apply a simple and efficient post-processing algorithm to obtain the final text instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Pyramid Enhancement Module</head><p>FPEM is a U-shaped module as illustrated in <ref type="figure">Fig. 4</ref>. It consists of two phases, namely, up-scale enhancement and down-scale enhancement. The up-scale enhancement acts on the input feature pyramid. In this phase, the enhancement is iteratively performed on the feature maps with strides of 32, 16, 8, 4 pixels. In the down-scale phase, the input is the feature pyramid generated by up-scale enhancement, and the enhancement is conducted from 4-stride to 32-stride.</p><p>Meanwhile, the output feature pyramid of down-scale enhancement is the final output of FPEM. We employ separable convolution <ref type="bibr" target="#b17">[18]</ref> (3×3 depthwise convolution <ref type="bibr" target="#b17">[18]</ref> followed by 1×1 projection) instead of the regular convolution to build the join part ⊕ of FPEM (see the dashed frames in <ref type="figure">Fig. 4</ref>). Therefore, FPEM is capable of enlarging the receptive field (3×3 depthwise convolution) and deepening the network (1×1 convolution) with a small computation overhead.</p><p>Similar to FPN <ref type="bibr" target="#b28">[29]</ref>, FPEM is able to enhance the features of different scales by fusing the low-level and highlevel information. In addition, different from FPN, there are two other advantages of FPEM. Firstly, FPEM is a cascadable module. With the increment of cascade number n c , the feature maps of different scales are fused more adequately and the receptive fields of features become larger. Secondly, FPEM is computationally cheap. FPEM is built by separa- <ref type="figure">Figure 5</ref>. The detail of FFM. "+" is element-wise addition. "C" is the operation of upsampling and concatenating.</p><formula xml:id="formula_1">+ + + + FPEM FPEM … ℂ</formula><p>ble convolution, which needs minimal computation. The FLOPS of FPEM is about 1/5 of FPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Fusion Module</head><p>Feature Fusion Module is applied to fuse the feature pyramids F 1 , F 2 ,..., F nc of different depths. Because both low-level and high-level semantic information are important for semantic segmentation. A direct and effective method to combine these feature pyramids is to upsample and concatenate them. However, the fused feature map given by this method has a large channel number (4×128×n c ), which slows down the final prediction. Thus, we propose another fusion method as shown in <ref type="figure">Fig. 5</ref>. We firstly combine the corresponding-scale feature maps by element-wise addition. Then, the feature maps after addition are upsampled and concatenated into a final feature map which only has 4×128 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pixel Aggregation</head><p>The text regions keep the complete shape of text instances, but the text regions of the text instances lying closely are often overlapping (see <ref type="figure" target="#fig_2">Fig. 3</ref> (g)). Contrarily, the text instances can be well distinguished using the kernels (see <ref type="figure" target="#fig_2">Fig.3</ref> (h)). However, the kernels are not the complete text instance. To rebuild the complete text instances, we need to merge the pixels in text regions to kernels. We propose a learnable algorithm, namely Pixel Aggregation, to guide the text pixels towards correct kernels.</p><p>In Pixel Aggregation, we borrow the idea of clustering to reconstruct the complete text instances from the kernels. Let us consider the text instances as clusters. The kernels of text instances are cluster centers. The text pixels are the samples to be clustered. Naturally, to aggregate the text pixels to the corresponding kernels, the distance between the text pixel and kernel of the same text instance should be small. In the training phase, we use aggregation loss L agg as Equ. 1 to implement this rule.</p><formula xml:id="formula_2">L agg = 1 N N i=1 1 |T i | p∈Ti ln(D(p, K i ) + 1),<label>(1)</label></formula><formula xml:id="formula_3">D(p, K i ) = max( F(p) − G(K i ) − δ agg , 0) 2 ,<label>(2)</label></formula><p>where the N is the number of text instances. The T i is the ith text instance. D(p, K i ) defines the distance between text pixel p and the kernel K i of text instance T i . δ agg is a constant, which is set to 0.5 experimentally and used to filter easy samples. F(p) is the similarity vector of the pixel p. G(·) is the similarity vector of the kernel K i , which can be calculated by q∈Ki F(q)/ |K i |.</p><p>In addition, the cluster centers need to keep discrimination. Therefore, the kernels of different text instances should maintain enough distance. We use discrimination loss L dis as Equ. 3 to describe this rule during the training.</p><formula xml:id="formula_4">L dis = 1 N (N − 1) N i=1 N j=1 j =i ln(D(K i , K j ) + 1),<label>(3)</label></formula><formula xml:id="formula_5">D(K i , K j ) = max(δ dis − G(K i ) − G(K j ) , 0) 2 . (4)</formula><p>L dis try to keep the distance among the kernels not less than δ dis which is set to 3 in all our experiments.</p><p>In the testing phase, we use the predicted similarity vector to guide the pixels in the text area to the corresponding kernel. The detailed post-processing steps are as follows: i) Finding the connected components in the kernels' segmentation result, and each connected component is a single kernel. ii) For each kernel K i , conditionally merging its neighbor text pixel (4-way) p in predicted text regions while the Euclidean distance of their similarity vectors is less than d. iii) Repeating step ii) until there is no eligible neighbor text pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>Our loss function can be formulated as:</p><formula xml:id="formula_6">L = L tex + αL ker + β(L agg + L dis ),<label>(5)</label></formula><p>where L tex is the loss of the text regions and L ker is the loss of the kernels. The α and β are used to balance the importance among L tex , L ker , L agg and L dis , and we set them to 0.5 and 0.25 respectively in all experiments.</p><p>Considering the extreme imbalance of text and non-text pixels, we follow <ref type="bibr" target="#b23">[24]</ref> and adopt dice loss <ref type="bibr" target="#b38">[39]</ref> to supervise the segmentation result P tex of the text regions and P ker of the kernels. Thus L tex and L ker can be written as follows:</p><formula xml:id="formula_7">L tex = 1 − 2 i P tex (i)G tex (i) i P tex (i) 2 + i G tex (i) 2 ,<label>(6)</label></formula><formula xml:id="formula_8">L ker = 1 − 2 i P ker (i)G ker (i) i P ker (i) 2 + i G ker (i) 2 ,<label>(7)</label></formula><p>where P tex (i) and G tex (i) refer to the value of the ith pixel in the segmentation result and the ground truth of the text regions respectively. The ground truth of the text regions is a binary image, in which text pixel is 1 and non-text pixel is 0. Similarly, P ker (i) and G ker (i) means the ith pixel value in the prediction and the ground truth of the kernels. The ground truth of the kernels is generated by shrinking original ground truth polygon, and we follows the method in <ref type="bibr" target="#b23">[24]</ref> to shrink the original polygon by ratio r. Note that, we adopt Online Hard Example Mining (OHEM) <ref type="bibr" target="#b42">[43]</ref> to ignore simple non-text pixels when calculating L tex , and we only take the text pixels in ground truth into consideration while calculating L ker , L agg and L dis .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>SynthText <ref type="bibr" target="#b9">[10]</ref> is a large scale synthetically generated dataset containing 800K synthetic images. Following <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref>, we pre-train our model on this dataset.</p><p>CTW1500 <ref type="bibr" target="#b30">[31]</ref> is a recent challenging dataset for curve text detection. It has 1000 training images and 500 testing images. The dataset focus on curve text instances which are labeled by 14-polygon.</p><p>Total-Text <ref type="bibr" target="#b1">[2]</ref> is also a newly-released dataset for curve text detection. This dataset includes horizontal, multioriented and curve text instances and consists of 1255 training images and 300 testing images.</p><p>ICDAR 2015 (IC15) <ref type="bibr" target="#b21">[22]</ref> is a commonly used dataset for text detection. It contains a total of 1500 images, 1000 of which are used for training and the remaining are for testing. The text instances are annotated by 4 vertices of the quadrangle.</p><p>MSRA-TD500 includes 300 training images and 200 test images with text line level annotations. It is a dataset with multi-lingual, arbitrary-oriented and long text lines. Because the training set is rather small, we follow the previous works <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref> to include the 400 images from HUST-TR400 <ref type="bibr" target="#b50">[51]</ref> as training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the ResNet <ref type="bibr" target="#b14">[15]</ref> or VGG16 <ref type="bibr" target="#b43">[44]</ref> pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref> as our backbone. The dimension of the similarity vector is set to 4. All the networks are optimized by using stochastic gradient descent (SGD). The pre-trained model is trained on SynthText for 50K iterations with a fixed learning rate of 1 × 10 −3 . Two training strategies are adopted in other experiments: i) Training from scratch. ii) Fine-tuning on SynthText pre-trained model. When training from scratch, we train PAN with batch size 16 on 4 GPUs for 36K iterations, and the initial learning rate is set to 1 × 10 −3 . Similar to <ref type="bibr" target="#b55">[56]</ref>, we use the "poly" learning rate strategy in which the initial rate is multiplied by (1− iter max iter ) power , and the power is set to 0.9 in all experiments. When fine-tuning on SynthText pre-trained model, the number of iterations is 36K, and the initial learning rate is 1 × 10 −3 . We use a weight decay of 5 × 10 −4 and a Nesterov momentum <ref type="bibr" target="#b44">[45]</ref> of 0.99. We adopt the weight initialization introduced by <ref type="bibr" target="#b12">[13]</ref>.</p><p>In the training phase, we ignore the blurred text regions labeled as DO NOT CARE in all datasets. The negativepositive ratio of OHEM is set to 3. We apply random scale, random horizontal flip, random rotation and random crop on training images. On ICDAR 2015 and MSRA-TD500, we fit a minimal area rectangle for each predicted text instance. The shrink ratio r of the kernels is set to 0.5 on ICDAR 2015 and 0.7 on other datasets. In the testing phase, the distance threshold d is set to 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To make the conclusion of ablation studies more generalized, all experiments of ablation studies are conducted on ICDAR 2015 (a quadrangle text dataset) and CTW1500 (a curve text dataset). Note that, in these experiments, all models are trained without any external dataset. The short sides of test images in ICDAR 2015 and CTW1500 are set to 736 and 640 respectively.</p><p>The influence of the number of cascaded FPEMs. We study the effect of the number of cascaded FPEMs by varying n c from 0 to 4. Note that, when n c = 0, we upsample and concatenate the feature maps in F r to get F f . From Table 1, we can find that the F-measures on the test sets keep rising with the growth of n c and begins to level off when n c ≥ 2. However, a large n c will slow down the model despite the low computational cost of FPEM. For each additional FPEM, the FPS will decrease by about 2-5 FPS. To keep a good balance of performance and speed, we set n c to 2 by default in the following experiments.</p><p>The effectiveness of FPEM. We design two groups of experiments to verify the effectiveness of FPEM. Firstly, we make a comparison between the model with FPEM and without FPEM. As shown in <ref type="table" target="#tab_0">Table 1</ref>, compared to the model without FPEM (n c = 0), the model with one FPEM (n c = 1) can make about 1.5% improvement on F-measure while bringing tiny extra computation. Secondly, we make comparison between a lightweight model equipped with FPEMs and a widely-used segmentation model. To ensure a fair comparison, under the same setting, we employ "ResNet18  <ref type="table">Table 3</ref>. The results of models with different settings. "Fuse" means the fusion method. "Concat" means direct concatenation. "F" means F-measure. + 2 FPEMs + FFM" or "ResNet50 + PSPNet <ref type="bibr" target="#b55">[56]</ref>" as the segmentation network. As shown in <ref type="table">Table.</ref> 2, even the backbone of "ResNet18 + 2 FPEMs + FFM" is lightweight, it can reach almost same performance as "ResNet50 + PSP-Net <ref type="bibr" target="#b55">[56]</ref>". In addition, "ResNet18 + 2 FPEMs + FFM" enjoy over 5 times faster speed than "ResNet50 + PSP-Net <ref type="bibr" target="#b55">[56]</ref>". The model size of "ResNet18 + 2 FPEMs + FFM" is 12.25M. The effectiveness of FFM. To investigate the effectiveness of FFM, we firstly remove FFM and concatenate the feature maps in the last feature pyramid F nc to make final segmentation. The F-measure drop 0.6%-0.8% when the FFM is removed (see <ref type="table">Table 3</ref> #1 and #2), which indicates that besides the features from deep layers, the shallow features are also important to semantic segmentation. We then compare FFM with the direct concatenation mentioned in Sec. 3.3. The proposed FFM can achieve performance comparable to the direct concatenation (see <ref type="table">Table 3</ref> #1 and #3), while FFM is more efficient.</p><p>The effectiveness of PA. We study the validity of PA by removing it from the pipeline. Specifically, we set β to 0 in Eqn. 5 in the training phase and merge all neighbor text pixels in step ii) of post-processing. Comparing the method with PA (see <ref type="table">Table 3</ref> #1), the F-measure of the model without PA (see <ref type="table">Table 3</ref> #4) drops over 1%, which indicate the effectiveness of PA.</p><p>The influence of the backbone. To better analyze the capability of the proposed PAN, we replace the lightweight backbone (ResNet18) to heavier backbone (ResNet50 and VGG16). As shown in <ref type="table">Table 3</ref> #5 and #6, under the same setting, both of ResNet50 and VGG16 can bring over 1% improvement on ICDAR 2015 and over 0.5% improvement on CTW1500. However, the reduction of FPS brought by the heavy backbone is apparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with State-of-the-Art Methods</head><p>Curve text detection. To evaluate the performance of our method for detecting curved text instance, we compare  <ref type="table">Table 4</ref>. The single-scale results on CTW1500. "P", "R" and "F" represent the precision, recall and F-measure respectively. "Ext." indicates external data. * indicates the results from <ref type="bibr" target="#b30">[31]</ref>.</p><p>the proposed PAN with other state-of-the-art methods on CTW1500 and Total-Text which include many curve text instances. In the testing phase, we set the short side of images to different scales (320, 512, 640) and evaluate the results using the same evaluation method with <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b1">[2]</ref>. We report the single-scale performance of PAN on CTW1500 and Total-Text in <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref>, respectively. Note that the backbone of PAN is set to ResNet18 by default. On CTW1500, PAN-320 (the short side of input image is 320), without external data pre-training, achieve the Fmeasure of 77.1% at an astonishing speed (84.2 FPS), in which the F-measure surpasses most of the counterparts, including the methods with external data pre-training, and the speed is 4 times faster than the fastest method. When finetuning on SynthText pre-trained model, the F-measure of PAN-320 can further be boosted to 79.9%, and PAN-512 outperform all other methods in F-measure by at least 1.2% while still keeping nearly real-time speed <ref type="bibr">(58 FPS)</ref>.</p><p>Similar conclusions can be obtained on Total-Text. Without external data pre-training, the speed of PAN-320 is real-time (82.4 FPS) while the performance is still very competitive (77.1%), and PAN-640 achieves the F-measure of 83.5%, surpassing all other state-of-the-art methods (including those with external data) over 0.6%. With Syn-thText pre-training, the F-measure of PAN-320 boosting to 79.9%, and the best F-measure achieve by PAN-640 is 85.0%, which is 2.1% better than second-best SPCNet <ref type="bibr" target="#b49">[50]</ref>. Meanwhile, the speed can still maintain nearly 40 FPS.</p><p>The performance on CTW1500 and Total-Text demonstrates the solid superiority of the proposed PAN to detect arbitrary-shaped text instances. We also illustrate several challenging results in <ref type="figure" target="#fig_3">Fig. 6</ref> (e)(f), which clearly demonstrate that PAN can elegantly distinguish very complex curve text instances.</p><p>Oriented text detection. We evaluate PAN on the IC-DAR 2015 to test its ability for oriented text detection. By default, ResNet18 is adopted as the backbone of PAN. During testing, we scale the short side of input images to 736. The comparisons with other state-of-the-art methods are  <ref type="table">Table 5</ref>. The single-scale results on Total-Text. "P", "R" and "F" represent the precision, recall and F-measure respectively. "Ext." indicates external data. * indicates the results from <ref type="bibr" target="#b34">[35]</ref>.   <ref type="table">Table 7</ref>. The single-scale results on MSRA-TD500. "P", "R" and "F" represent the precision, recall and F-measure respectively. "Ext." indicates external data.</p><p>shown in <ref type="table" target="#tab_5">Table 6</ref>. PAN achieves the F-measure of 80.4% at 26.1 FPS without external data pre-training. Compared with EAST <ref type="bibr" target="#b57">[58]</ref>, our method outperforms EAST 2.1% in F-measure, while the FPS of our method is 2 times of EAST. Fine-tuning on SynthText can further improve the F-measure to 82.9% which is on par with TextSnake <ref type="bibr" target="#b34">[35]</ref>, but our method can run 25 times faster than TextSnake. Although the performance of our method is not as well as some methods (e.g. PSENet, SPCNet), our method has a least 16 times faster speed (26.1 FPS) than these methods. Some qualitative illustrations are shown in <ref type="figure" target="#fig_3">Fig. 6 (g)</ref>. The   <ref type="table">Table 8</ref>. Time consumption of PAN on CTW-1500. The total time consists of backbone, segmentation head and post-processing. "F" represents the F-measure.</p><p>proposed PAN successfully detects text instances of arbitrary orientations and sizes.</p><p>Long straight text detection. To test the robustness of PAN to long straight text instance, we evaluate PAN on MSRA-TD500 benchmark. To ensure fair comparisons, we resize the short edge of test images to 736 as ICDAR 2015. As shown in <ref type="table">Table.</ref> 7, the proposed PAN achieve Fmeasures of 78.9% and 84.1% when the external data is not used and used respectively. Compared with other state-ofthe-art methods, PAN can achieve higher performance and run at a faster speed (30.2 FPS). Thus, PAN is also robust for long straight text detection (see <ref type="figure" target="#fig_3">Fig. 6</ref> (h)) and can indeed be deployed in complex natural scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Result Visualization and Speed Analysis</head><p>Result visualization. An example of PAN prediction is shown in <ref type="figure" target="#fig_3">Fig. 6 (a-d)</ref>. <ref type="figure" target="#fig_3">Fig. 6 (b)</ref> is the predicted text regions which keep the complete shape information of text instances. <ref type="figure" target="#fig_3">Fig. 6 (c)</ref> is the predicted kernels which clearly distinguish different text instances. <ref type="figure" target="#fig_3">Fig. 6 (d)</ref> is a visualization of similarity vectors. The dimensions of these vectors are reduced to 3 and 2 by PCA <ref type="bibr" target="#b48">[49]</ref> for visualization. We can easily find that pixels belonging to its kernels have similar color and narrow distance with its cluster center (kernels).</p><p>Speed analysis. We specially analyze the time consumption of PAN in different stages. As shown in <ref type="table">Table 8</ref>, the time costs of backbone and segmentation head are similar, and the time cost of post-processing is half of them. In practical applications, an obvious way to increase speed is to run the network and post-processing in parallel through a basic producer-consumer model, which can reduce the time cost to the original 4/5. The above experiments are conducted on CTW1500 test set. We evaluate all test images and calculate the average speed. All results in this paper are tested by PyTorch <ref type="bibr" target="#b39">[40]</ref> with batchsize of 1 on one 1080Ti GPU and one 2.20GHz CPU in a single thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed an efficient framework to detect arbitrary-shaped text in real-time. We firstly introduce a light-weight segmentation head consisting of Feature Pyramid Enhancement Module and Feature Fusion Module, which can benefit the feature extraction while bringing minor extra computation. Moreover, we propose Pixel Aggregation to predict similarity vectors between text kernels and surrounding pixels. These two advantages make the PAN become an efficient and accurate arbitrary-shaped text detector. Extensive experiments on Total-Text and CTW1500 demonstrate the superior advantages in speed and accuracy when compared to previous state-of-the-art text detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Robustness Analysis</head><p>To further demonstrate the robustness of the proposed PAN, we evaluate the model by training on one dataset and testing on other datasets. Based on the annotation level, we divide the datasets into two groups which are word level and text line level datasets. SynthText, ICDAR 2015 and Total-Text are annotated at word level, while CTW1500 and MSRA-TD500 are annotated at text line level. For fair comparisons, we train all model without any external dataset, and the short side of test images in ICDAR 2015, MSRA-TD500, CTW1500 and Total-Text are set to 736, 736, 640, 640 respectively.</p><p>The cross-dataset results of PAN are shown in <ref type="table">Table 9</ref>. Notably, the proposed PAN trained on SynthText (a synthetic dataset) have reluctantly satisfied performance on IC-DAR 2015 and Total-Text, which indicates that even without any manually annotated data, PAN can satisfy the scene with low precision requirements. The PAN trained on manually annotated dataset has over 64% F-measure in the cross-dataset evaluation, which is still competitive. Furthermore, in the cross-dataset evaluation at text line level, all models achieve the F-measure of nearly 75% even the training and the testing are performed on quadrangle and curved text datasets respectively. These cross-dataset experiments demonstrate that the proposed PAN is robust in generalizing to brand new datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparisons with Other Semantic Segmentation Methods</head><p>Unlike common semantic segmentation tasks, text detection needs to distinguish different text instances that lie closely. So feature map resolution matters and cannot be too small. However, most of high efficiency segmentation methods (i.e. BiSeNet <ref type="bibr" target="#b53">[54]</ref>) make prediction on 1/8 feature map, sacrificing accuracy for speed. Their speed will reduce sharply if using 1/4 feature map directly. Thus, 'how to keep the high efficiency and the high resolution feature map simultaneously?" is a challenging problem, and our answer is "ResNet18 + 2FPEM + FFM". We compare our method with two methods BiSeNet <ref type="bibr" target="#b53">[54]</ref> and CU-Net <ref type="bibr" target="#b45">[46]</ref> on CTW1500. For fair comparisons, we set the backbone  <ref type="table">Table 9</ref>. Cross-dataset results of PAN on word-level and line-level datasets. "P", "R" and "F" represent the precision, recall and Fmeasure respectively.  <ref type="table" target="#tab_0">Table 10</ref>. The results on CTW1500 of different segmentation methods. "F" means F-measure. "Ext." indicates external data.</p><p>(a) (b) (c) (d) <ref type="figure">Figure 7</ref>. Failure Samples.</p><p>of BiSeNet to ResNet18 and use one of default settings of CU-Net, which has the similar speed with our method. As shown in <ref type="table" target="#tab_0">Table 10</ref>, our method enjoys obviously better accuracy (+2.2% and +4.6%) at the similar speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Failure Samples</head><p>As demonstrated in previous experiments, the proposed PAN works well in most cases of arbitrary-shaped text detection. It still fails for some difficult cases, such as large character spacing (see <ref type="figure">Fig. 7</ref> (a)), symbols (see <ref type="figure">Fig. 7 (b)</ref>) and false positives (see <ref type="figure">Fig. 7</ref> (c)(d)). Large character spacing is an unresolved problem which also exists in other state-of-the-art methods such as RRD <ref type="bibr" target="#b27">[28]</ref>. For symbol detection and false positives, PAN is trained on small datasets (about 1000 images) and we believe this problem will be alleviated when increasing training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">More Detected Results on CTW1500, Total</head><p>Text, ICDAR 2015 and MSRA-TD500</p><p>In this section, we show more test examples produced by PAN on different datasets in <ref type="figure" target="#fig_4">Fig. 8</ref> (CTW1500) <ref type="figure" target="#fig_5">Fig. 9</ref> (Total-Text), <ref type="figure" target="#fig_0">Fig. 10 (ICDAR 2015)</ref> and <ref type="figure" target="#fig_7">Fig. 11</ref> (MSRA-TD500). From these results, we can find that the proposed PAN have the following abilities: i) separating adjacent text instances with narrow distances; ii) locating the arbitraryshaped text instances precisely; iii) detecting the text instances with various orientations; iv) detecting the long text instances; v) detecting the multiple Lingual text. Meanwhile, thanks to the strong feature representation, PAN can also locate the text instances with complex and unstable il-lumination, different colors and variable scales.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The overall pipeline of PAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FPEM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The overall architecture of PAN. The features from lightweight backbone network are enhanced by a low computational-cost segmentation head which is composed of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). The network predicts text regions, kernels and similarity vectors to describe the text instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results of PAN. (a) is the final result of PAN. (b) is the predicted text regions. (c) is the predicted kernels. (d) is the visualization of similarity vectors, which is the best viewed in color and scatter diagram. (e)-(h) are results on four standard benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Detection results on CTW1500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Detection results on Total-Text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Detection results on ICDAR 2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Detection results on MSRA-TD500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results of models with different number of cascaded FPEMs. "#FPEM" means the number of cascaded FPEMs. "F" means F-measure. The FLOPS are calculated for the input of 640 × 640 × 3.</figDesc><table><row><cell>#FPEM</cell><cell>GFLOPS</cell><cell cols="2">ICDAR 2015 F FPS</cell><cell cols="2">CTW1500 F FPS</cell></row><row><cell>0</cell><cell>42.17</cell><cell>78.4</cell><cell>33.7</cell><cell>78.8</cell><cell>49.7</cell></row><row><cell>1</cell><cell>42.92</cell><cell>79.9</cell><cell>29.5</cell><cell>80.4</cell><cell>44.7</cell></row><row><cell>2</cell><cell>43.67</cell><cell>80.3</cell><cell>26.1</cell><cell>81.0</cell><cell>39.8</cell></row><row><cell>3</cell><cell>44.43</cell><cell>80.4</cell><cell>23.0</cell><cell>81.3</cell><cell>35.2</cell></row><row><cell>4</cell><cell>45.18</cell><cell>80.5</cell><cell>20.1</cell><cell>81.5</cell><cell>32.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The comparison between "ResNet18 + 2 FPEMs + FFM" with "ResNet50 + PSPNet<ref type="bibr" target="#b55">[56]</ref>". "F" means F-measure.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="2">ICDAR 2015 F FPS</cell><cell cols="2">CTW1500 F FPS</cell></row><row><cell></cell><cell cols="3">ResNet18 + 2 FPEMs + FFM</cell><cell>80.3</cell><cell>26.1</cell><cell>81.0</cell><cell>39.8</cell></row><row><cell></cell><cell cols="2">ResNet50 + PSPNet [56]</cell><cell></cell><cell>80.5</cell><cell>4.6</cell><cell>81.1</cell><cell>7.1</cell></row><row><cell>#</cell><cell>Backbone</cell><cell>Fuse</cell><cell>PA</cell><cell cols="2">ICDAR 2015 F FPS</cell><cell cols="2">CTW1500 F FPS</cell></row><row><cell>1</cell><cell>ResNet18</cell><cell>FFM</cell><cell></cell><cell>80.3</cell><cell>26.1</cell><cell>81.0</cell><cell>39.8</cell></row><row><cell>2</cell><cell>ResNet18</cell><cell>-</cell><cell></cell><cell>79.7</cell><cell>26.2</cell><cell>80.2</cell><cell>40.0</cell></row><row><cell>3</cell><cell>ResNet18</cell><cell>Concat</cell><cell></cell><cell>80.4</cell><cell>22.3</cell><cell>81.2</cell><cell>35.9</cell></row><row><cell>4</cell><cell>ResNet18</cell><cell>FFM</cell><cell>-</cell><cell>79.3</cell><cell>26.1</cell><cell>79.8</cell><cell>39.9</cell></row><row><cell>5</cell><cell>ResNet50</cell><cell>FFM</cell><cell></cell><cell>81.4</cell><cell>16.7</cell><cell>81.6</cell><cell>26.0</cell></row><row><cell>6</cell><cell>VGG16</cell><cell>FFM</cell><cell></cell><cell>81.9</cell><cell>6.6</cell><cell>81.5</cell><cell>10.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The single-scale results on ICDAR 2015. "P", "R" and "F" represent the precision, recall and F-measure respectively. "Ext." indicates external data.</figDesc><table><row><cell>Method</cell><cell>Ext.</cell><cell>Venue</cell><cell>P</cell><cell cols="2">MSRA-TD500 R F</cell><cell>FPS</cell></row><row><cell>EAST [58]</cell><cell>-</cell><cell>CVPR'17</cell><cell>87.3</cell><cell>67.4</cell><cell>76.1</cell><cell>13.2</cell></row><row><cell>RRPN [38]</cell><cell>-</cell><cell>TMM'18</cell><cell>82.0</cell><cell>68.0</cell><cell>74.0</cell><cell>-</cell></row><row><cell>DeepReg [17]</cell><cell>-</cell><cell>ICCV'17</cell><cell>77.0</cell><cell>70.0</cell><cell>74.0</cell><cell>1.1</cell></row><row><cell>PAN</cell><cell>-</cell><cell>-</cell><cell>80.7</cell><cell>77.3</cell><cell>78.9</cell><cell>30.2</cell></row><row><cell>SegLink [42]</cell><cell></cell><cell>CVPR'17</cell><cell>86.0</cell><cell>70.0</cell><cell>77.0</cell><cell>8.9</cell></row><row><cell>PixelLink [3]</cell><cell></cell><cell>AAAI'18</cell><cell>83.0</cell><cell>73.2</cell><cell>77.8</cell><cell>3.0</cell></row><row><cell>Lyu et al. [37]</cell><cell></cell><cell>CVPR'18</cell><cell>87.6</cell><cell>76.2</cell><cell>81.5</cell><cell>5.7</cell></row><row><cell>RRD [28]</cell><cell></cell><cell>CVPR'18</cell><cell>87.0</cell><cell>73.0</cell><cell>79.0</cell><cell>10</cell></row><row><cell>MCN [32]</cell><cell></cell><cell>CVPR'18</cell><cell>88.0</cell><cell>79.0</cell><cell>83.0</cell><cell>-</cell></row><row><cell>TextSnake [35]</cell><cell></cell><cell>ECCV'18</cell><cell>83.2</cell><cell>73.9</cell><cell>78.3</cell><cell>1.1</cell></row><row><cell>PAN</cell><cell></cell><cell>-</cell><cell>84.4</cell><cell>83.8</cell><cell>84.1</cell><cell>30.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Chee Kheng Ch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Seng</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Analysis Recogn</title>
		<meeting>Int. Conf. Document Analysis Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10421</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<title level="m">Qibin Hou, Menglong Zhu, and Ming-Ming Cheng. Rethinking rgb-d salient object detection: Models, datasets, and large-scale benchmarks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qile</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu-Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Document Analysis Recogn</title>
		<meeting>Int. Conf. Document Analysis Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruo-Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Detecting curve text in the wild: New dataset and new solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaitao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning markov clustering networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">Ling</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. 3D Vision</title>
		<meeting>Int. Conf. 3D Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<title level="m">Yizhe Zhu, and Dimitris Metaxas. Cu-net: Coupled u-nets. In BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mixed link networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intell</title>
		<meeting>Int. Joint Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Principal component analysis. Chemometrics and Intelligent Laboratory Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03155</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
