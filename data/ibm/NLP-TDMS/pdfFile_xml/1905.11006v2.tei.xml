<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Levenshtein Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University Tigerobo Inc. †</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
							<email>changhan@fb.com‡jakezhao@cs.nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New York University Tigerobo Inc. †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University Tigerobo Inc. †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University Tigerobo Inc. †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Facebook</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University Tigerobo Inc. †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University Tigerobo Inc. †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Levenshtein Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the basic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable or even better performance with much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing. 1 1 Codes for reproducing this paper are released in https://github.com/pytorch/fairseq/tree/ master/examples/nonautoregressive_translation 33rd</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural sequence generation models are widely developed and deployed in tasks such as machine translation <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b28">Vaswani et al., 2017)</ref>. As we examine the current frameworks, the most popular autoregressive models generate tokens step-by-step. If not better, recent nonautoregressive approaches <ref type="bibr" target="#b10">(Gu et al., 2018;</ref><ref type="bibr" target="#b13">Kaiser et al., 2018;</ref> have proved it possible to perform generation within a much smaller number of decoding iterations.</p><p>In this paper, we propose Levenshtein Transformer (LevT), aiming to address the lack of flexibility of the current decoding models. Notably, in the existing frameworks, the length of generated sequences is either fixed or monotonically increased as the decoding proceeds. This remains incompatible with human-level intelligence where humans can revise, replace, revoke or delete any part of their generated text. Hence, LevT is proposed to bridge this gap by breaking the in-so-far standardized decoding mechanism and replacing it with two basic operations -insertion and deletion.</p><p>We train the LevT using imitation learning. The resulted model contains two policies and they are executed in an alternate manner. Empirically, we show that LevT achieves comparable or better results than a standard Transformer model on machine translation and summarization, while maintaining the efficiency advantages benefited from parallel decoding similarly to . With this model, we argue that the decoding becomes more flexible. For example, when the decoder is given an empty token, it falls back to a normal sequence generation model. On the other hand, the decoder acts as a refinement model when the initial state is a low-quality generated sequence. Indeed, we show that a LevT trained from machine translation is directly applicable to translation post-editing without any change. This would not be possible with any framework in the literature because generation and refinement are treated as two different tasks due to the model's inductive bias.</p><p>One crucial component in LevT framework is the learning algorithm. We leverage the characteristics of insertion and deletion -they are complementary but also adversarial. The algorithm we propose is called "dual policy learning". The idea is that when training one policy (insertion or deletion), we use the output from its adversary at the previous iteration as input. An expert policy, on the other hand, is drawn to provide a correction signal. Despite that, in theory, this learning algorithm is applicable to other imitation learning scenarios where a dual adversarial policy exists, in this work we primarily focus on a proof-of-concept of this algorithm landing at training the proposed LevT model.</p><p>To this end, we summarize the contributions as follows:</p><p>• We propose Levenshtein Transformer (LevT), a new sequence generation model composed of the insertion and deletion operations. This model achieves comparable or even better results than a strong Transformer baseline in both machine translation and text summarization, but with much better efficiency (up to ×5 speed-up in terms of actual machine execution time); • We propose a corresponding learning algorithm under the theoretical framework of imitation learning, tackling the complementary and adversarial nature of the dual policies; • We recognize our model as a pioneer attempt to unify sequence generation and refinement, thanks to its built-in flexibility. With this unification, we empirically validate the feasibility of applying a LevT model trained by machine translation directly to translation post-editing, without any change.</p><p>2 Problem Formulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence Generation and Refinement</head><p>We unify the general problems of sequence generation and refinement by casting them to a Markov Decision Process (MDP) defined by a tuple (Y, A, E, R, y 0 ). We consider the setup consisting an agent interacting with an environment E which receives the agent's editing actions and returns the modified sequence. We define Y = V Nmax as a set of discrete sequences up to length N max where V is a vocabulary of symbols. At every decoding iteration, the agent receives an input y drawn from scratch or uncompleted generation, chooses an action a and gets a reward r. We use A to denote the set of actions and R for the reward function. Generally the reward function R measures the distance between the generation and the ground-truth sequence, R(y) = −D(y, y * ) which can be any distance measurement such as the Levenshtein distance <ref type="bibr" target="#b17">(Levenshtein, 1965)</ref>. It is crucial to incorporate y 0 ∈ Y into the our formulation. As the initial sequence, the agent receives-when y 0 is an already generated sequence from another system, the agent essentially learns to do refinement while it falls back to generation if y 0 is an empty sequence. The agent is modeled by a policy, π, that maps the current generation over a probability distribution over A. That is, π : Y → P (A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Actions: Deletion &amp; Insertion</head><p>Following the above MDP formulation, with a subsequence y k = (y 1 , y 2 , ..., y n ), the two basic actions -deletion and insertion -are called to generate y k+1 = E(y k , a k+1 ). Here we let y 1 and y n be special symbols &lt;s&gt; and &lt;/s&gt;, respectively. Since we mainly focus on the policy of a single round generation, the superscripts are omitted in this section for simplicity. For conditional generation like MT, our policy also includes an input of source information x which is also omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deletion</head><p>The deletion policy reads the input sequence y, and for every token y i ∈ y, the deletion policy π del (d|i, y) makes a binary decision which is 1 (delete this token) or 0 (keep it). We additionally constrain π del (0|1, y) = π del (0|n, y) = 1 to avoid sequence boundary being broken. The deletion classifier can also be seen as a fine-grained discriminator used in GAN <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> where we predict "fake" or "real" labels for every predicted token.</p><p>Insertion In this work, it is slightly more complex to build the insertion atomic because it involves two phases: placeholder prediction and token prediction so that it is able to insert multiple tokens at the same slot. First, among all the possible inserted slots (y i , y i+1 ) in y, π plh (p|i, y) predicts the possibility of adding one or several placeholders. In what follows, for every placeholder predicted as  <ref type="figure">Figure 1</ref>: The illustration of the proposed Levenshtein Transformer decoder for one refinement iteration. The same architecture can be applied for three different tasks with specific classifiers. For simplicity, the encoder-decoder attention is omitted within each Transformer-Block.</p><p>above, a token prediction policy π tok (t|i, y) replaces the placeholders with actual tokens in the vocabulary. The two-stage insertion process can also be viewed as a hybrid of Insertion Transformer <ref type="bibr" target="#b26">(Stern et al., 2019)</ref> and masked language model (MLM, <ref type="bibr" target="#b4">Devlin et al., 2018;</ref><ref type="bibr" target="#b7">Ghazvininejad et al., 2019)</ref>.</p><p>Policy combination Recall that our two operations are complementary. Hence we combine them in an alternate fashion. For example in sequence generation from the empty, insertion policy is first called and it is followed by deletion, and then repeat till the certain stopping condition is fulfilled. Indeed, it is possible to leverage the parallelism in this combination. We essentially decompose one iteration of our sequence generator into three phases: "delete tokens -insert placeholdersreplace placeholders with new tokens". Within each stage, all operations are performed in parallel. More precisely, given the current sequence y = (y 0 , . . . , y n ), and suppose the action to predict is a = {d 0 , . . . d n d ; p 0 , . . . , p n−1 p ; t 1 0 , . . . t p0 0 , . . . , t pn−1 n−1 t }, the policy for one iteration is:</p><formula xml:id="formula_0">π(a|y) = di∈d π del (d i |i, y) · pi∈p π plh (p i |i, y ) · ti∈t π tok (t i |i, y ),<label>(1)</label></formula><p>where y = E(y, d) and y = E(y , p). We parallelize the computation within each sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Levenshtein Transformer</head><p>In this section, we cover the specs of Levenshtein Transformer and the dual-policy learning algorithm. Overall our model takes a sequence of tokens (or none) as the input then iteratively modify it by alternating between insertion and deletion, until the two policies combined converge. We describe the detailed learning and inference algorithms in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We use Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> as the basic building block. For conditional generation, the source x is included in each TransformerBlock. The states from the l-th block are:</p><formula xml:id="formula_1">h (l+1) 0 , h (l+1) 1 , ..., h (l+1) n = E y0 + P 0 , E y1 + P 1 , ..., E yn + P n , l = 0 TransformerBlock l (h (l) 0 , h (l) 1 , ..., h (l) n ), l &gt; 0<label>(2)</label></formula><p>where E ∈ R |V|×dmodel and P ∈ R Nmax×dmodel are the token and position embeddings, respectively. We show the illustration of the proposed LevT model for one refinement (delete, insert) as <ref type="figure">Figure 1</ref>.</p><p>Policy Classifiers The decoder outputs (h 0 , h 2 , ..., h n ) are passed to three policy classifiers:</p><p>1. Deletion Classifier: LevT scans over the input tokens (except for the boundaries) and predict "deleted" (0) or "kept" (1) for each token position,</p><formula xml:id="formula_2">π del θ (d|i, y) = softmax h i · A , i = 1, . . . n − 1,<label>(3)</label></formula><p>where A ∈ R 2×dmodel , and we always keep the boundary tokens. 2. Placeholder Classifier: LevT predicts the number of tokens to be inserted at every consecutive position pairs, by casting the representation to a categorical distribution:</p><formula xml:id="formula_3">π plh θ (p|i, y) = softmax concat(h i , h i+1 ) · B , i = 0, . . . n − 1,<label>(4)</label></formula><p>where B ∈ R (Kmax+1)×(2dmodel) . Based on the number (0 ∼ K max ) of tokens it predicts, we insert the considered number of placeholders at the current position. In our implementation, placehoder is represented by a special token &lt;PLH&gt; which was reserved in the vocabulary. 3. Token Classifier: following the placeholder prediction, LevT needs to fill in tokens replacing all the placeholders. This is achieved by training a token predictor as follow:</p><formula xml:id="formula_4">π tok θ (t|i, y) = softmax h i · C , ∀y i = &lt;PLH&gt;,<label>(5)</label></formula><p>where C ∈ R |V|×dmodel with parameters being shared with the embedding matrix.</p><p>Weight Sharing Our default implementation always assumes the three operations to share the same Transformer backbone to benefit features learned from other operations. However, it is also possible to disable weight sharing and train separate decoders for each operations, which increases the capacity of the model while does not affect the overall inference time.</p><p>Early Exit Although it is parameter-efficient to share the same Transformer architecture across the above three heads, there is room for improvement as one decoding iteration requires three full passes of the network. To make trade-off between performance and computational cost, we propose to perform early exit (attaching the classifier to an intermediate block instead of the last one) for π del and π plh to reduce computation while keeping π tok always based on the last block, considering that token prediction is usually more challenging than the other two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-policy Learning</head><p>Imitation Learning We use imitation learning to train the Levenshtein Transformer. Essentially we let the agent imitate the behaviors that we draw from some expert policy π * . The expert policy is derived from direct usage of ground-truth targets or less noisy version filtered by sequence distillation <ref type="bibr" target="#b14">(Kim and Rush, 2016)</ref>. The objective is to maximize the following expectation:</p><formula xml:id="formula_5">Ey del ∼dπ del d * ∼π * d * i ∈d * log π del θ (d * i |i, y del ) Deletion Objective + Ey ins ∼dπ ins p * ,t * ∼π *   p * i ∈p * log π plh θ (p * i |i, y ins ) + t * i ∈t * log π tok θ (t * i |i, y ins )   Insertion Objective ,</formula><p>where y ins is the output after inserting palceholders p * upon y ins .π del ,π ins are the roll-in polices and we repeatedly draw states (sequences) from their induced state distribution dπ del , dπ ins . These states are first executed by the expert policy returning the suggested actions by the expert, and then we maximize the conditional log-likelihood over them. By definition, the roll-in policy determines the state distribution fed to π θ during training. In this work, we have two strategies to construct the roll-in policy -adding noise to the ground-truth or using the output from the adversary policy. <ref type="figure" target="#fig_4">Figure 2</ref> shows a diagram of this learning paradigm. We formally write down the roll-in policies as follows.</p><p>1. Learning to Delete: we design theπ del as a stochastic mixture between the initial input y 0 or the output by applying insertion from the model with some mixture factor α ∈ [0, 1]:</p><formula xml:id="formula_6">dπ del = {y 0 if u &lt; α else E E (y , p * ) ,t , p * ∼ π * ,t ∼ π θ }<label>(6)</label></formula><p>where u ∼ Uniform[0, 1] and y is any sequence ready to insert tokens.t is obtained by sampling instead of doing argmax from Eq. (5).</p><p>y y y ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W t t 9 s 3 Y X X O e L s 2 7 B H a 4 k J 4 6 A 9 3 Y = "     2. Learning to Insert: similar to the deletion step, we apply a mixture of the deletion output and a random word dropping sequence of the round-truth, inspired by recent advances of training masked language model <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>. We use random dropping as a form of noise injection to encourage more exploration. Let β ∈ [0, 1] and u ∼ Uniform[0, 1],</p><formula xml:id="formula_7">&gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B P J T d K u i x 6 M V j B f u B 7 V q y a b Y N T b J L k h X K 0 n / h x Y M i X v 0 3 3 v w 3 Z t</formula><formula xml:id="formula_8">Z I x C P V D b C m n E n a M s x w 2 o 0 V x S L g t B N M b n K / 8 0 S V Z p G 8 N 2 l M f Y F H k o W M Y G O l h 3 4 s g i y d P r r l Q a X q 1 t w Z 0 D L x C l K F A s 1 B 5 a s / j E g i q D S E Y 6 1 7 n h s b P 8 P K M M L p t N x P N I 0 x m e A R 7 V k q s a D a z 2 Y X T 9 G p V Y Y o j J Q t a d B M / T 2 R Y a F 1 K g L b K b A Z 6 0 U v F / /</formula><formula xml:id="formula_9">= " &gt; A A A B 7 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B P J T d K u i x 6 M V j B f s B 7 V q y a b a N z S Z L k h X K 0 v / g x Y M i X v 0 / 3 v w 3 Z t s 9 a O u D g c d 7 M 8 z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P W l o m i t A m k V y q T o A 1 5 U z Q p m G G 0 0 6 s K I 4 C T t v B + C b z 2 0 9 U a S b F v Z n E 1 I / w U L C Q E W y s 1 O r F 7 O G s 1 C 9 X 3 K o 7 A 1 o m X k 4 q k K P R L 3 / 1 B p I k E R W G c K x 1 1 3 N j 4 6 d Y G U Y 4 n Z Z 6 i a Y x J m M 8 p F 1 L B Y 6 o 9 t P Z t V N 0 Y p U B C q W y J Q y a q b 8 n U h x p P Y k C 2 x l h M 9 K L X i b + 5 3 U T E 1 7 5 K R N x Y q g g 8 0 V h w p G R K H s d D Z i i x P C J J Z g o Z m 9 F Z I Q V J s Y G l I X g L b 6 8 T F q 1 q n d e r d 1 d V O r X e R x F O I J j O A U P L q E O t 9 C A J h B 4 h G d 4 h T d H O i / O u / M x b y 0 4 + c w h / I H z + Q O h p Y 6 B &lt; / l a t e x i t &gt; ⇡ rnd &lt; l a</formula><formula xml:id="formula_10">V v c 7 O k o U Z W 0 a i U j 1 P K K Z 4 J K 1 g Y N g v V g x E n q C d b 3 x V e 5 3 7 5 n S P J K 3 M I m Z G 5 K h 5 A G n B D J p Y F a d m N 8 5 w B 5 B 0 1 R J f 1 o Z m D W r b s 2 A l 4 l d k B o q 0 B q Y X 4 4 f 0 S R k E q g g W v d t K w Y 3 J Q o 4 F W x a c R L N Y k L H Z M j 6 G Z U k Z N p N Z 6 d P 8 X G m + D i I V F Y S 8 E z 9 P Z G S U O t J 6 G W d I Y G R X v R</formula><formula xml:id="formula_11">R l L V p L G L d C 9 A w w S V r W 2 4 F 6 y n N M A o E 6 w b T u 9 z v P j F t e C w f 7 U w x P 8 K x 5 C G n a D O p O 0 C h J l g Z V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j m C Y R k 5 Y K N K b v u c r 6 K W r L q W D z y i A x T C G d 4 p j 1 M y o x Y s Z P F + f O y U W m j E g Y</formula><formula xml:id="formula_12">dπ ins = {E y 0 , d * , d * ∼ π * if u &lt; β else E y * ,d ,d ∼ π RND } (7)</formula><p>Expert Policy It is crucial to construct an expert policy in imitation learning which cannot be too hard or too weak to learn from. Specifically, we considered two types of experts:</p><p>1. Oracle: One way is to build an oracle which accesses to the ground-truth sequence. It returns the optimal actions a * (either oracle insertion p * , t * or oracle deletion d * ) by: a * = argmin a D(y * , E(y, a))</p><p>Here, we use the Levenshtein distance (Levenshtein, 1965) 2 as D considering it is possible to obtain the action suggestions efficiently by dynamic programming. 2. Distillation: We also explore to use another teacher model to provide expert policy, which is known as sequence-level knowledge distillation <ref type="bibr" target="#b14">(Kim and Rush, 2016)</ref>. This technique has been widely used in previous approaches for nonauoregressive generation <ref type="bibr" target="#b10">(Gu et al., 2018)</ref>. More precisely, we first train an autoregressive teacher model using the same datasets and then replace the ground-truth sequence y * by the beam-search result of this teacher-model, y AR . We use the same mechanism to find the suggested option as using the ground-truth oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>Greedy Decoding At inference time, we apply the trained model over the initial sequence y 0 for several iterations. We greedily pick up the actions associated with high probabilities in Eq.</p><p>(3)(4)(5). Moreover, we find that using search (instead of greedy decoding) or nosiy parallel decoding <ref type="bibr" target="#b3">(Cho, 2016)</ref> does not yield much gain in LevT. This observation is quite opposite to what has been widely discovered in autoregressive decoding. We hypothesize there may be two reasons leading to this issue: (i) The local optimal point brought by greedy decoding in autoregressive models is often far from the optimal point globally. Search techniques resolve this issue with tabularization. In our case, however, because LevT inserts or deletes tokens dynamically, it could easily revoke the tokens that are found sub-optimal and re-insert better ones; (ii) the log-probability of LevT is not a good metric to select the best output. However, we do believe to see more improvements if we include an external re-ranker, e.g. an autoregressive teacher model. We leave this discussion in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Termination Condition</head><p>The decoding stops when one of the following two conditions is fulfilled:</p><p>1. Looping: Generation is terminated if two consecutive refinement iterations return the same output which can be (i) there are no words to delete or insert; (ii) the agent gets stuck in an infinite loop: i.e. the insertion and deletion counter each other and keep looping. 2. Timeout: We further set a maximum number of iterations (timeout) to guarantee a constant-time complexity in the worst case <ref type="bibr" target="#b7">Ghazvininejad et al., 2019)</ref>.</p><p>Penalty for Empty Placeholders Similar to <ref type="bibr" target="#b26">Stern et al. (2019)</ref>, we add a penalty to insert "empty" placeholder in decoding. Overly inserting "empty" placeholders may result in shorter output. A penalty term γ ∈ [0, 3] is subtracted from the logits of 0 in Eq. (4).  <ref type="figure">Figure 3</ref>: An example of WAT'17 En-Ja translation with two decoder iterations by LevT. We present the inserted tokens in purple and deleted tokens with red strikethrough .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validate the efficiency, effectiveness, and flexibility of Levenshtein Transformer extensively across three different tasks -machine translation (MT), text summarization (TS) and automatic post-editing (APE) for machine translation, from both generation ( §4.1) and refinement ( §4.2) perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sequence Generation</head><p>For the sequence generation perspective, we evaluate LevT model on MT and TS. As a special case, sequence generation assumes empty y 0 = &lt;S&gt;&lt;/S&gt; as input and no initial deletion is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data &amp; Evaluation</head><p>We use three diversified language pairs for MT experiments: WMT'16 Romanian-English (Ro-En) 3 , WMT'14 English-German (En-De) 4 and WAT2017 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017) 5 . The TS experiments use preprocessed data from the Annotated English Gigaword (Gigaword, <ref type="bibr" target="#b22">Rush et al., 2015)</ref> 6 . We learn byte-pair encoding (BPE, <ref type="bibr" target="#b24">Sennrich et al., 2016</ref>) vocabulary on tokenized data. Detailed dataset statistics can be found in the Appendix. For evaluation metrics, we use BLEU <ref type="bibr" target="#b21">(Papineni et al., 2002)</ref> for MT and ROUGE-1,2,L <ref type="bibr" target="#b18">(Lin, 2004)</ref> for TS. Before computing the BLEU scores for Japanese output, we always segment Japanese words using KyTea 7 .   trained on 8 Nvidia Volta GPUs with maximum 300K steps and a total batch-size of around 65, 536 tokens per step (We leave more details to the Appendix).</p><p>Overall results We present our main results on the generation quality and decoding speed in <ref type="table" target="#tab_1">Table 1</ref>. We measure the speed by the averaged generation latency of generating one sequence at a time on single Nvidia V100 GPU. To remove the implementation bias, we also present the number of decoder iterations as a reference. It can be concluded that for both MT and summarization tasks, our proposed LevT achieves comparable and sometimes better generation quality compared to the strong autoregressive baseline, while LevT is much more efficient at decoding. A translation example is shown in <ref type="figure">Figure 3</ref> and we leave more in Appendix. We conjecture that this is due to that the output of the teacher model possesses fewer modes and much less noisy than the real data. Consequently, LevT needs less number of iterations to converge to this expert policy.</p><p>Ablation on Efficiency As shown in <ref type="figure" target="#fig_6">Figure 4a</ref>, we plot the average number of iterations over the length of input over a monolingual corpus. LevT learns to properly adjust the decoding time accordingly. We also explore the variants of "early exit" where we denote LevT(m-n) as a model with m and n blocks for deletion (Eq. (3)) and placeholder prediction (Eq. (4)) respectively. <ref type="figure" target="#fig_6">Figure 4b</ref> shows that although it compromises the quality a bit, our model with early exit achieves up to ×5 speed-up (execution time) comparing against a strong autoregressive Transformer using beam-search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation on Weight Sharing</head><p>We also evaluate LevT with different weight sharing as noted in §3.1. The results of models trained with oracle or distillation are listed in <ref type="table" target="#tab_2">Table 2a</ref>. We observe that weight-sharing is beneficial especially between the two insertion operations (placeholder and token classifiers). Also, it shows another +0.5 BLEU improvement by not sharing the deletion operation with insertion compared to the default setting, which may indicate that insertion and deletion capture complementary information, requiring larger capacity by learning them separately.</p><p>Importance of mixture roll-in policy We perform an ablation study on the learning algorithm. Specifically, we train a model with no mixing of the π θ in Equation (6). We name this experiment by DAE due to its resemblance to a denoising autoencoder. We follow closely a standard pipeline established by . <ref type="table" target="#tab_2">Table 2b</ref> shows this comparison. As we can see that the deletion loss <ref type="table">Table 3</ref>: Performance (BLEU ↑ / case-sensitive TER ↓) comparison on APE. "do nothing" represents the results of the original MT system output; the autoregressive model uses beam-size 4. For the proposed LevT, we use "scratch" to denote training from scratch on the APE triple data, and use "zero-shot" to denote applying an MT pre-trained LevT model directly for post-editing tasks. The same model can be further fine-tuned. All scores with underlines are from the model trained with an autoregressive teacher model (distillation) as the expert policy.  from DAE is much smaller while the generation BLEU score is inferior. We conjecture that this is caused by the mismatch between the states from the model and the roll-in policy in training the DAE.</p><p>v.s. Exiting Refinement-based Models <ref type="table" target="#tab_2">Table 2a</ref> also includes results from two relevant recent works which also incorporate iterative refinement in non-autoregressive sequence generation. For fair comparison, we use the result with length beam 1 from <ref type="bibr" target="#b7">Ghazvininejad et al. (2019)</ref>. Although both approaches use similar "denosing" objectives to train the refinement process, our model explicitly learns "insertion" and "deletion" in a dual-policy learning fashion, and outperforms both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sequence Refinement</head><p>We evaluate LevT's capability of refining sequence outputs on the APE task. In this setting, inputs are pairs of the source sequence and a black-box MT system generation. The ground-truth outputs are from real human edits with expansion using synthetic data.</p><p>Dataset We follow a normal protocol in the synthetic APE experiments <ref type="bibr" target="#b9">(Grangier and Auli, 2017)</ref>: we first train the input MT system on half of the dataset. Then we will train a refinement model on the other half based on the output produced by the MT model trained in the previous phase. For the real APE tasks, we use the data from WMT17 Automatic Post-Editing Shared Task 8 on En-De. It contains both real PE triples and a large-scale synthetic corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models &amp; Evaluation</head><p>The baseline model is a standard Transformer encoding the concatenation of the source and the MT system's output. For the MT system here, we want some imperfect systems that need to be refined. We consider a statistical phrase-based MT system (PBMT, <ref type="bibr" target="#b15">Koehn et al., 2003)</ref> and an <ref type="bibr">RNN-based NMT system (Bahdanau et al., 2015)</ref>. Apart from BLEU scores, we additionally apply translation error rate <ref type="bibr">(TER, Snover et al., 2006)</ref> as it is widely used in the APE literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall results</head><p>We show the major comparison in <ref type="table">Table 3</ref>. When training from scratch, LevT consistently improves the performance of the input MT system (either PBMT or NMT). It also achieves better performance than the autoregressive Transformer in most of the cases.</p><p>Pre-training on MT Thanks to the generality of the LevT model, we show it is feasible to directly apply the LevT model trained by generation onto refinement tasks -in this case -MT and APE. We name this a "zero-shot post-editing" setting. According to <ref type="table">Table 3</ref>, the pre-trained MT models are always capable of improving the initial MT input in the synthetic tasks.</p><p>The real APE task, however, differs quite a bit from the synthetic tasks because human translators normally only fix a few spotted errors. This ends up with very high BLEU scores even for the "Do-nothing" column. However, the pre-trained MT model achieves the best results by fine-tuning on the PE data indicating that LevT is able to leverage the knowledge for generation and refinement.</p><p>Collaborate with Oracle Thanks to the saperation of insertion and deletion operations, LevT has better interpretability and controllability. For example, we test the ability that LevT adapts oracle (e.g. human translators) instructions. As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, both MT and PE tasks have huge improvement if every step the oracle deletion is given. This goes even further if the oracle provides both the correct deletion and the number of placehoders to insert. It also sheds some light upon computer-assisted text editing for human translators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Non-Autoregressive and Non-Monotonic Decoding Breaking the autoregressive constraints and monotonic (left-to-right) decoding order in classic neural sequence generation systems has recently attracted much interest. <ref type="bibr" target="#b27">Stern et al. (2018)</ref>; <ref type="bibr" target="#b29">Wang et al. (2018)</ref> designed partially parallel decoding schemes to output multiple tokens at each step. <ref type="bibr" target="#b10">Gu et al. (2018)</ref> proposed a non-autoregressive framework using discrete latent variables, which was later adopted in  as iterative refinement process. <ref type="bibr" target="#b7">Ghazvininejad et al. (2019)</ref> introduced the masked language modeling objective from BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> to non-autoregressively predict and refine translations. <ref type="bibr" target="#b30">Welleck et al. (2019)</ref>; <ref type="bibr" target="#b26">Stern et al. (2019)</ref>; <ref type="bibr" target="#b11">Gu et al. (2019)</ref> generate translations non-monotonically by adding words to the left or right of previous ones or by inserting words in arbitrary order to form a sequence.</p><p>Editing-Based Models Several prior works have explored incorporating "editing" operations for sequence generation tasks. For instance, <ref type="bibr" target="#b20">Novak et al. (2016)</ref> predict and apply token substitutions iteratively on phase-based MT system outputs using convolutional neural network. QuickEdit (Grangier and Auli, 2017) and deliberation network <ref type="bibr" target="#b31">(Xia et al., 2017)</ref> both consist of two autoregressive decoders where the second decoder refines the translation generated by the first decoder. <ref type="bibr" target="#b12">Guu et al. (2018)</ref> propose a neural editor which learned language modeling by first retrieving a prototype and then editing over that. <ref type="bibr" target="#b6">Freitag et al. (2019)</ref> correct patterned errors in MT system outputs using transformer models trained on monolingual data. Additionally, the use of Levenshtein distance with dynamic programming as the oracle policy were also proposed in <ref type="bibr" target="#b23">Sabour et al. (2018)</ref>; <ref type="bibr" target="#b5">Dong et al. (2019)</ref>. Different from these work, the proposed model learns a non-autoregressive model which simultaneously inserts and deletes multiple tokens iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose Levenshtein Transformer, a neural sequence generation model based on insertion and deletion. The resulted model achieves performance and decoding efficiency, and embraces sequence generation to refinement in one model. The insertion and deletion operations are arguably more similar to how human writes or edits text. For future work, it is potential to extend this model to human-in-the-loop generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Learning &amp; Inference Algorithm</head><p>We present the detailed algorithms for learning and decoding from Levenshtein Transformer as follows. For simplicity, we always omit the source information x in conditional sequence generation tasks such as machine translation which is handled by the cross-attention with an encoder on x.</p><p>The learning algorithm is shown in Algorithm 1. E is the environment and D is denoted as the Levenshtein distance, and we can easily back-track the optimal insertion and deletion operations through dynamic programming. We only show the the case with single batch-size for convenience. We also present the inference algorithm in Algorithm 2. If the initial sequence y 0 is empty (&lt;s&gt;&lt;/s&gt;), the proposed model will skip the first deletion and do sequence generation. Otherwise, the model starts with deletion operations and refine the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning for Levenshtein Transformer</head><p>Initialize: Training set T , expert policy π * , model policy π θ , random deletion policy π RND , α, β repeat Sample a training pair (y 0 , y * ) ∼ Y if expert π * is a teacher model then Set the teacher's output as the target y   Replace placeholders: y = E(y ,t), wheret = argmax t yi∈y ,yi=&lt;PLH&gt; log π tok θ (t i |i, y ) end if Update steps: t = t + 1 until Reach the maximum length t = T max return y Transformer models are used for autoregressive baselines as well as teacher models (for the expert policy). By default, we set d model = 512, d hidden = 2048, n heads = 8, n layers = 6, lr max = 0.0005, label-smooth = 0.1, warmup = 10000 and dropout = 0.3. Source and target side share embeddings in all the training pairs except for WAT En-Ja where BPE vocabularies of both side are learned separately and are almost non-overlapping.</p><formula xml:id="formula_14">* = y AR end if Sample u, v ∼ Uniform[0, 1] if u &lt; β then y ins = E(y 0 ,d), whered = argmin d D(y * , E y 0 , d) else y ins = E(y * ,d), whered ∼ π RND (·|y * ) end if y ins = E(y ins , p * ), where p * , t * = argmin p,t D(y * , E (y ins , {p, t})) if v &lt; α then y del = y 0 else y del = E(y ins ,t), wheret = argmax t yi∈y ins ,yi=&lt;PLH&gt; log π tok θ (t i |i, y ins ) end if L ins θ = − yi∈yins,p * i ∈p * log π plh θ (p * i |i, y ins ) + yi∈y ins ,yi=&lt;PLH&gt;,t * i ∈t * log π tok θ (t * i |i, y ins ) L del θ = − yi∈ydel,d * i ∈d * log π del θ (d * i |i, y del ), where d * = argmin d D(y * , E (y del , d)) θ = θ − λ · θ L ins θ + L del θ until</formula><p>Since the training objectives for Levenshtein Transformer contains randomness terms (Eq. (6) (7)), we instead use BLEU (for MT) or ROUGE-2 (for TS) to select the best checkpoint by validation scores. We do not average checkpoints in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Sequence Refinement Tasks</head><p>For synthetic APE tasks, we keep the same training conditions for LevT as those for MT tasks ( §C.1). As described earlier in §4.2, we build the baseline Transformer by concatenating the source and MT system's output as the input sequence for the encoder. Specially, we restart the positional embeddings  the administration of t@@ v@@ r has made constant efforts to reduce personnel and tv production expenses .</p><p>(iteration 1) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; nothing to delete &gt;&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Terminate]</head><p>the t@@ v@@ r administration has constantly constantly efforts to cut spending on personnel and tv production .</p><p>(iteration 1) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; delete &gt;&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Terminate]</head><p>the t@@ v@@ r made constant efforts to reduce expenditure on staff and tv production .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT</head><p>the t@@ v@@ r administration has constantly constantly efforts to cut spending on personnel and tv production .</p><p>the t@@ v@@ r administration has constantly made efforts to cut spending on personnel and tv production .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>delete &gt;&gt; insert &gt;&gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APE (zero-shot on PBMT)</head><p>the t@@ v@@ r administration administration has making constant efforts to reduce expenditure on staff and tv production .</p><p>the t@@ v@@ r administration administration has making constant efforts to reduce expenditure on staff and tv production .</p><p>the t@@ v@@ r administration has made constant efforts to reduce expenditure on staff and tv production . insert &gt;&gt; delete &gt;&gt; (iteration 2) (iteration 2) <ref type="figure">Figure 11</ref>: An example for machine translation and zero-shot post-editing over a PBMT system's output on WMT'16 Ro-En with the Levenshtein Transformer (LevT) trained for MT. It is clear to find that, the pre-trained LevT can directly adapt to the PBMT's output and have a different refinement results compared to translate from scratch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>s 9 a O u D g c d 7 M 8 z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P W j p K F K F N E v F I d Q K s K W e S N g 0 z n H Z i R b E I O G 0 H 4 5 v M b z 9 R p V k k 7 8 0 k p r 7 A Q 8 l C R r C x 0 k M v F k E 6 m T 6 e l f r l i l t 1 Z 0 D L x M t J B X I 0 + u W v 3 i A i i a D S E I 6 1 7 n p u b P w U K 8 M I p 9 N S L 9 E 0 x m S M h 7 R r q c S C a j + d X T x F J 1 Y Z o D B S t q R B M / X 3 R I q F 1 h M R 2 E 6 B z U g v e p n 4 n 9 d N T H j l p 0 z G i a G S z B e F C U c m Q t n 7 a M A U J Y Z P L M F E M X s r I i O s M D E 2 p C w E b / H l Z d K q V b 3 z a u 3 u o l K / z u M o w h E c w y l 4 c A l 1 u I U G N I G A h G d 4 h T d H O y / O u / M x b y 0 4 + c w h / I H z + Q M M f 5 C A &lt; / l a t e x i t &gt; y y y 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V 8 z K O 6 P 7 h d H / a X Q c p p 9 1 k W I o Q t Q = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 r G C / c A 2 l s 1 2 0 y 7 d 3 Y T d j R B C / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X x J x p 4 7 r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s 6 y h R h L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>z e o k J r / y M y T g x V J L 5 o j D h y E Q o f x 8 N m a L E 8 N Q S T B S z t y I y x g o T Y 0 P K Q / A W X 1 4 m 7 X r N O 6 / V 7 y 6 q j e s i j h I c w w m c g Q e X 0 I B b a E I L C E h 4 h l d 4 c 7 T z 4 r w 7 H / P W F a e Y O Y I / c D 5 / A B W d k I Y = &lt; / l a t e x i t &gt; ⇡ ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r B O h 8 7 3 Y W d s U V 2 d n S e / 9 s 9 K K 7 l 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 3 c k L k j 2 t z Z P k 6 p h Z l g 2 l T s 3 X k D s = " &gt; A A A B + n i c b V B N S 8 N A E N 3 U r 1 q / U j 1 6 W S y C p 5 J U Q Y 9 F L x 4 r 2 A 9 o Y t l s N u 3 S z S b s T t Q S + 1 O 8 e F D E q 7 / E m / / G p M 1 B W x 8 M P N 6 b Y W a e F w u u w b K + j d L K 6 t r 6 R n m z s r W 9 s 7 t n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y 8 T + v n 0 B w 4 a Z c x g k w S e e L g k R g i H C e A / a 5 Y h T E J C O E K p 7 d i u m I K E I h S y s P w V 5 8 e Z l 0 G n X 7 t N 6 4 O a s 1 L 4 s 4 y u g Q H a E T Z K N z 1 E T X q I X a i K I H 9 I x e 0 Z v x Z L w Y 7 8 b H v L V k F D M H 6 A + M z x + E 7 J Q p &lt; / l a t e x i t &gt; ⇡ ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j + D O H / x p X e G P s D U 2 x T b W 7 a 1 i H 5 s = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 r G C / Y A k l M 1 2 0 y 7 d 7 I b d i V B C f 4 Y X D 4 p 4 9 d d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X p Y I b c N 1 v Z 2 1 9 Y 3 N r u 7 J T 3 d 3 b P z i s H R 1 3 j c o 0 Z R 2 q h N L 9 i B g m u G Q d 4 C B Y P 9 W M J J F g v W h y V / i 9 J 6 Y N V / I R p i k L E z K S P O a U g J X 8 I O W D A M Y M S H V Q q 7 s N d w 6 8 S r y S 1 F G J 9 q D 2 F Q w V z R I m g Q p i j O + 5 K Y Q 5 0 c C p Y L N q k B m W E j o h I + Z b K k n C T J j P T 5 7 h c 6 s M c a y 0 L Q l 4 r v 6 e y E l i z D S J b G d C Y G y W v U L 8 z / M z i G / C n M s 0 A y b p Y l G c C Q w K F / / j I d e M g p h a Q q j m 9 l Z M x 0 Q T C j a l I g R v + e V V 0 m 0 2 v M t G 8 + G q 3 r o t 4 6 i g U 3 S G L p C H r l E L 3 a M 2 6 i C K F H p G r + j N A e f F e X c + F q 1 r T j l z g v 7 A + f w B 4 X + Q / A = = &lt; / l a t e x i t &gt; y y y 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L i t o k t b S s w R y r 4 W T d h p Y Z E S x 9 S 4 = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 6 K k k V 9 F j 0 4 r G C / Z A 2 l M 1 2 0 y 7 d 3 Y T d j R B C f 4 U X D 4 p 4 9 e d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X x J x p 4 7 r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s 6 y h R h L Z I x C P V D b C m n E n a M s x w 2 o 0 V x S L g t B N M b n O / 8 0 S V Z p F 8 M G l M f Y F H k o W M Y G O l x 3 4 s g i y d n p U H l a p b c 2 d A y 8 Q r S B U K N A e V r / 4 w I o m g 0 h C O t e 5 5 b m z 8 D C v D C K f T c j / R N M Z k g k e 0 Z 6 n E g m o / m x 0 8 R a d W G a I w U r a k Q T P 1 9 0 S G h d a p C G y n w G a s F 7 1 c / M / r J S a 8 9 j M m 4 8 R Q S e a L w o Q j E 6 H 8 e z R k i h L D U 0 s w U c z e i s g Y K 0 y M z S g P w V t 8 e Z m 0 6 z X v o l a / v 6 w 2 b o o 4 S n A M J 3 A O H l x B A + 6 g C S 0 g I O A Z X u H N U c 6 L 8 + 5 8 z F t X n G L m C P 7 A + f w B U K 2 Q F Q = = &lt; / l a t e x i t &gt; y y y 00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 x A / 5 U B N u C m Q C M s E l G e h Q T u S S D U = " &gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R a p p 7 J b B T 0 W v X i s Y D + w X U o 2 z b a h S X Z J s k J Z + i + 8 e F D E q / / G m / / G b L s H b X 0 w 8 H h v h p l 5 Q c y Z N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R W 0 e J I r R F I h 6 p b o A 1 5 U z S l m G G 0 2 6 s K B Y B p 5 1 g c p v 5 n S e q N I v k g 5 n G 1 B d 4 J F n I C D Z W e u z H I k i n s 2 q 1 N C h X 3 J o 7 B 1 o l X k 4 q k K M 5 K H / 1 h x F J B J W G c K x 1 z 3 N j 4 6 d Y G U Y 4 n Z X 6 i a Y x J h M 8 o j 1 L J R Z U + + n 8 4 h k 6 s 8 o Q h Z G y J Q 2 a q 7 8 n U i y 0 n o r A d g p s x n r Z y 8 T / v F 5 i w m s / Z T J O D J V k s S h M O D I R y t 5 H Q 6 Y o M X x q C S a K 2 V s R G W O F i b E h Z S F 4 y y + v k n a 9 5 l 3 U 6 v e X l c Z N H k c R T u A U z s G D K 2 j A H T S h B Q Q k P M M r v D n a e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H 7 Q X k E Y = &lt; / l a t e x i t &gt; ↵ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w l E S 0 P l L D l y l r 5 M e n e q M f V Q V d 5 w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V 7 A e 0 o U y 2 m 3 b p Z r P s b o Q S + i O 8 e F D E q 7 / H m / / G p M 1 B W x 8 M P N 6 b Y W Z e o A Q 3 1 n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>6 6 y k J Q v 1 9 0 S K k T G z K M g 6 I 7 Q T s + r l 4 n 9 e P 7 H h r Z 9 y q R L L J F 0 u C h N B b E z y 3 8 m I a 0 a t m G U E q e b Z r Y R O U C O 1 W U J 5 C N 7 q y + u k 0 6 h 7 V / X G w 3 W t 2 S z i K M M Z n M M l e H A D T b i H F r S B w h S e 4 R X e H O W 8 O O / O x 7 K 1 5 B Q z p / A H z u c P w q 2 P L g = = &lt; / l a t e x i t &gt; 1 ↵ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J d 9 o P X U Z V f R W v e 3 / a E Z 4 + i t y y N k = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 s S R V 0 G P B i 8 c K 9 g P S U C b b T b t 0 s x t 2 N 0 I p / R l e P C j i 1 V / j z X 9 j 0 u a g r Q 8 G H u / N M D M v T A Q 3 1 n W / n b X 1 j c 2 t 7 d J O e X d v / + C w c n T c N i r V l L W o E k p 3 Q z R M c M l a l l v B u o l m G I e C d c L x X e 5 3 n p g 2 X M l H O 0 l Y E O N Q 8 o h T t J n k e + S S 9 F A k I y z 3 K 1 W 3 5 s 5 B V o l X k C o U a P Y r X 7 2 B o m n M p K U C j f E 9 N 7 H B F L X l V L B Z u Z c a l i A d 4 5 D 5 G Z U Y M x N M 5 y f P y H m m D E i k d F b S k r n 6 e 2 K K s T G T O M w 6 Y 7 Q j s + z l 4 n + e n 9 r o N p h y m a S W S b p Y F K W C W E X y / 8 m A a 0 a t m G Q E q e b Z r Y S O U C O 1 W U p 5 C N 7 y y 6 u k X a 9 5 V 7 X 6 w 3 W 1 0 S j i K M E p n M E F e H A D D b i H J r S A g o J n e I U 3 x z o v z r v z s W h d c 4 q Z E / g D 5 / M H S 2 G P 9 A = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 T Y o 9 l S Q q Q i s U t / I Y 8 Y D G e W A k 1 I = " &gt; A A A B 8 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B i y W p g h 4 L X j x W s B / Y h r L Z T t q l m 0 3 Y n Q g l 9 F 9 4 8 a C I V / + N N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U c 2 j y W M a 6 E z A D U i h o o k A J n U Q D i w I J 7 W B 8 O / P b T 6 C N i N U D T h L w I z Z U I h S c o Z U e P X p B e w E g K / X L F b f q z k F X i Z e T C s n R 6 J e / e o O Y p x E o 5 J I Z 0 / X c B P 2 M a R R c w r T U S w 0 k j I / Z E L q W K h a B 8 b P 5 x V N 6 Z p U B D W N t S y G d q 7 8 n M h Y Z M 4 k C 2 x k x H J l l b y b + 5 3 V T D G / 8 T K g k R V B 8 s S h M J c W Y z t 6 n A 6 G B o 5 x Y w r g W 9 l b K R 0 w z j j a k W Q j e 8 s u r p F W r e p f V 2 v 1 V p V 7 P 4 y i S E 3 J K z o l H r k m d 3 J E G a R J O F H k m r + T N M c 6 L 8 + 5 8 L F o L T j 5 z T P 7 A + f w B g q S P g A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F Z A Y z f r + 4 h X u 2 u Q 6 O z U j u U r R 4 b E = " &gt; A A A B 7 X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j w 4 r G C / Y A 2 l M 1 2 0 q 7 d b M L u R C i h / 8 G L B 0 W 8 + n + 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 h x a P Z a y 7 A T M g h Y I W C p T Q T T S w K J D Q C S a 3 u d 9 5 A m 1 E r B 5 w m o A f s Z E S o e A M r d T u B 4 C s P K h U 3 Z o 7 B 1 0 l X k G q p E B z U P n q D 2 O e R q C Q S 2 Z M z 3 M T 9 D O m U X A J s 3 I / N Z A w P m E j 6 F m q W A T G z + b X z u i 5 V Y Y 0 j L U t h X S u / p 7 I W G T M N A p s Z 8 R w b J a 9 X P z P 6 6 U Y 3 v i Z U E m K o P h i U Z h K i j H N X 6 d D o Y G j n F r C u B b 2 V s r H T D O O N q A 8 B G / 5 5 V X S r t e 8 y 1 r 9 / q r a a B R x l M g p O S M X x C P X p E H u S J O 0 C C e P 5 J m 8 k j c n d l 6 c d + d j 0 b r m F D M n 5 A + c z x / 6 l o 6 6 &lt; / l a t e x i t &gt; Learn to Insert Learn to Delete y y y ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W t t 9 s 3 Y X X O e L s 2 7 B H a 4 k J 4 6 A 9 3 Y = " &gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B P J T d K u i x 6 M V j B f u B 7 V q y a b Y N T b J L k h X K 0 n / h x Y M i X v 0 3 3 v w 3 Z t s 9 a O u D g c d 7 M 8 z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P W j p K F K F N E v F I d Q K s K W e S N g 0 z n H Z i R b E I O G 0 H 4 5 v M b z 9 R p V k k 7 8 0 k p r 7 A Q 8 l C R r C x 0 k M v F k E 6 m T 6 e l f r l i l t 1 Z 0 D L x M t J B X I 0 + u W v 3 i A i i a D S E I 6 1 7 n p u b P w U K 8 M I p 9 N S L 9 E 0 x m S M h 7 R r q c S C a j + d X T x F J 1 Y Z o D B S t q R B M / X 3 R I q F 1 h M R 2 E 6 B z U g v e p n 4 n 9 d N T H j l p 0 z G i a G S z B e F C U c m Q t n 7 a M A U J Y Z P L M F E M X s r I i O s M D E 2 p C w E b / H l Z d K q V b 3 z a u 3 u o l K / z u M o w h E c w y l 4 c A l 1 u I U G N I G A h G d 4 h T d H O y / O u / M x b y 0 4 + c w h / I H z + Q M M f 5 C A &lt; / l a t e x i t &gt; y y y ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W t t 9 s 3 Y X X O e L s 2 7 B H a 4 k J 4 6 A 9 3 Y = " &gt; A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B P J T d K u i x 6 M V j B f u B 7 V q y a b Y N T b J L k h X K 0 n / h x Y M i X v 0 3 3 v w 3 Z t s 9 a O u D g c d 7 M 8 z M C 2 L O t H H d b 6 e w s r q 2 v l H c L G 1 t 7 + z u l f c P W j p K F K F N E v F I d Q K s K W e S N g 0 z n H Z i R b E I O G 0 H 4 5 v M b z 9 R p V k k 7 8 0 k p r 7 A Q 8 l C R r C x 0 k M v F k E 6 m T 6 e l f r l i l t 1 Z 0 D L x M t J B X I 0 + u W v 3 i A i i a D S E I 6 1 7 n p u b P w U K 8 M I p 9 N S L 9 E 0 x m S M h 7 R r q c S C a j + d X T x F J 1 Y Z o D B S t q R B M / X 3 R I q F 1 h M R 2 E 6 B z U g v e p n 4 n 9 d N T H j l p 0 z G i a G S z B e F C U c m Q t n 7 a M A U J Y Z P L M F E M X s r I i O s M D E 2 p C w E b / H l Z d K q V b 3 z a u 3 u o l K / z u M o w h E c w y l 4 c A l 1 u I U G N I G A h G d 4 h T d H O y / O u / M x b y 0 4 + c w h / I H z + Q M M f 5 C A &lt; / l a t e x i t &gt; The data-flow of learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>BLEU v.s. speed-up for LevT across variant early-exits and the autoregressive baselines on the test set of Ro-En.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Plots showing the decoding efficiency of the proposed Levenshtein Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>MT &amp; PE Performance v.s. Timeout iterations w/o oracle instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Post-editing examples for WMT'17-APE En-De with the Levenshtein Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Generation quality (BLEU ↑, ROUGE-1/2/L ↑) and latency (ms ↓) as well as the average number of decoder iterations (I DEC ) on the standard test sets for LevT and the autoregressive baseline (with both greedy and beam-search outputs). We show the results of LevT trained from both oracle and the autoregressive teacher model.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Metric</cell><cell cols="2">Transformer greedy beam4</cell><cell cols="2">Levenshtein Transformer oracle distillation</cell></row><row><cell></cell><cell>Ro-En</cell><cell>BLEU</cell><cell>31.67</cell><cell>32.30</cell><cell>33.02</cell><cell>33.26</cell></row><row><cell></cell><cell>En-De</cell><cell>BLEU</cell><cell>26.89</cell><cell>27.17</cell><cell>25.20</cell><cell>27.27</cell></row><row><cell>Quality ↑</cell><cell>En-Ja</cell><cell>BLEU</cell><cell>42.86</cell><cell>43.68</cell><cell>42.36</cell><cell>43.17</cell></row><row><cell></cell><cell>Gigaword</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">The latter coil generated 2.2 T in liquid helium .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">nothing to delete &gt;&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(iteration 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">insert &gt;&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">delete &gt;&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(iteration 2)</cell><cell cols="2">insert &gt;&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">nothing to delete, nothing to insert &gt;&gt;</cell><cell></cell><cell></cell><cell></cell><cell>[Terminate]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for Levenshtein Transformer on En-De (a) and Ro-En (b) translation tasks.(a) Test BLEU for variant weight sharing. Baseline scores from,Ghazvininejad et al. (MaskT, 2019)  are included for reference.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Test BLEU and deletion loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>with variant roll-in polices.</cell></row><row><cell cols="8">sharing none plh, ins ins, del</cell><cell>all</cell><cell>IT</cell><cell>MaskT</cell><cell>roll-in BLEU NLL(del)</cell></row><row><cell cols="3">oracle distill</cell><cell>− 25.11</cell><cell cols="2">25.50 27.73</cell><cell cols="3">− 24.90 27.27 21.61 26.56 25.20 − −</cell><cell>Ours DAE</cell><cell>33.02 31.78</cell><cell>≈ 0.202 ≈ 0.037</cell></row><row><cell>number of iterations</cell><cell>4 6 8 10 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LevT Translation Logarithm Time Linear Time Constant Time (4) Constant Time (10)</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell cols="2">60 sentence length</cell><cell>80</cell><cell>100</cell><cell>120</cell></row><row><cell cols="9">(a) Average number of refinement iterations v.s. length measured</cell></row><row><cell cols="9">on monolingual corpus. For most of the time, LevT decodes with</cell></row><row><cell cols="9">much smaller number (generally, 1∼4) of iterations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>and 5 list the statistics (# of sentences, vocabulary) for all the datasets used in this work. We learn BPE vocabulary with 32, 000 joint operations for WMT En-De and Gigaword and 40, 000 joint operations for WMT Ro-En. For WAT En-Ja, we adopt the official 16, 384 BPE vocabularies learned separately on source and target side.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics for sequence generation tasks (MT and TS). Decoding for Levenshtein TransformerInitialize: Input y = y 0 , step t = 0, maximum step T max , model policy π θ . repeat if y = &lt;s&gt;&lt;/s&gt; then Empty sequence, skip deletion: y = y elseDelete tokens: y = E(y,d), whered = argmax d yi∈y log π del θ (d i |i, y) Insert placeholders: y = E(y ,p), wherep = argmax p yiyi+1∈y log π plh θ (p i |i, y ) if y = y = y then Termination condition satisfied: nothing to delete, nothing to insert.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell><cell>Vocabulary</cell></row><row><cell></cell><cell>WMT'16 Ro-En</cell><cell>608,319</cell><cell cols="2">1999 1999</cell><cell>34,983</cell></row><row><cell>Translation</cell><cell>WMT'14 En-De</cell><cell>4,500,966</cell><cell cols="2">3000 3003</cell><cell>37,009</cell></row><row><cell></cell><cell>WAT'17 En-Ja</cell><cell>2,000,000</cell><cell cols="3">1790 1812 17,952 / 17,801</cell></row><row><cell cols="5">Summarization English Gigaword 3,803,957 189,651 1951</cell><cell>30,004</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Dataset statistics for sequence refinement tasks (APE).</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>MT-Train</cell><cell cols="2">APE-Train Valid</cell><cell>Test</cell><cell>Vocabulary</cell></row><row><cell></cell><cell>WMT'16 Ro-En</cell><cell>300,000</cell><cell cols="3">308,319 1999 1999</cell><cell>34,983</cell></row><row><cell>Synthetic</cell><cell cols="2">WMT'14 En-De 2,250,000</cell><cell cols="3">2,250,967 3000 3003</cell><cell>37,009</cell></row><row><cell></cell><cell>WAT'17 En-Ja</cell><cell>1,000,000</cell><cell cols="4">1,000,000 1790 1812 17,952 / 17,801</cell></row><row><cell>Real</cell><cell>WMT'17 APE En-De</cell><cell>4,391,180</cell><cell>526,368 (fake) + 24,000 (real)</cell><cell cols="2">2000 2000</cell><cell>40,349</cell></row><row><cell cols="3">C Model and Training Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">C.1 Sequence Generation Tasks</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>D More Decoding Examples We present more examples from the proposed Levenshtein Transformer as follows. Translation examples for WAT'17 Small-NMT En-Ja with the Levenshtein Transformer. Translation examples for WMT'16 Ro-En with the Levenshtein Transformer. Translation examples for WMT'14 En-De with the Levenshtein Transformer. Translation examples for English Gigaword with the Levenshtein Transformer.</figDesc><table><row><cell>(a) (a)</cell><cell></cell></row><row><cell>(a)</cell><cell>nothing to delete &gt;&gt; delete &gt;&gt;</cell></row><row><cell cols="2">nothing to delete &gt;&gt; insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; nothing to delete, nothing to insert &gt;&gt; (iteration 1) (iteration 2) (iteration 3) nothing to delete &gt;&gt; insert &gt;&gt; nothing to delete &gt;&gt; (iteration 1) (iteration 2) insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; nothing to delete, nothing to insert &gt;&gt; Figure 6: (iteration 1) (b) (iteration 2) insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; nothing to delete, nothing to insert &gt;&gt; nothing to delete &gt;&gt; insert &gt;&gt; (iteration 1) (iteration 2) delete &gt;&gt; insert &gt;&gt; nothing to delete, nothing to insert &gt;&gt; nothing to delete &gt;&gt; insert &gt;&gt; (iteration 1) (iteration 2) delete &gt;&gt; insert &gt;&gt; nothing to delete, nothing to insert &gt;&gt; nothing to delete &gt;&gt; (a) (b) (c) Figure 7: (iteration 1) (iteration 2) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; insert &gt;&gt; (iteration 3) (iteration 4) nothing to delete &gt;&gt; (iteration 1) (iteration 2) insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; nothing to delete, nothing to insert &gt;&gt; nothing to delete &gt;&gt; (b) Figure 8: (iteration 1) (iteration 2) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; nothing to delete &gt;&gt; (a) (iteration 1) (iteration 2) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; delete &gt;&gt; insert &gt;&gt; nothing to delete &gt;&gt; (b) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; (iteration 1) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; delete &gt;&gt; (b) (iteration 1) nothing to delete, nothing to insert &gt;&gt; insert &gt;&gt; delete &gt;&gt; Figure 9: (iteration 1) (c)</cell><cell>[Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate] [Terminate]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We only consider the variant which only computes insertion and deletion. No substitution is considered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Models &amp; TrainingWe adopt the model architecture of Transformer base<ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> for the proposed LevT model and the autoregressive baseline. All the Transformer-based models are 3 http://www.statmt.org/wmt16/translation-task.html 4 http://www.statmt.org/wmt14/translation-task.html 5 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/snmt/index.html 6 https://github.com/harvardnlp/sent-summary 7 http://www.phontron.com/kytea/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://www.statmt.org/wmt17/ape-task.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Kyunghyun Cho, Marc'Aurelio Ranzato, Douwe Kiela, Qi Liu and our colleagues at Facebook AI Research for valuable feedback, discussions and technical assistance.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for the MT output, add an additional language embedding for each token of the input sequence to show its language type. The detailed hyperpameters are the same as the standard Transformer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En-De</forename><surname>Latency</surname></persName>
		</author>
		<idno>DEC 343 / 28.1 369 / 28.1 126 / 2.88 92 / 2.05</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En-Ja</forename><surname>Latency</surname></persName>
		</author>
		<idno>DEC 261 / 22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>References Dzmitry Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Noisy parallel approximate decoding for conditional recurrent language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03835</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Editnts: An neural programmer-interpreter model for sentence simplification through explicit editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08104</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Text repair model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04790</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Constant-time machine translation with conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1904.09324</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04805</idno>
		<title level="m">Quickedit: Editing text &amp; translations by crossing words out</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Insertion-based decoding with automatically inferred generation order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2395" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir Iosifovich Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady Akademii Nauk</title>
		<imprint>
			<date type="published" when="1965" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
	<note>Russian Academy of Sciences</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the 4th workshop on Asian translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Higashiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Asian Translation (WAT2017)</title>
		<meeting>the 4th Workshop on Asian Translation (WAT2017)<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Iterative refinement for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06602</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Optimal completion distillation for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01398</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Blockwise parallel decoding for deep autoregressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10107" to="10116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Non-monotonic sequential text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kianté</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">As described in §4.2, we consider the following two different imperfect MT systems to provide the refinement inputs. Firstly, we consider the traditional statistical phrase-based machine translation system (PBMT)</title>
		<imprint/>
	</monogr>
	<note>As for the NMT-based model, we use a single layer attention-based model composed by LSTM. We build this model on fairseq-py 10 with the default configuration</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synthetic corpus has two subsets: a 500K one and a 4M one. We over-sample real data by 10 times and merge it with the 500K synthetic data to train APE models. Besides, we also train a LevT MT model on the bigger (4M) synthetic corpus where we only use the source and target pairs</title>
	</analytic>
	<monogr>
		<title level="m">For the real APE task, we follow the procedures introduced in Junczys-Dowmunt and Grundkiewicz</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">We also presented in general how many sentences will be generated using the maximum iteration (for instance 10)</title>
		<imprint/>
	</monogr>
	<note>As shown in Table 6, surprisingly. most predictions are gotten in 1-4 iterations, and the average number of iterations is 2.43. Only a tiny portion (∼ 0.1%) require the maximum number of iterations demonstrating the efficiency of the proposed approach</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
