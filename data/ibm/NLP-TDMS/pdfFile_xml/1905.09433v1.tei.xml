<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sina Weibo Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sina Weibo Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Display Advertising</term>
					<term>CTR Prediction</term>
					<term>Factorization Machines</term>
					<term>Squeeze- Excitation network</term>
					<term>Neural Network</term>
					<term>Bilinear Function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two realworld datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computer systems organization → Factorization methods; • Theory of computation → Computational advertising theory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. The main technique behind these tasks is click-through rate prediction which is known as CTR. Many models have been proposed in this field such as logistic regression (LR) <ref type="bibr" target="#b17">[17]</ref>, polynomial-2 (Poly2) <ref type="bibr" target="#b10">[10]</ref>, tree based models <ref type="bibr" target="#b7">[7]</ref>, tensor-based models <ref type="bibr" target="#b13">[13]</ref>, Bayesian models <ref type="bibr" target="#b3">[3]</ref>, and factorization machines based models <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20</ref>].</p><p>With the great success of deep learning in many research fields such as computer vision <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b14">14]</ref> and natural language processing <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b18">18]</ref>, many deep learning based CTR models have been proposed in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. As a result, the deep learning for CTR prediction has also been a research trend in this field. Some neural network based models have been proposed and achieved success such as Factorization-Machine Supported Neural Networks (FNN) <ref type="bibr" target="#b25">[25]</ref>, Wide&amp;Deep model(WDL) <ref type="bibr" target="#b0">[1]</ref>, Attentional Factorization Machines(AFM) <ref type="bibr" target="#b23">[23]</ref>, DeepFM <ref type="bibr" target="#b4">[4]</ref>, XDeepFM <ref type="bibr" target="#b15">[15]</ref> etc.</p><p>In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and finegrained feature interactions. As far as we know, different features have various importances for the target task. For example, the feature occupation is more important than the feature hobby when we predict a person's income. Taking this into consideration, we introduce a Squeeze-and-Excitation network (SENET) <ref type="bibr" target="#b8">[8]</ref> to learn the weights of features dynamically. Besides, feature interaction is a key challenge in CTR prediction field and many related works calculate the feature interactions in a simple way such as Hadamard product and inner product. We propose a new fine-grained way in this paper to calculate the feature interactions with the bilinear function. Our main contributions are listed as follows:</p><p>• Inspired by the success of SENET in the computer vision field, we use the SENET mechanism to learn the weights of features dynamically. • We introduce three types of Bilinear-Interaction layer to learn feature interactions in a fine-grained way. This is also in contrast to the previous work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23]</ref>, which calculates the feature interactions with Hadamard product or inner product. • Combining SENET mechanism with bilinear feature interaction, our shallow model achieves state-of-the-art among the shallow models such as FFM on Criteo and Avazu datasets. • For further performance gains, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models on Criteo and Avazu datasets.</p><p>The rest of this paper is organized as follows. In Section 2, we review related works which are relevant with our proposed model, followed by introducing our proposed model in Section 3. We will present experimental explorations on Criteo and Avazu datasets in Section 4. Finally, we discuss empirical results and conclude this work in Section 5. storage, and it works well on large sparse data. FFM introduced field aware latent vectors and won two competitions hosted by Criteo and Avazu <ref type="bibr" target="#b9">[9]</ref>. However, FFM was restricted by the need of large memory and cannot easily be used in Internet companies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning based CTR Models</head><p>Deep learning has achieved great success in many research fields such as computer vision <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b14">14]</ref> and natural language processing <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b18">18]</ref>. As a result, many deep learning based CTR models have also been proposed in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. How to effectively model the feature interactions is the key factor for most of these neural network based models.</p><p>Factorization-Machine Supported Neural Networks (FNN) <ref type="bibr" target="#b25">[25]</ref> is a forward neural network using FM to pre-train the embedding layer. However, FNN can capture only high-order feature interactions. Wide &amp; Deep model(WDL) <ref type="bibr" target="#b0">[1]</ref> was initially introduced for application recommendation in google play. WDL jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems. However, expertise feature engineering is still needed on the input to the wide part of WDL, which means that the cross-product transformation also requires to be manually designed. To alleviate manual efforts in feature engineering, DeepFM <ref type="bibr" target="#b4">[4]</ref> replaces the wide part of WDL with FM and shares the feature embedding between the FM and deep component. DeepFM is regarded as one state-of-the-art model in CTR estimation field.</p><p>Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b22">[22]</ref> efficiently captures feature interactions of bounded degrees in an explicit fashion. Similarly, eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b15">[15]</ref> also models the low-order and high-order feature interactions in an explicit way by proposing a novel Compressed Interaction Network (CIN) part.</p><p>As <ref type="bibr" target="#b23">[23]</ref> mentioned, FM can be hindered by its modeling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. And they propose the Attentional Factorization Machines(AFM) <ref type="bibr" target="#b23">[23]</ref> model, which uses an attention network to learn the weights of feature interactions. Deep Interest Network (DIN) <ref type="bibr" target="#b26">[26]</ref> represents users' diverse interests with an interest distribution and designs an attention-like network structure to locally activate the related interests according to the candidate ad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SENET Module</head><p>Hu <ref type="bibr" target="#b8">[8]</ref> proposed the "Squeeze-and-Excitation Network" (SENET) to improve the representational power of a network by explicitly modeling the interdependencies between the channels of convolutional features in various image classification tasks. The SENET is proved to be successful in image classification tasks and won first place in the ILSVRC 2017 classification task.</p><p>There are also other applications about SENET except for the image classification <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24]</ref>. <ref type="bibr" target="#b21">[21]</ref> introduces three variants of the SE modules for semantic segmentation task. Classifying common thoracic diseases as well as localizing suspicious lesion regions on chest X-rays <ref type="bibr" target="#b24">[24]</ref> is another application field. <ref type="bibr" target="#b16">[16]</ref> extends SENET module with a global-and-local attention (GALA) module to get state-of-the-art accuracy on ILSVRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED MODEL</head><p>We aim to dynamically learn the importance of features and feature interactions in a fine-grained way. To this end, we propose the Feature Importance and Bilinear feature Interaction NETwork(FiBiNET) for CTR prediction tasks.</p><p>In this section, we will describe the architecture of our proposed model as depicted in <ref type="figure">Figure 1</ref>. For clarity purpose, we omit the logistic regression part which can be trivially incorporated. Our proposed model consists of the following parts: sparse input layer, embedding layer, SENET layer, Bilinear-Interaction layer, combination layer, multiple hidden layers and output layer. The sparse input layer and embedding layer are the same with DeepFM <ref type="bibr" target="#b4">[4]</ref>, which adopts a sparse representation for input features and embeds the raw feature input into a dense vector. The SENET layer can convert an embedding layer into the SENET-Like embedding features, which helps to boost feature discriminability. The following Bilinear-Interaction layer models second order feature interactions on the original embedding and the SENET-Like embedding respectively. Subsequently, these cross features are concatenated by a combination layer which merges the outputs of Bilinear-Interaction layer. At last, we feed the cross features into a deep neural network and the network outputs the prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Input and Embedding layer</head><p>The sparse input layer and embedding layer are widely used in deep learning based CTR models such as DeepFM <ref type="bibr" target="#b4">[4]</ref> and AFM <ref type="bibr" target="#b23">[23]</ref>. The sparse input layer adopts a sparse representation for raw input features. The embedding layer is able to embed the sparse feature into a low dimensional, dense real-value vector. The output of embedding layer is a wide concatenated field embedding 1 vector: E = [e 1 , e 2 , · · · , e i , · · · , e f ], where f denotes the number of fields, e i ∈ R k denotes the embedding of i-th field , and k is the dimension of embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SENET Layer</head><p>As far as we know, different features have various importances for the target task. For example, the feature occupation is more important than the feature hobby when we predict a person's income. Inspired by the success of SENET in the computer vision field, we introduce a SENET mechanism to let the model pay more attention to the feature importance. For specific CTR prediction task, we can dynamically increase the weights of important features and decrease the weights of uninformative features via the SENET mechanism.</p><p>Using the feature embedding as input, the SENET produces weight vector A = {a 1 , · · · , a i , · · · , a f } for field embeddings and then rescales the original embedding E with vector A to get a new embedding (SENET-Like embedding)  As <ref type="figure">Figure 2</ref> illustrated, the SENET is comprised of three steps: squeeze step, excitation step and re-weight step. These steps can be described in detail as follows: Squeeze. This step is used for calculating 'summary statistics' of each field embedding. Concretely speaking, we use some pooling methods such as max or mean to squeeze the original embedding</p><formula xml:id="formula_0">V = [v 1 , · · · , v i , · · · , v f ], where a i ∈ R is a scalar that denotes the weight of the i-th field embedding v i , v i ∈ R k denotes the SENET-Like embedding of i-th field, i ∈ [1, 2, · · · , f ] , V ∈ R f ×k ,</formula><formula xml:id="formula_1">E = [e 1 , · · · , e f ] into a statistic vector Z = [z 1 , · · · , z i , · · · , z f ], where i ∈ [1, · · · , f ]</formula><p>, z i is a scalar value which represents the global information about the i-th feature representation. z i can be calculated as the following global mean pooling:</p><formula xml:id="formula_2">z i = F sq (e i ) = 1 k k t =1 e (t ) i<label>(1)</label></formula><p>The squeeze function in original SENET paper <ref type="bibr" target="#b8">[8]</ref> is max pooling. However, our experimental results show that the mean pooling performs better than the max pooling.</p><p>Excitation. This step can be used to learn the weight of each field embedding based on the statistic vector Z . We use two full connected (FC) layers to learn the weights. The first FC layer is a dimensionality-reduction layer with parameters W 1 with reduction ratio r which is a hyper-parameter and it uses σ 1 as nonlinear function. The second FC layer increases dimensionality with parameters W 2 . Formally, the weight of field embedding can be calculated as follows:</p><formula xml:id="formula_3">A = F ex (Z ) = σ 2 (W 2 σ 1 (W 1 Z ))<label>(2)</label></formula><p>where A ∈ R f is a vector, σ 1 and σ 2 are activation functions, the</p><formula xml:id="formula_4">learning parameters are W 1 ∈ R f × f r , W 2 ∈ R f</formula><p>r ×f , and r is reduction ratio. Re-Weight. The last step in SENET is a reweight step which is called re-scale in original paper <ref type="bibr" target="#b8">[8]</ref>. It does field-wise multiplication between the original field embedding E and field weight vector A and outputs the new embedding(SENET-Like embedding)</p><formula xml:id="formula_5">V = {v 1 , · · · , v i , · · · , v f }.</formula><p>The SENET-Like embedding V can be calculated as follows:</p><formula xml:id="formula_6">V = F ReW eiдht (A, E) = [a 1 · e 1 , · · · , a f · e f ] = [v 1 , · · · , v f ] (3) where a i ∈ R, e i ∈ R k , and v i ∈ R k .</formula><p>In short, the SENET uses two FCs to dynamically learn the importance of features. For a specific task, it increases the weights of important features and decreases the weights of uninformative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bilinear-Interaction Layer</head><p>The Interaction layer is a layer to calculate the second order feature interactions. The classical methods of feature interactions in Interaction layer are inner product and Hadamard product. Inner product is widely used in shallow models such as FM and FFM while the Hadamard product is commonly used in deep models such as </p><formula xml:id="formula_7">v i · v j )x i x j } (i, j)∈R x and {(v i ⊙ v j )x i x j } (i, j)∈R x , where R x = {(i, j)} i ∈ {1, ··· , f }, j ∈ {1, ··· , f }, j &gt;i , v i</formula><p>is the i-th field embedding vector, · denotes the regular inner product, and ⊙ denotes the Hadamard product, for example,</p><formula xml:id="formula_8">[a 1 , a 2 , a 3 ] ⊙ [b 1 , b 2 , b 3 ] = [a 1 b 1 , a 2 b 2 , a 3 b 3 ]</formula><p>. Inner product and Hadamard product in Interaction layer are too simple to effectively model the feature interactions in sparse dataset. Therefore, we propose a more fine-grained method which combines the inner product and Hadamard product to learn the feature interactions with extra parameters. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>.c, the inner product is used between the matrix W and vector v i and the Hadamard product is used between the matrix W and the vector v j . Specifically, we propose three types of bilinear functions in this layer and call this layer Bilinear-Interaction layer. Taking the i-th field embedding v i and the j-th field embedding v j as an example, the result of feature interaction p i j can be calculated as follows: a.Field-All Type</p><formula xml:id="formula_9">p i j = v i · W ⊙ v j (4) where W ∈ R k ×k , and v i , v j ∈ R k are the i-th and j-th field em- bedding, 1 ≤ i ≤ f , i ≤ j ≤ f . Here W is shared among all (v i , v j )</formula><p>field interaction pairs and there are k × k parameters in Bilinear-Interaction layer, so here we call this type 'Field-All'. b.Field-Each Type</p><formula xml:id="formula_10">p i j = v i · W i ⊙ v j (5) where W i ∈ R k ×k , v i , v j ∈ R k are the i-th and j-th field embedding, 1 ≤ i ≤ f , i ≤ j ≤ f .</formula><p>Here W i is the corresponding parameter matrix of the i-th field and there are f ×k ×k parameters in Bilinear-Interaction layer because we have f different fields, so here we call this type 'Field-Each'. c.Field-Interaction Type</p><formula xml:id="formula_11">p i j = v i · W i j ⊙ v j<label>(6)</label></formula><p>where W i j ∈ R k ×k is the corresponding parameter matrix of interaction between field i and field j and 1 ≤ i ≤ f , i ≤ j ≤ f . The total number of learning parameters in this layer is n × k × k, n is the number of field interactions, which is equal to</p><formula xml:id="formula_12">f (f −1)</formula><p>2 . Here we call this type 'Field-Interaction'.</p><p>As shown in <ref type="figure">Figure 1</ref>, we have two embeddings(original embedding and SENET-like embedding) and we can adopt either the bilinear function or Hadamard product as feature interaction operation to any embeddings. So we have several combinations of feature interaction in this layer. In Section 4.3, we will discuss the performance of different combinations of bilinear function and Hadamard product in detail. In addition, we have three different types of proposed feature interaction methods(Field-All, Field-Each, Field-Interaction) to apply in our model and we will discuss the performance of different field types in Section 4.4.</p><p>In this section, the Bilinear-Interaction layer can output an interaction vector p = [p 1 , ..., p i , ..., p n ] from the original embedding E and a SENET-Like interaction vector q = [q 1 , ..., q i , ..., q n ] from the SENET-Like embedding V , where p i , q i ∈ R k are vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combination Layer</head><p>The combination layer concatenates interaction vector p and q and feeds the concatenated vector into the following layer in FiBiNET which is a standard neural network layer. It can be expressed as the following forms:</p><formula xml:id="formula_13">c = F concat (p, q) = [p 1 , · · · , p n , q 1 , · · · , q n ] = [c 1 , · · · , c 2n ] (7)</formula><p>If we sum each element in vector c and then use a sigmoid function to output a prediction value, we have a shallow CTR model. For further performance gains, we combine the shallow component and a classical deep neural network(DNN) which will be described in Section 3.5 into a unified model to form the deep network structure, this unified model is called deep model in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Deep Network</head><p>The deep network is comprised of several full-connected layers, which implicitly captures high-order features interactions. As shown in <ref type="figure">Figure 1</ref>, the input of deep network is the output of the combination layer. Let a (0) = [c 1 , c 2 , · · · , c 2n ] denotes the outputs of the combination layer, where c i ∈ R k and n is the number of field interactions. Then, a (0) is fed into the deep neural network and the feed forward process is:</p><formula xml:id="formula_14">a (l ) = σ (W (l ) a (l −1) + b (l ) )<label>(8)</label></formula><p>where l is the depth and σ is the activation function. W (l ) ,b (l ) ,a (l ) are the model weight, bias and output of the l-th layer. After that, a dense real-value feature vector is generated which is finally fed into the sigmoid function for CTR prediction:</p><formula xml:id="formula_15">y d = σ (W |L |+1 a |L | + b |L |+1 ),</formula><p>where |L| is the depth of DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Output Layer</head><p>To summarize, we give the overall formulation of our proposed model' output as:ŷ</p><formula xml:id="formula_16">= σ (w 0 + m i=0 w i x i + y d )<label>(9)</label></formula><p>whereŷ ∈ (0, 1) is the predicted value of CTR, σ is the sigmoid function, m is the feature size, x is a input and w i is the i-th weight of linear part. The parameters of our model are θ</p><formula xml:id="formula_17">= {w 0 , {w i } m i=1 , {e i } m i=1 , {W i } 2 i=1 , {W (i) } |L | i=1 }.</formula><p>The learning process aims to minimize the following objective function (cross entropy):</p><formula xml:id="formula_18">loss = − 1 N N i=1 (y i loд(ŷ i ) + (1 − y i ) * loд(1 −ŷ i ))<label>(10)</label></formula><p>where y i is the ground truth of i-th instance,ŷ i is the predicted CTR, and N is the total size of samples.</p><p>3.6.1 Relationship with FM and FNN. Suppose we remove the SENET layer and Bilinear-Interaction layer, it's not hard to find that our model will be degraded as the FNN. When we further remove the DNN part, and at the same time use a constant sum, then the shallow FiBiNET is downgraded to the traditional FM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to answer the following questions: (RQ1) How does our model perform as compared to the state-ofthe-art methods for CTR prediction? (RQ2) Can the different combinations of bilinear and Hadamard functions in Bilinear-Interaction layer impact its performance? (RQ3) Can the different field types(Field-All, Field-Each and Field-Interaction) of Bilinear-Interaction layer impact its performance? (RQ4) How do the settings of networks influence the performance of our model? (RQ5) Which is the most important component in FiBiNET?</p><p>We will answer these questions after presenting some fundamental experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Testbeds and Setup</head><p>4.1.1 Data Sets. 1) Criteo. The Criteo 2 dataset is widely used in many CTR model evaluation. It contains click logs with 45 millions of data instances. There are 26 anonymous categorical fields and 13 continuous feature fields in Criteo dataset. We split the dataset randomly into two parts: 90% is for training, while the rest is for testing. 2) Avazu. The Avazu 3 dataset consists of several days of ad click-through data which is ordered chronologically. It contains click logs with 40 millions of data instances. For each click data, there are 24 fields which indicate elements of a single ad impression. We split it randomly into two parts: 80% is for training, while the rest is for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics.</head><p>In our experiment, we adopt two metrics: AUC(Area Under ROC) and Log loss.</p><p>AUC: Area under ROC curve is a widely used metric in evaluating classification problems. Besides, some work validates AUC as a good measurement in CTR prediction <ref type="bibr" target="#b3">[3]</ref>. AUC is insensitive to the classification threshold and the positive ratio. The upper bound of AUC is 1, and the larger the better.</p><p>Log loss: Log loss is widely used metric in binary classification, measuring the distance between two distributions. The lower bound of log loss is 0, indicating the two distributions perfectly match, and a smaller value indicates better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baseline Methods.</head><p>To verify the efficiency of combining SENET layer with Bilinear-Interaction layer in shallow model and deep model, we split our experiments into two groups: shallow group and deep group. We also split the baseline models into two parts: shallow baseline models and deep baseline models. The shallow baseline models include LR(logistic regression) <ref type="bibr" target="#b17">[17]</ref>, FM <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>, FFM <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>, AFM <ref type="bibr" target="#b23">[23]</ref>, and the deep baseline models include FNN <ref type="bibr" target="#b25">[25]</ref>, DCN <ref type="bibr" target="#b22">[22]</ref>, DeepFM <ref type="bibr" target="#b4">[4]</ref>, XDeepFM <ref type="bibr" target="#b15">[15]</ref>.</p><p>Note that an improvement of 1‰ in AUC is usually regarded as significant for the CTR prediction because it will bring a large increase in a company's revenue if the company has a very large user base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement all the models with Tensorflow 4 in our experiments. For the embedding layer, the dimension of embedding layer is set to 10 for Criteo dataset and 50 for Avazu dataset. For the optimization method, we use the Adam <ref type="bibr" target="#b11">[11]</ref> with a mini-batch size of 1000 for Criteo and 500 for Avazu datasets, and the learning rate is set to 0.0001. For all deep models, the depth of layers is set to 3, all activation functions are RELU, the number of neurons per layer is 400 for Criteo dataset and 2000 for Avazu dataset, and the dropout rate is set to 0.5. For the SENET part, the activation functions in two FCs are RELU function, and the reduction ratio is set to 3. We conduct our experiments with 2 Tesla K40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison(RQ1)</head><p>In this subsection, we summarize the overall performance of shallow models and deep models on Criteo and Avazu test sets in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> respectively.  <ref type="table" target="#tab_0">Table 1</ref> shows the results of the shallow models on Criteo and Avazu datasets. We find our shallow SE-FM-All model consistently outperforms other models such as FM, FFM, AFM etc. On the one hand, the results indicate that combining the SENET mechanism with the bilinear interaction over sparse features is an effective method for many real world datasets; on the other hand, for the classical shallow models, the state-of-the-art model is FFM which is restricted by the need of large memory and cannot be easily used in Internet companies, our shallow model has fewer parameters but still performs better than FFM. So it can be regarded as an alternative solution for FFM. For further performance gains, we combine the shallow part and DNN into a deep model. The overall performance of deep models is shown in <ref type="table" target="#tab_1">Table 2</ref> and we have the following observations:</p><p>• Combining the shallow part and DNN into a unified model, the shallow model can gain further performance improvement. We can infer from experimental results that the implicit high-order feature interactions help the shallow model to gain more expressive power. • Among all the compared methods, our proposed deep FiB-iNET achieves the best performance. Our deep model outperforms FNN by relatively 0.571% and 0.386% in terms of AUC(0.918% and 0.4% in terms of log loss) and outperforms DeepFM by 0.222% and 0.59% in terms of AUC(0.494% and 0.6% in terms of log loss) on Criteo and Avazu datasets. • The results indicate that combining the SENET mechanism with Bilinear-Interaction in DNN for prediction is effective. On the one hand, the SENET intrinsically introduces dynamics conditioned on the input, helping to boost feature discriminability; on the other hand, the bilinear function is an effective method to model the feature interaction as compared with other methods such as the inner product or Hadamard product as described in Section 4.3.</p><p>For further performance gains, we will discuss the different combinations of Bilinear-Interaction layer in Section 4.3 and the field types of Bilinear-Interaction layer in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combinations of Bilinear-Interaction Layer(RQ2)</head><p>In this section, we will discuss the influence of different type of combinations between bilinear function and Hadamard product in Bilinear-Interaction layer. For convenience, we use 0 and 1 to represent which function is used in Bilinear-Interaction layer. The '1' denotes that bilinear function is used while 0 means Hadamard product is used. We have two embeddings so two numbers are used. The first number denotes the feature interaction method used on original embedding and the second number denotes the feature interaction method used on SENET-Like embedding. For example, '10' denotes that bilinear function is used as feature interaction method on the original embedding while the Hadamard function is used as feature interaction method on the SENET like embedding. Similarly, we conduct the experiments on shallow and deep models and summarize the results in <ref type="table" target="#tab_2">Table 3</ref>. Overall, we can not draw any obvious conclusions, but we can find some empirical observations as follows:</p><p>• On Criteo dataset, the combination '11' outperforms other type of combinations among shallow models. However, the combination '11' performs worst among the deep models. • The preferred combination in deep models should be '01'.</p><p>This combination means the bilinear function is only applied to the SENET-Like embedding layer, which is beneficial for designing an effective network architectures in our model. In this section, we study impact of different field types(Field-All, Field-Each and Field-Interaction) of Bilinear-Interaction layer. We first fix the combination of the bilinear and Hadamard product in Bilinear-Interaction layer. The combination of Bilinear-Interaction layer is set to '01' for deep model and '11' for shallow model. And the mark '01' and '11' are illustrated in Section 4.3. We summarize the experimental results in <ref type="table" target="#tab_3">Table 4</ref> and have the following observations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Field Types of Bilinear-Interaction (RQ3)</head><p>• For the shallow models, compared to the Field-All type of our shallow model (in <ref type="table" target="#tab_0">Table 1</ref>), the Field-Interaction type can gains 0.382% (relatively 0.476%) improvements in terms of AUC on Criteo dataset. • For the deep models, compared to the Field-All type of our deep model (in <ref type="table" target="#tab_1">Table 2</ref>), the type of Field-Interaction for Criteo dataset and Field-Each for Avazu dataset can gain some improvements respectively. • The performances of different types of Bilinear-Interaction layer depend on datasets. On Criteo dataset, the performance ranking is as follows: Field-Interaction, Field-Each, and Field-All. While on Avazu dataset, we cannot draw the obvious conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Investigation(RQ4)</head><p>In this subsection, we will conduct some hyper-parameter investigations in our model. We focus on hyper-parameters in the following two components in FiBiNET: the embedding part and the DNN part. Specifically, we change the following hyper-parameters:(1) the dimension of embeddings; (3) the number of neurons per layer in DNN; (4) the depth of DNN. Unless specially mentioned in our paper, the default parameter of our network is set as the Section 4.1.4.  <ref type="table" target="#tab_4">Table 5</ref>. We can find some observations as follows:</p><p>• As the dimension is expanded from 10 to 50, our model can obtain a substantial improvement on Avazu dataset. • The performance degrades when we increase the embedding size on Criteo dataset. Enlarging embedding size indicates increasing the number of parameters in embedding layer and DNN part. We guess that it may be the much more features in Criteo dataset as opposed to Avazu dataset that leads to optimization difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">DNN Part.</head><p>In deep part, we can change the number of neurons per layer, the depths of DNN, the activation functions and the dropout rates. For brevity, we just study the impact of different neural units per layer and different depths in DNN part. As a matter of fact, increasing the number of layers can increase the model complexity. We can observe from <ref type="figure" target="#fig_3">Figure 4</ref> that increasing number of layers improves model performance at the beginning. However, the performance is degraded if the number of layers keeps increasing. This is because an over-complicated model is easy to overfit. It's a good choice that the number of hidden layers is set to 3 for Avazu dataset and Criteo dataset. Likewise, increasing the number of neurons per layer introduces complexity. In <ref type="figure" target="#fig_4">Figure 5</ref>, we find that it is better to set 400 neurons per layer for Criteo dataset and 2000 neurons per layer for Avazu dataset.   Although we have demonstrated strong empirical results, the results presented so far have not isolated the specific contributions from each component of the FiBiNET. In this section, we perform ablation experiments over FiBiNET in order to better understand their relative importance. We set 'DeepSE-FM-Interaction' as the base model and perform it in the following ways: 1) No BI: remove the Bilinear-Interaction layer from FiBiNET 2) No SE: remove the SENET layer from FiBiNET.</p><p>If we remove the SENET layer and Bilinear-Interaction layer, our shallow FiBiNET and deep FiBiNET will downgrade to FM and FNN. We can find the following observations in <ref type="table" target="#tab_5">Table 6</ref>:</p><p>• Both the Bilinear-Interaction layer and SENET layer are necessary for FiBiNET's performance. We can see that the the performance will drop apparently when we remove any component.</p><p>• The Bilinear-Interaction layer is as important as the SENET layer in FiBiNET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Motivated by the drawbacks of the state-of-the-art models, we propose a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork and aim to dynamically learn the feature importance and fine-grained feature interactions. Our proposed FiBiNET makes a contribution to improving performance in these following aspects: 1) For CTR task, the SENET module can learn the importance of features dynamically. It boosts the weight of the important feature and suppresses the weight of unimportant features. 2) We introduce three types of Bilinear-Interaction layers to learn feature interaction rather than calculating the feature interactions with Hadamard product or inner product. 3) Combining the SENET mechanism with bilinear feature interaction in our shallow model outperforms other shallow models such as FM and FFM. 4) In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and XdeepFM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>k is an embedding size, and f is the number of fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>The architecture of our proposed FiBiNET The SENET Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The different methods to calculate the feature interactions. (a): Inner product. (b): Hadamard product. (c): our proposed bilinear interaction. Here p i j in inner product method is a scalar while it is a vector in Hadamard product and our proposed bilinear function. AFM and NFM. The forms of inner product and Hadamard product are respectively expressed as {(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The performance of different number of layers in DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The performance of different number of neurons per layer in DNN 4.6 Ablation Study (RQ5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The overall performance of shallow models on Criteo and Avazu datasets. The SE-FM-ALL denotes the shallow model with the Field-All type of Bilinear-Interaction layer.</figDesc><table><row><cell></cell><cell cols="2">Criteo</cell><cell cols="2">Avazu</cell></row><row><cell>Model</cell><cell>AUC</cell><cell>Logloss</cell><cell>AUC</cell><cell>Logloss</cell></row><row><cell>LR</cell><cell>0.7808</cell><cell>0.4681</cell><cell>0.7633</cell><cell>0.3891</cell></row><row><cell>FM</cell><cell>0.7923</cell><cell>0.4584</cell><cell>0.7745</cell><cell>0.3832</cell></row><row><cell>FFM</cell><cell>0.8001</cell><cell>0.4525</cell><cell>0.7795</cell><cell>0.3810</cell></row><row><cell>AFM</cell><cell>0.7965</cell><cell>0.4541</cell><cell>0.7740</cell><cell>0.3839</cell></row><row><cell cols="5">SE-FM-All 0.8021 0.4495 0.7803 0.3800</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The overall performance of deep models on Criteo and Avazu datasets. The DeepSE-FM-ALL denotes the deep model with the Field-All type of Bilinear-Interaction layer.</figDesc><table><row><cell></cell><cell cols="2">Criteo</cell><cell cols="2">Avazu</cell></row><row><cell>Model</cell><cell>AUC</cell><cell>Logloss</cell><cell>AUC</cell><cell>Logloss</cell></row><row><cell>FNN</cell><cell cols="2">0.8057 0.4464</cell><cell>0.7802</cell><cell>0.3800</cell></row><row><cell>DeepFM</cell><cell cols="2">0.8085 0.4445</cell><cell>0.7786</cell><cell>0.3810</cell></row><row><cell>DCN</cell><cell cols="2">0.7978 0.4617</cell><cell>0.7681</cell><cell>0.3940</cell></row><row><cell>XDeepFM</cell><cell cols="2">0.8091 0.4461</cell><cell>0.7808</cell><cell>0.3818</cell></row><row><cell cols="5">DeepSE-FM-All 0.8103 0.4423 0.7832 0.3786</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The performance of different combinations of bilinear and Hadamard functions in Bilinear-Interaction layer. The field type of Bilinear-Interaction layer is set to Field-Each.</figDesc><table><row><cell></cell><cell cols="2">Criteo</cell><cell cols="2">Avazu</cell></row><row><cell>Combinations</cell><cell>AUC</cell><cell>Logloss</cell><cell>AUC</cell><cell>Logloss</cell></row><row><cell>SE-FM_00</cell><cell cols="2">0.7989 0.4525</cell><cell>0.7782</cell><cell>0.3818</cell></row><row><cell>SE-FM_01</cell><cell cols="2">0.8018 0.4500</cell><cell>0.7797</cell><cell>0.3808</cell></row><row><cell>SE-FM_10</cell><cell cols="2">0.8029 0.4488</cell><cell>0.7794</cell><cell>0.3807</cell></row><row><cell>SE-FM_11</cell><cell cols="2">0.8037 0.4479</cell><cell>0.7770</cell><cell>0.3815</cell></row><row><cell cols="3">DeepSE-FM-00 0.8105 0.4425</cell><cell>0.7828</cell><cell>0.3785</cell></row><row><cell cols="3">DeepSE-FM-01 0.8104 0.4423</cell><cell cols="2">0.7833 0.3783</cell></row><row><cell cols="3">DeepSE-FM-10 0.8100 0.4427</cell><cell>0.7810</cell><cell>0.3809</cell></row><row><cell cols="3">DeepSE-FM-11 0.8099 0.4428</cell><cell>0.7805</cell><cell>0.3807</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The performance of different field types of Bilinear-Interaction layer.</figDesc><table><row><cell>Criteo</cell><cell>Avazu</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The performance of different embedding sizes on Criteo and Avazu datasets Embedding Part. We change the embedding sizes from 10 to 50 and summarize the experimental results in</figDesc><table><row><cell></cell><cell cols="2">Criteo</cell><cell cols="2">Avazu</cell></row><row><cell>Embedding-Size</cell><cell>AUC</cell><cell>Logloss</cell><cell>AUC</cell><cell>Logloss</cell></row><row><cell>10</cell><cell cols="2">0.8104 0.4423</cell><cell>0.7809</cell><cell>0.3801</cell></row><row><cell>20</cell><cell cols="2">0.8093 0.4435</cell><cell>0.7810</cell><cell>0.3796</cell></row><row><cell>30</cell><cell cols="2">0.8071 0.4460</cell><cell>0.7812</cell><cell>0.3799</cell></row><row><cell>40</cell><cell cols="2">0.8071 0.4464</cell><cell>0.7824</cell><cell>0.3790</cell></row><row><cell>50</cell><cell cols="2">0.8072 0.4468</cell><cell cols="2">0.7833 0.3787</cell></row><row><cell>4.5.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The performance of different components in FiBi-NET.</figDesc><table><row><cell></cell><cell>Criteo</cell><cell cols="2">Avazu</cell></row><row><cell>Model</cell><cell cols="3">AUC Logloss AUC Logloss</cell></row><row><cell>BASE</cell><cell>0.8037 0.4479</cell><cell>0.7797</cell><cell>0.3812</cell></row><row><cell>NO-SE</cell><cell>0.7962 0.4552</cell><cell>0.7763</cell><cell>0.3825</cell></row><row><cell>NO-BI</cell><cell>0.7986 0.4525</cell><cell>0.7754</cell><cell>0.3829</cell></row><row><cell>FM</cell><cell>0.7923 0.4584</cell><cell>0.7745</cell><cell>0.3832</cell></row><row><cell cols="2">Deep-BASE 0.8104 0.4423</cell><cell>0.7833</cell><cell>0.3783</cell></row><row><cell>NO-SE</cell><cell>0.8098 0.4427</cell><cell>0.7822</cell><cell>0.3790</cell></row><row><cell>NO-BI</cell><cell>0.8093 0.4435</cell><cell>0.7827</cell><cell>0.3785</cell></row><row><cell>FNN</cell><cell>0.8057 0.4464</cell><cell>0.7802</cell><cell>0.3800</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The field embedding is also known as the feature embedding. If the field is multivalent, the sum of feature embedding is used as the field embedding. For consistency with previous literature, we preserve "feature" in some terminologies, e.g., feature interaction, and feature representation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Criteo http://labs.criteo.com/downloads/download-terabyte-click-logs/ 3 Avazu http://www.kaggle.com/c/avazu-ctr-prediction</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">TensorFlow: https://www.tensorflow.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:cs.CL/1406.1078</idno>
		<title level="m">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><forename type="middle">Quinonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Omnipress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080777</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080777" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines in a real-world online advertising system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Lefortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Skin lesion classification with ensemble of squeeze-and-excitation networks and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Kitada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Iyatomi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02568</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05170</idno>
		<title level="m">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Global-andlocal attention networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Scheibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Abhijit Guha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wachinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08127</idno>
		<title level="m">Recalibrating Fully Convolutional Networks with Spatial and Channel&apos;Squeeze &amp; Excitation&apos;Blocks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<meeting>the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
