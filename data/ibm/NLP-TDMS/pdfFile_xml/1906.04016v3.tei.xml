<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Temporal Pose Estimation from Sparsely-Labeled Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Temporal Pose Estimation from Sparsely-Labeled Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames-a labeled Frame A and an unlabeled Frame B-we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows us to obtain state-of-the-art pose detection results on PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github. com/facebookresearch/PoseWarper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-Person Pose Detection in Images. The traditional approaches for pose estimation leverage pictorial structures model <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>, which represents human body as a tree-structured graph with pairwise potentials between the connected body parts. These approaches have been highly successful in the past, but they tend to fail if some of body parts are occluded. These issues have been partially addressed by the models that assume a non-tree graph structure <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. However, most modern approaches for single image pose estimation are based on convolutional neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b45">[45]</ref>. The method in <ref type="bibr" target="#b2">[3]</ref> regresses (x, y) joint coordinates directly from the images. More recent work [25] instead predicts pose heatmaps, which leads to an easier optimization problem. Several approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">46]</ref> propose an iterative pose estimation pipeline where the predictions are refined at different stages inside a CNN or via a recurrent network. The methods in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">45]</ref> tackle pose estimation problem in a top-down fashion, first detecting bounding boxes of people, and then predicting the pose heatmaps from the cropped images. The work in <ref type="bibr" target="#b23">[24]</ref> proposes part affinity fields module that captures pairwise relationships between different body parts. The approaches in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> leverage a bottom-up pipeline first predicting the keypoints, and then assembling them into instances. Lastly, a recent work in <ref type="bibr" target="#b26">[27]</ref>, proposes an architecture that preserves high resolution feature maps, which is shown to be highly beneficial for the multi-person pose estimation task.</p><p>Multi-Person Pose Detection in Video. Due to a limited number of large scale benchmarks for video pose detection, there has been significantly fewer methods in the video domain. Several prior methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48</ref>] tackle a video pose estimation task as a two-stage problem, first detecting the keypoints in individual frames, and then applying temporal smoothing techniques. The method in <ref type="bibr" target="#b49">[49]</ref> proposes a spatiotemporal CRF, which is jointly optimized for the pose prediction in video. The work in [50] proposes a personalized video pose estimation framework, which is accomplished by finetuning the model on a few frames with high confidence keypoints in each video. The approaches in <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref> leverage flow based representations for aligning features temporally across multiple frames, and then using such aligned features for pose detection in individual frames.</p><p>In contrast to these prior methods, our primary objective is to learn an effective video pose detector from sparsely labeled videos. Our approach has similarities to the methods in <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref>, which use flow representations for feature alignment. However, unlike our model, the methods in [51, 52] do not optimize their flow representations end-to-end with respect to the pose detection task. As we will show in our experiments, this is important for strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PoseWarper Network</head><p>Overview. Our goal is to design a model that learns to detect pose from sparsely labeled videos. Specifically, we assume that pose annotations in training videos are available every k frames. Inspired by a recent self-supervised approach for learning facial attribute embeddings [53], we formulate the following task. Given two video frames-a labeled Frame A and an unlabeled Frame B-our model</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, visual understanding methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> have made tremendous progress, partly because of advances in deep learning <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, and partly due to the introduction of large-scale annotated datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In this paper we consider the problem of pose estimation, which has greatly benefitted from the recent creation of large-scale datasets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Most of the recent advances in this area, though, have concentrated on the task of pose estimation in still-images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. However, directly applying these image-level models to video is challenging due to nuisance factors such as motion blur, video defocus, and frequent pose occlusions. Additionally, the process of collecting annotated pose data in multi-person videos is costly and time consuming. A video typically contains hundreds of frames that need to be densely-labeled by human annotators. As a result, datasets for video pose estimation <ref type="bibr" target="#b21">[22]</ref> are typically smaller and less diverse compared to their image counterparts <ref type="bibr" target="#b20">[21]</ref>. This is problematic because modern deep models require large amounts of labeled data to achieve good performance. At the same time, videos have high informational redundancy as the content changes little from frame to frame. This raises the question of whether every single frame in a training video needs to be labeled in order to achieve good pose estimation accuracy. <ref type="bibr">33rd</ref> Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1906.04016v3 [cs.CV] 11 Dec 2019</head><p>To reduce the reliance on densely annotated video pose data, in this work, we introduce the Pose-Warper network, which operates on sparsely annotated videos, i.e., videos where pose annotations are given only every k frames. Given a pair of frames from the same video-a labeled Frame A and an unlabeled Frame B-we train our model to detect pose in Frame A, using the features from Frame B. To achieve this goal, our model leverages deformable convolutions <ref type="bibr" target="#b27">[28]</ref> across space and time. Through this mechanism, our model learns to sample features from an unlabeled Frame B to maximize pose detection accuracy in a labeled Frame A.</p><p>Our trained PoseWarper can then be used for several applications. First, we can leverage PoseWarper to propagate pose information from a few manually-labeled frames across the entire video. Compared to modern optical flow propagation methods such as FlowNet2 <ref type="bibr" target="#b28">[29]</ref>, our PoseWarper produces more accurate pose annotations (88.7% mAP vs 83.8% mAP), while also employing a much more compact warping mechanism (6M vs 39M parameters). Furthermore, we show that our propagated poses can serve as effective pseudo labels for training a more accurate pose detector. Finally, our PoseWarper can be used to aggregate temporal pose information from neighboring frames during inference. This naturally renders the approach more robust to occlusion or motion blur in individual frames, and leads to state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets <ref type="bibr" target="#b21">[22]</ref>.  Our training procedure is designed to achieve two goals: 1) our model must be able to extract motion offsets relating these two frames. 2) Using these motion offsets our model must then be able to rewarp the detected pose heatmap extracted from an unlabeled Frame B in order to optimize the accuracy of pose detection in a labeled Frame A. After training, we can apply our model in reverse order to propagate pose information across the entire video from ground truth poses given for only a few frames.</p><p>is allowed to compare Frame A to Frame B but it must predict Pose A (i.e., the pose in Frame A) using the features from Frame B, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref> (top).</p><p>At first glance, this task may look overly challenging: how can we predict Pose A by merely using features from Frame B? However, suppose that we had body joint correspondences between Frame A and Frame B. In such a scenario, this task would become trivial, as we would simply need to spatially "warp" the feature maps computed from frame B according to the set of correspondences relating frame B to frame A. Based on this intuition, we design a learning scheme that achieves two goals: 1) By comparing Frame A and Frame B, our model must be able to extract motion offsets relating these two frames. 2) Using these motion offsets our model must be able to rewarp the pose extracted from an unlabeled Frame B in order to optimize pose detection accuracy in a labeled Frame A.</p><p>To achieve these goals, we first feed both frames through a backbone CNN that predicts pose heatmaps for each of the frames. Then, the resulting heatmaps from both frames are used to determine which points from Frame B should be sampled for detection in Frame A. Finally, the resampled pose heatmap from Frame B is used to maximize accuracy of Pose A.</p><p>Backbone Network. Due to its high efficiency and accuracy, we use the state-of-the-art High Resolution Network (HRNet-W48) <ref type="bibr" target="#b26">[27]</ref> as our backbone CNN. However, we note that our system can easily integrate other architectures as well. Thus, we envision that future improvements in still-image pose estimation will further improve the effectiveness of our approach.</p><p>Deformable Warping. Initially, we feed Frame A and Frame B through our backbone CNN, which outputs pose heatmaps f A and f B . Then, we compute the difference ψ A,B = f A − f B . The resulting feature tensor ψ A,B is provided as input to a stack of 3 × 3 simple residual blocks (as in standard ResNet-18 or ResNet-34 models), which output a feature tensor φ A,B . The feature tensor φ A,B is then fed into five 3 × 3 convolutional layers each using a different dilation rate d ∈ {3, 6, 12, 18, 24}   to predict five sets of offsets o (d) (p n ) at all pixel locations p n . The motivation for using different dilation rates at the offset prediction stage comes from the need to consider motion cues at different spatial scales. When the body motion is small, a smaller dilation rate may be more useful as it captures subtle motion cues. Conversely, if the body motion is large, using large dilation rate allows us to incorporate relevant information further away. Next, the predicted offsets are used to spatially rewarp the pose heatmap f B . We do this for each of the five sets of offsets o (d) , and then sum up all five rewarped pose heatmaps to obtain a final output g A,B , which is used to predict pose in Frame A.</p><formula xml:id="formula_0">Labeled Frame A (time t) Offsets Warped Feature Map Difference &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x b J V L R y 5 z V 8 j t L s j A i s 5 D X w M T 4 U = " &gt; A A A B 7 n i c b V D J S g N B E K 1 x j X G L e v T S G A R P Y U Y E P Q a 9 e I x g F k i G 0 N P p S Z r 0 M v Q i h C E f 4 c W D I l 7 9 H m / + j Z 1 k D p r 4 o O D x X h V V 9 Z K M M 2 P D 8 D t Y W 9 / Y 3 N o u 7 Z R 3 9 / Y P D i t H x y 2 j n C a 0 S R R X u p N g Q z m T t G m Z 5 b S T a Y p F w m k 7 G d / N / P Y T 1 Y Y p + W g n G Y 0 F H k q W M o K t l 9 o 9 J Z h 0 p l + p h r V w D r R K o o J U o U C j X / n q D R R x g k p L O D a m G 4 W Z j X O s L S O c T s s 9 Z 2 i G y R g P a d d T i Q U 1 c T 4 / d 4 r O v T J A q d K + p E V z 9 f d E j o U x E 5 H 4 T o H t y C x 7 M / E / r + t s e h P n T G b O U k k W i 1 L H k V V o 9 j s a M E 2 J 5 R N P M N H M 3 4 r I C G t M r E + o 7 E O I l l 9 e J a 3 L W h T W o o e r a v 2 2 i K M E p 3 A G F x D B N d T h H h r Q B A J j e I Z X e A u y 4 C V 4 D z 4 W r W t B M X M C f x B 8 / g C V C Y + 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x b J V L R y 5 z V 8 j t L s j A i s 5 D X w M T 4 U = " &gt; A A A B 7 n i c b V D J S g N B E K 1 x j X G L e v T S G A R P Y U Y E P Q a 9 e I x g F k i G 0 N P p S Z r 0 M v Q i h C E f 4 c W D I l 7 9 H m / + j Z 1 k D p r 4 o O D x X h V V 9 Z K M M 2 P D 8 D t Y W 9 / Y 3 N o u 7 Z R 3 9 / Y P D i t H x y 2 j n C a 0 S R R X u p N g Q z m T t G m Z 5 b S T a Y p F w m k 7 G d / N / P Y T 1 Y Y p + W g n G Y 0 F H k q W M o K t l 9 o 9 J Z h 0 p l + p h r V w D r R K o o J U o U C j X / n q D R R x g k p L O D a m G 4 W Z j X O s L S O c T s s 9 Z 2 i G y R g P a d d T i Q U 1 c T 4 / d 4 r O v T J A q d K + p E V z 9 f d E j o U x E 5 H 4 T o H t y C x 7 M / E / r + t s e h P n T G b O U k k W i 1 L H k V V o 9 j s a M E 2 J 5 R N P M N H M 3 4 r I C G t M r E + o 7 E O I l l 9 e J a 3 L W h T W o o e r a v 2 2 i K M E p 3 A G F x D B N d T h H h r Q B A J j e I Z X e A u y 4 C V 4 D z 4 W r W t B M X M C f x B 8 / g C V C Y + 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x b J V L R y 5 z V 8 j t L s j A i s 5 D X w M T 4 U = " &gt; A A A B 7 n i c b V D J S g N B E K 1 x j X G L e v T S G A R P Y U Y E P Q a 9 e I x g F k i G 0 N P p S Z r 0 M v Q i h C E f 4 c W D I l 7 9 H m / + j Z 1 k D p r 4 o O D x X h V V 9 Z K M M 2 P D 8 D t Y W 9 / Y 3 N o u 7 Z R 3 9 / Y P D i t H x y 2 j n C a 0 S R R X u p N g Q z m T t G m Z 5 b S T a Y p F w m k 7 G d / N / P Y T 1 Y Y p + W g n G Y 0 F H k q W M o K t l 9 o 9 J Z h 0 p l + p h r V w D r R K o o J U o U C j X / n q D R R x g k p L O D a m G 4 W Z j X O s L S O c T s s 9 Z 2 i G y R g P a d d T i Q U 1 c T 4 / d 4 r O v T J A q d K + p E V z 9 f d E j o U x E 5 H 4 T o H t y C x 7 M / E / r + t s e h P n T G b O U k k W i 1 L H k V V o 9 j s a M E 2 J 5 R N P M N H M 3 4 r I C G t M r E + o 7 E O I l l 9 e J a 3 L W h T W o o e r a v 2 2 i K M E p 3 A G F x D B N d T h H h r Q B A J j e I Z X e A u y 4 C V 4 D z 4 W r W t B M X M C f x B 8 / g C V C Y + 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x b J V L R y 5 z V 8 j t L s j A i s 5 D X w M T 4 U = " &gt; A A A B 7 n i c b V D J S g N B E K 1 x j X G L e v T S G A R P Y U Y E P Q a 9 e I x g F k i G 0 N P p S Z r 0 M v Q i h C E f 4 c W D I l 7 9 H m / + j Z 1 k D p r 4 o O D x X h V V 9 Z K M M 2 P D 8 D t Y W 9 / Y 3 N o u 7 Z R 3 9 / Y P D i t H x y 2 j n C a 0 S R R X u p N g Q z m T t G m Z 5 b S T a Y p F w m k 7 G d / N / P Y T 1 Y Y p + W g n G Y 0 F H k q W M o K t l 9 o 9 J Z h 0 p l + p h r V w D r R K o o J U o U C j X / n q D R R x g k p L O D a m G 4 W Z j X O s L S O c T s s 9 Z 2 i G y R g P a d d T i Q U 1 c T 4 / d 4 r O v T J A q d K + p E V z 9 f d E j o U x E 5 H 4 T o H t y C x 7 M / E / r + t s e h P n T G b O U k k W i 1 L H k V V</formula><formula xml:id="formula_1">v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a</formula><formula xml:id="formula_2">v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a</formula><p>We implement the warping mechanism via a deformable convolution <ref type="bibr" target="#b27">[28]</ref>, which takes 1) the offsets o (d) (p n ), and 2) the pose heatmap f B as its inputs, and then outputs a newly sampled pose heatmap g A,B . The subscript (A, B) is used to indicate that even though g A,B was resampled from tensor f B , the offsets for rewarping were computed using ψ A,B , which contains information from both frames. An illustration of our architecture is presented in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Loss Function. As in <ref type="bibr" target="#b26">[27]</ref>, we use a standard pose estimation loss function which computes a mean squared error between the predicted, and the ground-truth heatmaps. The ground-truth heatmap is generated by applying a 2D Gaussian around the location of each joint.</p><p>Pose Annotation Propagation. During training, we force our model to warp pose heatmap f B from an unlabeled frame B such that it would match the ground-truth pose heatmap in a labeled Frame A. Afterwards, we can reverse the application direction of our network. This then, allows us to propagate pose information from manually annotated frames to unlabeled frames (i.e. from a labeled Frame A to an unlabeled Frame B). Specifically, given a pose annotation in Frame A, we can generate its respective ground-truth heatmap y A by applying a 2D Gaussian around the location of each joint (identically to how it was done in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. Then, we can predict the offsets for warping ground-truth heatmap y A to an unlabeled Frame B, from the feature difference ψ B,A = f B − f A . Lastly, we use our deformable warping scheme to warp the ground-truth pose heatmap y A to Frame B, thus, propagating pose annotations to unlabeled frames in the same video. See <ref type="figure" target="#fig_1">Figure 1 (bottom)</ref> for a high-level illustration of this scheme.</p><p>Temporal Pose Aggregation at Inference Time. Instead of using our model to propagate pose annotations on training videos, we can also use our deformable warping mechanism to aggregate pose information from nearby frames during inference in order to improve the accuracy of pose detection. For every frame at time t, we want to aggregate information from all frames at times t + δ where δ ∈ {−3, −2, −1, 0, 1, 2, 3}. Such a pose aggregation procedure renders pose estimation more robust to occlusions, motion blur, and video defocus.  <ref type="figure">Figure 3</ref>: The results of a video pose propagation task by our PoseWarper and FlowNet2 <ref type="bibr" target="#b28">[29]</ref>. The first frame in each 3-frame sequence illustrates a labeled reference frame at time t. For simplicity, we show only the "right ankle" body joint for one person, denoted by a pink circle in each of the frames (please zoom in for a clearer view). The second frame depicts our propagated "right ankle" detection from the labeled frame in time t to the unlabeled frame in time t+1. The third frame shows the propagated detection in frame t+1 produced by the FlowNet2 baseline. In contrast to our method, FlowNet2 fails to propagate poses when there is large motion, blurriness or occlusions.</p><p>Consider a pair of frames, I t and I t+δ . In this case, we want to use pose information from frame I t+δ to improve pose detection in frame I t . To do this, we first feed both frames through our trained PoseWarper model, and obtain a spatially rewarped (resampled) pose heatmap g t,t+δ , which is aligned with respect to frame I t using the features from frame I t+δ . We can repeat this procedure for every δ value, and then aggregate pose information from multiple frames via a summation as δ g t,t+δ .</p><p>Implementation Details. Following the framework in <ref type="bibr" target="#b26">[27]</ref>, for training, we crop a 384 × 288 bounding box around each person and use it as input to our model. During training, we use ground truth person bounding boxes. We also employ random rotations, scaling, and horizontal flipping to augment the data. To learn the network, we use the Adam optimizer <ref type="bibr" target="#b54">[54]</ref> with a base learning rate of 10 −4 , which is reduced to 10 −5 and 10 −6 after 10, and 15 epochs, respectively. The training is performed using 4 Tesla M40 GPUs, and is terminated after 20 epochs. We initialize our model with a HRNet-W48 <ref type="bibr" target="#b26">[27]</ref> pretrained for a COCO keypoint estimation task. To train the deformable warping module, we select Frame B, with a random time-gap δ ∈ [−3, 3] relative to Frame A. To compute features relating the two frames, we use twenty 3 × 3 residual blocks each with 128 channels. Even though this seems like many convolutional layers, due to a small number of channels in each layer, this amounts to only 5.8M parameters (compared to 39M required to compute optical flow in <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref>: first, we detect the bounding boxes for each person in the image using the detector in <ref type="bibr" target="#b48">[48]</ref>, and then feed the cropped images to our pose estimation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our results on the PoseTrack <ref type="bibr" target="#b21">[22]</ref> dataset. We demonstrate the effectiveness of our approach on three applications: 1) video pose propagation, 2) training a network on annotations augmented with propagated pose pseudo-labels, 3) temporal pose aggregation during inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Pose Propagation</head><p>Quantitative Results. To verify that our model learns to capture pose correspondences, we apply it to the task of video pose propagation, i.e., propagating poses across time from a few labeled frames. Initially, we train our PoseWarper in a sparsely labeled video setting according to the procedure described above. In this setting, every 7 th frame of a training video is labeled, i.e. there are 6 unlabeled frames between each pair of manually labeled frames. Since each video contains on average 30 frames, we have approximately 5 annotated frames uniformly spaced out in each video. Our goal then, is to use our learned PoseWarper to propagate pose annotations from manually-labeled frames to all unlabeled frames in the same video. Specifically, for each labeled frame in a video, we propagate its pose information to the three preceding and three subsequent frames. We train our PoseWarper on sparsely labeled videos from the training set of PoseTrack2017 <ref type="bibr" target="#b21">[22]</ref> and then perform our evaluations on the validation set.</p><p>To evaluate the effectiveness of our approach, we compare our model to several relevant baselines. As our weakest baseline, we use our trained HRNet <ref type="bibr" target="#b26">[27]</ref> model that simply predicts pose for every single frame in a video. Furthermore, we also include a few propagation baselines based on warping annotations using optical flow. The first of these uses a standard Farneback optical flow <ref type="bibr" target="#b55">[55]</ref> to warp the manually-labeled pose in each labeled frame to its three preceding and three subsequent frames. We also include a more advanced optical flow propagation baseline that uses FlowNet2 optical flow <ref type="bibr" target="#b28">[29]</ref>. Finally, we evaluate our PoseWarper model.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we present our quantitative results for video pose propagation. The evaluation is done using an mAP metric as in <ref type="bibr" target="#b41">[42]</ref>. Our best model achieves a 88.7% mAP, while the optical flow propagation baseline using FlowNet2 <ref type="bibr" target="#b28">[29]</ref> yields an accuracy of 83.8% mAP. We also note that compared to the FlowNet2 <ref type="bibr" target="#b28">[29]</ref> propagation baseline, our PoseWarper warping mechanism is not only more accurate, but also significantly more compact (6M vs 39M parameters).</p><p>Ablation Studies on Dilated Convolution. In <ref type="table" target="#tab_1">Table 1</ref>, we also present the results investigating the effect of different levels of dilated convolutions in our PoseWarper architecture. We evaluate all these variants on the task of video pose propagation. First, we report that removing dilated convolution blocks from the original architecture reduces the accuracy from 88.7 mAP to 87.2 mAP. We also note that a network with a single dilated convolution (using a dilation rate of 3) yields 87.0 mAP. Adding a second dilated convolution level (using dilation rates of 3, 6) improves the accuracy to 88.0. Three dilation levels (with dilation rates of 3, 6, 12) yield a mAP of 88.4 and four levels (dilation rates of 3, 6, 12, 18) give a mAP of 88.6. A network with 5 dilated convolution levels yields 88.7 mAP. Adding more dilated convolutions does not improve the performance further. Additionally, we also experimented with two networks that use dilation rates of 1, 2, 3, 4, 5, and 4, 8, 16, 24, 32, and report that such models yield mAPs of 88.6 and 88.5, respectively, which are slightly lower.</p><p>Qualitative Comparison to FlowNet2. In <ref type="figure">Figure 3</ref>, we include an illustration of the motion encoded by PoseWarper, and compare it to the optical flow computed by FlowNet2 for the video pose propagation task. The first frame in each 3-frame sequence illustrates a labeled reference frame at time t. For a cleaner visualization, we show only the "right ankle" body joint for one person, which is marked with a pink circle in each of the frames. The second frame depicts our propagated "right  <ref type="figure">Figure 4</ref>: A figure illustrating the value of a) training a standard HRNet <ref type="bibr" target="#b26">[27]</ref> using our propagated pose pseudo labels (left), and b) our temporal pose aggregation scheme during inference. In both settings, we study pose detection performance as a function of 1) number of sparsely-labeled training videos (with 1 manually-labeled frame per video), and 2) number of labeled frames per video (with 50 sparsely-labeled videos in total). All baselines are based on retraining the standard HRNet <ref type="bibr" target="#b26">[27]</ref> model on the different training sets. The "GT (1x)" baseline is trained in a standard way on sparsely labeled video data. The "GT (7x)" baseline uses 7x more manually annotated data relative to the "GT (1x)" baseline. Our approach on the left subfigure ("GT (1x) + pGT (6x)"), augments the original sparsely labeled video data with our propagated pose pseudo labels (6 nearby frames for every manually-labeled frame). Lastly, in b) "GT (1x) + T-Agg" denotes the use of PoseWarper to fuse pose information from multiple neighboring frames during inference (training is done as in "GT (1x)" baseline). From the results, we observe that both application modalities of PoseWarper provide an effective way to achieve strong pose accuracy while reducing the number of manual annotations.</p><p>ankle" detection from the labeled frame in time t to the unlabeled frame in time t+1. The third frame shows the propagated detection in frame t+1 produced by the FlowNet2 baseline. These results suggest that FlowNet2 struggles to accurately warp poses if 1) there is large motion, 2) occlusions, or 3) blurriness. In contrast, our PoseWarper handles these cases robustly, which is also indicated by our results in <ref type="table" target="#tab_1">Table 1</ref> (i.e., 88.7 vs 83.8 mAP w.r.t. FlowNet2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Augmentation with PoseWarper</head><p>Here we consider the task of propagating poses on sparsely labeled training videos using PoseWarper, and then using them as pseudo-ground truth labels (in addition to the original manual labels) to train a standard HRNet-W48 <ref type="bibr" target="#b26">[27]</ref>. For this experiment, we study the pose detection accuracy as a function of two variables: 1) the total number of sparsely-labeled videos, and 2) the number of manually-annotated frames per video. We aim to study how much we can reduce manual labeling through our mechanism of pose propagation, while maintaining strong pose accuracy. Note, that we first train our PoseWarper on sparsely labeled videos from the training set of PoseTrack2017 <ref type="bibr" target="#b21">[22]</ref>. Then, we propagate pose annotations on the same set of training videos. Afterwards, we retrain the model on the joint training set comprised of sparse manual pose annotations and our propagated poses. Lastly, we evaluate this trained model on the validation set.</p><p>All results are based on a standard HRNet <ref type="bibr" target="#b26">[27]</ref> model trained on different forms of training data. "GT (1x)" refers to a model trained on sparsely labeled videos using ground-truth annotations only. "GT (7x)" baseline employs 7x more manually-annotated poses relative to "GT (1x)" (the annotations are part of the PoseTrack2017 training set). In comparison, our approach ("GT (1x) + pGT (6x)"), is trained on a joint training set consisting of sparse manual pose annotations (same as "GT (1x)" baseline) and our propagated poses (on the training set of PoseTrack2017), which we use as pseudo ground truth data (pGT). As before, for every labeled frame we propagate the ground truth pose to the 3 previous and the 3 subsequent frames, which allows us to expand the training set by 7 times.</p><p>Based on the results in the left subfigure of <ref type="figure">Figure 4</ref>, we can draw several conclusions. First, we note that when there are very few labeled videos (i.e., 5), all three baselines perform poorly (leftmost <ref type="figure">figure)</ref>. This indicates that in this setting there is not enough data to learn an effective pose detection model. Second, we observe that when the number of labeled videos is somewhat reasonable (e.g., 50 − 100), our approach significantly outperforms the "GT (1x)" baseline, and is only slightly worse relative to the "GT (7x)" baseline. As we increase the number of labeled videos, the gaps among the three methods shrink, suggesting that the model becomes saturated. As we vary the number of labeled frames per video (second leftmost <ref type="figure">figure)</ref>, we notice several interesting patterns. First, we note that for a small number of labeled frames per video (i.e., 1 − 2) our approach outperforms the "GT (1x)" baseline by a large margin. Second, we note that the performance of our approach and the "GT (7x)" becomes very similar as we add 2 or more labeled frames per video. These findings further strengthen our previous observation that PoseWarper allows us to reduce the annotation cost without a significant loss in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improved Pose Estimation via Temporal Pose Aggregation</head><p>In this subsection we assess the ability of PoseWarper to improve the accuracy of pose estimation at test time by using our deformable warping mechanism to aggregate pose information from nearby frames. We visualize our results in <ref type="figure">Figure 4</ref> b), where we evaluate the effectiveness of our temporal pose aggregation during inference for models trained a) with a different number of labeled videos (second rightmost <ref type="figure">figure)</ref>, and b) with a different number of manually-labeled frames per video (rightmost figure). We compare our approach ("GT (1x) + T-Agg.") to the same "GT (7x)" and "GT (1x)" baselines defined in the previous subsection. Note that our method in this case is trained exactly as "GT (1x)" baseline, the only difference comes from the inference procedure.</p><p>When the number of training videos and/or manually labeled frames is small, our approach provides a significant accuracy boost with respect to the "GT (1x)" baseline. However, once, we increase the number of labeled videos/frames, the gap between all three baselines shrinks, and the model becomes more saturated. Thus, our temporal pose aggregation scheme during inference is another effective way to maintain strong performance in a sparsely-labeled video setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to State-of-the-Art</head><p>We also test the effectiveness of our temporal pose aggregation scheme, when the model is trained on the full PoseTrack <ref type="bibr" target="#b21">[22]</ref> dataset. <ref type="table" target="#tab_2">Table 2</ref> compares our method to the most recent approaches in this area <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. These results suggest that although we designed our method to improve pose estimation when training videos are sparsely-labeled, our temporal pose aggregation scheme applied at inference is also useful for models trained on densely-labeled videos. Our PoseWarper obtains 81.2 mAP and 77.9 mAP on PoseTrack2017 validation and test sets respectively, and 79.7 mAP and 78.0 mAP on PoseTrack2018 validation and test sets respectively, thus outperforming prior single frame baselines <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame t</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame t+5</head><p>Farneback Flow Predicted Human Motion Offset Magnitudes Channel 99 (x,y) Channel 123 (x,y) <ref type="figure">Figure 5</ref>: In the first two columns, we show a pair of video frames used as input for our model. The 3 rd and 4 th columns depict 2 randomly selected offset channels visualized as a motion field. Different channels appear to capture the motion of different body parts. In the 5 th column, we display the offset magnitudes, which highlight salient human motion. Finally, the last two columns illustrate the standard Farneback flow, and the human motion predicted from our learned offsets. To predict human motion we train a linear classifier to regress the ground-truth (x, y) displacement of each joint from the offset maps. The color wheel, at the bottom right corner encodes motion direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Interpreting Learned Offsets</head><p>Understanding what information is encoded in our learned offsets is nearly as difficult as analyzing any other CNN features <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b60">60]</ref>. The main challenge comes from the high dimensionality of offsets: we are predicting c × k h × k w (x, y) displacements for every pixel for each of the five dilation rates d, where c is the number of channels, and k h , k w are the convolutional kernel height and width respectively.</p><p>In columns 3, 4 of <ref type="figure">Figure 5</ref>, we visualize two randomly-selected offset channels as a motion field. Based on this figure, it appears that different offset maps encode different motions rather than all predicting the same solution (say, the optical flow between the two frames). This makes sense, as the network may decide to ignore motions of uninformative regions, and instead capture the motion of different human body parts in different offset maps (say, a hand as opposed to the head). We also note that the magnitudes of our learned offsets encode salient human motion (see Column 5 of <ref type="figure">Figure 5</ref>).</p><p>Lastly, to verify that our learned offsets encode human motion, for each point p n denoting a body joint, we extract our predicted offsets and train a linear classifier to regress the ground truth (x, y) motion displacement of this body joint. In Column 7 of <ref type="figure">Figure 5</ref>, we visualize our predicted motion outputs for every pixel. We show Farneback's optical flow in Column 6. Note that in regions containing people, our predicted human motion matches Farneback optical flow. Furthermore, we point out that compared to the standard Farneback optical flow, our motion fields look less noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we introduced PoseWarper, a novel architecture for pose detection in sparsely labeled videos. Our PoseWarper can be effectively used for multiple applications, including video pose propagation, and temporal pose aggregation. In these settings, we demonstrated that our approach reduces the need for densely labeled video data, while producing strong pose detection performance. Furthermore, our state-of-the-art results on PoseTrack2017 and PoseTrack2018 datasets demonstrate that our PoseWarper is useful even when the training videos are densely-labeled. Our future work involves improving our model ability to propagate labels and aggregate temporal information when the input frames are far away from each other. We are also interested in exploring self-supervised learning objectives for our task, which may further reduce the need of pose annotations in video. We will release our source code and our trained models upon publication of the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 .</head><label>2</label><figDesc>Video Pose Propagation (Labeled Unlabeled) Unlabeled Frame B (time t+ ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 51 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l at e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; Frame A (time t+ ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A high level overview of our approach for using sparsely labeled videos for pose detection.Faces in the figure are artificially masked for privacy reasons. In each training video, pose annotations are available only every k frames. During training, our system considers a pair of frames-a labeled Frame A, and an unlabeled Frame B, and aims to detect pose in Frame A, using the features from Frame B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>o 9 j s a M E 2 J 5 R N P M N H M 3 4 r I C G t M r E + o 7 E O I l l 9 e J a 3 L W h T W o o e r a v 2 2 i K M E p 3 A G F x D B N d T h H h r Q B A J j e I Z X e A u y 4 C V 4 D z 4 W r W t B M X M C f x B 8 / g C V C Y + 3 &lt; / l a t e x i t &gt; Unlabeled Frame B (time t+ ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A U / v q + D A g M m 1 x 9 R h U c 2 0 a A 7 B J w Y = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a U D a b T b t 2 k w 2 7 E 6 G E / g c v H h T x 6 v / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K U w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j c o 0 4 y 2 m p N L d g B o u R c J b K F D y b q o 5 j Q P J O 8 H 4 d u Z 3 n r g 2 Q i U P O E m 5 H 9 N h I i L B K F q p 3 Q + 5 R D q o 1 t y 6 O w d Z J V 5 B a l C g O a h + 9 U P F s p g n y C Q 1 p u e 5 K f o 5 1 S i Y 5 N N K P z M 8 p W x M h 7 x n a U J j b v x 8 f u 2 U n F k l J J H S t h I k c / X 3 R E 5 j Y y Z x Y D t j i i O z 7 M 3 E / 7 x e h t G 1 n 4 s k z Z A n b L E o y i R B R W a v k 1 B o z l B O L K F M C 3 s r Y S O q K U M b U M W G 4 C 2 / v E r a F 3 X P r X v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 o Q U M H u E Z X u H N U c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A k Y y P H A = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " N 4 z t u 7 r A 6 G c W P A 6 n A D Z 8 t M O c v q E = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u z q V m e 2 V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 3 N s o Q r Z J J a 2 w n 8 F M M J N S i Y 5 N N S N 7 M 8 p W x E B 7 z j q K I J t + F k f u 2 U n D m l T 2 J t X C k k c / X 3 x I Q m 1 o 6 T y H U m F I d 2 2 Z u J / 3 m d D O P r c C J U m i F X b L E o z i R B T W a v k 7 4 w n K E c O 0 K Z E e 5 W w o b U U I Y u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M Y P M I z v M K b p 7 0 X 7 9 3 7 W L Q W v H z m G P 7 A + / w B z / 6 P R Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 4 z t u 7 r A 6 G c W P A 6 n A D Z 8 t M O c v q E = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u z q V m e 2 V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 3 N s o Q r Z J J a 2 w n 8 F M M J N S i Y 5 N N S N 7 M 8 p W x E B 7 z j q K I J t + F k f u 2 U n D m l T 2 J t X C k k c / X 3 x I Q m 1 o 6 T y H U m F I d 2 2 Z u J / 3 m d D O P r c C J U m i F X b L E o z i R B T W a v k 7 4 w n K E c O 0 K Z E e 5 W w o b U U I Y u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M Y P M I z v M K b p 7 0 X 7 9 3 7 W L Q W v H z m G P 7 A + / w B z / 6 P R Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 4 z t u 7 r A 6 G c W P A 6 n A D Z 8 t M O c v q E = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u z q V m e 2 V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 3 N s o Q r Z J J a 2 w n 8 F M M J N S i Y 5 N N S N 7 M 8 p W x E B 7 z j q K I J t + F k f u 2 U n D m l T 2 J t X C k k c / X 3 x I Q m 1 o 6 T y H U m F I d 2 2 Z u J / 3 m d D O P r c C J U m i F X b L E o z i R B T W a v k 7 4 w n K E c O 0 K Z E e 5 W w o b U U I Y u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M Y P M I z v M K b p 7 0 X 7 9 3 7 W L Q W v H z m G P 7 A + / w B z / 6 P R Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N 4 z t u 7 r A 6 G c W P A 6 n A D Z 8 t M O c v q E = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o M e g F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u z q V m e 2 V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 3 N s o Q r Z J J a 2 w n 8 F M M J N S i Y 5 N N S N 7 M 8 p W x E B 7 z j q K I J t + F k f u 2 U n D m l T 2 J t X C k k c / X 3 x I Q m 1 o 6 T y H U m F I d 2 2 Z u J / 3 m d D O P r c C J U m i F X b L E o z i R B T W a v k 7 4 w n K E c O 0 K Z E e 5 W w o b U U I Y u o J I L I V h + e Z U 0 L 6 q B X w 3 u L y u 1 m z y O I p z A K Z x D A F d Q g z u o Q w M Y P M I z v M K b p 7 0 X 7 9 3 7 W L Q W v H z m G P 7 A + / w B z / 6 P R Q = = &lt; / l a t e x i t &gt; An illustration of our PoseWarper architecture. Given a labeled Frame A and an unlabeled Frame B, which are separated by δ steps in time, our goal is to detect pose in a labeled Frame A using the features from an unlabeled Frame B.First, we predict pose heatmaps for both frames. Then, we compute the difference between pose heatmaps in Frame A and Frame B and feed it through a stack of 3 × 3 residual blocks. Afterwards, we attach five 3 × 3 convolutional layers with dilation rates d ∈ {3, 6, 12, 18, 24} and predict five sets of offsets o (d) (p n ) for each pixel location p n . The predicted offsets are used to rewarp pose heatmap B. All five rewarped heatmaps are then summed and the resulting tensor is used to predict pose in Frame A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). To compute the offsets o (d) , we use five 3 × 3 convolutional layers, each using a different dilation rate (d = 3, 6, 12, 18, 24). To resample the pose heatmap f B , we employ five 3 × 3 deformable convolutional layers, each applied to one of the five predicted offset maps o(d)  . The five deformable convolution layers too employ different dilation rates of 3, 6, 12, 18, 24. During testing, we follow the same two-stage framework used in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The results of video pose propagation on the PoseTrack2017<ref type="bibr" target="#b21">[22]</ref> validation set (measured in mAP). We propagate pose information across the entire video from the manual annotations provided in few frames. To study the effect of different levels of dilated convolutions in our PoseWarper architecture, we also include several ablation baselines (see the bottom half of the table).</figDesc><table><row><cell>Method</cell><cell cols="3">Head Shoulder Elbow Wrist Hip Knee Ankle Mean</cell></row><row><cell>Pseudo-labeling w/ HRNet [27]</cell><cell>79.1</cell><cell>86.5</cell><cell>81.4 74.7 81.4 79.4 72.3 79.3</cell></row><row><cell cols="2">Optical Flow Propagation (Farneback [55]) 76.5</cell><cell>82.3</cell><cell>74.3 69.2 80.8 74.8 70.1 75.5</cell></row><row><cell cols="2">Optical Flow Propagation (FlowNet2 [29]) 82.7</cell><cell>91.0</cell><cell>83.8 78.4 89.7 83.6 78.1 83.8</cell></row><row><cell>PoseWarper (no dilated convs)</cell><cell>86.1</cell><cell>91.7</cell><cell>88.0 83.5 90.2 87.3 84.6 87.2</cell></row><row><cell>PoseWarper (1 dilated conv)</cell><cell>85.0</cell><cell>91.6</cell><cell>88.0 83.7 89.6 87.3 84.7 87.0</cell></row><row><cell>PoseWarper (2 dilated convs)</cell><cell>85.8</cell><cell>92.4</cell><cell>88.8 84.9 91.0 88.4 86.0 88.0</cell></row><row><cell>PoseWarper (3 dilated convs)</cell><cell>86.1</cell><cell>92.6</cell><cell>89.2 85.5 91.3 88.8 86.3 88.4</cell></row><row><cell>PoseWarper (4 dilated convs)</cell><cell>86.3</cell><cell>92.6</cell><cell>89.5 85.9 91.9 88.8 86.4 88.6</cell></row><row><cell>PoseWarper (5 dilated convs)</cell><cell>86.0</cell><cell>92.7</cell><cell>89.5 86.0 91.5 89.1 86.6 88.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Multi-person pose estimation results on the validation and test sets of PoseTrack2017 and PoseTrack2018 datasets. Even though our model is designed to improve pose detection in scenarios involving sparsely-labeled videos, here we show that our temporal pose aggregation scheme during inference is also useful for models trained on densely labeled videos. We improve upon the state-of-the-art single-frame baselines<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b56">56]</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Mean</cell></row><row><cell></cell><cell cols="2">Girdhar et al. [48] 72.8</cell><cell>75.6</cell><cell cols="6">65.3 54.3 63.5 60.9 51.8 64.1</cell></row><row><cell></cell><cell>Xiu et al. [57]</cell><cell>66.7</cell><cell>73.3</cell><cell cols="6">68.3 61.1 67.5 67.0 61.3 66.5</cell></row><row><cell>PoseTrack17 Val Set</cell><cell>Bin et al [23] HRNet [27]</cell><cell>81.7 82.1</cell><cell>83.4 83.6</cell><cell cols="6">80.0 72.4 75.3 74.8 67.1 76.7 80.4 73.3 75.5 75.3 68.5 77.3</cell></row><row><cell></cell><cell>MDPN [56]</cell><cell>85.2</cell><cell>88.5</cell><cell cols="6">83.9 77.5 79.0 77.0 71.4 80.7</cell></row><row><cell></cell><cell>PoseWarper</cell><cell>81.4</cell><cell>88.3</cell><cell cols="6">83.9 78.0 82.4 80.5 73.6 81.2</cell></row><row><cell></cell><cell>Girdhar et al. [48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.6</cell></row><row><cell></cell><cell>Xiu et al. [57]</cell><cell>64.9</cell><cell>67.5</cell><cell cols="6">65.0 59.0 62.5 62.8 57.9 63.0</cell></row><row><cell>PoseTrack17 Test Set</cell><cell>Bin et al [23]</cell><cell>80.1</cell><cell>80.2</cell><cell cols="6">76.9 71.5 72.5 72.4 65.7 74.6</cell></row><row><cell></cell><cell>HRNet [27]</cell><cell>80.1</cell><cell>80.2</cell><cell cols="6">76.9 72.0 73.4 72.5 67.0 74.9</cell></row><row><cell></cell><cell>PoseWarper</cell><cell>79.5</cell><cell>84.3</cell><cell cols="6">80.1 75.8 77.6 76.8 70.8 77.9</cell></row><row><cell></cell><cell>AlphaPose [58]</cell><cell>63.9</cell><cell>78.7</cell><cell cols="6">77.4 71.0 73.7 73.0 69.7 71.9</cell></row><row><cell>PoseTrack18 Val Set</cell><cell>MDPN [56]</cell><cell>75.4</cell><cell>81.2</cell><cell cols="6">79.0 74.1 72.4 73.0 69.9 75.0</cell></row><row><cell></cell><cell>PoseWarper</cell><cell>79.9</cell><cell>86.3</cell><cell cols="6">82.4 77.5 79.8 78.8 73.2 79.7</cell></row><row><cell></cell><cell cols="2">AlphaPose++ [56, 58] -</cell><cell>-</cell><cell>-</cell><cell cols="2">66.2 -</cell><cell>-</cell><cell cols="2">65.0 67.6</cell></row><row><cell>PoseTrack18 Test Set</cell><cell>MDPN [56]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">74.5 -</cell><cell>-</cell><cell cols="2">69.0 76.4</cell></row><row><cell></cell><cell>PoseWarper</cell><cell>78.9</cell><cell>84.4</cell><cell cols="6">80.9 76.8 75.6 77.5 71.8 78.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Zürich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1612.01925</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond trees: common-factor models for 2d human pose recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="470" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measure locally, reason globally: Occlusion-sensitive articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2041" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multiple tree models for occlusion and spatial constraints in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<editor>David Forsyth, Philip Torr, and Andrew Zisserman</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="710" to="724" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schieke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Detect-and-Track: Efficient Pose Estimation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4420" to="4229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Personalizing human video pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Poseflow: A deep motion representation for understanding human behaviors in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Scandinavian Conference on Image Analysis, SCIA&apos;03</title>
		<meeting>the 13th Scandinavian Conference on Image Analysis, SCIA&apos;03<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-domain pose network for multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfu</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Mai</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<idno>abs/1506.06579</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
