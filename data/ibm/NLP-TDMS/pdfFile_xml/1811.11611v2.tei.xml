<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Appearance Model for End-to-end Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
							<email>joakim.johnander@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<settlement>Zenuity</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<email>martin.danelljan@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
							<email>emil.brissman@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Saab</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<email>fahad.khan@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
							<email>michael.felsberg@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Generative Appearance Model for End-to-end Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the fundamental challenges in video object segmentation is to find an effective representation of the target and background appearance. The best performing approaches resort to extensive fine-tuning of a convolutional neural network for this purpose. Besides being prohibitively expensive, this strategy cannot be truly trained end-to-end since the online fine-tuning procedure is not integrated into the offline training of the network.</p><p>To address these issues, we propose a network architecture that learns a powerful representation of the target and background appearance in a single forward pass. The introduced appearance module learns a probabilistic generative model of target and background feature distributions. Given a new image, it predicts the posterior class probabilities, providing a highly discriminative cue, which is processed in later network modules. Both the learning and prediction stages of our appearance module are fully differentiable, enabling true end-to-end training of the entire segmentation pipeline. Comprehensive experiments demonstrate the effectiveness of the proposed approach on three video object segmentation benchmarks. We close the gap to approaches based on online fine-tuning on DAVIS17, while operating at 15 FPS on a single GPU. Furthermore, our method outperforms all published approaches on the largescale YouTube-VOS dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) is the task of tracking and segmenting one or multiple target objects in a video sequence. In this work, we consider the semi-supervised setting, where the ground-truth segmentation is only given in the first frame. The task is generic, i.e., the targets are Image RGMP <ref type="bibr" target="#b30">[31]</ref> Ours <ref type="bibr">Figure 1</ref>. Comparison between our proposed approach and the recently proposed RGMP <ref type="bibr" target="#b30">[31]</ref>. In RGMP, the input features are concatenated with the initial mask and feature map. In contrast, we explicitly capture the target and background appearance, including distractor objects, by generative modelling. While RGMP severely struggles, the proposed approach successfully identifies and accurately segments all annotated targets. As in RGMP, we do not invoke computationally intensive fine-tuning in the first frame, but instead aim to learn the appearance model in a single forward pass. The figure is best viewed in colour.</p><p>arbitrary and no further assumptions regarding the object classes are made. The VOS problem is challenging from several aspects. The target may undergo significant appearance changes and may be subject to fast motion or occlusion. Moreover, the scene may contain distractor objects that are visually or semantically similar to the target. To tackle the aforementioned challenges, the standard strategy is to invoke extensive iterative optimization in the first frame <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1]</ref>, given the initial image-mask pair.</p><p>However, this strategy comes at an immense computational cost, rendering real-time operation infeasible. Furthermore, these methods do not train the segmentation pipeline endto-end, since the online fine-tuning step is excluded from the offline learning stage. In response to the these issues, we explore the problem of finding a feedforward network architecture for video object segmentation that completely avoids online optimization.</p><p>Recent works have posed video object segmentation as a feedforward mask-refinement process <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31]</ref>, where the previous mask prediction is adapted to fit the target in the current frame using a convolutional neural network. However, since no explicit modelling of the target appearance is performed, such approaches inherently fail if the target is occluded or out of view. This problem has been approached by incorporating simple appearance models based on e.g., concatenation of the feature map from the first frame <ref type="bibr" target="#b30">[31]</ref>, or utilization of a set of foreground and background feature vectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>. However, these appearance models are either too simplistic, achieving unsatisfactory discriminative power, or cannot be fully trained end-to-end due to the reliance of non-differentiable components.</p><p>In this work, we propose a novel neural network architecture for video object segmentation that integrates a powerful appearance model of the scene. In contrast to previous methods, our network internally learns a generative probabilistic model of the foreground and background feature distributions. For this purpose, we employ a class-conditional mixture of Gaussians, which is inferred through a single forward pass. Our appearance model outputs the posterior class probabilities, thus providing a powerful cue containing discriminative information about the image content. This completely removes the need for online fine-tuning, as target-specific appearance information is captured in a single forward pass. We demonstrate our approach in <ref type="figure">fig. 1</ref>.</p><p>The proposed generative appearance model is seamlessly integrated as a module in our video object segmentation network. Our complete architecture is composed of a backbone feature extractor, the generative appearance module, a mask propagation branch, a fusion component, and a final upsampling and prediction module. For our generative appearance module, both the model inference and the prediction stages are fully differentiable. This ensures that the entire segmentation pipeline can be trained end-to-end, which is not the case for methods invoking online finetuning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref> or K-Nearest-Neighbor prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>. Finally, our appearance module is lightweight, enabling efficient online inference.</p><p>We perform extensive experiments on 3 datasets, including the recent large-scale YouTubeVOS dataset <ref type="bibr" target="#b31">[32]</ref>. We obtain a final score of 66.0% on YouTube-VOS, outperforming all previously published methods. Further, our approach achieves the best mean IoU of 67.2% on Davis17 among all causal video object segmentation methods. We perform a comprehensive analysis of our method in terms of an ablation study. Our analysis clearly underlines the effectiveness of the proposed generative appearance module and the importance of full end-to-end learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this work we address the problem of video object segmentation where an initial segmentation mask is provided, defining the target in the first frame. In recent years interest in this problem has surged and a wide variety of approaches have been proposed. Caelles et al. <ref type="bibr" target="#b1">[2]</ref> proposed to use a convolutional neural network pre-trained for the semantic segmentation task, and fine-tune this in the first frame to segment out the foreground and background. This approach was extended in a number of works: continuous training during the sequence <ref type="bibr" target="#b29">[30]</ref>; adding instance-level semantic information <ref type="bibr" target="#b20">[21]</ref>; incorporating motion information via optical flow <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6]</ref>; performing temporal propagation via a Markov random field <ref type="bibr" target="#b0">[1]</ref>; location-specific embeddings <ref type="bibr" target="#b7">[8]</ref>; sophistic data augmentation <ref type="bibr" target="#b15">[16]</ref>; or a combination of these <ref type="bibr" target="#b19">[20]</ref>. While these approaches obtain satisfactory results in many scenarios, they have one critical drawback in common: they learn the target appearance in the initial frame via extensive training of deep neural networks with stochastic gradient descent. This leads to a significant time-delay before these methods can start tracking, and an average computation time that renders real-time processing infeasible.</p><p>Despite reduced accuracy, several approaches avoid invoking expensive fine-tuning procedures in the first frame. Some methods rely on optical flow coupled with refinement <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>. Li et al. proposed DyeNet <ref type="bibr" target="#b17">[18]</ref>, which combines optical flow with an object proposal network, interleaving bidirectional mask-propagation and target re-identification. DyeNet provides outstanding performance, but it is not causal and relies on future video frames to make predictions. Jampani et al. <ref type="bibr" target="#b13">[14]</ref> explicitly try to avoid optical flow and propose an approach based on bilateral filters. Cheng et al. <ref type="bibr" target="#b4">[5]</ref> track different parts of the target with visual object tracking techniques, and refine the final solution with a convolutional neural network. Xu et al. <ref type="bibr" target="#b31">[32]</ref> instead train a convolutional LSTM [11] to track and segment the target.</p><p>More closely related to our work, Perazzi et al. <ref type="bibr" target="#b22">[23]</ref> pose video object segmentation as a mask refinement problem. Based on an input image, the mask predicted from the previous frame is refined with a neural network. The network is recurrent in time, with a particularly deep recurrent connection, an entire VGG16 <ref type="bibr" target="#b27">[28]</ref>. In the work by Yang et al. <ref type="bibr" target="#b33">[34]</ref>, the mask was reduced to a rough spatial prior on the target location, and this together with a channel-wise attention mechanism provided improved performance. Wug et al. <ref type="bibr" target="#b30">[31]</ref> extend <ref type="bibr" target="#b22">[23]</ref> and concatenate the initial frame feature map and mask with the current feature map and previous mask, and train a standard convolutional neural network to match and segment in a fully recurrent fashion. Also more explicit matching mechanisms have been proposed, where the input features are matched with a set of features with known class membership <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref> using K-Nearest-Neighbour (KNN). While these methods model the target appearance, the non-parametric nature of KNN requires the entire training set to be stored. Additionally, the process of finding the K nearest neighbours is not differentiable. In contrast to existing work, our approach learns a compact appearance model of the scene in a single differentiable forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The aim of this work is to develop a network architecture for video object segmentation with the capability of learning accurate models of the target and background appearance through a single forward pass. That is, the network must learn in a one-shot manner to discriminate between target and background pixels, without invoking stochastic gradient descent. We tackle this problem by integrating a generative model of the foreground and background appearance. This model directly aids the segmentation process by providing discriminative posterior class probabilities. The learning and inference is computationally efficient and endto-end differentiable, enabling a seamless integration of our generative component into a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Our approach is divided into five components that jointly address the video object segmentation task and are trained jointly end-to-end. The model is illustrated in <ref type="figure">fig. 2</ref>. Given an input image, features are first extracted with a backbone network. These are then passed to the appearance-and mask-propagation modules. The outputs of these two modules are combined in the fusion module, comprising two convolutional layers and outputting a coarse mask encoding. The encoding is handed to a predictor that generates a coarse segmentation mask. This prediction is used to update the appearance module and further used as input to the mask-propagation layer in the next frame to provide a rough spatial prior. The mask encoding output by the fusion component is also passed through an upsampling module, in which the coarse encoding is combined with successively more shallow features in order to produce a final refined segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Appearance Module</head><p>The task of our appearance module is to learn a generative model of the video content in a deep feature space. Our generative model is conditioned on the class variable, indicating target or background. Given a new frame, the appearance module returns the posterior class probabilities at each image location. This output forms an extremely strong cue for foreground/background discrimination, as the proposed module explicitly models their respective appearance in a probabilistic manner. Model learning: Formally, let the set of features extracted from the image be denoted as {x p } p . The feature x p at each spatial location p is a D-dimensional vector of real numbers. We model these observed feature vectors as i.i.d. samples drawn from the underlying distribution</p><formula xml:id="formula_0">p(x p ) = K k=1 p(z p = k)p(x p |z p = k) .<label>(1)</label></formula><p>Each class-conditional density is a multi-variate Gaussian with mean µ k and covariance matrix Σ k ,</p><formula xml:id="formula_1">p(x p |z p = k) = N (x p |µ k , Σ k ) .<label>(2)</label></formula><p>The discrete random variable z p in (1) assigns the observation x p to a specific component z p = k. We use a uniform prior p(z p = k) = 1/K for this variable, where K is the number of components. In our model, we use separate components z p = k to describe the foreground and background respectively. Each z p = k is thus strictly assigned to either foreground or background, making our model conditioned on the class. As further detailed below, we use two Gaussian components for each class.</p><p>In the first frame, our generative mixture model is inferred from the extracted features and the initial target mask. In subsequent frames, we update the model using the network predictions as soft class labels. In general, to update the mixture model in a frame i we require a set of features x i p together with a set of soft component assignment variables α i pk ∈ [0, 1]. These variables can be thought of as soft labels, describing the level of assignment of the vector x i p to component k. For example, in the first frame i = 0, the feature vectors would be strictly assigned to either foreground or background α 0 pk ∈ {0, 1} using the initial target mask. Given the variables α i pk , we compute the model parameter updates as,</p><formula xml:id="formula_2">µ i k = p α i pk x i p p α i pk ,<label>(3a)</label></formula><formula xml:id="formula_3">Σ i k = p α i pk diag{(x i p −μ i k ) 2 + r k } p α i pk .</formula><p>(3b)</p><p>For efficiency, we limit the covariance matrix to be diagonal, where diag(v) is a diagonal matrix with entries corresponding to the input vector v. To avoid singularities, the covariance is regularized with a vector r k , which is a trainable parameter in our network. In the first frame, the mixture model parameters in <ref type="formula" target="#formula_1">(2)</ref> are directly achieved from <ref type="formula">(3)</ref>, i.e. µ 0 k =μ 0 k and Σ 0 k =Σ 0 k . In subsequent frames, these <ref type="figure">Figure 2</ref>. Full architecture of the proposed approach, illustrating both model initialization and frame processing. Model Initialization: A feature map is extracted from the initial frame, which is then fed together with the mask to the mask propagation module. This pair is furthermore used to initialize the appearance model. Frame processing: A feature map is extracted from the current frame and fed to both the appearance and mask-propagation modules whose outputs are combined, generating a coarse mask-encoding. Our upsampling module then refines the mask-encoding by also considering low-level information contained in the shallow features. The predictor then generates a final segmentation, based on this encoding. Moreover, the mask-encoding and appearance model parameters are fed back via a recurrent connection. During training, we use two cross-entropy losses applied to the coarse and fine segmentations, respectively.</p><p>parameters are updated with new information (3) using a learning rate λ,</p><formula xml:id="formula_4">µ i k = (1 − λ)µ i−1 k + λμ i k , Σ i k = (1 − λ)Σ i−1 k + λΣ i k .<label>(4)</label></formula><p>Assignment variables: Next, we describe the computation of the assignment variables α i pk . Note that <ref type="formula">(3)</ref> resembles the M-step in the Expectation Maximization (EM) algorithm for a mixture of Gaussians. In EM, the variables z i p are treated as latent and <ref type="formula">(3)</ref> is derived by maximizing the expected complete-data log-likelihood. In that case the assignment variables are computed in the E-step as</p><formula xml:id="formula_5">α i pk = p(z i p = k|x i p , θ i−1 ), where θ i−1 = {µ i−1 k , Σ i−1 k } k are</formula><p>the previous estimates of the parameters. However, the setting is different in our case. The discrete assignment variables z i p are fully observed in the first frame. Moreover, in the subsequent frames, the network refines the posteriors p(z i p = k|x i p , θ i−1 ), providing even better assignment estimates. We therefore exploit these factors in the estimation of the assignment variables α i kp . Our model consists of one base component for background k = 0 and foreground k = 1, respectively. Given the ground truth binary target mask in the first frame y p , where y p = 1 for foreground and y p = 0 otherwise, we set α 0 p0 = 1 − y p and α 0 p1 = y p . That is, the feature vectors x i p are strictly assigned to the foreground and background base components according to the initial mask. In subsequent frames, where the ground-truth is not available, we use the final prediction of our segmentation network according to</p><formula xml:id="formula_6">α i p0 = 1 −ỹ p (I i , θ i−1 , Φ) α i p1 =ỹ p (I i , θ i−1 , Φ) .<label>(5)</label></formula><formula xml:id="formula_7">Here,ỹ p (I i , θ i−1 , Φ) ∈ [0, 1]</formula><p>is the probability of the target class, given the input image I i , neural network parameters Φ, and current mixture model parameter estimates θ i−1 .</p><p>A drawback of using a single Gaussian component per class is that only uni-modal distributions can be accurately represented. However, the background appearance is typically multi-modal, especially in the presence of background objects that are similar to the target, often termed distractors. To obtain satisfactory discrimination between foreground and background, it is therefore critical to capture the feature distribution of such distractors. We therefore add Gaussian components in our model that are dedicated to the task of modeling hard examples. These components are explicitly learned to counter the errors of the two base components. Ideally, we would wish the base components alone to correctly predict the assignment variables, i.e. p(z i p = k|x i p , µ i k , Σ i k ) = α i pk , k = 0, 1. The additional components are trained on data where this does not hold by considering incorrectly classified background (k = 2) and foreground (k = 3) respectively. Their corresponding assignment variables are computed as,</p><formula xml:id="formula_8">α i p2 = ReLU(p(z i p = 0|x i p , µ i 0 , Σ i 0 ) − α i p0 ) α i p3 = ReLU(p(z i p = 1|x i p , µ i 1 , Σ i 1 ) − α i p1 ) .<label>(6)</label></formula><p>Here, the posteriors p(z i p = k|x i p , µ i k , Σ i k ) are evaluated using only the base components. Given (6), we finally update the parameters of the latter components k = 2, 3 using (3) and (4). Module output: Given the mixture model parameters computed in the previous frame, θ i−1 , our model can predict the component posteriors,</p><formula xml:id="formula_9">p(z i p = k|x i p , θ i−1 ) = p(z i p = k)p(x i p |z i p = k) k p(z i p = k)p(x i p |z i p = k) .<label>(7)</label></formula><p>Note that each component k belongs to either foreground or background, and that the outputs <ref type="formula" target="#formula_9">(7)</ref> thus provide a discriminative mask encoding. In practice, we found it beneficial to feed the log-probabilities log(p(z i p = k)p(x i p |z i p = k)) into the conv layers in the fusion module. By canceling out constant factors, the outputs are calculated as,</p><formula xml:id="formula_10">s i pk = − ln |Σ i−1 k | + (x i p − µ i−1 k ) T (Σ i−1 k ) −1 (x i p − µ i−1 k ) 2 . (8)</formula><p>The component posteriors <ref type="formula" target="#formula_9">(7)</ref> can be reconstructed from s i pk by a simple soft-max operation. The output <ref type="formula">(8)</ref> should therefore be interpreted as component scores, encoding foreground and background assignment. The entire appearance modelling procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Segmentation Architecture</head><p>As our backbone feature extractor, we use ResNet101 <ref type="bibr" target="#b9">[10]</ref> with dilated convolutions <ref type="bibr" target="#b2">[3]</ref> to reduce the stride of the deepest layer from 32 to 16. It is pretrained on ImageNet and all layers up to the last block, layer4, are frozen. The mask-propagation module is based on the concept proposed in <ref type="bibr" target="#b30">[31]</ref>. The module constructs a mask encoding based on the mask predicted in the previous frame, a feature map predicted in the current frame, and a feature map extracted from the initial frame together with the given ground-truth mask. The entire module consists of three convolutional layers, where the middle layer is a dilation pyramid <ref type="bibr" target="#b2">[3]</ref>.</p><p>The outputs of the mask propagation and appearance modules are concatenated and fed into the fusion module, comprising two convolutional layers. The output is then sent as input to the upsampling module from which a predicted soft segmentationŷ p is obtained. The output of the fusion module is also fed into a predictor that produces a coarse segmentationỹ p , which is input to the mask propagation step and appearance module (using (5)) in the next timestep. By separating the feature extractor and upsampling path from the recurrent module we get a shorter path between variables of different time steps. We experienced the coarse mask to be a sufficient representation of the previous target segmentation. As a special case, during sequences with multiple objects, we run our approach once per object, and combine the resulting soft segmentations with softmax-aggregation <ref type="bibr" target="#b30">[31]</ref>. The aggregated soft segmentations then replaces the coarse segmentationỹ p in the recurrent connection.</p><p>The output of the fusion module provides coarse maskencoding that is used to locate and segment the target. There have been considerable efforts in semantic segmentation and instance segmentation litterature to refine final segmentations. We adopt an upsampling path similar to <ref type="bibr" target="#b24">[25]</ref>, where the coarse representation is successively combined with successively shallower features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1:</head><p>The appearance module inference and update. Inference: Based on the appearance model parameters, µ i k , Σ i k , and the input feature map x i p , a soft segmentation is constructed for the background, foreground, and the two residual components. Update: The appearance model parameters are updated based on the coarse segmentationỹ i p .</p><formula xml:id="formula_11">1 Inference(x i p , µ i k , Σ i k ): 2 for k = 0, 1, 2, 3: compute s i pk from (8) 3 return s i pk 4 Update(x i p ,ỹ i p , µ i k , Σ i k ): 5</formula><p>for k = 0, 1: compute α i pk from (5) <ref type="bibr" target="#b5">6</ref> for k = 0, 1: computeμ i k ,Σ i k based on (3) <ref type="bibr" target="#b6">7</ref> for k = 0, 1: compute s i pk based on (8) <ref type="bibr" target="#b7">8</ref> for k = 0, 1: compute</p><formula xml:id="formula_12">p(z i p = k|x i p , µ i 0 , Σ i 0 ) = Softmax(s i p0 , s i p1 ) 12 return µ i k and Σ i k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Training</head><p>We train the proposed neural network end-to-end in a recurrent fashion. Based on a video and a single ground-truth segmentation, the network predicts segmentation masks for each frame in the video. We train on three datasets: DAVIS2017 <ref type="bibr" target="#b25">[26]</ref>: The DAVIS2017 training set comprises 60 videos containing one or several annotated objects to track. Each video is between 25 and 100 frames long, each of which is labeled with a ground-truth segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube-VOS [32]:</head><p>The YouTube-VOS training set consists of 3471 videos with one or several target objects. Each video is 20 to 180 frames long, where every fifth frame is labelled. We use only the labelled frames during training. SynthVOS: In order to cover a wide varity of classes we follow <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> and utilize objects from the salient object segmentation dataset MSRA10k <ref type="bibr" target="#b6">[7]</ref>. It contains 10 4 images where a single object is segmented. We paste 1 to 5 such objects onto an image from VOC2012 <ref type="bibr" target="#b8">[9]</ref>. A synthetic video is obtained by moving the objects across the image.</p><p>One training sample consists of a video snippet of n frames and a given ground-truth for the first frame. Images are normalized with ImageNet <ref type="bibr" target="#b26">[27]</ref> mean and standard deviation. We let our model predict segmentation masks in each frame and apply a cross-entropy loss. We also place an auxillary loss on the coarse segmentationsỹ p . The losses are summed and minimized with Adam in two stages: Initial training: First we train for 80 epochs using all three datasets on half resolution images (240 × 432). The batch-size is set to 4 video snippets, using 8 frames in each snippet. We use a learning rate of 10 −4 , exponential learning rate decay of 0.95 per epoch, and a weight decay of 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetuning:</head><p>We then finetune for 100 epochs on the DAVIS2017 and YouTube-VOS training sets, using full image resolution. During this step we sample sequences from both datasets with equal probability. The batchsize is lowered to 2 snippets, to accomodate longer sequences of 14 frames. We use a learning rate of 10 −5 , exponential learning rate decay of 0.985 per epoch, and a weight decay of 10 −6 . The training is stopped early by observing the performance on a held-out set of 300 sequences from the YouTube-VOS training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first conduct an ablation study of the proposed approach on the Youtube-VOS benchmark <ref type="bibr" target="#b31">[32]</ref>. Then we compare with the state-of-the-art on three video object segmentation benchmarks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. Our method is implemented in PyTorch <ref type="bibr" target="#b21">[22]</ref> and trained on a single Nvidia V100 GPU. Our code will be made available upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We perform an extensive ablative analysis of our approach on the large-scale YouTube-VOS dataset. We use the official validation set, since the test set is closed for submissions after the challenge. The validation set comprises 474 videos, each labelled with one or multiple objects. Groundtruth masks are withheld, and results are obtained through an online evaluation server. Performance is measured in terms of the mean Jaccard index J <ref type="bibr" target="#b25">[26]</ref>, i.e. intersectionover-union (IoU), and the mean contour accuracy F. The two measures are separately calculated for seen and unseen classes, resulting in four performance measures. The overall performance (G) is the average of all four measures.</p><p>In our ablative experiments, we analyze six key modifications of our approach, as explained below. Results are shown in table 1. For each version, we retrain the entire network from scratch using the exact same procedure.</p><p>Appearance module: We first analyze the impact of the proposed appearance module (see section 3.2) by remov-  <ref type="table">Table 1</ref>. Ablation study on YouTube-VOS. We report the overall performance G along with segmentation accuracy J on classes seen and unseen during training. See text for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Final segmentation Appearance <ref type="figure">Figure 3</ref>. Visualization of the appearance module on five videos from YouTube-VOS. The final segmentation of our approach is shown (middle) together with output of the appearance module (right). The appearance module accurately locates the target (red) with the foreground representation while accentuating potential distractors (green) with the secondary mixture component.</p><p>ing it from the network (No appearance module in <ref type="table">table 1)</ref>. This leads to a major reduction in overall performance, from 66.0% to 50.0%. The results clearly demonstrate that the introduced appearance module is an essential component in our video object segmentation approach. Further insights are obtained by studying the performance on seen and unseen classes in table 1. Note that removing the appearance module causes a 9.1% decrease for classes that are seen during training, and a remarkable 20.6% decrease for unseen classes. Thus, our generative appearance model component is crucial for the generalization to arbitrary objects that are unseen during training. This is explained by the target specific and class-agnostic nature of our appearance module. Mask-propagation module: Secondly, we investigate the importance of the mask-propagation module (see section 3.3). Refraining from propagating the mask predicted in the previous frame (No mask-prop module in table 1) results in a 2.0% reduction in performance. While this reduction is significant, the importance of the mask-propagation module is small compared to that of the appearance module. Gaussian mixture components: As described in section 3.2, we employ two Gaussian mixture components to model the foreground and background, respectively. In addition to the base mixture component, a secondary Gaussian mixture component is added to capture hard examples that are not accurately modelled by a unimodal distribution. We investigate the impact of this additional mixture compo- The impact of the multi-modal generative model is also analyzed qualitatively in <ref type="figure">fig. 3</ref>. The mixture component dedicated to hard negative image regions is able to model other objects in the vicinity of the target (row 1 and 2) and accurately captures other objects of the same class (row <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Note that both the appearance module's output and the final segmentations are soft, and only for the purpose of visualization we show the arguments of the maxima. Model update: We investigate the impact of updating the generative model in each frame using (4). The version No update (table 1) only uses the initial frame to compute the mixture model parameters <ref type="bibr" target="#b2">(3)</ref>, and no update (4) is performed during training and inference. Updating the generative model to capture changes in the target and background appearance leads to a 1.1% improvement in performance. Appearance module output: As previously described, our appearance module outputs the log-probability scores <ref type="bibr" target="#b7">(8)</ref>. To validate this choice, we also compare with outputting the posterior probabilities (Appearance SoftMax in table 1), obtained by adding a SoftMax layer after computing the scores <ref type="bibr" target="#b7">(8)</ref>, between the appearance and fusion modules. This leads to a significant degradation in performance (−10.2%).</p><p>These results are in line with conventional techniques in segmentation <ref type="bibr" target="#b18">[19]</ref> and classification <ref type="bibr" target="#b16">[17]</ref>, where activations in the network are not converted to probabilities until the final output layer. End-to-end learning: Finally, we analyze the impact of end-to-end differentiation and training in our approach. Specifically, we investigate the importance of end-to-end differentiability in the learning stage of the appearance module. The comparison is performed by not backpropagating through the model inference computation (3) during the training of the network. Note that, the rest of the framework remains unchanged. The resulting method (No end-to-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>O-Ft Causal J (%) CINM <ref type="bibr" target="#b0">[1]</ref> 67.2 OSVOS-S <ref type="bibr" target="#b20">[21]</ref> 64.7 OnAVOS <ref type="bibr" target="#b29">[30]</ref> 61.6 OSVOS <ref type="bibr" target="#b1">[2]</ref> 56.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">State-of-the-Art Comparison</head><p>We compare our approach with the state-of-the-art on three video object segmentation benchmarks: YouTube-VOS <ref type="bibr" target="#b31">[32]</ref>, DAVIS2017 <ref type="bibr" target="#b25">[26]</ref>, and DAVIS2016 <ref type="bibr" target="#b23">[24]</ref>. YouTube-VOS: This recently introduced large-scale dataset contains 474 sequences with 91 classes, 26 of which are not included in the YouTube-VOS training set. We use the official validation set, as in section 4.1. We compare our approach with all, to our best knowledge, published results <ref type="bibr" target="#b31">[32]</ref>. Additionally, we evaluate the RGMP method, using the code provided by the authors. The results are shown in  <ref type="figure">Figure 4</ref>. Qualitative comparison between our approach and 3 state-of-the-art approaches. Our approach is able to accurately segment all targets, demonstrating robustness to occlusions and successfully discriminating between different objects. This is largely thanks to the powerful appearance model in our architecture. method employs online fine-tuning (O-Ft) and if it is causal, i.e. if the segmentation output depends on future frames in the video. Here we let and ×indicate yes and no, respectively. Among previous approaches performing extensive online fine-tuning in the first frame, OSVOS and On-AVOS achieve final scores of 58.8% and 55.2%. For the S2S method, we compare with two versions: one with and one without online fine-tuning, obtaining 64.4% and 57.6%, respectively. Our approach obtains a final score of 66.0%, significantly outperforming state-of-the-art without invoking any online fine-tuning. Furthermore, our method performs notably well on the unseen category, which only considers objects that are not seen during training. Again, this demonstrates the effectiveness of our class-agnostic appearance module, which generalizes to arbitrary target objects. DAVIS2017: The dataset comprises 30 videos with one or multiple target objects. The results are shown in table 3. Among existing methods, DyeNet is the only approach that is non-causal, since it processes the entire video in a bidirectional manner. It is therefore not applicable to real-time or online systems. The RGMP method, achieving a score of 64.8%, relies on mask propagation and an appearance model constructed by simply concatenating image features from the first frame. VideoMatch (VM) stores foreground and background feature vectors that are then matched with feature vectors in the test image. This method obtains a final result of 56.5%. The proposed method, employing an end-to-end differentiable generative probabilistic appearance model, achieves a score of 67.2%. Our ap-proach outperforms all causal methods not invoking online fine-tuning, and is even on par with the best non-causal and online fine-tuning-based techniques. DAVIS2016: For completeness, we also evaluate our approach on DAVIS2016. It is a subset of DAVIS2017, containing 20 videos labeled with a single object. The small size and number of objects in DAVIS2016 limits the diversity. It has therefore become highly saturated over the years. In table 4 we show the final result of each method, along with computational time reported by the respective authors. Our approach obtains a competitive performance of 82.0% compared to state-of-the-art. Unlike our method, the top performing approaches on DAVIS2016, such as OSVOS, OnAVOS, and FAVOS do not generalize well to the larger and more diverse YouTube-VOS and DAVIS2017 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Evaluation</head><p>We qualitatively compare our approach with three stateof-the-art approaches (RGMP <ref type="bibr" target="#b30">[31]</ref>, CINM <ref type="bibr" target="#b0">[1]</ref>, FAVOS <ref type="bibr" target="#b4">[5]</ref>) on three videos from DAVIS2017. The results are shown in <ref type="figure">fig. 4</ref>. RGMP tends to lose parts of objects, and struggles with discrimination between different objects. While CINM can produce detailed segmentation masks (row 5), it suffers from several failure modes (row 2, 4, 6). FAVOS struggles with discriminating targets (row 2, 6) and fails to capture details (row 6) or precise boundaries (row 4). The proposed approach succeeds to accurately segment both targets in all scenarios while being one or several orders of magnitude faster compared to FAVOS and CINM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose to address the VOS problem by learning the appearance of the target in an efficient and differentiable manner, avoiding the drawbacks of existing matching or online-finetuning based approaches. The target appearance is modelled as a mixture of Gaussians in an embedding space, and we show that both learning and inference of this model can be expressed in closed form. This permits the implementation of the appearance model as a component in a neural network that is trained on end-to-end. We thoroughly analyze the proposed approach and demonstrate its effectiveness on three benchmarks, resulting in state-ofthe-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>in table 1) obtains poor results, with a total degradation of 7.2% in overall performance. This highlights the importance of permitting true end-to-end learning.</figDesc><table><row><cell cols="5">State-of-the-art comparison on the DAVIS2017 valida-</cell></row><row><cell cols="5">tion set. For each method we report whether it employs online</cell></row><row><cell cols="5">fine-tuning (O-Ft), is causal, and the final performance J (%).</cell></row><row><cell cols="5">Our approach obtains superior results compared to state-of-the-art</cell></row><row><cell cols="5">methods without online fine-tuning. Further, our approach closes</cell></row><row><cell cols="5">the performance gap to existing methods employing online fine-</cell></row><row><cell>tuning.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">O-Ft Causal Speed J (%)</cell></row><row><cell>OnAVOS [30]</cell><cell></cell><cell></cell><cell>13s</cell><cell>86.1</cell></row><row><cell>OSVOS-S [21]</cell><cell></cell><cell></cell><cell>4.5s</cell><cell>85.6</cell></row><row><cell>MGCRN [12]</cell><cell></cell><cell></cell><cell>0.73s</cell><cell>84.4</cell></row><row><cell>CINM [1]</cell><cell></cell><cell></cell><cell>&gt;30s</cell><cell>83.4</cell></row><row><cell>LSE [8]</cell><cell></cell><cell></cell><cell></cell><cell>82.9</cell></row><row><cell>OSVOS [2]</cell><cell></cell><cell></cell><cell>9s</cell><cell>79.8</cell></row><row><cell>MSK [23]</cell><cell></cell><cell></cell><cell>12s</cell><cell>79.7</cell></row><row><cell>SFL [6]</cell><cell></cell><cell></cell><cell>7.9s</cell><cell>74.8</cell></row><row><cell>DyeNet [18]</cell><cell>×</cell><cell>×</cell><cell>0.42s</cell><cell>84.7</cell></row><row><cell>FAVOS [5]</cell><cell>×</cell><cell></cell><cell>1.80s</cell><cell>82.4</cell></row><row><cell>RGMP [31]</cell><cell>×</cell><cell></cell><cell>0.13s</cell><cell>81.5</cell></row><row><cell>VM [13]</cell><cell>×</cell><cell></cell><cell>0.32s</cell><cell>81.0</cell></row><row><cell>MGCRN [12]</cell><cell>×</cell><cell></cell><cell>0.36s</cell><cell>76.4</cell></row><row><cell>PML [4]</cell><cell>×</cell><cell></cell><cell>0.28s</cell><cell>75.5</cell></row><row><cell>OSMN [34]</cell><cell>×</cell><cell></cell><cell>0.14s</cell><cell>74.0</cell></row><row><cell>CTN [15]</cell><cell>×</cell><cell></cell><cell>1.30s</cell><cell>73.5</cell></row><row><cell>VPN [14]</cell><cell>×</cell><cell></cell><cell>0.63s</cell><cell>70.2</cell></row><row><cell>MSK [23]</cell><cell>×</cell><cell></cell><cell>0.15s</cell><cell>69.9</cell></row><row><cell>Ours</cell><cell>×</cell><cell></cell><cell>0.07s</cell><cell>82.0</cell></row><row><cell cols="5">Table 4. State-of-the-art comparison on DAVIS2016 validation set,</cell></row><row><cell cols="5">which is a subset of DAVIS2017. For each method we report</cell></row><row><cell cols="5">whether it employs online fine-tuning (O-Ft), is causal, the com-</cell></row><row><cell cols="5">putation time (if available), and the final performance J (%). Our</cell></row><row><cell cols="5">approach obtains competitive results compared to causal methods</cell></row><row><cell cols="2">without online fine-tuning.</cell><cell></cell><cell></cell><cell></cell></row></table><note>end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table 2 .</head><label>2</label><figDesc>For each approach, we indicate if the</figDesc><table><row><cell>Image</cell><cell>Ground Truth</cell><cell>RGMP [31]</cell><cell>CINM [1]</cell><cell>FAVOS [5]</cell><cell>Ours</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">for k = 2, 3: compute α i pk from (6)10 for k = 2, 3: computeμ i k ,Σ i k based on (3) 11for k = 0, 1, 2, 3: update µ i k and Σ i k from(4)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Msra10k database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motionguided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Premvos: Proposalgeneration, refinement and merging for the davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequenceto-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
