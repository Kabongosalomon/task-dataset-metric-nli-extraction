<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Table-to-text Generation by Structure-aware Seq2seq Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
							<email>tianyu0421@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
							<email>shalei@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Table-to-text Generation by Structure-aware Seq2seq Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table-</ref><p>to-text generation aims to generate a description for a factual table which can be viewed as a set of field-value records. To encode both the content and the structure of a table, we propose a novel structure-aware seq2seq architecture which consists of field-gating encoder and description generator with dual attention. In the encoding phase, we update the cell memory of the LSTM unit by a field gate and its corresponding field value in order to incorporate field information into table representation. In the decoding phase, dual attention mechanism which contains word level attention and field level attention is proposed to model the semantic relevance between the generated description and the table. We conduct experiments on the WIKIBIO dataset which contains over 700k biographies and corresponding infoboxes from Wikipedia. The attention visualizations and case studies show that our model is capable of generating coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table. Automatic evaluations also show our model outperforms the baselines by a great margin. Code for this work is available on https://github.com/tyliupku/wiki2bio.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Generating natural language description for a structured table is an important task for text generation from structured data. Previous researches include weather forecast based on a set of weather records <ref type="bibr" target="#b13">(Liang, Jordan, and Klein 2009)</ref> and sportscasting based on temporally ordered events <ref type="bibr" target="#b3">(Chen and Mooney 2008)</ref>. However, previous work models the structured data in the limited pre-defined schemas. For example, a weather record rainChance(time:06:00-21:00, mode:SSE, value:20) is represented by a fixed-length onehot vector by its record type, record time, record value and record value. To this end, we focus on table-to-text generation which involves comprehensive representation for the complex structure of a table rather than pre-defined schemas. In contrast to previous work experimented on small datasets which contain only a few tens of thousands of records such as WEATHERGOV <ref type="bibr" target="#b13">(Liang, Jordan, and Klein 2009)</ref> and ROBOCUP (Chen and Mooney 2008), we focus on a more challenging task to generate biographies Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="figure">Figure 1</ref>: The Wikipedia infobox of Charles Winstead, the corresponding introduction on his wiki page reads "Charles Winstead <ref type="bibr">(1891 -1973)</ref> was an FBI agent in the 1930s -40s, famous for being one of the agents who shot and killed John Dillinger.". based on the Wikipedia infoboxes. As shown in <ref type="figure">Fig 1,</ref> a biographic infobox is a fixed-format table that describes a person with many field-value records like (Name, Charles B. Winstead), (Nationality, American), (Occupation, FBI Agent), etc. We utilize WIKIBIO dataset proposed by <ref type="bibr" target="#b12">Lebret, Grangier, and Auli (2016)</ref> which contains 700k biographies from Wikipedia, with 400k words in total as the benchmark dataset.</p><p>Previous work has made significant progress on this task. <ref type="bibr" target="#b12">Lebret, Grangier, and Auli (2016)</ref> proposed a statistical n-gram model with local and global conditioning on a Wikipedia infobox. However the field content of a record is likely to be a sequence of words, the statistical language model is not good at capturing long-range dependencies between words. <ref type="bibr" target="#b17">Mei, Bansal, and Walter (2015)</ref> proposed a selective generation method based on an encoder-alignerdecoder framework. The model utilizes a sparse one-hot vector to represent a weather record. However it's inefficient to represent the complex structure of a table by one-hot vectors.</p><p>We propose a structure-aware sequence to sequence (seq2seq) generation framework to model both content and structure of the table by local and global addressing. When a human writes a biography for a person based on the related Wikipedia infobox, he will firstly determine which records in the table should be included in the introduction and how to arrange the order of these records before wording. After that, the writer will further consider which words or phrases in the table should be more focused on to paraphrase. We summarize the two phases of generation as two scopes of addressing: local and global addressing. Local addressing determines which particular word in the table should be focused on while generating a piece of description at certain time step. However, the word level addressing can not fully address the table-to-text generation problem as the factual tables usually have complex structures which might confuse the generator. Global addressing is proposed to determine which records of the table should be more focused on while generating corresponding description. Global addressing is necessary as the description of a table may not cover all the records. For example, the 'cause of death' field in <ref type="figure">Fig 1</ref> is not mentioned in the description. Furthermore, the order of records in the tables may not always be homogeneous. For example, we can introduce a person as an order of his (Birth-Death-Nationality-Occupation) according to his Wikipedia infobox. However the other infoboxes may be arranged as (Occupation-Nationality-Birth-Death). Local addressing is realized by content encoding of the LSTM encoder and word level attention while global addressing is realized by field encoding of the field-gating LSTM variation and field level attention in our model.</p><p>The structure-aware seq2seq architecture we proposed exploits encoder-decoder framework using long short-term memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref> units with local and global addressing on the structured table. In the encoding phase, our model first encodes the sets of fieldvalue records in the table by integrating field information and content representation. To make better use of field information, we add a field gate to the cell state of the encoder LSTM unit to incorporate the field embedding into the structural representation of the table. The model next employs a LSTM decoder to generate natural language description by the structural representation of the table. In the decoding phase, we also propose a novel dual attention mechanism which consists of two parts: word-level attention for local addressing and field-level attention for global addressing.</p><p>Our contributions are three-fold: (1) We propose an endto-end structure-aware encoder-decoder architecture to encode field information into the representation of a structured table.</p><p>(2) Field-gating encoder and dual attention mechanism are proposed to operate local and global addressing between the content and the field information of a structured table.</p><p>(3) Experiments on WIKIBIO dataset show that our model achieves substantial improvement over baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Most generation systems can be divided into two independent modules: (1)content selection involves choosing a subset of relevant records in a table to talk about. (2)surface realization is concerned with generating natural language descriptions for this subset.</p><p>Many approaches have been proposed to learn the individual modules. For content selection module, one approach builds a content selection model by aligning records and sentences <ref type="bibr" target="#b1">(Barzilay and Lapata 2005;</ref><ref type="bibr" target="#b4">Duboue and McKeown 2002)</ref>. A hierarchical semi-Markov method is proposed by <ref type="bibr" target="#b13">(Liang, Jordan, and Klein 2009)</ref> which first associates the text sequences to corresponding records and then generates corresponding descriptions from these records. Surface realization is often treated as a concept-to-text generation task from a given representation. <ref type="bibr" target="#b18">Reiter and Dale (2000)</ref>, <ref type="bibr" target="#b22">Walker, Rambow, and Rogati (2001)</ref> and <ref type="bibr">Stent, Prasad, and Walker (2001)</ref> utilize various linguistic features to train sentence planners for sentence generation. Context-free grammars are also used to generate natural language sentences from formal meaning representations <ref type="bibr" target="#b14">(Lu and Ng 2011;</ref><ref type="bibr" target="#b2">Belz 2008)</ref>. Other effective approaches include hybrid alignment tree <ref type="bibr" target="#b9">(Kim and Mooney 2010)</ref>, tree conditional random fields <ref type="bibr" target="#b15">(Lu, Ng, and Lee 2009)</ref>, tree adjoining grammar <ref type="bibr" target="#b6">(Gyawali 2016)</ref> and template extraction in a log-linear framework <ref type="bibr" target="#b0">(Angeli, Liang, and Klein 2010)</ref>. Recent work combines content selection and surface realization in a unified framework (Ratnaparkhi 2002; <ref type="bibr" target="#b10">Konstas and Lapata 2012;</ref><ref type="bibr" target="#b19">Sha et al. 2017)</ref> Our model borrowed the idea of representing a structured table by its field and content information from (Lebret, Grangier, and Auli 2016). However, their n-gram model is inefficient to model long-range dependencies while generating descriptions. <ref type="bibr" target="#b17">Mei, Bansal, and Walter (2015)</ref> also proposed a seq2seq model with an aligner between weather records and weather broadcast. The model used one-hot encoding to represent the weather records as they are relatively simple and highly structured. However, the model is not capable to represent the tables with complex structure like Wikipedia infoboxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition</head><p>We model the table-to-text generation in an end-to-end structure-aware seq2seq framework. The given table T can be viewed as a combination of n field-value records { R 1 , R 2 , · · · , R n }. Each record R i consists of a sequence of words { d 1 , d 2 , · · · , d m } and their corresponding field rep-</p><formula xml:id="formula_0">resent { Z d1 , Z d2 , · · · , Z dm }.</formula><p>The output of the model is the generated description S for table T which contains p tokens {w 1 , w 2 , · · · , w p } with w t being the word at time t. We formulate the table-to-text generation as the inference over a probabilistic model. The goal of the inference is to generate a sequence w * 1:p which maximizes P (w 1:p |R 1:n ).</p><formula xml:id="formula_1">w * 1:p = arg max w1:p p t=1 P (w t |w 0:t−1 , R 1:n )<label>(1)</label></formula><p>Structure-aware Seq2seq</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field representation</head><p>A Wikipedia infobox can be viewed as a set of field-value records, in which values are sequences or segments of words corresponding to certain fields. The structural representation of an infobox consists of context embedding and field embedding. The context embedding is formulated as an em-  bedding for a segment of words in the field content. The field embedding is a key point to label each word in the field content by its corresponding field name and occurrence in the table. <ref type="bibr" target="#b12">Lebret, Grangier, and Auli (2016)</ref> represented the field embeddding Z w = {f w ; p w } for a word w in the table with corresponding field name f w and position information p w . The position information can be further represented as a tuple (p + w , p − w ) which includes the positions of the token w counted from the begining and the end of the field respectively. So the field embedding of token w is extended to a triple:</p><formula xml:id="formula_2">Z w = {f w ; p + w ; p − w }<label>(2)</label></formula><p>As shown in <ref type="figure" target="#fig_0">Fig 2,</ref> the infobox of George Mikell contains several field-value records, the field content for the record (birthname, Jurgis Mikelatitis) is 'Jurgis Mikelatitis'. The word 'Jurgis' is the first token counted from the beginning of the field 'birthname' and also the second token counted from the end. So the field embedding for the word 'Jurgis' is described as {birthname; 1; 2}. Each token in the table has an unique field embedding even if there exists two same words in the same field due to the unique (field, position) pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field-gating Table Encoder</head><p>The table encoder aims to encode each word d j in the table together with its field embedding Z dj into the hidden state h j using LSTM encoder. We present a novel field-gating LSTM unit to incorporate field information into table encoding. LSTM is a recurrent neural network (RNN) architecture which uses a vector of cell state c t and a set of element-wise multiplication gates to control how information is stored, forgotten and exploited inside the network. Following the design for an LSTM cell in <ref type="bibr" target="#b5">(Graves, Mohamed, and Hinton 2013)</ref> , the architecture used in the table encoder is defined by following equations:</p><formula xml:id="formula_3">   i t f t o t c t    =    sigmoid sigmoid sigmoid tanh    W c 4n,2n d t h t−1 (3) c t = f t c t−1 + i t ĉ t (4) h t = o t tanh(c t )<label>(5)</label></formula><p>where i t , f t , o t ∈ [0, 1] n are input, forget and output gates respectively, andĉ t and c t are proposed cell value and true cell value in time t. n is the hidden size.</p><p>To make better understanding of the structure of a table, the field information should also be encoded into the encoder. One simple way is to take the concatenation of word embedding and corresponding field embedding as the input for the vanilla LSTM unit. Actually, the method is indeed proved to be useful in our experiments and serves as a baseline for comparison. However, the concatenation of word embedding and field embedding only treats the field information as an additional label of certain token which loses the structural information of the table.</p><p>To better encode the structural information of a table, we propose a field-gating variation on the vanilla LSTM unit to update the cell memory by a field gate and its corresponding field value. The field-gating cell state is described as follows:</p><formula xml:id="formula_4">l t z t = sigmoid tanh W d 2n,2n (z t ) (6) c t = f t c t−1 + i t ĉ t + l t ẑ t (7)</formula><p>where z t is the field embedding described before, l t ∈ [0, 1] n is the field gate to determine how much field information should be kept in the cell memory,ẑ t is the proposed field value corresponding to field gate. The cell state c t is updated from the original c t by incorporating field information of the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description Decoder with Dual Attention</head><p>To conduct local and global addressing towards the structured table, we use LSTM architecture with dual attention mechanism as our description generator. As defined in the equation 1, the generated token w t at time t in the decoder is predicated based on all the previously generated tokens w &lt;t before w t , the hidden states H = {h t } L t=1 of the table encoder and the field embeddings Z = {z t } L t=1 . To be more specific:</p><formula xml:id="formula_5">P (w t |H, Z, w &lt;t ) = sof tmax(W s g t ) (8) g t = tanh(W t [s t , a t ])<label>(9)</label></formula><formula xml:id="formula_6">s t = LST M (w t−1 , s t−1 )<label>(10)</label></formula><p>where s t is the t-th hidden state of the decoder calculated by the LSTM unit. The computational details can be referred in Equation 3, 4 and 5. a t is the attention vector which is widely used in many applications <ref type="bibr" target="#b23">(Xu et al. 2015;</ref><ref type="bibr" target="#b16">Luong et al. 2014;</ref><ref type="bibr" target="#b16">Ma et al. 2017)</ref>. Vanilla attention mechanism is proposed to encode the semantic relevance between the encoder states {h t } L t=1 and and the decoder state s t at time t. The attention vector is usually represented by the weighted sum of encoder hidden states. α ti = e g(st,hi) N j=1 e g(st,hj )  where g(s t , h i ) is a relevant score between decoder hidden state s t and encoder hidden state h i . There are many different ways to calculate the relevant scores. In our paper, we use the following dot product to measure the similarity be-</p><formula xml:id="formula_7">; a t = L i=1 α ti h i<label>(11)</label></formula><formula xml:id="formula_8">) / ) $ ) # 0 # 0 $ 0 % 0 $&amp; 0 $' 0 %(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description Decoder</head><formula xml:id="formula_9">tween s t and h i . W s , W t , W p , W q are all model parameters. g(s t , h i ) = tanh(W p h i ) tanh(W q s t )<label>(12)</label></formula><p>However, the word level attention described above can only capture the semantic relevance between generated tokens and the content information in the table, ignoring the structure information of the table. To fully utilize the structure information, we propose a higher level attention over generated tokens and the field embedding of the table. Field level attention can locate the particular field-value record which should be focused on while generating next token in the description by modeling the relevance between all field embeddings {z t } L t=1 and the decoder state s t at t-th time. Field level attention weight β ti is presented as Equation 13. We use the same relevant score function g(s t , z i ) as equation 12. Dual attention weight γ t is the element-wise production between field level attention weight β t and word level attention weight α t . The dual attention vector a t is updated as the weighted sum of encoder states {h t } t=1 by γ t <ref type="figure">(Equation  15</ref>):</p><formula xml:id="formula_10">β ti = e g(st,zi) N j=1 e g(st,zj ) (13) g(s t , z i ) = tanh(W x z i ) tanh(W y s t )<label>(14)</label></formula><formula xml:id="formula_11">γ ti = α ti · β ti N j=1 α tj · β tj ; a t = L i=1 γ ti h i<label>(15)</label></formula><p>Furthermore, we utilize a post-process operation for the generated unknown (UNK) tokens to alleviate the out-ofvocabulary (OOV) problem. We replace a specific generated UNK token with the most relevant token in the corresponding table according to the related dual attention matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local and Global Addressing</head><p>Local and global addressing determine which part of the table should be more focused on in different steps of description generation. The two scopes of addressings play a very important role in understanding and representing the innerstructure of a table. Next we will introduce how our model conducts local and global addressing on table-to-text generation with the help of <ref type="figure" target="#fig_1">Fig 3.</ref> Local addressing: A table can be treated as a set of fieldvalue records. Local addressing tends to encode the table content inside each record. The value in each field-value record is a sequence of words which contains 2.7 tokens on average. Some records in the Wikipedia infoboxes even contain several phrases or sentences. Previous models which used one-hot encoding or statistical language model to encode field content are inefficient to capture the semantic relevance between words inside a field. The seq2seq structure itself has a strong ability to model the context of a piece of words. For one thing, the LSTM encoder can capture longrange dependencies between words in the table. For another, the word level attention of the proposed dual attention mechanism can also build a connection between the words in the description and the tokens in the table. The generated word 'actor' in <ref type="figure" target="#fig_1">Fig 3 refers</ref> to the word 'actor' in the 'Occupation' field.</p><p>Global addressing: The goal of local addressing is to represent inner-record information while global addressing aims to model inter-record relevance within the table. For example, it's noteworthy that the generated token 'actor' in <ref type="figure" target="#fig_1">Fig 3</ref> is mapped to the 'occupation' field in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Field-gating table representation and field level attention mechanism are proposed for global addressing. For table representation, we encode the structure of a table by incorporating field and position embedding into table representation apart from word embedding. The position of a token in # tokens per sentence #  the field content of a table is determined only by its field and position information. Even two same words in the table can be distinguished by their field and position. We propose a novel field-gating LSTM to incorporate the field embedding into the cell memory of LSTM unit. Furthermore, the information in a table is likely to be redundant. Some records in the table are unimportant or even useless for generating description. We should make appropriate choices on selecting useful information from all the table records. The order of records may also influence the performance of generation <ref type="bibr" target="#b21">(Vinyals, Bengio, and Kudlur 2015)</ref>. We should make it clear which records the token to be generated is focused on by global addressing between the field information of a table and its description. The field level attention of dual attention mechanism is introduced to determine which field the generator focused on in certain time step. Experiments show that our dual attention mechanism is of great help to generate description from certain table and insensible to different orders of table records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We first introduce the dataset, evaluation metrics and experimental setups in our experiments. Then we compare our model with several baselines. After that, we assess the performance of our model on table-to-text generation. Furthermore, we also conduct experiments on the disordered tables to show the efficiency of global addressing mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Evaluation Metrics</head><p>We use WIKBIO dataset proposed by <ref type="bibr" target="#b12">Lebret, Grangier, and Auli (2016)</ref> as the benchmark dataset. WIKBIO contains 728,321 articles from English Wikipedia <ref type="bibr">(Sep 2015)</ref>. The dataset uses the first sentence of each article as the description of the corresponding infobox. <ref type="table" target="#tab_2">Table 1</ref> summarizes the dataset statistics: on average, the tokens in the table (53.1) are twice as long as those in the first sentence (26.1). 9.5 tokens in the description text also occur in the table. The corpus has been divided in to training (80%), testing (10%) and validation (10%) sets.</p><p>We assess the generation quality automatically with BLEU-4 and ROUGE-4 (F measure) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare the proposed structure-aware seq2seq model with several statistical language models and the vanilla encoder-decoder model. The baselines are listed as follows:</p><p>• KN: The Kneser-Ney (KN) model is a widely used language model proposed by <ref type="bibr" target="#b7">Heafield et al. (2013)</ref>. We use the KenLM toolkit to train 5-gram models without pruning. • Template KN: Template KN is a KN model over templates which also serves as a baseline in <ref type="bibr" target="#b12">(Lebret, Grangier, and Auli 2016)</ref>. The model replaces the words occurring in both the table and the training sentences with a special token reflecting its field. The introduction section of the table in <ref type="figure" target="#fig_0">Fig 2 looks</ref> as follows under this scheme: " name 1 name 2 (born birthname 1 ... birthdate 3) is a Lithuanian-Australian occupation 1 and occupation 3 best known for his performances in known for 1 ... known for 4 (1961) and known for 5 ... known for 7 (1963) ". During inference, the decoder is constrained to emit words from the regular vocabulary or special tokens occurring in the input table. • NLM: A naive statistical language model proposed by <ref type="bibr" target="#b12">(Lebret, Grangier, and Auli 2016)</ref> for comparison. The model uses only the field content as input without field and position information. <ref type="table">• Table NLM</ref>: The most competitive statistical language model proposed by <ref type="bibr" target="#b12">(Lebret, Grangier, and Auli 2016)</ref>, which includes local and global conditioning over the table by integrating related field and position embedding into the table representation. • Vanilla Seq2seq: The vanilla seq2seq neural architecture is also provided as a strong baseline which uses the concatenation of word embedding, field embedding and position embedding as the model input. The model can operate local addressing over the table by the natural advantages of LSTM units and word level attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setup</head><p>In the table encoding phase, we use a sequence of word embeddings and their corresponding field and position embedding as input. We select the most frequent 20,000 words in the training set as the word vocabulary. For field embedding, we select 1480 fields occurring more than 100 times from the training set as field vocabulary. Additionally, we filter all empty fields whose values are none while feeding field information to the network. We also limit the largest position number as 30. Any position number over 30 will be counted as 30. While generating description for the table, a special start token sos is feed into the generator in the beginning of the <ref type="figure">Figure 4</ref>: An example of word level, field level and aggregated dual attention on generating the biography of Frédéric Fonteyne. Note there are two adjacent 'belgium's in 'birthplace-3' and 'nationality-1' field, respectively. The word level attention focuses improperly on the first 'belgium' while generating 'a belgian film director'. In contrast, the field level attention and dual attention can locate the second 'belgium' properly by word-field modeling (marked in the black boxes).  <ref type="table">Table 3</ref>: BLEU-4 and ROUGE-4 for structure-aware seq2seq model (last three rows), statistical language model (first four rows) and vanilla seq2seq model with field and position input (three rows in the middle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>decoding phase. Then we use the last generated token as the input at the next time step. A special end token eos is used to mark the end of decoding. We also restrict the generated text by a pre-defined max length to avoid redundant or irrelevant generation. We also try beam search with beam size 2-10 to enhance the performance. We use grid search to determine the parameters of our model. The detail of model parameters is listed in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation Assessment</head><p>The assessment for description generation is listed in Table 3. We have following observations: (1) Neural network models perform much better than statistical language models. Even vanilla seq2seq architecture with word level atten-tion outperform the most competitive statistical model by a great margin.</p><p>(2)The proposed structure-aware seq2seq architecture can further improve the table-to-text generation compared with the competitive vanilla seq2seq. Dual attention mechanism is able to boost the model performance by over 1 BLEU compared to vanilla attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research on Disordered Tables</head><p>We view a structured table as a set of field-value records and then feed the records into the generator sequentially as the order they are presented in the table. The order of records can guide the description generator to produce an introduction in the pre-defined schemas <ref type="bibr" target="#b21">(Vinyals, Bengio, and Kudlur 2015)</ref>. However, not all the tables are arranged in the proper order. So global addressing between the generated descriptions and the records of the table is necessary for table-to-text generation. Furthermore, the schemas of various types of tables differ greatly from each other. A biography about a politician may emphasize his or her social activities and working experience while a biography of a soccer player is likely to highlight which team he or she used to serve in or the performance in his or her career. To cope with various schemas of different tables, it's essential to model inter-record information within the tables by global addressing.</p><p>For these reasons, we propose a pair of disordered training and testing set based on WIKIBIO by randomly shuffling the records of a infobox. For example, the order of several records in a specific infobox is 'name-birthdateoccupation-spouse', we randomly shuffle the table records as 'occupation-name-spouse-birthdate', without changing the field content inside the 'occupation', 'name', 'spouse' and 'birthdate' records. Reference john joseph ''binky '' 'jones <ref type="bibr">( july 11 , 1899 in st. louis , missourimay 13 , 1961)</ref>, was a professional baseball player who played shortstop for the brooklyn robins in ten games during the 1924 season.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla seq2seq</head><p>binky jones <ref type="bibr">( july 11 , 1899 --may 13 , 1961</ref>) was a shortstop in major league baseball . + pos &amp; field binky jones ( july 11 , 1899 in st. louis , missouri --may 13 , 1961 in st. louis , missouri ) was a professional baseball player who played shortstop in the major leagues.</p><p>Struct-aware seq2seq binky jones ( july 11 , 1899 in st. louis , missouri --may 13 , 1961 in st. louis , missouri ), is a former professional baseball player who played shortstop from april 15 to april 27 for the brooklyn robins in 1924 . <ref type="figure">Figure 5</ref>: The generated descriptions for Binky Jones and the corresponding reference in the Wikipedia. Our proposed structaware seq2seq model can generate more informative and accurate description compared to vanilla seq2seq model. <ref type="table" target="#tab_7">Table 4</ref> shows that all three neural network models perform not as good as before, which means the order of table records is an essential aspect for table-to-text generation. However, the BLEU and ROUGE decreases on the structureaware seq2seq model are much smaller than the other two models, which proves the efficiency of global addressing mechanism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on Dual Attention</head><p>Dual attention mechanism models the relationship between the generated tokens and table content inside each record by word level attention while encoding the relevance of generated description and inter-record information within the table by field level attention. The aggregation of word level attention and field level attention can model more precise connection between the table and its generated description. <ref type="figure">Fig 4 shows</ref> an example of the three attention mechanisms while generating a piece of description for Frédéric Fonteyne based on his Wikipedia infobox. We can find out that the name, birthdate, nationality and occupation information contained in the generated sentence can properly refer to the related table content by the aggregated dual attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>Fig 5 shows the generated descriptions for different variants of our model based on the related Wikipedia infobox. All three neural network generators can produce coherent and understandable sentences with the help of local addressing mechanism. All of them contain the word 'baseball' which is not directly mentioned in the infobox. It means the generators deduce from table content that Binky Jones is a baseball player.</p><p>However, the two vanilla seq2seq models also generate 'major league baseball' or 'major leagues' which are not mentioned in the table and probably not correct. Vanilla seq2seq model without global addressing on the table just generates the most possible league in Wikipedia for a baseball player to play in. Furthermore, the two biographies generated by vanilla seq2seq model fail to contain the information from the infobox which team he served in, as well as the time period of his playing in that team. The biography generated by our proposed structure-aware seq2seq model is able to cover nearly all the information mentioned in the table. The generated segment 'who played shortstop from april 15 to april 27 for the brooklyn robins in 1924' (15 words) includes information in five fields of the table: 'position', 'debutdate', 'finaldate', 'debutteam' and 'finalteam', which is achieved by the global addressing between the fields and the generated tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We propose a structure-aware seq2seq architecture to encode both the content and the structure of a table for table-to-text generation. The model consists of field-gating encoder and description generator with dual attention. We add a field gate to the encoder LSTM unit to incorporate the field information. Furthermore, dual attention mechanism which contains word level attention and field level attention can operate local and global addressing to the content and the structure of a table. A series of visualizations, case studies and generation assessments show that our model outperforms the competitive baselines by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The wiki infobox of George Mikell (left) and the table of its field representation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The overall diagram of structure-aware seq2seq architecture for generating description for George Mikell inFig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>table token per sent. # tokens per table # fields per table Statistics of WIKIBIO dataset.</figDesc><table><row><cell>Mean</cell><cell>26.1</cell><cell>9.5</cell><cell>53.1</cell><cell></cell><cell>19.7</cell><cell></cell></row><row><cell cols="7">Word dimension Field dimension Position dimension Hidden size Batch size Learning rate Optimizer</cell></row><row><cell>400</cell><cell>50</cell><cell>5</cell><cell>500</cell><cell>32</cell><cell>0.0005</cell><cell>Adam</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Parameter settings of our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Experiments on the disordered tables to show the efficiency of global addressing.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use standard scripts NIST mteval-v13a.pl (for BLEU), and rouge-1.5.5 (for ROUGE).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Our work is supported by the National Key Research and Development Program of China under Grant No.2017YFB1002101 and project 61772040 supported by NSFC. The corresponding authors of this paper are Baobao Chang and Zhifang Sui.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domainindependent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="502" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="431" to="455" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to sportscast: a test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Content planner construction via evolutionary algorithms and a corpus-based fitness function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG 2002</title>
		<meeting>INLG 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Surface Realisation from Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gyawali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Universite de Lorraine</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable modified kneser-ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative alignment and semantic parsing for learning from ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, 543-551. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, 543-551. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised conceptto-text generation with hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A global model for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07771</idno>
		<title level="m">Neural text generation from structured data with application to the biography domain</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A probabilistic forest-to-string model for language generation from typed lambda calculus expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1611" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language generation with tree conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving semantic relevance for sequence-tosequence learning of chinese social media text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="635" to="640" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Canada</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trainable approaches to surface natural language generation and their application to conversational dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00838</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="455" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>What to talk about and how? selective generation using lstms with coarse-to-fine alignment</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Order-planning neural text generation from structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno>abs/1709.00155</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trainable sentence planning for complex information presentation in spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on association for computational linguistics, 79. Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on association for computational linguistics, 79. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spot: A trainable sentence planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rogati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</title>
		<meeting>the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
