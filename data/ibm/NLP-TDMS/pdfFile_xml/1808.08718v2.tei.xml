<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wide Activation for Efficient and Accurate Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
							<email>yuchenf4@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
							<email>yangjianchao@bytedance.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
							<email>ning.xu@snap.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
							<email>zhawang@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
						</author>
						<title level="a" type="main">Wide Activation for Efficient and Accurate Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report we demonstrate that with same parameters and computational budgets, models with wider features before ReLU activation have significantly better performance for single image super-resolution (SISR). The resulted SR residual network has a slim identity mapping pathway with wider (2× to 4×) channels before activation in each residual block. To further widen activation (6× to 9×) without computational overhead, we introduce linear low-rank convolution into SR networks and achieve even better accuracy-efficiency tradeoffs. In addition, compared with batch normalization or no normalization, we find training with weight normalization leads to better accuracy for deep super-resolution networks. Our proposed SR network WDSR achieves better results on large-scale DIV2K image super-resolution benchmark in terms of PSNR with same or lower computational complexity. Based on WDSR, our method also won 1st places in NTIRE 2018 Challenge on Single Image Super-Resolution in all three realistic tracks. Experiments and ablation studies support the importance of wide activation for image super-resolution. Code is released at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional neural networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>. SISR aims at recovery of a high resolution (HR) image from its low resolution (LR) counterpart (typically a bicubic downsampled version of HR). It has many applications in security, surveillance, satellite, medical imaging <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref> and can serve as a built-in module for other image restoration or recognition tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Previous image super-resolution networks including SRCNN <ref type="bibr" target="#b3">[4]</ref>, FSRCNN <ref type="bibr" target="#b2">[3]</ref>, ESPCN <ref type="bibr" target="#b28">[29]</ref> utilized relatively shallow convolutional neural networks (with its depth from 3 to 5). They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR <ref type="bibr" target="#b13">[14]</ref>, SRResNet <ref type="bibr" target="#b16">[17]</ref> and EDSR <ref type="bibr" target="#b18">[19]</ref>). The increasing of depth brings benefits to representation power <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> but meanwhile under-use the feature information from shallow layers (usually represent low-level features). To address this issue, methods including SRDenseNet <ref type="bibr" target="#b35">[36]</ref>, RDN <ref type="bibr" target="#b41">[42]</ref>, MemNet <ref type="bibr" target="#b32">[33]</ref> introduce various skip connections and concatenation operations between shallow layers and deep layers, formalizing holistic structures for image super-resolution.</p><p>In this work we address this issue in a different perspective. Instead of adding various shortcut connections, we conjecture that the non-linear ReLUs impede information flow from shallow layers to deeper ones <ref type="bibr" target="#b25">[26]</ref>. Based on residual SR network, we demonstrate that without additional parameters <ref type="figure">Figure 1</ref>: Left: vanilla residual block. Middle WDSR-A: residual block with wide activation. Right WDSR-B: residual block with wider activation and linear low-rank convolution. We demonstrate different residual building blocks for image super-resolution networks. Compared with vanilla residual blocks used in EDSR <ref type="bibr" target="#b18">[19]</ref>, we introduce WDSR-A which has a slim identity mapping pathway with wider (2× to 4×) channels before activation in each residual block. We further introduce WDSR-B with linear low-rank convolution stack and even widen activation (6× to 9×) without computational overhead. In WDSR-A and WDSR-B, all ReLU activation layers are only applied between two wide features (features with larger channel numbers). and computation, simply expanding features before ReLU activation leads to significant improvements for single image super-resolution, beating SR networks with complicated skip connections and concatenations including SRDenseNet <ref type="bibr" target="#b35">[36]</ref> and MemNet <ref type="bibr" target="#b32">[33]</ref>. The intuition of our work is that expanding features before ReLU allows more information pass through while still keeps highly non-linearity of deep neural networks. Thus low-level SR features from shallow layers may be easier to propagate to the final layer for better dense pixel value predictions.</p><p>The central idea of wide activation leads us to explore efficient ways to expand features before ReLU, since simply adding more parameters is inefficient for real-time image SR scenarios <ref type="bibr" target="#b7">[8]</ref>. We first introduce SR residual network WDSR-A, which has a slim identity mapping pathway with wider (2× to 4×) channels before activation in each residual block. However when the expansion ratio is above 4, channels of the identity mapping pathway have to be further slimmed and we find it dramatically deteriorates accuracy. Thus as the second step, we keep constant channel numbers of identity mapping pathway, and explore more efficient ways to expand features. We first consider group convolution <ref type="bibr" target="#b37">[38]</ref> and depthwise separable convolution <ref type="bibr" target="#b0">[1]</ref>. However, we find both of them have unsatisfactory performance for the task of image super-resolution. To this end, we propose linear low-rank convolution that factorizes a large convolution kernel into two low-rank convolution kernels. With wider activation and linear low-rank convolutions, we construct our SR network WDSR-B. It has even wider activation (6× to 9×) without additional parameters or computation, and boosts accuracy further for image super-resolution. The illustration of WDSR-A and WDSR-B is shown in <ref type="figure">Figure 1</ref>. Experiments show that wider activation consistently beats their baselines under different parameter budgets.</p><p>Additionally, compared with batch normalization <ref type="bibr" target="#b11">[12]</ref> or no normalization, we find training with weight normalization <ref type="bibr" target="#b24">[25]</ref> leads to better accuracy for deep super-resolution networks. Previous works including EDSR <ref type="bibr" target="#b18">[19]</ref>, BTSRN <ref type="bibr" target="#b6">[7]</ref> and RDN <ref type="bibr" target="#b41">[42]</ref> found that batch normalization <ref type="bibr" target="#b11">[12]</ref> deteriorates the accuracy of image super-resolution, which is also confirmed in our experiments. We provide three intuitions and related experiments showing that batch normalization, due to 1) mini-batch dependency, 2) different formulations in training and inference and 3) strong regularization side-effects, is not suitable for training SR networks. However, with the increasing depth of neural networks for SR (e.g. MDSR <ref type="bibr" target="#b18">[19]</ref> has depth around 180), the networks without batch normalization become difficult to train. To this end, we introduce weight normalization for training deep SR networks. The weight normalization enables us to train SR network with an order of magnitude higher learning rate, leading to both faster convergence and better performance.</p><p>In summary, our contributions are as follows. 1) We demonstrate that in residual networks for SISR, wider activation has better performance with same parameter complexity. Without additional computational overhead, we propose network WDSR-A which has wider (2× to 4×) activation for better performance. 2) To further improve efficiency, we also propose linear low-rank convolution as basic building block for construction of our SR network WDSR-B. It enables even wider activation (6× to 9×) without additional parameters or computation, and boosts accuracy further. 3) We suggest batch normalization <ref type="bibr" target="#b11">[12]</ref> is not suitable for training deep SR networks, and introduce weight normalization <ref type="bibr" target="#b24">[25]</ref> for faster convergence and better accuracy. 4) We train proposed WDSR-A and WDSR-B built on the principle of wide activation with weight normalization, and achieve better results on large-scale DIV2K image super-resolution benchmark. Our method also won 1st places in NTIRE 2018 Challenge on Single Image Super-Resolution in all three realistic tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Super-Resolution Networks</head><p>Deep learning-based methods for single image super-resolution significantly outperform conventional ones <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref> in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). SRCNN <ref type="bibr" target="#b3">[4]</ref> was the first work utilizing an end-to-end convolutional neural network as a mapping function from LR images to their HR counterparts. Since then, various convolutional neural network architectures were proposed for improving the accuracy and efficiency. In this section, we review these approaches under several groups.</p><p>Upsampling layers Super-resolution involves upsampling operation of image resolution. The first super-resolution network SRCNN <ref type="bibr" target="#b3">[4]</ref> applied convolution layers on the pre-upscaled LR image. It is inefficient because all convolutional layers have to compute on high-resolution feature space, yielding S 2 times computation than on low-resolution space, where S is the upscaling factor. To accelerate processing speed without loss of accuracy, FSRCNN <ref type="bibr" target="#b2">[3]</ref> utilized parametric deconvolution layer at the end of SR network <ref type="bibr" target="#b2">[3]</ref>, making all convolution layers compute on LR feature space. Another non-parametric efficient alternative is pixel shuffling <ref type="bibr" target="#b28">[29]</ref> (a.k.a., sub-pixel convolution). Pixel shuffling is also believed to introduce less checkerboard artifacts <ref type="bibr" target="#b21">[22]</ref> than the deconvolutional layer.</p><p>Very deep and recursive neural networks The depth of neural networks is of central importance for deep learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. It is also experimentally proved in single image super-resolution task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. These very deep networks (usually more than 10 layers) stack many small-kernel (i.e., 3 × 3) convolutions and have higher accuracy than shallow ones <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. However, the increasing depth of convolutional neural networks introduces over-parameterization and difficulty of training. To address these issues, recursive neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref> are proposed by re-using weights repeatedly.</p><p>Skip connections On one hand, deeper neural networks have better performance in various tasks <ref type="bibr" target="#b29">[30]</ref>, on the other hand low-level features are also important for image super-resolution task <ref type="bibr" target="#b41">[42]</ref>. To address this contradictory, VDSR <ref type="bibr" target="#b13">[14]</ref> proposed a very deep VGG-like <ref type="bibr" target="#b29">[30]</ref> network with global residual connection (i.e. identity skip connection) for SISR. SRResNet <ref type="bibr" target="#b16">[17]</ref> proposed a ResNetlike <ref type="bibr" target="#b8">[9]</ref> network. Densely connected networks <ref type="bibr" target="#b10">[11]</ref> are also adapted for SISR in SRDenseNet <ref type="bibr" target="#b35">[36]</ref>. MemNet <ref type="bibr" target="#b32">[33]</ref> integrated skip connections and recursive unit for low-level image restoration tasks. To further exploit the hierarchical features from all the convolutional layers, residual dense networks (RDN) <ref type="bibr" target="#b41">[42]</ref> are proposed. All these works benefit from additional skip connections between different levels of features in deep neural networks.</p><p>Normalization layers As image super-resolution networks going deeper and deeper (from 3-layer SRCNN <ref type="bibr" target="#b3">[4]</ref> to 160-layer MDSR <ref type="bibr" target="#b18">[19]</ref>), training becomes more difficult. Batch normalization layers are one of the cures for this problem in many tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>. It is also introduced in SISR networks in SRResNet <ref type="bibr" target="#b16">[17]</ref>. However, empirically it is found that batch normalization <ref type="bibr" target="#b11">[12]</ref> hinders the accuracy of image super-resolution. Thus, in recent image SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>, batch normalization is abandoned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter-Efficient Convolutions</head><p>In this subsection, we also review several related methods proposed for improving efficiency of convolutions.</p><p>Flattened convolution Flattened convolutions <ref type="bibr" target="#b12">[13]</ref> consist of consecutive sequence of onedimensional filters across all directions in 3D space (lateral, vertical and horizontal) to approximate conventional convolutions. The number of parameters in flattened convolution decreases from XY C to X + Y + C, where C is the number of input planes, X and Y denote filter width and height.</p><p>Group convolution Group convolutions <ref type="bibr" target="#b37">[38]</ref> divide features into groups channel-wisely and perform convolutions inside the group individually, followed by a concatenation to form the final output. In group convolutions, the number of parameters can be reduced by g times, where g is the group number. Group convolutions are the key components to many efficient models (e.g. ResNeXt <ref type="bibr" target="#b37">[38]</ref>).</p><p>Depthwise separable convolution Depthwise separable convolution is a stack of depthwise convolution (i.e. a spatial convolution performed independently over each channel of an input) followed by a pointwise convolution (i.e. a 1x1 convolution) without non-linearities. It can also be viewed as a specific type of group convolution where the number of groups g is the number of channels. The depthwise separable convolution formulates the basic architecture in many efficient models including Xception <ref type="bibr" target="#b0">[1]</ref>, MobileNet <ref type="bibr" target="#b9">[10]</ref> and MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>.</p><p>Inverted residuals Another work <ref type="bibr" target="#b26">[27]</ref> expands features before activation for image recognition tasks (named inverted residuals). The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. The inverted residual shares similar merits with our proposed wide activation, however we found the inverted residual proposed in <ref type="bibr" target="#b26">[27]</ref> has unsatisfactory performance on the task of image SR. In this work we mainly explore different network architectures to improve the accuracy and efficiency for the task of image super-resolution with the central idea of wide activation.</p><p>3 Proposed Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wide Activation: WDSR-A</head><p>In this part, we mainly describe how we expand features before ReLU activation layer without computational overhead. We consider the effects of wide activation inside a residual block. A naive way is to directly add channel numbers of all features. However, it proves nothing except that more parameters lead to better performance. Thus, in this section, we design our SR network to study the importance of wide features before activation with same parameter and computational budgets. Our first step towards wide activation is extremely simple: we slim the features of residual identity mapping pathway while expand the features before activation, as shown in <ref type="figure">Figure 1</ref>.</p><p>Two-layer residual blocks are specifically studied following baseline EDSR <ref type="bibr" target="#b18">[19]</ref>. Assume the width of identity mapping pathway <ref type="figure">(Fig. 2)</ref> is w 1 and width before activation inside residual block is w 2 . We introduce expansion factor before activation as r thus w 2 = r × w 1 . In the vanilla residual networks (e.g., used in EDSR and MDSR) we have w 2 = w 1 and the number of parameters are 2 × w 2 1 × k 2 in each residual block. The computational (Mult-Add operations) complexity is a constant scaling of parameter numbers when we fix the input patch size. To have same complexity w 2 1 =ŵ 1 ×ŵ 2 = r ×ŵ 1 2 , the residual identity mapping pathway need to be slimmed as a factor of √ r and the activation can be expanded with √ r times meanwhile.</p><p>This simple idea forms our first widely-activated SR network WDSR-A. Experiments show that WDSR-A is extremely effective for improving accuracy of SISR when r is between 2 to 4. However, for r larger than this threshold the performance drops quickly. This is likely due to the identity mapping pathway becoming too slim. For example, in our baseline EDSR (16 residual blocks with 64 filters) for ×3 super-resolution, when r is beyond 6, w 1 will be even smaller than the final HR image representation space S 2 * 3 (we use pixel shuffle as upsampling layer) where S is the scaling factor and 3 represents RGB. Thus we seek for parameter-efficient convolution to further improve accuracy and efficiency with wider activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Wider Activation: WDSR-B</head><p>To address the above limitation, we keep constant channel numbers of identity mapping pathway, and explore more efficient ways to expand features. Specifically we consider 1 × 1 convolutions.</p><p>1 × 1 convolutions are widely used for channel number expansion or reduction in ResNets <ref type="bibr" target="#b8">[9]</ref>, ResNeXts <ref type="bibr" target="#b37">[38]</ref> and MobileNetV2 <ref type="bibr" target="#b26">[27]</ref>. In WDSR-B <ref type="figure">(Fig. 1)</ref> we first expand channel numbers by using 1 × 1 and then apply non-linearity (ReLUs) after the convolution layer. We further propose an efficient linear low-rank convolution which factorizes a large convolution kernel to two low-rank convolution kernels. It is a stack of one 1 × 1 convolution to reduce number of channels and one 3 × 3 convolution to perform spatial-wise feature extraction. We find adding ReLU activation in linear low-rank convolutions significantly reduces accuracy, which also supports wide activation hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Weight Normalization vs. Batch Normalization</head><p>In this part, we mainly analyze the different purposes and effects of batch normalization (BN) <ref type="bibr" target="#b11">[12]</ref> and weight normalization (WN) <ref type="bibr" target="#b24">[25]</ref>. We offer three intuitions why batch normalization is not appropriate for image SR tasks. </p><formula xml:id="formula_0">B = x B − E B [x B ] V ar B [x B ] + ,<label>(1)</label></formula><p>where x B is the features of current training batch, is a small value (e.g. 1e-5) to avoid zero-division. The first order and second order statistics are then updated to global statistics in a moving average way:</p><formula xml:id="formula_1">E[x] ← E B [x B ],<label>(2)</label></formula><formula xml:id="formula_2">V ar[x] ← V ar B [x B ],<label>(3)</label></formula><p>where ← means assigning moving average. During inference, these global statistics are used instead to normalize the features:</p><formula xml:id="formula_3">x test = x test − E[x] V ar[x] + .<label>(4)</label></formula><p>As shown in the formulations of BN, it will cause following problems. 1) For image super-resolution, commonly only small image patches (e.g. 48 × 48) and small mini-batch size (e.g. 16) are used to speedup training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, thus the mean and variance of small image patches differ a lot among mini-batches, making theses statistics unstable, which is demonstrated in the section of experiments. 2) BN is also believed to act as a regularizer and in some cases can eliminate the need for Dropout <ref type="bibr" target="#b11">[12]</ref>. However, it is rarely observed that SR networks overfit on training datasets. Instead, many kinds of regularizers, for examples, weight decaying and dropout, are not adopted in SR networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. 3) Unlike image classification tasks where softmax (scale-invariant) is used at the end of networks to make prediction, for image SR, the different formulations of training and testing may deteriorate the accuracy for dense pixel value predictions.</p><p>Weight normalization Weight normalization, on the other hand, is a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. It does not introduce dependencies between the examples in a mini-batch, and has the same formulation in training and testing. Assume the output y is with the form:</p><formula xml:id="formula_4">y = w · x + b,<label>(5)</label></formula><p>where w is a k-dimensional weight vector, b is a scalar bias term, x is a k-dimensional vector of input features. WN re-parameterizes the weight vectors in terms of the new parameters using</p><formula xml:id="formula_5">w = g ||v|| v,<label>(6)</label></formula><p>where v is a k-dimensional vector, g is a scalar, and ||v|| denotes the Euclidean norm of v. With this formalization, we will have ||w|| = g, independent of parameters v. As shown in <ref type="bibr" target="#b24">[25]</ref>, the decouples of length and direction speed up convergence of deep neural networks. And more importantly, for image SR, it does not introduce troubles of BN as described above, since it is just a reparameterization technique and has exact same representation ability.</p><p>It is also noteworthy that introducing WN allows training with higher learning rate (i.e. 10×), and improves both training and testing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Structure</head><p>Figure 2: Demonstration of our simplified SR network compared with EDSR <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this part, we overview the WDSR network architectures. We made two major modifications based on EDSR <ref type="bibr" target="#b18">[19]</ref> super-resolution network.</p><p>Global residual pathway Firstly we find that the global residual pathway is a linear stack of several convolution layers, which is computational expensive. We argue that these linear convolutions are redundant ( <ref type="figure">Fig. 2)</ref> and can be absorbed into residual body to some extent. Thus, we slightly modify the network structure and use single convolution layer with kernel size 5 × 5 that directly take 3 × H × W LR RGB image/patch as input and output 3S 2 × H × W HR counterparts, where S is the scale. This results in less parameters and computation. In our experiments we have not found any accuracy drop with our simpler form.</p><p>Upsampling layer Different from previous state-of-the-arts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> where one or more convolutional layers are inserted after upsampling, our proposed WDSR extracts all features in low-resolution stage <ref type="figure">(Fig. 2</ref>). Empirically we find it does not affect accuracy of SR networks while improves speed by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We train our models on DIV2K dataset <ref type="bibr" target="#b34">[35]</ref> since the dataset is relatively large and contains highquality (2K resolution) images. The default splits of DIV2K dataset consist 800 training images, 100 validation images and 100 testing images. We use 800 training images for training and 10 validation images for validation during training. The trained models are evaluated on 100 validation images (testing images are not publicly available) of DIV2K dataset. We mainly measure PSNR on RGB space. ADAM optimizer <ref type="bibr" target="#b15">[16]</ref> is used with β 1 = 0.9, β 2 = 0.999 and = 10 −8 . The batch size is set to 16. The learning rate is initialized the maximum convergent value (10-4 for models without weight normalization and 10-3 for models with weight normalization). The learning rate is halved at every 2 × 10 5 iterations.</p><p>We crop 96 × 96 RGB input patches from HR image and its bicubic downsampled image as training output-input pairs. Training data is augmented with random horizontal flips and rotations following common data augmentation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. During training, the input images are also subtracted with the mean RGB values of the DIV2K training images.  <ref type="table">Table 1</ref>: Model comparisons at different parameters budgets by controlling the number of residual blocks with fixed number of channels. We mainly compare the number of parameters and validation PSNR to measure efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Wide and Efficient Wider Activation:</head><p>In this part, we show results of baseline model EDSR <ref type="bibr" target="#b18">[19]</ref> and our proposed WDSR-A and WDSR-B for the task of image bicubic x2 super-resolution on DIV2K dataset. To ensure fairness, each model is evaluated at different parameters and computational budgets by controlling the number of residual blocks with fixed number of channels. The results are shown in <ref type="table">Table 1</ref>. We compare each model with its number of residual blocks. The results suggest that our proposed WDSR-A and WDSR-B have better accuracy and efficiency than EDSR <ref type="bibr" target="#b18">[19]</ref>. WDSR-B with wider activation also has better or similar performance compared with WDSR-A, which supports our wide activation hypothesis and demonstrates the effectiveness of our proposed linear low-rank convolution. We also demonstrate the effectiveness of weight normalization for improved training of SR networks. We compare the training and testing accuracy (PSNR) when train the same model with different normalization methods, i.e. weight normalization, batch normalization or no normalization. The results in <ref type="figure" target="#fig_0">Figure 3</ref> show that the model trained with weight normalization has faster convergence and better accuracy. The model trained with batch normalization is unstable during testing, which is likely due to different formulations of BN in training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Normalization layers:</head><p>To further study whether this is because the learning rate is too large for models trained with batch normalization, we also train the same model with different learning rates. The results are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Even with lr = 10 −4 when the training curves are stable, the validation PSNR is still not stable across training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this report, we introduce two super-resolution networks WDSR-A and WDSR-B based on the central idea of wide activation. We demonstrate in our experiments that with same parameter and computation complexity, models with wider features before ReLU activation have better accuracy for single image super-resolution. We also find training with weight normalization leads to better accuracy for deep super-resolution networks comparing to batch normalization or no normalization. The proposed methods may help to other low-level image restoration tasks like denoising and dehazing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Training L1 loss and validation PSNR of same model trained with weight normalization, batch normalization or no normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Training L1 loss and validation PSNR of model trained with batch normalization but different learning rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Then we demonstrate that weight normalization does not have these drawbacks like BN, and it can be effectively used to ease the training difficulty of deep SR networks.</figDesc><table /><note>Batch normalization BN re-calibrates the mean and variance of intermediate features to solve the problem of internal covariate shift [12] in training deep neural networks. It has different formulations in training and testing. For simplicity, here we ignore the re-scaling and re-centering learnable parameters of BN. During training, features in each layer are normalized with mean and variance of the current training mini-batch:x</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the expressive power of deep learning: A tensor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="698" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The power of depth for feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="907" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wide-activated Deep Residual Networks based Restoration for BPG-compressed Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Balanced two-stage residual networks for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1157" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-resolution System for 4K-HDTV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomio</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4453" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flattened convolutional neural networks for feedforward acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In: (</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Why deep neural networks for function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04161</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust single image super-resolution via deep networks with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3194" to="3207" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deconvolution and Checkerboard Artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00003</idno>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-resolution image reconstruction: a technical overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Kyu</forename><surname>Sung Cheol Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moon Gi</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Superresolution in MRI: application to human white matter fiber tract visualization by diffusion tensor imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehezkel</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Magnetic resonance in medicine</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="29" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note>In: ArXiv e-prints. cs.CV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="15" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient subpixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sub-pixel mapping of rural land cover objects from fine spatial resolution satellite sensor imagery using super-resolution pixel-swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Peter M Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="491" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV). IEEE. 2017</title>
		<imprint>
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Studying very low resolution recognition using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4792" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Free-Form Image Inpainting with Gated Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generative Image Inpainting with Contextual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07892</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual Dense Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08797</idno>
	</analytic>
	<monogr>
		<title level="m">ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
