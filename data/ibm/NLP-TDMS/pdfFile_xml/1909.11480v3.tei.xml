<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INPUT COMPLEXITY AND OUT-OF-DISTRIBUTION DETECTION WITH LIKELIHOOD-BASED GENERATIVE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
							<email>joan.serra@dolby.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dolby Laboratories</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Telefónica Research</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davidálvarez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Telefónica Research</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicenç</forename><surname>Gómez</surname></persName>
							<email>vicen.gomez@upf.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Slizovskaia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Telefónica Research</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">F</forename><surname>Núñez</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Luque</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Telefónica Research</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">INPUT COMPLEXITY AND OUT-OF-DISTRIBUTION DETECTION WITH LIKELIHOOD-BASED GENERATIVE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Likelihood-based generative models are a promising resource to detect out-ofdistribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates.</p><p>Recent works, however, have shown that likelihoods derived from generative models fail to distinguish between training data and some OOD input types <ref type="bibr" target="#b2">(Choi et al., 2018;</ref><ref type="bibr" target="#b20">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b7">Hendrycks et al., 2019)</ref>. This occurs for different likelihood-based generative models, and even when inputs are unrelated to training data or have totally different semantics. For instance, when 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Assessing whether input data is novel or significantly different than the one used in training is critical for real-world machine learning applications. Such data are known as out-of-distribution (OOD) inputs, and detecting them should facilitate safe and reliable model operation. This is particularly necessary for deep neural network classifiers, which can be easily fooled by OOD data <ref type="bibr" target="#b24">(Nguyen et al., 2015)</ref>. Several approaches have been proposed for OOD detection on top of or within a neural network classifier <ref type="bibr" target="#b6">(Hendrycks &amp; Gimpel, 2017;</ref><ref type="bibr" target="#b13">Lakshminarayanan et al., 2017;</ref><ref type="bibr" target="#b16">Liang et al., 2018;</ref><ref type="bibr" target="#b15">Lee et al., 2018)</ref>. Nonetheless, OOD detection is not limited to classification tasks nor to labeled data sets. Two examples of that are novelty detection from an unlabeled data set and next-frame prediction from video sequences.</p><p>A rather obvious strategy to perform OOD detection in the absence of labels (and even in the presence of them) is to learn a density model M that approximates the true distribution p * (X ) of training inputs x ∈ X <ref type="bibr" target="#b1">(Bishop, 1994)</ref>. Then, if such approximation is good enough, that is, p(x|M) ≈ p * (x), OOD inputs should yield a low likelihood under model M. With complex data like audio or images, this strategy was long thought to be unattainable due to the difficulty of learning a sufficiently good model. However, with current approaches, we start having generative models that are able to learn good approximations of the density conveyed by those complex data. Autoregressive and invertible models such as PixelCNN++ <ref type="bibr" target="#b28">(Salimans et al., 2017)</ref> and Glow <ref type="bibr" target="#b9">(Kingma &amp; Dhariwal, 2018)</ref> perform well in this regard and, in addition, can approximate p(x|M) with arbitrary accuracy.  <ref type="figure">Figure 1</ref>: Likelihoods from a Glow model trained on CIFAR10. Qualitatively similar results are obtained for other generative models and data sets (see also results in <ref type="bibr" target="#b2">Choi et al., 2018;</ref><ref type="bibr" target="#b20">Nalisnick et al., 2019a)</ref>. trained on CIFAR10, generative models report higher likelihoods for SVHN than for CIFAR10 itself ( <ref type="figure">Fig. 1</ref>; data descriptions are available in Appendix A). Intriguingly, this behavior is not consistent across data sets, as other ones correctly tend to produce likelihoods lower than the ones of the training data (see the example of TrafficSign in <ref type="figure">Fig. 1</ref>). A number of explanations have been suggested for the root cause of this behavior <ref type="bibr" target="#b2">(Choi et al., 2018;</ref><ref type="bibr" target="#b20">Nalisnick et al., 2019a;</ref><ref type="bibr" target="#b26">Ren et al., 2019)</ref> but, to date, a full understanding of the phenomenon remains elusive.</p><p>In this paper, we shed light to the above phenomenon, showing that likelihoods computed from generative models exhibit a strong bias towards the complexity of the corresponding inputs. We find that qualitatively complex images tend to produce the lowest likelihoods, and that simple images always yield the highest ones. In fact, we show a clear negative correlation between quantitative estimates of complexity and the likelihood of generative models. In the second part of the paper, we propose to leverage such estimates of complexity to detect OOD inputs. To do so, we introduce a widely-applicable OOD score for individual inputs that corresponds, conceptually, to a likelihoodratio test statistic. We show that such score turns likelihood-based generative models into practical and effective OOD detectors, with performances comparable to, or even better than the state-of-theart. We base our experiments on an extensive collection of alternatives, including a pool of 12 data sets, two conceptually-different generative models, increasing model sizes, and three variants of complexity estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">COMPLEXITY BIAS IN LIKELIHOOD-BASED GENERATIVE MODELS</head><p>From now on, we shall consider the log-likelihood of an input x given a model M: M (x) = log 2 p(x|M). Following common practice in evaluating generative models, negative log-likelihoods − M will be expressed in bits per dimension <ref type="bibr" target="#b30">(Theis et al., 2016)</ref>, where dimension corresponds to the total size of x (we resize all images to 3×32×32 pixels). Note that the qualitative behavior of log-likelihoods is the same as likelihoods: ideally, OOD inputs should have a low M , while in-distribution data should have a larger M .</p><p>Most literature compares likelihoods of a given model for a few data sets. However, if we consider several different data sets at once and study their likelihoods, we can get some insight. In <ref type="figure">Fig. 2</ref>, we show the log-likelihood distributions for the considered data sets (Appendix A), computed with a Glow model trained on CIFAR10. We observe that the data set with a higher log-likelihood is Constant, a data set of constant-color images, followed by Omniglot, MNIST, and FashionMNIST; all of those featuring gray-scale images with a large presence of empty black background. On the other side of the spectrum, we observe that the data set with a lower log-likelihood is Noise, a data set of uniform random images, followed by TrafficSign and TinyImageNet; both featuring colorful images with non-trivial background. Such ordering is perhaps more clear by looking at the average log-likelihood of each data set (Appendix D). If we think about the visual complexity of the images in those data sets, it would seem that log-likelihoods tend to grow when images become simpler and with less information or content.  To further confirm the previous observation, we design a controlled experiment where we can set different decreasing levels of image complexity. We train a generative model with some data set, as before, but now compute likelihoods of progressively simpler inputs. Such inputs are obtained by average-pooling the uniform random Noise images by factors of 1, 2, 4, 8, 16, and 32, and re-scaling back the images to the original size by nearest-neighbor up-sampling. Intuitively, a noise image with a pooling size of 1 (no pooling) has the highest complexity, while a noise image with a pooling of 32 (constant-color image) has the lowest complexity. Pooling factors from 2 to 16 then account for intermediate, decreasing levels of complexity. The result of the experiment is a progressive growing of the log-likelihood M <ref type="figure" target="#fig_0">(Fig. 3)</ref>. Given that the only difference between data is the pooling factor, we can infer that image complexity plays a major role in generative models' likelihoods.</p><p>Until now, we have consciously avoided a quantitative definition of complexity. However, to further study the observed phenomenon, and despite the difficulty in quantifying the multiple aspects that affect the complexity of an input (cf. <ref type="bibr" target="#b18">Lloyd, 2001)</ref>, we have to adopt one. A sensible choice would be to exploit the notion of Kolmogorov complexity <ref type="bibr" target="#b10">(Kolmogorov, 1963)</ref> which, unfortunately, is noncomputable. In such cases, we have to deal with it by calculating an upper bound using a lossless compression algorithm <ref type="bibr" target="#b3">(Cover &amp; Thomas, 2006)</ref>. Given a set of inputs x coded with the same bit depth, the normalized size of their compressed versions, L(x) (in bits per dimension), can be considered a reasonable estimate of their complexity. That is, given the same coding depth, a highly complex input will require more bits per dimension, while a less complex one will be compressed with fewer bits per dimension. For images, we can use PNG, JPEG2000, or FLIF compressors (Appendix C). For other data types such as audio or text, other lossless compressors should be available to produce a similar estimate.   If we study the relation between generative models' likelihoods and our complexity estimates, we observe that there is a clear negative correlation ( <ref type="figure" target="#fig_2">Fig. 4</ref>). Considering all data sets, we find Pearson's correlation coefficients below −0.75 for models trained on FashionMNIST, and below −0.9 for models trained on CIFAR10, independently of the compressor used (Appendix D). Such significant correlations, all of them with infinitesimal p-values, indicate that likelihood-based measures are highly influenced by the complexity of the input image, and that this concept accounts for most of their variance. In fact, such strong correlations suggest that one may replace the computed likelihood values for the negative of the complexity estimate and obtain almost the same result (Appendix D). This implies that, in terms of detecting OOD inputs, a complexity estimate would perform as well (or bad) as the likelihoods computed from our generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TESTING OUT-OF-DISTRIBUTION INPUTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DEFINITION</head><p>As complexity seems to account for most of the variability in generative models' likelihoods, we propose to compensate for it when testing for possible OOD inputs. Given that both negative loglikelihoods − M (x) and the complexity estimate L(x) are expressed in bits per dimension (Sec. 2), we can express our OOD score as a subtraction between the two:</p><formula xml:id="formula_0">S(x) = − M (x) − L(x).<label>(1)</label></formula><p>Notice that, since we use negative log-likelihoods, the higher the S, the more OOD the input x will be (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INTERPRETATION: OCCAM'S RAZOR AND THE OUT-OF-DISTRIBUTION PROBLEM</head><p>Interestingly, S can be interpreted as a likelihood-ratio test statistic. For that, we take the point of view of Bayesian model comparison or minimum description length principle <ref type="bibr" target="#b19">(MacKay, 2003)</ref>. We can think of a compressor M 0 as a universal model, adjusted for all possible inputs and general enough so that it is not biased towards a particular type of data semantics. Considering the probabilistic model associated with the size of the output produced by the lossless compressor, we have p(x|M 0 ) = 2 −L(x) and, correspondingly,</p><formula xml:id="formula_1">L(x) = − log 2 p(x|M 0 ).<label>(2)</label></formula><p>In Bayesian model comparison, we are interested in comparing the posterior probabilities of different models in light of data X . In our setting, the trained generative model M is a 'simpler' version of the universal model M 0 , targeted to a specific semantics or data type. With it, one aims to approximate the marginal likelihood (or model evidence) for x ∈ X , which integrates out all model parameters:</p><formula xml:id="formula_2">p(x|M) = p(x|θ, M)p(θ|M)dθ.</formula><p>This integral is intractable, but current generative models can approximate p(x|M) with arbitrary accuracy <ref type="bibr" target="#b9">(Kingma &amp; Dhariwal, 2018)</ref>. Choosing between one or another model is then reduced to a simple likelihood ratio:</p><formula xml:id="formula_3">log 2 p(M 0 |x) p(M|x) = log 2 p(x|M 0 )p(M 0 ) p(x|M)p(M) .<label>(3)</label></formula><p>For uniform priors p(M 0 ) = p(M) = 1/2, this ratio is reduced to</p><formula xml:id="formula_4">S(x) = − log 2 p(x|M) + log 2 p(x|M 0 )</formula><p>which, using Eq. 2 for the last term, becomes Eq. 1.</p><p>The ratio S accommodates the Occam's razor principle. Consider simple inputs that can be easily compressed by M 0 using a few bits, and that are not present in the training of M. These cases have a high probability under M 0 , effectively correcting the abnormal high likelihood given by the learned model M. The same effect will occur with complex inputs that are not present in the training data. In these cases, both likelihoods will be low, but the universal lossless compressor M 0 will predict those better than the learned model M. The two situations will lead to large values of S. In contrast, inputs that belong to the data used to train the generative model M will always be better predicted by M than by M 0 , resulting in lower values of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">USING S IN PRACTICE</head><p>Given a training set X of in-distribution samples and the corresponding scores S(x) for each x ∈ X , we foresee a number of strategies to perform OOD detection in practice for new instances z. The first and more straightforward one is to use S(z) as it is, just as a score, to perform OOD ranking. This can be useful to monitor the top-k, potentially more problematic instances z in a new set of unlabeled data Z. The second strategy is to interpret S(z) as the corresponding Bayes factor in Eq. 3, and directly assign z to be OOD for S(z) &gt; 0, or in-distribution otherwise <ref type="bibr">(cf. MacKay, 2003)</ref>. The decision is then taken with stronger evidence for higher absolute values of S(z). A third strategy is to consider the empirical or null distribution of S for the full training set, S(X ). We could then choose an appropriate quantile as threshold, adopting the notion of frequentist hypothesis testing (see for instance <ref type="bibr" target="#b21">Nalisnick et al., 2019b)</ref>. Finally, if ground truth OOD data Y is available, a fourth strategy is to optimize a threshold value for S(z). Using X and Y, we can choose a threshold that targets a desired percentage of false positives or negatives.</p><p>The choice of a specific strategy will depend on the characteristics of the particular application under consideration. In this work, we prefer to keep our evaluation generic and to not adopt any specific thresholding strategy (that is, we use S directly, as a score). This also allows us to compare with the majority of reported values from the existing literature, which use the AUROC measure (see below). <ref type="bibr" target="#b26">Ren et al. (2019)</ref> have recently proposed the use of likelihood-ratio tests for OOD detection. They posit that "background statistics" (for instance, the number of zeros in the background of MNISTlike images) are the source of abnormal likelihoods, and propose to exploit them by learning a background model which is trained on random surrogates of input data. Such surrogates are generated according to a Bernoulli distribution, and an L2 regularization term is added to the background model, which implies that the approach has two hyper-parameters. Moreover, both the background model and the model trained using in-distribution data need to capture the background information equally well. In contrast to their method, our method does not require additional training nor extra conditions on a specific background model for every type of training data. <ref type="bibr" target="#b2">Choi et al. (2018)</ref> and <ref type="bibr" target="#b21">Nalisnick et al. (2019b)</ref> suggest that typicality is the culprit for likelihoodbased generative models not being able to detect OOD inputs. While <ref type="bibr" target="#b2">Choi et al. (2018)</ref> do not explicitly address typicality, their estimate of the Watanabe-Akaike information criterion using ensembles of generative models performs well in practice. <ref type="bibr" target="#b21">Nalisnick et al. (2019b)</ref> propose an explicit test for typicality employing a Monte Carlo estimate of the empirical entropy, which limits their approach to batches of inputs of the same type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORKS</head><p>The works of Høst-Madsen et al. <ref type="formula" target="#formula_0">(2019)</ref> and  combine the concepts of typicality and minimum description length to perform novelty detection. Although concepts are similar to the ones employed here, their focus is mainly on bit sequences. They consider atypical sequences those that can be described (coded) with fewer bits in itself rather than using the (optimum) code for typical sequences. We find their implementation to rely on strong parametric assumptions, which makes it difficult to generalize to generative or other machine learning models.</p><p>A number of methods have been proposed to perform OOD detection under a classification-based framework <ref type="bibr" target="#b6">(Hendrycks &amp; Gimpel, 2017;</ref><ref type="bibr" target="#b13">Lakshminarayanan et al., 2017;</ref><ref type="bibr" target="#b16">Liang et al., 2018;</ref><ref type="bibr" target="#b15">Lee et al., 2018;</ref><ref type="bibr" target="#b7">Hendrycks et al., 2019)</ref>. Although achieving promising results, these methods do not generally apply to the more general case of non-labeled or self-supervised data. The method of <ref type="bibr" target="#b7">Hendrycks et al. (2019)</ref> extends to such cases by leveraging generative models, but nonetheless makes use of auxiliary, outlier data to learn to distinguish OOD inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We now study how S performs on the OOD detection task. For that, we train a generative model M on the train partition of a given data set and compute scores for such partition and the test partition of a different data set. With both sets of scores, we then calculate the area under the receiver operating characteristic curve (AUROC), which is a common evaluation measure for the OOD detection task <ref type="bibr" target="#b7">(Hendrycks et al., 2019)</ref> and for classification tasks in general. Note that AUROC represents a good performance summary across different score thresholds <ref type="bibr" target="#b5">(Fawcett, 2005)</ref>.</p><p>First of all, we want to assess the improvement of S over log-likelihoods alone (− M ). When considering likelihoods from generative models trained on CIFAR10, the problematic results reported by previous works become clearly apparent <ref type="table" target="#tab_3">(Table 1</ref>). The unintuitive higher likelihoods for SVHN observed in Sec. 1 now translate into a poor AUROC below 0.1. This not only happens for SVHN, but also for Constant, Omniglot, MNIST, and FashionMNIST data sets, for which we observed consistently higher likelihoods than CIFAR10 in Sec. 2. Likelihoods for the other data sets yield AUROCs above the random baseline of 0.5, but none above 0.67. The only exception is the Noise data set, which is perfectly distinguishable from CIFAR10 using likelihood alone. For completeness, we include the AUROC values when trying to perform OOD with the test partition of CIFAR10. We see those are close to the ideal value of 0.5, showing that, as expected, the reported measures do not generally consider those samples to be OOD.</p><p>We now look at the AUROCs obtained with S <ref type="table" target="#tab_3">(Table 1)</ref>. We see that, not only results are reversed for less complex datasets like MNIST or SVHN, but also that all AUROCs for the rest of the data sets improve as well. The only exception to the last assertion among all studied combinations is the combination of TinyImageNet with PixelCNN++ and FLIF (see Appendix D for other combinations). In general, we obtain AUROCs above 0.7, with many of them approaching 0.9 or 1. Thus, we can conclude that S clearly improves over likelihoods alone in the OOD detection task, and that S is able to revert the situation with intuitively less complex data sets that were previously yielding a low AUROC.</p><p>We also study how the training set, the choice of compressor/generative model, or the size of the model affects the performance of S (Appendix D). In terms of models and compressors, we do not observe a large difference between the considered combinations, except for a few isolated cases whose investigation we defer for future work. In terms of model size, we do observe a tendency to provide better discrimination with increasing size. In terms of data sets, we find the OOD detection task to be easier with FashionMNIST than with CIFAR10. We assume that this is due to the ease of the generative model to learn and approximate the density conveyed by the data. A similar but less marked trend is also observed for compressors, with better compressors yielding slightly improved AUROCs than other, in principle, less powerful ones. A takeaway from all these observations would be that using larger generative models and better compressors will yield a more reliable S and a better AUROC. The conducted experiments support that, but a more in-depth analysis should be carried out to further confirm this hypothesis.</p><p>Finally, we want to assess how S compares to previous approaches in the literature. For that, we compile a number of reported AUROCs for both classifier-and generative-based approaches and compare them with S. Note that classifier-based approaches, as mentioned in Sec. 1, are less applicable than generative-based ones. In addition, as they exploit label information, they might have an advantage over generative-based approaches in terms of performance (some also exploit external or outlier data; Sec. 4).</p><p>We observe that S is competitive with both classifier-and existing generative-based approaches ( <ref type="table" target="#tab_4">Table 2)</ref>. When training with FashionMNIST, S achieves the best scores among all considered approaches. The results with further test sets are also encouraging, with almost all AUROCs approaching 1 (Appendix D). When training with CIFAR10, S achieves similar or better performance than existing approaches. Noticeably, within generative-based approaches, S is only outperformed in two occasions by the same approach, WAIC, which uses ensembles of generative models (Sec. 4).</p><p>On the one hand, it would be interesting to see how S could perform when using ensembles of models and compressors to produce better estimates of − M and L, respectively. On the other hand, however, the use of a single generative model together with a single fast compression library makes S an efficient alternative compared to WAIC and some other existing approaches. It is also worth noting that many existing approaches have a number of hyper-parameters that need to be tuned, sometimes with the help of outlier or additional data. In contrast, S is a parameter-free measure, which makes it easy to use and deploy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We illustrate a fundamental insight with regard to the use of generative models' likelihoods for the task of detecting OOD data. We show that input complexity has a strong effect in those likelihoods, and pose that it is the main culprit for the puzzling results of using generative models' likelihoods for OOD detection. In addition, we show that an estimate of input complexity can be used to com-  <ref type="bibr" target="#b26">Ren et al. (2019)</ref>, (b) by <ref type="bibr" target="#b15">Lee et al. (2018)</ref>, and (c) by <ref type="bibr" target="#b2">Choi et al. (2018)</ref>. Results for Typicality test correspond to using batches of 2 samples of the same type.</p><p>Trained on: FashionMNIST CIFAR10 OOD data:</p><p>MNIST Omniglot SVHN CelebA CIFAR100 Classifier-based approaches ODIN <ref type="bibr">(Liang et al., 2018) a,b</ref> 0.697 -0.966 --VIB <ref type="bibr">(Alemi et al., 2018) c</ref> 0.941 0.943 0.528 0.735 -Mahalanobis <ref type="bibr" target="#b15">(Lee et al., 2018)</ref> 0.986 -0.991 --Outlier exposure <ref type="bibr" target="#b7">(Hendrycks et al., 2019)</ref> --0.984 -0.933 Generative-based approaches WAIC <ref type="bibr" target="#b2">(Choi et al., 2018)</ref> 0.766 0.796 1.000 0.997 -Outlier exposure <ref type="bibr" target="#b7">(Hendrycks et al., 2019)</ref> --0.758 -0.685 Typicality test <ref type="bibr" target="#b21">(Nalisnick et al., 2019b)</ref> 0.140 -0.420 --Likelihood-ratio <ref type="bibr" target="#b26">(Ren et al., 2019)</ref> 0.997 -0.912 --S using Glow and FLIF (ours) 0.998 1.000 0.950 0.863 0.736 S using PixelCNN++ and FLIF (ours) 0.967 1.000 0.929 0.776 0.535 pensate standard negative log-likelihoods in order to produce an efficient and reliable OOD score. We also offer an interpretation of our score as a likelihood-ratio akin to Bayesian model comparison. Such score performs comparably to, or even better than several state-of-the-art approaches, with results that are consistent across a range of data sets, models, model sizes, and compression algorithms. The proposed score has no hyper-parameters besides the definition of a generative model and a compression algorithm, which makes it easy to employ in a variety of practical problems and situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DATA SETS</head><p>In our experiments, we employ well-known, publicly-available data sets. In addition to those, and to facilitate a better understanding of the problem, we develop another two self-created sets of synthetic images: Noise and Constant images. The Noise data set is created by uniformly randomly sampling a tensor of 3×32×32 and quantizing the result to 8 bits. The Constant data set is created similarly, but using a tensor of 3×1×1 and repeating the values along the last two dimensions to obtain a size of 3×32×32. The complete list of data sets is available in <ref type="table" target="#tab_5">Table 3</ref>. In the case of data sets with different variations, such as CelebA or FaceScrub, which have both plain and aligned versions of the faces, we select the aligned versions. Note that, for models trained on CIFAR10, it is important to notice the overlap of certain classes between that and other sets, namely TinyImageNet and CIFAR100 (they overlap, for instance, in classes of certain animals or vehicles). Therefore, strictly speaking, such data sets are not entirely OOD, at least semantically.  <ref type="bibr" target="#b31">(Xiao et al., 2017)</ref> 1×28×28 10 70,000 SVHN <ref type="bibr" target="#b22">(Netzer et al., 2011)</ref> 3× 32×32 10 99,289 CIFAR10 <ref type="bibr" target="#b11">(Krizhevsky, 2009)</ref> 3×32×32 10 60,000 CIFAR100 <ref type="bibr" target="#b11">(Krizhevsky, 2009)</ref> 3×32×32 100 60,000 CelebA <ref type="bibr" target="#b17">(Liu et al., 2015)</ref> 3×178×218 10,177 182,732 FaceScrub <ref type="bibr" target="#b23">(Ng &amp; Winkler, 2014)</ref> 3×300×300 530 91,712 TinyImageNet <ref type="bibr" target="#b4">(Deng et al., 2009)</ref> 3×64×64 200 100,000 TrafficSign <ref type="bibr" target="#b29">(Stallkamp et al., 2011)</ref> 3×32×32 43 51,839 Noise (Synthetic) 3×32×32 1 40,000</p><p>In order to split the data between train, validation, and test, we follow two simple rules: (1) if the data set contains some predefined train and test splits, we respect them and create a validation split using a random 10% of the training data;</p><p>(2) if no predefined splits are available, we create them by randomly assigning 80% of the data to the train split and 10% to both validation and test splits. In order to create consistent input sizes for the generative models, we work with 3-channel images of size 32×32. For those data sets which do not match this configuration, we follow a classic bi-linear resizing strategy and, to simulate the three color components from a gray-scale image, we triplicate the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MODELS AND TRAINING</head><p>The results of this paper are obtained using two generative models of different nature: one autoregressive model and one invertible model. As autoregressive model we choose PixelCNN++ <ref type="bibr" target="#b28">(Salimans et al., 2017)</ref>, which has been shown to obtain very good results in terms of likelihood for image data. As invertible model we choose Glow <ref type="bibr" target="#b9">(Kingma &amp; Dhariwal, 2018)</ref>, which is also capable of inferring exact log-likelihoods using large stacks of bijective transformations. We implement the Glow model using the default configuration of the original implementation 1 , except that we zeropad and do not use ActNorm inside the coupling network. The model has 3 blocks of 32 flows, using an affine coupling with an squeezing factor of 2. As for PixelCNN++, we set 5 residual blocks per stage, with 80 filters and 10 logistic components in the mixture. The non-linearity of the residual layers corresponds to an exponential linear unit 2 .</p><p>We train both Glow and PixelCNN++ using the Adam optimizer with an initial learning rate of 10 −4 . We reduce this initial value by a factor of 1/5 every time that the validation loss does not decrease during 5 consecutive epochs. The training finishes when the learning rate is reduced by factor of 1/100. The batch size of both models is set to 50. The final model weights are the ones yielding the best validation loss. The likelihoods obtained in validation with both Glow and PixelCNN++ match the ones reported in the literature for CIFAR10 <ref type="bibr" target="#b9">(Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b28">Salimans et al., 2017)</ref>.</p><p>We also make sure that the generated images are of comparable quality to the ones shown in those references.</p><p>We use PyTorch version 1.2.0 <ref type="bibr" target="#b25">(Paszke et al., 2017)</ref>. All models have been trained with a single NVIDIA GeForce GTX 1080Ti GPU. Training takes some hours under that setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPRESSORS AND COMPLEXITY ESTIMATE</head><p>We explore three different options to compress input images. As a mandatory condition, they need to provide lossless compression. The first format that we consider is PNG, and old-classic format which is globally used and well-known. We use OpenCV 3 to compress from raw Numpy matrices, with compression set to the maximum possible level. The second format that we consider is JPEG2000. Although not as globally known as the previous one, it is a more modern format with several new generation features such as progressive decoding. Again, we use the default OpenCV implementation to obtain the size of an image using this compression algorithm. The third format that we consider is FLIF, the most modern algorithm of the list. According to its website 4 , it promises to generate up to 53% smaller files than JPEG2000. We use the publicly-available compressor implementation in their website. We do not include header sizes in the measurement of the resulting bits per dimension.</p><p>To compute our complexity estimate L(x), we compress the input x with one of the compressors C above. With that, we obtain a string of bits C(x). The length of it, |C(x)|, is normalized by the size or dimensionality of x, which we denote by d, to obtain the complexity estimate:</p><formula xml:id="formula_5">L(x) = |C(x)| d .</formula><p>We also experimented with an improved version of L,</p><formula xml:id="formula_6">L (x) = min (L 1 (x), L 2 (x), . . . ) ,</formula><p>where L i corresponds to different compression schemes. This forces S to work always with the best compressor for every x. In our case, as FLIF was almost always the best compressor, we did not observe a clear difference between using L or L . However, in cases where it is not clear which compressor to use or cases in which we do not have a clear best/winner, L could be of use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS</head><p>The additional results mentioned in the main paper are the following:</p><p>• In <ref type="table" target="#tab_6">Table 4</ref>, we report the average log-likelihood M for every data set. We sort data sets from highest to lowest log-likelihood. • In <ref type="table" target="#tab_7">Table 5</ref>, we report the global Pearson's correlation coefficient for different models, train sets, and compressors. Due to the large sample size, Scipy version 1.2.1 reports a p-value of 0 in all cases. • In <ref type="table" target="#tab_8">Table 6</ref>, we report the AUROC values obtained from log-likelihoods M , complexity estimates L, a simple two-tail test T taking into account lower and higher log-likelihoods, T = | M − M |, and the proposed score S.</p><p>• In <ref type="table" target="#tab_9">Table 7</ref>, we report the AUROC values obtained from S across different Glow model sizes, using a PNG compressor. • In <ref type="table" target="#tab_10">Table 8</ref>, we report the AUROC values obtained from S across different data sets, models, and compressors.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Pooled-image log-likelihoods obtained from a Glow model trained on CIFAR10. Qualitatively similar results are obtained for a PixelCNN++ model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Normalized compressed lengths using a PNG compressor with respect to likelihoods of a PixelCNN++ model trained on CIFAR10 (for visualization purposes we here employ a sample of 200 images per data set). Similar results are obtained for a Glow model and other compressors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>AUROC values using negative log-likelihood − M and the proposed score S for Glow and PixelCNN++ models trained on CIFAR10 using the FLIF compressor. Results for models trained on FashionMNIST and other compressors are available in Appendix D.</figDesc><table><row><cell>Data set</cell><cell>Glow</cell><cell></cell><cell cols="2">PixelCNN++</cell></row><row><cell></cell><cell>− M</cell><cell>S</cell><cell>− M</cell><cell>S</cell></row><row><cell>Constant</cell><cell>0.024</cell><cell>1.000</cell><cell>0.006</cell><cell>1.000</cell></row><row><cell>Omniglot</cell><cell>0.001</cell><cell>1.000</cell><cell>0.001</cell><cell>1.000</cell></row><row><cell>MNIST</cell><cell>0.001</cell><cell>1.000</cell><cell>0.002</cell><cell>1.000</cell></row><row><cell>FashionMNIST</cell><cell>0.010</cell><cell>1.000</cell><cell>0.013</cell><cell>1.000</cell></row><row><cell>SVHN</cell><cell>0.083</cell><cell>0.950</cell><cell>0.083</cell><cell>0.929</cell></row><row><cell>CIFAR100</cell><cell>0.582</cell><cell>0.736</cell><cell>0.526</cell><cell>0.535</cell></row><row><cell>CelebA</cell><cell>0.621</cell><cell>0.863</cell><cell>0.624</cell><cell>0.776</cell></row><row><cell>FaceScrub</cell><cell>0.646</cell><cell>0.859</cell><cell>0.643</cell><cell>0.760</cell></row><row><cell>TinyImageNet</cell><cell>0.663</cell><cell>0.716</cell><cell>0.642</cell><cell>0.589</cell></row><row><cell>TrafficSign</cell><cell>0.609</cell><cell>0.931</cell><cell>0.599</cell><cell>0.870</cell></row><row><cell>Noise</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>CIFAR10 (test)</cell><cell>0.564</cell><cell>0.618</cell><cell>0.506</cell><cell>0.514</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of AUROC values for the OOD detection task. Results as reported by the original references except (a) by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Summary of the considered data sets.</figDesc><table><row><cell>Data set</cell><cell>Original size</cell><cell>Num. classes</cell><cell>Num. images</cell></row><row><cell>Constant (Synthetic)</cell><cell>3×32×32</cell><cell>1</cell><cell>40,000</cell></row><row><cell>Omniglot (Lake et al., 2015)</cell><cell>1×105×105</cell><cell>1,623</cell><cell>32,460</cell></row><row><cell>MNIST (LeCun et al., 2010)</cell><cell>1×28×28</cell><cell>10</cell><cell>70,000</cell></row><row><cell>FashionMNIST</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average log-likelihoods from a PixelCNN++ model trained on CIFAR10.</figDesc><table><row><cell>Data set</cell><cell>M</cell></row><row><cell>Constant (Test)</cell><cell>−0.25</cell></row><row><cell>Omniglot (Test)</cell><cell>−0.43</cell></row><row><cell>MNIST (Test)</cell><cell>−0.55</cell></row><row><cell>FashionMNIST (Test)</cell><cell>−0.83</cell></row><row><cell>SVHN (Test)</cell><cell>−1.19</cell></row><row><cell>CIFAR10 (Train)</cell><cell>−2.20</cell></row><row><cell>CIFAR10 (Test)</cell><cell>−2.21</cell></row><row><cell>CIFAR100 (Test)</cell><cell>−2.27</cell></row><row><cell>CelebA (Test)</cell><cell>−2.42</cell></row><row><cell>FaceScrub (Test)</cell><cell>−2.43</cell></row><row><cell>TinyImageNet (Test)</cell><cell>−2.51</cell></row><row><cell>TrafficSign (Test)</cell><cell>−2.51</cell></row><row><cell>Noise (Test)</cell><cell>−8.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Pearson's correlation coefficient between normalized compressed length, using different compressors, and model likelihood. All correlations are statistically significant (see text).</figDesc><table><row><cell>Model</cell><cell>Trained with</cell><cell></cell><cell>Compressor</cell><cell></cell></row><row><cell></cell><cell></cell><cell>PNG</cell><cell>JPEG2000</cell><cell>FLIF</cell></row><row><cell>Glow</cell><cell>FashionMNIST</cell><cell>−0.77</cell><cell>−0.75</cell><cell>−0.77</cell></row><row><cell>PixelCNN++</cell><cell>FashionMNIST</cell><cell>−0.77</cell><cell>−0.77</cell><cell>−0.78</cell></row><row><cell>Glow</cell><cell>CIFAR10</cell><cell>−0.94</cell><cell>−0.90</cell><cell>−0.90</cell></row><row><cell>PixelCNN++</cell><cell>CIFAR10</cell><cell>−0.96</cell><cell>−0.94</cell><cell>−0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>AUROC values using negative log-likelihood − M , the complexity measure L, a simple two-tail test T (see text), and our score S for Glow and PixelCNN++ models trained on CIFAR10 and using a PNG compressor. Qualitatively similar results were obtained for FashionMNIST and other compressors.</figDesc><table><row><cell>Data set</cell><cell></cell><cell>Glow</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PixelCNN++</cell></row><row><cell></cell><cell>− M</cell><cell>L</cell><cell>T</cell><cell>S</cell><cell>− M</cell><cell>L</cell><cell>T</cell><cell>S</cell></row><row><cell>Constant</cell><cell cols="4">0.024 0.000 0.963 1.000</cell><cell cols="4">0.006 0.000 0.987 1.000</cell></row><row><cell>Omniglot</cell><cell cols="4">0.001 0.000 0.999 1.000</cell><cell cols="4">0.001 0.000 0.995 1.000</cell></row><row><cell>MNIST</cell><cell cols="4">0.001 0.000 0.998 1.000</cell><cell cols="4">0.002 0.000 0.992 1.000</cell></row><row><cell>FashionMNIST</cell><cell cols="4">0.010 0.003 0.987 1.000</cell><cell cols="4">0.013 0.003 0.966 1.000</cell></row><row><cell>SVHN</cell><cell cols="4">0.083 0.077 0.845 0.950</cell><cell cols="4">0.083 0.077 0.832 0.929</cell></row><row><cell>CIFAR100</cell><cell cols="4">0.582 0.483 0.576 0.736</cell><cell cols="4">0.526 0.483 0.540 0.535</cell></row><row><cell>CelebA</cell><cell cols="4">0.621 0.414 0.458 0.863</cell><cell cols="4">0.624 0.414 0.414 0.776</cell></row><row><cell>FaceScrub</cell><cell cols="4">0.646 0.452 0.472 0.859</cell><cell cols="4">0.643 0.452 0.425 0.760</cell></row><row><cell>TinyImageNet</cell><cell cols="4">0.663 0.548 0.585 0.716</cell><cell cols="4">0.642 0.548 0.544 0.589</cell></row><row><cell>TrafficSign</cell><cell cols="4">0.609 0.356 0.689 0.931</cell><cell cols="4">0.599 0.357 0.657 0.870</cell></row><row><cell>Noise</cell><cell cols="4">1.000 1.000 1.000 1.000</cell><cell cols="4">1.000 1.000 1.000 1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>AUROC values for S using a Glow model trained on CIFAR10 and a PNG compressor. Results for different, increasing sizes of the model (blocks × flow steps). Qualitatively similar results are obtained for other compressors.</figDesc><table><row><cell>Data set</cell><cell>2×16</cell><cell>3×16</cell><cell>3×32</cell></row><row><cell>Constant</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>Omniglot</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>MNIST</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>FashionMNIST</cell><cell>0.997</cell><cell>0.998</cell><cell>1.000</cell></row><row><cell>SVHN</cell><cell>0.765</cell><cell>0.783</cell><cell>0.950</cell></row><row><cell>CIFAR100</cell><cell>0.641</cell><cell>0.685</cell><cell>0.736</cell></row><row><cell>CelebA</cell><cell>0.741</cell><cell>0.794</cell><cell>0.863</cell></row><row><cell>FaceScrub</cell><cell>0.697</cell><cell>0.755</cell><cell>0.859</cell></row><row><cell>TinyImageNet</cell><cell>0.664</cell><cell>0.715</cell><cell>0.716</cell></row><row><cell>TrafficSign</cell><cell>0.946</cell><cell>0.957</cell><cell>0.931</cell></row><row><cell>Noise</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison of AUROC values for the proposed OOD score S using different compressors: Glow and PixelCNN++ models trained on FashionMNIST (top) and CIFAR10 (bottom).</figDesc><table><row><cell>Data set</cell><cell></cell><cell>Glow</cell><cell></cell><cell></cell><cell>PixelCNN++</cell><cell></cell></row><row><cell></cell><cell>PNG</cell><cell>JPEG2000</cell><cell>FLIF</cell><cell>PNG</cell><cell>JPEG2000</cell><cell>FLIF</cell></row><row><cell>Constant</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>Omniglot</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>MNIST</cell><cell>0.841</cell><cell>0.493</cell><cell>0.997</cell><cell>0.821</cell><cell>0.687</cell><cell>0.967</cell></row><row><cell>SVHN</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>CIFAR10</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>0.998</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>CIFAR100</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>0.997</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>CelebA</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>FaceScrub</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>TinyImageNet</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>TrafficSign</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>Noise</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>Constant</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>Omniglot</cell><cell>1.000</cell><cell>0.994</cell><cell>1.000</cell><cell>1.000</cell><cell>0.997</cell><cell>1.000</cell></row><row><cell>MNIST</cell><cell>1.000</cell><cell>0.996</cell><cell>1.000</cell><cell>1.000</cell><cell>0.995</cell><cell>1.000</cell></row><row><cell>FashionMNIST</cell><cell>0.998</cell><cell>0.998</cell><cell>1.000</cell><cell>0.998</cell><cell>0.995</cell><cell>1.000</cell></row><row><cell>SVHN</cell><cell>0.787</cell><cell>0.974</cell><cell>0.950</cell><cell>0.787</cell><cell>0.965</cell><cell>0.929</cell></row><row><cell>CIFAR100</cell><cell>0.683</cell><cell>0.757</cell><cell>0.736</cell><cell>0.583</cell><cell>0.514</cell><cell>0.535</cell></row><row><cell>CelebA</cell><cell>0.794</cell><cell>0.701</cell><cell>0.863</cell><cell>0.756</cell><cell>0.640</cell><cell>0.776</cell></row><row><cell>FaceScrub</cell><cell>0.750</cell><cell>0.797</cell><cell>0.859</cell><cell>0.710</cell><cell>0.704</cell><cell>0.760</cell></row><row><cell>TinyImageNet</cell><cell>0.710</cell><cell>0.875</cell><cell>0.716</cell><cell>0.657</cell><cell>0.735</cell><cell>0.589</cell></row><row><cell>TrafficSign</cell><cell>0.953</cell><cell>0.955</cell><cell>0.931</cell><cell>0.916</cell><cell>0.840</cell><cell>0.870</cell></row><row><cell>Noise</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell><cell>1.000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/openai/glow</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/pclucas14/pixel-cnn-pp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://flif.info</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Ilias Leontiadis and Santi Pascual for useful discussions at the beginning of this project.</p><p>Vicenç Gómez is supported by the Ramon y Cajal program RYC-2015-18878 (AEI/MINEICO/FSE,UE). The project leading to these results has received funding from "la Caixa" Foundation (ID 100010434), under agreement LCF/PR/PR16/51110009.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty in the variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Deep Learning Workshop, UAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Novelty detection and neural network validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proceedings -Vision, Image and Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">WAIC, but why? Generative ensembles for robust anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<idno>1810.01392</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>Hoboken, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Learning Representations (ICLR</title>
		<meeting>of the Int. Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>of the Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Høst-Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sabeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Walton</surname></persName>
		</author>
		<title level="m">Data Discovery and Anomaly Detection Using Atypicality: Theory. IEEE Trans. on Information Theory</title>
		<imprint>
			<publisher>In press</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glow: generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On tables of random numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhya Ser. A</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MSc Thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallac, R. Fergus, S. Vishwanatan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>of the Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measures of complexity: a nonexhaustive list</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="7" to="8" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>of the Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting out-of-distribution inputs to deep generative models using a test for typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Image Processing (ICIP)</title>
		<meeting>of the IEEE Int. Conf. on Image essing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: high confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on The Future of Gradientbased Machine Learning Software &amp; Techniques (NIPS-Autodiff</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data discovery and anomaly detection using atypicality for realvalued data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sabeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Høst-Madsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">219</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PixelCNN++: improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Learning Representations (ICLR</title>
		<meeting>of the Int. Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>of the Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
