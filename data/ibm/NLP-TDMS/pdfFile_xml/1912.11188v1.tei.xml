<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVERSARIAL AUTOAUGMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-24">24 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
							<email>zhangxinyu10@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
							<email>wangqiang168@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Huawei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
							<email>zhangjian157@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Huawei</surname></persName>
						</author>
						<title level="a" type="main">ADVERSARIAL AUTOAUGMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-24">24 Dec 2019</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, Au-toAugment (Cubuk et al., 2018)  can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12× reduction in computing cost and 11× shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Massive amount of data have promoted the great success of deep learning in academia and industry. The performance of deep neural networks (DNNs) would be improved substantially when more supervised data is available or better data augmentation method is adapted. Data augmentation such as rotation, flipping, cropping, etc., is a powerful technique to increase the amount and diversity of data. Experiments show that the generalization of a neural network can be efficiently improved through manually designing data augmentation policies. However, this needs lots of knowledge of human expert, and sometimes shows the weak transferability across different tasks and datasets in practical applications. Inspired by neural architecture search (NAS) <ref type="bibr" target="#b34">(Zoph &amp; Le, 2016;</ref><ref type="bibr" target="#b35">Zoph et al., 2017;</ref><ref type="bibr" target="#b32">Zhong et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b7">Guo et al., 2018)</ref>, a reinforcement learning (RL) <ref type="bibr" target="#b29">(Williams, 1992)</ref> method called AutoAugment is proposed by <ref type="bibr" target="#b1">Cubuk et al. (2018)</ref>, which can automatically learn the augmentation policy from data and provide an exciting performance improvement on image classification tasks. However, the computing cost is huge for training and evaluating thousands of sampled policies in the search process. Although proxy tasks, i.e., smaller models and reduced datasets, are taken to accelerate the searching process, tens of thousands of GPU-hours of consumption are still required. In addition, these data augmentation policies optimized on proxy tasks are not guaranteed to be optimal on the target task, and the fixed augmentation policy is also sub-optimal for the whole training process.</p><p>… … <ref type="figure">Figure 1</ref>: The overview of our proposed method. We formulate it as a Min-Max game. The data of each batch is augmented by multiple pre-processing components with sampled policies {τ 1 , τ 2 , · · · , τ M }, respectively. Then, a target network is trained to minimize the loss of a large batch, which is formed by multiple augmented instances of the input batch. We extract the training losses of a target network corresponding to different augmentation policies as the reward signal. Finally, the augmentation policy network is trained with the guideline of the processed reward signal, and aims to maximize the training loss of the target network through generating adversarial policies.</p><p>In this paper, we propose an efficient data augmentation method to address the problems mentioned above, which can directly search the best augmentation policy on the full dataset during training a target network, as shown in <ref type="figure">Figure 1</ref>. We first organize the network training and augmentation policy search in an adversarial and online manner. The augmentation policy is dynamically changed along with the training state of the target network, rather than fixed throughout the whole training process like normal AutoAugment <ref type="bibr" target="#b1">(Cubuk et al., 2018)</ref>. Due to reusing the computation in policy evaluation and dispensing with the retraining of the target network, the computing cost and time overhead are extremely reduced. Then, the augmentation policy network is taken as an adversary to explore the weakness of the target network. We augment the data of each min-batch with various adversarial policies in parallel, rather than the same data augmentation taken in batch augmentation (BA) <ref type="bibr" target="#b14">(Hoffer et al., 2019)</ref>. Then, several augmented instances of each mini-batch are formed into a large batch for target network learning. As an indicator of the hardness of augmentation policies, the training losses of the target network are used to guide the policy network to generate more aggressive and efficient policies based on REINFORCE algorithm <ref type="bibr" target="#b29">(Williams, 1992)</ref>. Through adversarial learning, we can train the target network more efficiently and robustly.</p><p>The contributions can be summarized as follows:</p><p>• Our method can directly learn augmentation policies on target tasks, i.e., target networks and full datasets, with a quite low computing cost and time overhead. The direct policy search avoids the performance degradation caused by the policy transfer from proxy tasks to target tasks.</p><p>• We propose an adversarial framework to jointly optimize target network training and augmentation policy search. The harder samples augmented by adversarial policies are constantly fed into the target network to promote robust feature learning. Hence, the generalization of the target network can be significantly improved.</p><p>• The experiment results show that our proposed method outperforms previous augmentation methods. For instance, we achieve a top-1 test error of 1.36% with PyramidNet+ShakeDrop <ref type="bibr" target="#b30">(Yamada et al., 2018)</ref> on CIFAR-10, which is the state-of-the-art performance. On Ima-geNet, we improve the top-1 accuracy of ResNet-50 <ref type="bibr" target="#b10">(He et al., 2016)</ref> from 76.3% to 79.4% without extra data, which is even 1.77% better than AutoAugment <ref type="bibr" target="#b1">(Cubuk et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Common data augmentation, which can generate extra samples by some label-preserved transformations, is usually used to increase the size of datasets and improve the generalization of networks, such as on MINST, CIFAR-10 and ImageNet <ref type="bibr" target="#b19">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b27">Wan et al., 2013;</ref><ref type="bibr" target="#b25">Szegedy et al., 2015)</ref>. However, human-designed augmentation policies are specified for different datasets. For example, flipping, the widely used transformation on CIFAR-10/CIFAR-100 and ImageNet, is not suitable for MINST, which will destroy the property of original samples.</p><p>Hence, several works <ref type="bibr" target="#b20">(Lemley et al., 2017;</ref><ref type="bibr" target="#b1">Cubuk et al., 2018;</ref><ref type="bibr" target="#b21">Lin et al., 2019;</ref><ref type="bibr" target="#b12">Ho et al., 2019)</ref> have attempted to automatically learn data augmentation policies. <ref type="bibr" target="#b20">Lemley et al. (2017)</ref> propose a method called Smart Augmentation, which merges two or more samples of a class to improve the generalization of a target network. The result also indicates that an augmentation network can be learned when a target network is being training. Through well designing the search space of data augmentation policies, AutoAugment (Cubuk et al., 2018) takes a recurrent neural network (RNN) as a sample controller to find the best data augmentation policy for a selected dataset. To reduce the computing cost, the augmentation policy search is performed on proxy tasks. Population based augmentation (PBA) <ref type="bibr" target="#b12">(Ho et al., 2019)</ref> replaces the fixed augmentation policy with a dynamic schedule of augmentation policy along with the training process, which is mostly related to our work. Inspired by population based training (PBT) <ref type="bibr" target="#b16">(Jaderberg et al., 2017)</ref>, the augmentation policy search problem in PBA is modeled as a process of hyperparameter schedule learning. However, the augmentation schedule learning is still performed on proxy tasks. The learned policy schedule should be manually adjusted when the training process of a target network is non-matched with proxy tasks.</p><p>Another related topic is Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref>, which has recently attracted lots of research attention due to its fascinating performance, and also been used to enlarge datasets through directly synthesizing new images <ref type="bibr" target="#b26">(Tran et al., 2017;</ref><ref type="bibr" target="#b23">Perez &amp; Wang, 2017;</ref><ref type="bibr" target="#b0">Antoniou et al., 2017;</ref><ref type="bibr" target="#b8">Gurumurthy et al., 2017;</ref><ref type="bibr" target="#b4">Frid-Adar et al., 2018)</ref>. Although we formulate our proposed method as a Min-Max game, there exists an obvious difference with traditional GANs. We want to find the best augmentation policy to perform image transformation along with the training process, rather than synthesize new images. <ref type="bibr" target="#b22">Peng et al. (2018)</ref> also take such an idea to optimize the training process of a target network in human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we present the implementation of Adversarial AutoAugment. First, the motivation for the adversarial relation between network learning and augmentation policy is discussed. Then, we introduce the search space with the dynamic augmentation policy. Finally, the joint framework for network training and augmentation policy search is presented in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MOTIVATIONS</head><p>Although some human-designed data augmentations have been used in the training of DNNs, such as randomly cropping and horizontally flipping on CIFAR-10/CIFAR-100 and ImageNet, limited randomness will make it very difficult to generate effective samples at the tail end of the training.</p><p>To struggle with the problem, more randomness about image transformation is introduced into the search space of AutoAugment (Cubuk et al., 2018) (described in Section 3.2). However, the learned policy is fixed for the entire training process. All of possible instances of each example will be send to the target network repeatedly, which still results in an inevitable overfitting in a long-epoch training. This phenomenon indicates that the learned policy is not adaptive to the training process of a target network, especially found on proxy tasks. Hence, the dynamic and adversarial augmentation policy with the training process is considered as the crucial feature in our search space.</p><p>Another consideration is how to improve the efficiency of the policy search. In AutoAugment <ref type="bibr" target="#b1">(Cubuk et al., 2018)</ref>, to evaluate the performance of augmentation policies, a lot of child models should be trained from scratch nearly to convergence. The computation in training and evaluating the performance of different sampled policies can not be reused, which leads to huge waste of computation resources. In this paper, we propose a computing-efficient policy search framework through reusing prior computation in policy evaluation. Only one target network is used to evaluate the performance of different policies with the help of the training losses of corresponding augmented  instances. The augmentation policy network is learned from the intermediate state of the target network, which makes generated augmentation policies more aggressive and adaptive. On the contrary, to combat harder examples augmented by adversarial policies, the target network has to learn more robust features, which makes the training more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SEARCH SPACE</head><p>In this paper, the basic structure of the search space of AutoAugment (Cubuk et al., 2018) is reserved. An augmentation policy is defined as that it is composed by 5 sub-policies, each sub-policy contains two image operations to be applied orderly, each operation has two corresponding parameters, i.e., the probability and magnitude of the operation. Finally, the 5 best policies are concatenated to form a single policy with 25 sub-policies. For each image in a mini-batch, only one sub-policy will be randomly selected to be applied. To compare with AutoAugment (Cubuk et al., 2018) conveniently, we just slightly modify the search space with removing the probability of each operation. This is because that we think the stochasticity of an operation with a probability requires a certain epochs to take effect, which will detain the feedback of the  <ref type="bibr" target="#b15">(Inoue, 2018)</ref>. The range of the magnitude is also discretized uniformly into 10 values. To guarantee the convergence during adversarial learning, the magnitude of all the operations are set in a moderate range. 1 Besides, the randomness during the training process is introduced into our search space. Hence, the search space of the policy in each epoch has |S| = (16×10) 10 ≈ 1.1×10 22 possibilities. Considering the dynamic policy, the number of possible policies with the whole training process can be expressed as |S| #epochs . An example of dynamically learning the augmentation policy along with the training process is shown in <ref type="figure" target="#fig_0">Figure  2</ref>. We observe that the magnitude (an indication of difficulty) gradually increases with the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ADVERSARIAL LEARNING</head><p>In this section, the adversarial framework of jointly optimizing network training and augmentation policy search is presented in detail. We use the augmentation policy network A(·, θ) as an adversary, which attempts to increase the training loss of the target network F (·, w) through adversarial learning. The target network is trained by a large batch formed by multiple augmented instances of each batch to promote invariant learning <ref type="bibr" target="#b24">(Salazar et al., 2018)</ref>, and the losses of different augmentation policies applied on the same data are used to train the augmentation policy network by RL algorithm.</p><p>Considering the target network F (·, w) with a loss function L <ref type="figure">[F (x, w)</ref>, y], where each example is transformed by some random data augmentation o(·), the learning process of the target network can be defined as the following minimization problem</p><formula xml:id="formula_0">w * = arg min w E x∼Ω L[F (o(x), w), y],<label>(1)</label></formula><p>where Ω is the training set, x and y are the input image and the corresponding label, respectively. The problem is usually solved by vanilla SGD with a learning rate η and batch size N , and the training procedure for each batch can be expressed as</p><formula xml:id="formula_1">w t+1 = w t − η 1 N N n=1 ∇ w L[F (o(x n ), w, y n ].<label>(2)</label></formula><p>To improve the convergence performance of DNNs, more random and efficient data augmentation is performed under the help of the augmentation policy network. Hence, the minimization problem should be slightly modified as</p><formula xml:id="formula_2">w * = arg min w E x∼Ω E τ ∼A(·,θ) L[F (τ (x), w), y],<label>(3)</label></formula><p>where τ (·) represents the augmentation policy generated by the network A(·, θ). Accordingly, the training rule can be rewritten as</p><formula xml:id="formula_3">w t+1 = w t − η 1 M · N M m=1 N n=1 ∇ w L[F (τ m (x n ), w), y n ],<label>(4)</label></formula><p>where we introduce M different instances of each input example augmented by adversarial policies {τ 1 , τ 2 , · · · , τ M }. For convenience, we denote the training loss of a mini-batch corresponding to the augmentation policy τ m as</p><formula xml:id="formula_4">L m = 1 N N n=1 L[F (τ m (x n ), w), y n ].<label>(5)</label></formula><p>Hence, we have an equivalent form of Equation 4</p><formula xml:id="formula_5">w t+1 = w t − η 1 M M m=1 ∇ w L m .<label>(6)</label></formula><p>Note that the training procedure can be regarded as a larger N · M batch training or an average over M instances of gradient computation without changing the learning rate, which will lead to a reduction of gradient variance and a faster convergence of the target network <ref type="bibr" target="#b14">Hoffer et al. (2019)</ref>. However, overfitting will also come. To overcome the problem, the augmentation policy network is designed to increase the training loss of the target network with harder augmentation policies. Therefore, we can mathematically express the object as the following maximization problem</p><formula xml:id="formula_6">θ * = arg max θ J(θ), where J(θ) = E x∼Ω E τ ∼A(·,θ) L[F (τ (x), w), y].<label>(7)</label></formula><p>Similar to <ref type="bibr" target="#b1">AutoAugment (Cubuk et al., 2018)</ref>, the augmentation policy network is also implemented as a RNN shown in <ref type="figure">Figure 3</ref>. At each time step of the RNN controller, the softmax layer will predict  <ref type="figure">Figure 3</ref>: The basic architecture of the controller for generating a sub-policy, which consists of two operations with corresponding parameters, the type and magnitude of each operation. When a policy contains Q sub-policies, the basic architecture will be repeated Q times. Following the setting of AutoAugment <ref type="bibr" target="#b1">(Cubuk et al., 2018)</ref>, the number of sub-policies Q is set to 5 in this paper.</p><p>an action corresponding to a discrete parameter of a sub-policy, and then an embedding of the predicted action will be fed into the next time step. In our experiments, the RNN controller will predict 20 discrete parameters to form a whole policy.</p><p>However, there has a severe problem in jointly optimizing target network training and augmentation policy search. This is because that non-differentiable augmentation operations break gradient flow from the target network F to the augmentation policy network A <ref type="bibr" target="#b22">Peng et al., 2018)</ref>. As an alternative approach, REINFORCE algorithm <ref type="bibr" target="#b29">(Williams, 1992)</ref> is applied to optimize the augmentation policy network as</p><formula xml:id="formula_7">∇ θ J(θ) = ∇ θ E x∼Ω E τ ∼A(·,θ) L[F (τ (x), w), y] ≈ m L m ∇ θ p m = m L m p m ∇ θ log p m = E τ ∼A(·,θ) L m ∇ θ log p m ≈ 1 M M m=1 L m ∇ θ log p m ,<label>(8)</label></formula><p>where p m represents the probability of the policy τ m . To reduce the variance of gradient ∇ θ J(θ), we replace the training loss of a mini-batch L m with L m a moving average over a certain minibatches 2 , and then normalize it among M instances as L m . Hence, the training procedure of the augmentation policy network can be expressed as</p><formula xml:id="formula_8">∇ θ J(θ) ≈ 1 M M m=1 L m ∇ θ log p m , θ e+1 = θ e + β 1 M M m=1 L m ∇ θ log p m ,<label>(9)</label></formula><p>The adversarial learning of target network training and augmentation policy search is summarized as Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND ANALYSIS</head><p>In this section, we first reveal the details of experiment settings. Then, we evaluate our proposed method on CIFAR-10/CIFAR-100, ImageNet, and compare it with previous methods. Results in <ref type="figure" target="#fig_1">Figure 4</ref> show our method achieves the state-of-the-art performance with higher computing and time efficiency 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Joint Training of Target Network and Augmentation Policy Network</head><p>Initialization: target network F (·, w), augmentation policy network A(·, θ) Input: input examples x, corresponding labels y 1: for 1 ≤ e ≤ epochs do 2:</p><p>Initialize L m = 0, ∀m ∈ {1, 2, · · · , M };</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Generate M policies with the probabilities {p 1 , p 2 , · · · , p M };</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for 1 ≤ t ≤ T do 5:</p><p>Augment each batch data with M generated policies, respectively; 6:</p><p>Update w e,t+1 according to Equation 4; 7:</p><p>Update L m through moving average, ∀m ∈ {1, 2, · · · , M };</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Collect { L 1 , L 2 , · · · , L M };</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Normalize L m among M instances as L m , ∀m ∈ {1, 2, · · · , M };</p><p>10:</p><p>Update θ e+1 via Equation 9; 11: Output w * , θ *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENT SETTINGS</head><p>The RNN controller is implemented as a one-layer LSTM <ref type="bibr" target="#b13">(Hochreiter &amp; Schmidhuber, 1997)</ref>. We set the hidden size to 100, and the embedding size to 32. We use Adam optimizer <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref> with a initial learning rate 0.00035 to train the controller. To avoid unexpected rapid convergence, an entropy penalty of a weight of 0.00001 is applied. All the reported results are the mean of five runs with different initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTS ON CIFAR-10 AND CIFAR-100</head><p>CIFAR-10 dataset <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009</ref>) has totally 60000 images. The training and test sets have 50000 and 10000 images, respectively. Each image in size of 32 × 32 belongs to one of 10 classes. We evaluate our proposed method with the following models: Wide-ResNet-28-10 <ref type="bibr" target="#b31">(Zagoruyko &amp; Komodakis, 2016)</ref>, Shake-Shake (26 2x32d) <ref type="bibr" target="#b5">(Gastaldi, 2017)</ref>, Shake-Shake (26 2x96d) <ref type="bibr" target="#b5">(Gastaldi, 2017)</ref>, Shake-Shake (26 2x112d) <ref type="bibr" target="#b5">(Gastaldi, 2017)</ref>, PyramidNet+ShakeDrop <ref type="bibr" target="#b9">(Han et al., 2017;</ref><ref type="bibr" target="#b30">Yamada et al., 2018)</ref>. All the models are trained on the full training set.</p><p>Training details: The Baseline is trained with the standard data augmentation, namely, randomly cropping a part of 32 × 32 from the padded image and horizontally flipping it with a probability of 0.5. The Cutout <ref type="bibr" target="#b3">(Devries &amp; Taylor, 2017)</ref> randomly select a 16 × 16 patch of each image, and then set the pixels of the selected patch to zeros. For our method, the searched policy is applied in addition to standard data augmentation and Cutout. For each image in the training process, standard data augmentation, the searched policy and Cutout are applied in sequence. For Wide-ResNet-28-10, the step learning rate (LR) schedule is adopted. The cosine LR schedule is adopted for the other models. More details about model hyperparameters are supplied in A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of M :</head><p>To choose the optimal M , we select Wide-ResNet-28-10 as a target network, and evaluate the performance of our proposed method verse different M , where M ∈ {2, 4, 8, 16, 32}. From <ref type="figure">Figure 5</ref>, we can observe that the test accuracy of the model improves rapidly with the increase of M up to 8. The further increase of M does not bring a significant improvement. Therefore, to balance the performance and the computing cost, M is set to 8 in all the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 results:</head><p>In <ref type="table" target="#tab_3">Table 1</ref>, we report the test error of these models on CIFAR-10. For all of these models, our proposed method can achieve better performance compared to previous methods. We achieve 0.78% and 0.68% improvement on Wide-ResNet-28-10 compared to AutoAugment and PBA, respectively. We achieve a top-1 test error of 1.36% with PyramidNet+ShakeDrop, which is 0.1% better than the current state-of-the-art reported in <ref type="bibr" target="#b12">Ho et al. (2019)</ref>. As shown in <ref type="figure">Figure 6</ref>(a) and 6(b),we further visualize the probability distribution of the parameters of the augmentation policies learned with PyramidNet+ShakeDrop on CIFAR-10 over time. From <ref type="figure">Figure 6(a)</ref>, we can find that the percentages of some operations, such as TranslateY, Rotate, Posterize, and SampleParing, gradually increase along with the training process. Meanwhile, more geometric transformations, such as TranslateX, TranslateY, and Rotate, are picked in the sampled augmentation policies, which  is different from color-focused AutoAugment (Cubuk et al., 2018) on CIFAR-10. <ref type="figure">Figure 6(b)</ref> shows that large magnitudes gain higher percentages during training. However, at the tail of training, low magnitudes remain considerable percentages. This indicates that our method does not simply learn the transformations with the extremes of the allowed magnitudes to spoil the target network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100 results:</head><p>We also evaluate our proposed method on CIFAR-100, as shown in <ref type="table" target="#tab_4">Table 2</ref>. As we can observe from the table, we also achieve the state-of-the-art performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXPERIMENTS ON IMAGENET</head><p>As a great challenge in image recognition, ImageNet dataset <ref type="bibr" target="#b2">(Deng et al., 2009</ref>) has about 1.2 million training images and 50000 validation images with 1000 classes. In this section, we directly search the augmentation policy on the full training set and train ResNet-50 <ref type="bibr" target="#b10">(He et al., 2016)</ref>, ResNet-50-D <ref type="bibr" target="#b11">(He et al., 2018)</ref> and <ref type="bibr">ResNet-200 (He et al., 2016)</ref> from scratch.</p><p>Training details: For the baseline augmentation, we randomly resize and crop each input image to a size of 224 × 224, and then horizontally flip it with a probability of 0.5. For AutoAugment <ref type="bibr" target="#b1">(Cubuk et al., 2018)</ref> and our method, the baseline augmentation and the augmentation policy are both used for each image. The cosine LR schedule is adopted in the training process. The model hyperparameters on ImageNet is also detailed in A.1.</p><p>ImageNet results: The performance of our proposed method on ImageNet is presented in <ref type="table" target="#tab_5">Table 3</ref>. It can be observed that we achieve a top-1 accuracy 79.40% on ResNet-50 without extra data. To  the best of our knowledge, this is the highest top-1 accuracy for ResNet-50 learned on ImageNet.</p><p>Besides, we only replace the ResNet-50 architecture with ResNet-50-D, and achieve a consistent improvement with a top-1 accuracy of 80.00%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDY</head><p>To check the effect of each component in our proposed method, we report the test error of ResNet-50 on ImageNet the following augmentation methods in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>• Baseline: Training regularly with the standard data augmentation and step LR schedule.</p><p>• Fixed: Augmenting all the instances of each batch with the standard data augmentation fixed throughout the entire training process.</p><p>• Random: Augmenting all the instances of each batch with randomly and dynamically generated policies.</p><p>• Ours: Augmenting all the instances of each batch with adversarial policies sampled by the policy network along with the training process.</p><p>From the table, we can find that Fixed can achieve 0.99% error reduction compared to Baseline. This shows that a large-batch training with multiple augmented instances of each mini-batch can indeed improve the generalization of the model, which is consistent with the conclusion presented in <ref type="bibr" target="#b14">Hoffer et al. (2019)</ref>. In addition, the test error of Random is 1.02% better than Fixed. This indicates that augmenting batch with randomly generated policies can reduce overfitting in a certain extent. Furthermore, our method achieves the best test error of 20.60% through augmenting samples with adversarial policies. From the result, we can conclude that these policies generated by the policy network are more adaptive to the training process, and make the target network have to learn more robust features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">COMPUTING COST AND TIME OVERHEAD</head><p>Computing Cost: The computation in target network training is reused for policy evaluation. This makes the computing cost in policy search become negligible. Although there exists an increase of computing cost in target network training, the total computing cost in training one target network with augmentation policies is quite small compared to prior work.</p><p>Time Overhead: Since we just train one target network with a large batch distributedly and simultaneously, the time overhead of the large-batch training is equal to the regular training. Meanwhile, the joint optimization of target network training and augmentation policy search dispenses with the process of offline policy search and the retraining of a target network, which leads to a extreme time overhead reduction.</p><p>In <ref type="table" target="#tab_7">Table 5</ref>, we take the training of ResNet-50 on ImageNet as an example to compare the computing cost and time overhead of our method and AutoAugment. From the table, we can find that our method is 12× less computing cost and 11× shorter time overhead than AutoAugment. To further show the higher efficiency of our method, the transferability of the learned augmentation policies is evaluated in this section. We first take a snapshot of the adversarial training process of ResNet-50 on ImageNet, and then directly use the learned dynamic augmentation policies to regularly train the following models: Wide-ResNet-28-10 on CIFAR-10/100, ResNet-50-D on ImageNet and ResNet200 on ImageNet. <ref type="table" target="#tab_8">Table 6</ref> presents the experimental results of the transferability. From the table, we can find that a competitive performance can be still achieved through direct policy transfer. This indicates that the learned augmentation policies transfer well across datasets and architectures. However, compared to the proposed method, the policy transfer results in an obvious performance degradation, especially the transfer across datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce the idea of adversarial learning into automatic data augmentation. The policy network tries to combat the overfitting of the target network through generating adversarial policies with the training process. To oppose this, robust features are learned in the target network, which leads to a significant performance improvement. Meanwhile, the augmentation policy search is performed along with the training of a target network, and the computation in network training is reused for policy evaluation, which can extremely reduce the search cost and make our method more computing-efficient. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An example of dynamic augmentation policies learned with ResNet-50 on ImageNet. With the training process of the target network, harder augmentation policies are sampled to combat overfitting. Intuitively, more geometric transformations, such as TranslateX, ShearY and Rotate, are picked in our sampled policies, which is obviously different from AutoAugment (Cubuk et al., 2018) concentrating on color-based transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The Comparison of normalized performance between AutoAugment and our method. Please refer to the following tables for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The Top-1 test accuracy of Wide-ResNet-28-10 on CIFAR-10 verse different M , where M ∈ {2, 4, 8, 16, 32}. Probability distribution of the parameters in the learned augmentation policies on CIFAR-10 over time. The number in (b) represents the magnitude of one operation. Larger number stands for more dramatic image transformations. The probability distribution of each parameter is the mean of each five epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Top-1 test error (%) on CIFAR-10. We replicate the results of Baseline, Cutout and Au-toAugment methods from<ref type="bibr" target="#b1">Cubuk et al. (2018)</ref>, and the results of PBA from<ref type="bibr" target="#b12">Ho et al. (2019)</ref> in all of our experiments.</figDesc><table><row><cell>Model</cell><cell cols="5">Baseline Cutout AutoAugment PBA Our Method</cell></row><row><cell>Wide-ResNet-28-10</cell><cell>3.87</cell><cell>3.08</cell><cell>2.68</cell><cell>2.58</cell><cell>1.90±0.15</cell></row><row><cell>Shake-Shake (26 2x32d)</cell><cell>3.55</cell><cell>3.02</cell><cell>2.47</cell><cell>2.54</cell><cell>2.36±0.10</cell></row><row><cell>Shake-Shake (26 2x96d)</cell><cell>2.86</cell><cell>2.56</cell><cell>1.99</cell><cell>2.03</cell><cell>1.85±0.12</cell></row><row><cell>Shake-Shake (26 2x112d)</cell><cell>2.82</cell><cell>2.57</cell><cell>1.89</cell><cell>2.03</cell><cell>1.78±0.05</cell></row><row><cell>PyramidNet+ShakeDrop</cell><cell>2.67</cell><cell>2.31</cell><cell>1.48</cell><cell>1.46</cell><cell>1.36±0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Top-1 test error (%) on CIFAR-100.</cell></row><row><cell>Model</cell><cell cols="4">Baseline Cutout AutoAugment PBA Our Method</cell></row><row><cell>Wide-ResNet-28-10</cell><cell>18.80</cell><cell>18.41</cell><cell>17.09</cell><cell>16.73 15.49±0.18</cell></row><row><cell>Shake-Shake (26 2x96d)</cell><cell>17.05</cell><cell>16.00</cell><cell>14.28</cell><cell>15.31 14.10±0.15</cell></row><row><cell>PyramidNet+ShakeDrop</cell><cell>13.99</cell><cell>12.19</cell><cell>10.67</cell><cell>10.94 10.42±0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Top-1 / Top-5 test error (%) on ImageNet. Note that the result of ResNet-50-D is achieved</cell></row><row><cell cols="3">only through substituting the architecture.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Baseline</cell><cell cols="2">AutoAugment PBA</cell><cell>Our Method</cell></row><row><cell>ResNet-50</cell><cell>23.69 / 6.92</cell><cell>22.37 / 6.18</cell><cell>-</cell><cell>20.60±0.15 / 5.53±0.05</cell></row><row><cell cols="2">ResNet-50-D 22.84 / 6.48</cell><cell>-</cell><cell>-</cell><cell>20.00±0.12 / 5.25±0.03</cell></row><row><cell>ResNet-200</cell><cell>21.52 / 5.85</cell><cell>20.00 / 4.90</cell><cell>-</cell><cell>18.68±0.18 / 4.70±0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Top-1 test error (%) of ResNet-50 with different augmentation methods on ImageNet.</figDesc><table><row><cell>Method</cell><cell cols="4">Aug. Policy Enlarge Batch LR Schedule Test Error</cell></row><row><cell>Baseline</cell><cell>standard</cell><cell>M = 1</cell><cell>step</cell><cell>23.69</cell></row><row><cell>Fixed</cell><cell>standard</cell><cell>M = 8</cell><cell>cosine</cell><cell>22.70</cell></row><row><cell>Random</cell><cell>random</cell><cell>M = 8</cell><cell>cosine</cell><cell>21.68</cell></row><row><cell>Ours</cell><cell>adversarial</cell><cell>M = 8</cell><cell>cosine</cell><cell>20.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The comparison of computing cost (GPU hours) and time overhead (days) in training ResNet-50 on ImageNet between AutoAugment and our method. The computing cost and time overhead are estimated on 64 NVIDIA Tesla V100s.</figDesc><table><row><cell>Method</cell><cell cols="2">Computing Cost</cell><cell></cell><cell cols="2">Time Overhead</cell><cell></cell></row><row><cell></cell><cell cols="6">Searching Training Total Searching Training Total</cell></row><row><cell>AutoAugment</cell><cell>15000</cell><cell>160</cell><cell>15160</cell><cell>10</cell><cell>1</cell><cell>11</cell></row><row><cell>Our Method</cell><cell>∼0</cell><cell>1280</cell><cell>1280</cell><cell>∼0</cell><cell>1</cell><cell>1</cell></row><row><cell cols="5">4.6 TRANSFERABILITY ACROSS DATASETS AND ARCHITECTURES</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Top-1 test error (%) of the transfer of the augmentation policies learned with ResNet-50 on ImageNet.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell cols="3">AutoAugment Our Method Policy Transfer</cell></row><row><cell cols="2">Wide-ResNet-28-10 CIFAR-10</cell><cell>2.68</cell><cell>1.90</cell><cell>2.45±0.13</cell></row><row><cell cols="2">Wide-ResNet-28-10 CIFAR-100</cell><cell>17.09</cell><cell>15.49</cell><cell>16.48±0.15</cell></row><row><cell>ResNet-50-D</cell><cell>ImageNet</cell><cell>-</cell><cell>20.00</cell><cell>20.20±0.05</cell></row><row><cell>ResNet-200</cell><cell>ImageNet</cell><cell>20.00</cell><cell>18.68</cell><cell>19.05±0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Model hyperparameters on CIFAR-10/CIFAR-100 and ImageNet. LR represents learning rate, and WD represents weight decay. We do not specifically tune these hyperparameters, and all of these are consistent with previous works, expect for the number of epochs.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Batch Size (N · M )</cell><cell cols="2">LR WD Epoch</cell></row><row><cell>CIFAR-10</cell><cell>Wide-ResNet-28-10</cell><cell>128 · 8</cell><cell>0.1 5e-4</cell><cell>200</cell></row><row><cell>CIFAR-10</cell><cell>Shake-Shake (26 2x32d)</cell><cell>128 · 8</cell><cell>0.2 1e-4</cell><cell>600</cell></row><row><cell>CIFAR-10</cell><cell>Shake-Shake (26 2x96d)</cell><cell>128 · 8</cell><cell>0.2 1e-4</cell><cell>600</cell></row><row><cell>CIFAR-10</cell><cell>Shake-Shake (26 2x112d)</cell><cell>128 · 8</cell><cell>0.2 1e-4</cell><cell>600</cell></row><row><cell>CIFAR-10</cell><cell>PyramidNet+ShakeDrop</cell><cell>128 · 8</cell><cell>0.1 1e-4</cell><cell>600</cell></row><row><cell>CIFAR-100</cell><cell>Wide-ResNet-28-10</cell><cell>128 · 8</cell><cell>0.1 5e-4</cell><cell>200</cell></row><row><cell cols="2">CIFAR-100 Shake-Shake (26 2x96d)</cell><cell>128 · 8</cell><cell cols="2">0.1 5e-4 1200</cell></row><row><cell cols="2">CIFAR-100 PyramidNet+ShakeDrop</cell><cell>128 · 8</cell><cell cols="2">0.5 1e-4 1200</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The more details about the parameter setting please refer to<ref type="bibr" target="#b1">AutoAugment (Cubuk et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The length of the moving average is fixed to an epoch in our experiments.3  To clearly present the advantage of our proposed method, we normalize the performance of our method in theFigure 4, and the performance of AutoAugment is plotted accordingly.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<title level="m">Learning augmentation policies from data. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database. CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthetic data augmentation using GAN for improved liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayit</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno>abs/1705.07485</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">IRLAS: inverse reinforcement learning for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1812.05285</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deligan : Generative adversarial networks for diverse and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Babu Radhakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>abs/1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Augment your batch: better training with larger batches. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
		<idno>abs/1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.09846</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Smart augmentation -learning an optimal data augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shabab</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Corcoran</surname></persName>
		</author>
		<idno>abs/1703.08383</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Online hyper-parameter learning for auto-augmentation strategy. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1712.04621</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Invariant representation learning for robust deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A bayesian data augmentation approach for learning deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">J</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Yann LeCun, and Rob Fergus. Regularization of neural networks using dropconnect. ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
		<idno>abs/1802.02375</idno>
		<title level="m">Shakedrop regularization. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Wide residual networks. British Machine Vision Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Practical network blocks design with Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">BlockQNN: Efficient block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1808.05584</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
