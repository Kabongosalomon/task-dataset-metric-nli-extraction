<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;BNN -BN = ?&quot;: Training Binary Neural Networks without Batch Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<email>tianlong.chen@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
							<email>zechunl@andrew.cmu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
							<email>zhiqians@andrew.cmu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;BNN -BN = ?&quot;: Training Binary Neural Networks without Batch Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Batch normalization (BN) is a key facilitator and considered essential for state-of-the-art binary neural networks (BNN). However, the BN layer is costly to calculate and is typically implemented with non-binary parameters, leaving a hurdle for the efficient implementation of BNN training. It also introduces undesirable dependence between samples within each batch. Inspired by the latest advance on Batch Normalization Free (BN-Free) training [7], we extend their framework to training BNNs, and for the first time demonstrate that BNs can be completed removed from BNN training and inference regimes. By plugging in and customizing techniques including adaptive gradient clipping, scale weight standardization, and specialized bottleneck block, a BN-free BNN is capable of maintaining competitive accuracy compared to its BN-based counterpart. Extensive experiments validate the effectiveness of our proposal across diverse BNN backbones and datasets. For example, after removing BNs from the state-of-the-art ReActNets <ref type="bibr" target="#b37">[38]</ref>, it can still be trained with our proposed methodology to achieve 92.08%, 68.34%, and 68.0% accuracy on CIFAR-10, CIFAR-100, and ImageNet respectively, with marginal performance drop (0.23% ∼ 0.44% on CIFAR and 1.40% on ImageNet). Codes and pre-trained models are available at: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite widespread success <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45]</ref>, state-of-the-art deep networks usually have hundreds of millions of parameters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref>, and suffer from burdensome computational cost. It is questionable how practical they are when it comes to deployment on real-world resource-constrained platforms, e.g., FPGA, ASICs, and mobile devices. Binary neural network (BNN) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b72">73]</ref> are therefore proposed for the efficiency purpose. It takes only 1-bit with two discrete values, i.e., {−1, 1} to represent networks' weights and activations, leading to sig-  The orange bar presents the existing state-of-the-art (SOTA) method, i.e., ReActNet <ref type="bibr" target="#b37">[38]</ref>. The green bar shows the performance of ReActNet if naively dropping Batch Normalization (BN) modules. The red bar indicates our proposed Batch Normalization Free (BN-Free) binary neural network, which reaches competitive performance compared to its counterpart with BN.</p><p>nificantly accelerated and energy-efficient inference as the 1-bit convolution operation can be efficiently implemented with XNOR and Bitcount operations <ref type="bibr" target="#b50">[51]</ref>. Despite these appeals, BNNs are notoriously difficult to train, and undergo performance degradation. Particularly, <ref type="bibr" target="#b54">[55]</ref> shows that the Batch Normalization (BN) <ref type="bibr" target="#b30">[31]</ref> is critical to train BNNs successfully, allowing for stable training under larger learning rates, from both theoretical and empirical perspectives. Unfortunately, the batch normalization implementation <ref type="bibr" target="#b30">[31]</ref> hinges on high precision values to compute the sum of squares, square-root and reciprocal. Therefore, it comes as no surprise that most BNNs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref> kept BN layers in full precision during training, and some used reduced-precision such as 8-bit <ref type="bibr" target="#b3">[4]</ref>. Although BNs can be absorbed into the BNN weights (e.g., scaling factors) post-training, their presence becomes a bottleneck for BNNs training efficiency on hardware <ref type="bibr" target="#b64">[65]</ref>. Moreover, BNs often account for a substantial fraction of run-time, are hard to accelerate <ref type="bibr" target="#b16">[17]</ref>, and incurs memory overhead <ref type="bibr" target="#b8">[9]</ref>. That applies to both inference and the feedforward stage of a training pass. Besides, BN causes discrepant behaviors between the network training and inference stages <ref type="bibr" target="#b57">[58]</ref>, which may break down the independence assumption between samples within each batch.</p><p>We hence anticipate finding an alternative to eliminate the unwanted properties of BNs in BNNs, while maintaining competitive performance. Motivated by the recent advance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, we propose Batch Normalization Free (BN-Free) binary neural networks. Specifically, we leverage the adaptive gradient clipping to constraining BNN's gradient distribution and mitigate gradient explosion due to removing BNs <ref type="bibr" target="#b54">[55]</ref>. Then, the scaled weight standardization and specialized bottleneck block <ref type="bibr" target="#b6">[7]</ref> are integrated for preserving the variances and preventing the mean shifts of activations. Our contributions are outlined as follows:</p><p>• We provide the first proof-of-concept study that general BNNs can be successfully trained without BNs but maintain competitive performance.</p><p>• We introduce adaptive gradient clipping, scaled weight standardization, and specialized block to BNNs, and show these techniques can be easily plugged in various BNN backbones to make them BN-Free.</p><p>• Comprehensive experiments validate the effectiveness of our proposed mechanisms. For example, BN-Free ReActNet achieves 92.08%, 68.34%, and 68.0% accuracy on CIFAR-10, CIFAR-100, and ImageNet respectively, with only marginal performance drops compared to state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Binary neural networks. Numerous model compression and acceleration algorithms have been proposed to reduce the latency of models while maintaining comparable accuracy performance. General model compression approaches fall under multiple forms <ref type="bibr" target="#b11">[12]</ref>: pruning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b62">63]</ref>, quantization <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b15">16]</ref>, knowledge distillation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44]</ref>, as well as their compositions <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71]</ref>. A Binary Neural Network (BNN) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b65">66]</ref> represents the most extreme form of model quantization as it quantizes weights in convolution layers to only 1 bit, enjoying great speed-up compared with its full-precision counterpart. <ref type="bibr" target="#b49">[50]</ref> roughly divides previous BNN literature into two categories: (i) native BNN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> which directly applies binarization to a full-precision model by a pre-defined binarization function. Straight-through estimator (STE) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref> is usually adopted to enable the backpropagation in binarized models <ref type="bibr" target="#b12">[13]</ref>. (ii) optimizationbased BNNs techniques, including minimizing the quantization error <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>, improving the network loss function <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b71">72]</ref>, and reducing the gradient error <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>However, such aggressive quantization usually results in severe accuracy decline. To tackle this limitation, <ref type="bibr" target="#b13">[14]</ref> proposes an end-to-end gradient back-propagation framework for training the discrete binary weights and activations, establishing great successes on small datasets, such as MNIST <ref type="bibr" target="#b59">[60]</ref> and CIFAR10 <ref type="bibr" target="#b61">[62]</ref>, while still has unsatisfactory performance on large datasets like ImageNet <ref type="bibr" target="#b37">[38]</ref>. Follow-up researches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref> devote themselves to build state-of-the-art (SOTA) accuracies on ImageNet. Among these works, ReActNet <ref type="bibr" target="#b37">[38]</ref> proposes generalized activation functions and a distributional loss, reaching the superior performance which reduces the gap to its fullprecision counterpart within 3.0% accuracy on ImageNet. Note that, all mentioned SOTA BNNs are not sustained without batch normalization.</p><p>Batch normalization and normalization-free networks. Batch normalization (BN) <ref type="bibr" target="#b30">[31]</ref> is a well-known and widely used technique to stabilize model training. It also plays a critical role in the BNN training, as evidenced by <ref type="bibr" target="#b54">[55]</ref>. However, bath normalization is an expensive computational primitive <ref type="bibr" target="#b16">[17]</ref>, and its inefficiency is further amplified in low bits precision context which hinders the deployment of BNN to resource-limited hardware <ref type="bibr" target="#b64">[65]</ref>.</p><p>To seek a simple and effective alternative for batch normalization, various studies <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> are proposed. <ref type="bibr" target="#b69">[70]</ref> introduces an initialization and rescaling rule (i.e., fixed-update initialization) to stabilizes the training of very deep models in place of BN. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> share similar observations that appropriately initializing weights and scaling residual modules benefit avoiding the gradient exploding and vanishing, leading to a stabilized training. Another promising substitution is weight standardization <ref type="bibr" target="#b48">[49]</ref>, which subtracts the mean from weights and divides weights by their standard deviation. <ref type="bibr" target="#b5">[6]</ref> proposes a modified variant, i.e., scaled weight standardization, to suppresses the quickly enlarging of the mean in hidden activations. Recently, <ref type="bibr" target="#b6">[7]</ref> proposes Adaptive Gradient Clipping (AGC) to enable the larger batch size training of normalization-free networks, and to overcome the instabilities from eliminating BN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head><p>In this section, we present the detailed normalizationfree methodologies for binary neural networks (BNN) in Section 3.1 and adopt BNN backbone architecture in Sec-  <ref type="figure">Figure 2</ref>. The architecture overview of baseline network block (a) and proposed BN-Free network block (b). The baseline network blocks are inherited from the recent state-of-the-art (SOTA) BNN framework, i.e., ReActNet <ref type="bibr" target="#b37">[38]</ref>, which are modified from MobileNetV1 <ref type="bibr" target="#b29">[30]</ref> and have the same configuration of channel and layer numbers. For the reduction block, <ref type="bibr" target="#b37">[38]</ref> duplicates the input activation and concatenate the outputs to increase the channel number, which is also maintained in our proposed BN-Free network block. The most important thing is that all original Batch Normalization modules are removed, replaced by scaling factors (e.g., α, 1/β1, 1/β2) and adjusted convolutional layers with scaled weight standardization (i.e., WS-Conv). tion 3.2. Before that, we briefly list the main Batch Normalization benefits from previous literature.</p><p>Understanding Batch Normalization. The Batch Normalization (BN) can (i) reduces the scale of hidden activations on the residual branches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b67">68]</ref>, and maintains well-behaved gradients early in training; (ii) eliminates mean-shift by enforcing the mean activation of each channel to zero across the current training batch <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref>; (iii) serves an implicit regularization <ref type="bibr" target="#b40">[41]</ref> and enhances the models' generalization <ref type="bibr" target="#b27">[28]</ref>; (iv) enables large-batch training <ref type="bibr" target="#b17">[18]</ref> and smoothens the loss landscapes <ref type="bibr" target="#b54">[55]</ref>. Removing batch normalization directly usually leads to an inferior performance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref>. It is further aggravated in training the binary neural network, due to its challenge regime with discrete values of variables <ref type="bibr" target="#b54">[55]</ref>. Particularly, <ref type="bibr" target="#b54">[55]</ref> provides both theoretical and empirical analyses to demonstrate the critical role of BN is to alleviate exploding gradients in the case of binary neural networks, which motivates us the introduce adaptive gradient clipping to establish the framework of BN-Free BNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Normalization-free Training Methodology</head><p>Adaptive gradient clipping (AGC). Gradient clipping is typically adopted to constrain the norm of gradients <ref type="bibr" target="#b45">[46]</ref>, leading to stabilized training <ref type="bibr" target="#b42">[43]</ref>. Recently <ref type="bibr" target="#b6">[7]</ref> proposes adaptive gradient clipping (AGC) to ameliorate the NF-ResNets <ref type="bibr" target="#b5">[6]</ref>'s performance, which clips gradients based on the unit-wise ratios of gradient norms to parameter norms. It can be described as follows:</p><formula xml:id="formula_0">G l i →      λ W l i * F G l i F G l i if G l i F W l i * F &gt; λ G l i otherwise.<label>(1)</label></formula><p>Where G l i denotes the i th row of gradient matrix G l ; similarly, W l i is the i th row of weight matrix W l ; l is the layer index of the considered network; W l i * F = max{ W i F , }, = 10 −3 and · F is the Frobenius norm.</p><p>The clipping threshold λ is a crucial hyperparameter, which is usually tuned by a grid search. Equipped with AGC, BNN training tends to have a constrained gradient distribution as evidenced in <ref type="figure" target="#fig_8">Figure 8</ref>, avoiding the gradient explosion issue.</p><p>Scaled weight standardization. To deal with the meanshift in the hidden activation distributions caused by removing BN, we also introduced the Scaled Weight Standardization from <ref type="bibr" target="#b5">[6]</ref>. Specifically, we modify all convolutional layers in BNN backbones as follows:</p><formula xml:id="formula_1">W i,j = γ · W i,j − µ i √ N σ i<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">µ i = (1/N )Σ j W i,j , σ 2 i = (1/N )Σ j (W i,j − µ i ) 2 ,</formula><p>N is the fan-in, andŴ i,j is the corresponding standardized weights. γ is a fixed scalar for variance preserving, and has diverse values for different adopted activation functions <ref type="bibr" target="#b5">[6]</ref>. For example, γ = 2/(1 − (1/π)) for the ReLU activation function <ref type="bibr" target="#b0">[1]</ref>. We name the modified convolutional layer as WS-Conv for simplicity. Note that, such WS-Conv has consistent performance between training and inference, mitigating the discrepancy behaviour of the batch normalization <ref type="bibr" target="#b6">[7]</ref> and leading to a hardware-friendly implementation of BN-Free binary neural networks.</p><p>Specialized bottleneck block. For the batch normalization benefits preserving purpose, we inherit the specialized bottlenecks block from <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> that applies input/output normalization with hand-crafted scaling factor (e.g., α, β). As shown in <ref type="figure">Figure 2</ref>, we utilize x i0 and x i1 to present the input of the i th BN-Free block and activation after the ReAct PReLU function. In order to normalize the input variance, β 1 = Var(x i0 ) is adopted before the 3x3 WS-Conv operation. We then multiply it with a scalar α and feed it to the ReAct PReLU. Similarly, we divide the obtain activation x i1 with β 2 = Var(x i1 ) and multiply it with α. Blessed by the variance preserving design <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, the output variance of the i th BN-Free block is Var(x i1 )+α 2 . Note that, β 1 and β 2 are usually the expected empirical standard deviation of the corresponding activation at initialization <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Backbone Architecture of BNN</head><p>Generalized activation functions. <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b37">38]</ref> advocate that enforcing binary neural networks to learn similar distribution as full-precision (i.e., real-valued or 32 bits) networks plays a significant role in the final achievable performance of BNN. Specifically, XNOR-Net <ref type="bibr" target="#b50">[51]</ref> pursues close logits distribution as real-valued ones by calculating analytical real-valued scaling factors and multiplying them with the activations. <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b7">8]</ref> introduce further improvements by learning these factors through back-propagation. Re-ActNet <ref type="bibr" target="#b37">[38]</ref> explores an orthogonal perspective that mimics the activation distribution from a pre-trained full precision model. However, it is challenging for binary neural networks with a highly limited capacity to learn appropriate activation distribution, since even small variations to their activation distribution can substantially affect the feature representations in BNNs <ref type="bibr" target="#b37">[38]</ref>.</p><p>To tackle this issue, <ref type="bibr" target="#b37">[38]</ref> proposes the generalized activation functions with learnable parameters, for sign and PReLU <ref type="bibr" target="#b22">[23]</ref> functions, which are termed as RSign and RPReLU respectively. Such learnable parameters enable the adaptive reshape and shift of BNNs' activation to match the desired distributions. Following <ref type="bibr" target="#b37">[38]</ref>'s definition, we introduce adopted activation functions.</p><formula xml:id="formula_3">(RSign) x b i = h(x r i ) = +1, if x r i &gt; α i −1, if x r i ≤ α i<label>(3)</label></formula><p>where x r i is full-precision input of the RSign function h(·) on the ith channel, x b i is the binary output and α i is a learnable coefficient controlling the threshold. The superscripts b and r above x i denote the corresponding binary and fullprecision values.</p><formula xml:id="formula_4">(RPReLU) f (x i ) = x i − γ i + ζ i , if x i &gt; γ i β i × (x i − γ i ) + ζ i , if x i ≤ γ i<label>(4)</label></formula><p>In RPReLU function f (·), x i is the input in the ith channel, γ i and ζ i are learnable shifts, and β i is a learnable coefficient determines the slope of the negative half.</p><p>Meanwhile, we also use the default setting that adding parameter-free shortcuts to blocks, similar to <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b37">[38]</ref>. As shown in <ref type="figure">Figure 2</ref>, our proposed Batch Normalization free (BN-Free) network block maintains the duplication of input activation from <ref type="bibr" target="#b37">[38]</ref>, replaces by scaling factors (e.g., α, 1/β 1 , 1/β 2 ) and adjusts convolutional layers with scaled weight standardization (i.e., WS-Conv).</p><p>Distillation loss functions. To establish the state-of-theart BNN results, we also introduce the distribution loss function <ref type="bibr" target="#b37">[38]</ref> to enforce the similarity of distributions between full-precision networks and binary neural networks. It can also be regarded as a knowledge distillation technique. Specifically, the formulation is depicted as follows:</p><formula xml:id="formula_5">L Dis = − 1 n c n i=1 ρ R c (X i ) × log( ρ B c (X i ) ρ R c (X i ) )<label>(5)</label></formula><p>where L Dis is the Kullback-Leibler (KL) divergence, X i is the input image, c represents classes and n denotes the batch size. ρ R c is the softmax output of the full-precision (i.e., real-valued) model and ρ B c is the softmax output of the binary neural network. With the assistance of introduced distribution loss, BNN is capable of imitating the prediction distribution from full-precision models, leading to a superior performance. In the implementation, the full-precision NFNet <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> is utilized, which is also a BN-Free network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>We conduct experiments on three binary models, i.e., XNOR-Net <ref type="bibr" target="#b50">[51]</ref>, Bi-RealNet <ref type="bibr" target="#b38">[39]</ref>, and ReActNet <ref type="bibr" target="#b37">[38]</ref> with two widely used backbones, i.e., ResNet-18 <ref type="bibr" target="#b23">[24]</ref> and Mo-bileNetV1 <ref type="bibr" target="#b29">[30]</ref>. Meanwhile, we evaluate their BN-free counterparts and report the performance on three representative classification datasets, i.e., CIFAR-10 <ref type="bibr" target="#b34">[35]</ref>, CIFAR-100 <ref type="bibr" target="#b34">[35]</ref>, and ILSVRC12 ImageNet <ref type="bibr" target="#b53">[54]</ref>.</p><p>Implementation details on ImageNet. We use the Ima-geNet dataset with 1000 classes. There are 1, 281, 167 images for training and 50, 000 images for validation. Considering the superior performance of ReActNet <ref type="bibr" target="#b36">[37]</ref> on the Im-ageNet classification task, we apply our BN-Free network design on ReActNet-18 and ReActNet-A, which are the modifications of ResNet-18 and MobileNetv1 respectively. We also adopt the adaptive gradient clipping (AGC) <ref type="bibr" target="#b6">[7]</ref> in the back-propagation when training our BN-Free BNNs with the upper bound value set to 0.02.</p><p>When training the model, We follow the original twostep training strategy <ref type="bibr" target="#b37">[38]</ref>, where we only binarize the activations and train the network from scratch in the first step, then we fine-tune the network with both binary activations and weights in the second step. In both steps, we train the network for 120 epochs with the Adam optimizer and an initial learning rate of 5 × 10 −4 , which follows a linear decreasing scheduler to zero. The weight decay is set to 1 × 10 −5 for the first step and 0 for the second. Besides, the data augmentation method we used in our experiments follows <ref type="bibr" target="#b29">[30]</ref>, which contains random cropping, lighting, and random horizontal flipping. The input resolution is 224 and the top-1 accuracy on the validation set will be reported in the following section.</p><p>Implementation details on CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR-100 contain 50, 000 training images and 10, 000 testing images from 10 and 100 classes respectively. To comprehensively investigate the effectiveness of BN-Free networks, We conduct the classification experiments with four binary networks: XNORNet-18, Bi-RealNet-18, ReActNet-18, and ReActNet-A on CIFAR-10 and CIFAR-100. The first three networks all have a modified ResNet-18 backbone while ReActNet-A is constructed on MobileNetv1. We follow the two-step training strategy consistent with the ImageNet experiments and train the network for 256 epochs in each step. The upper bound of clipping value in AGC is set to 0.001 by default, according to the grid searching in Section 4.3. Additionally, other training hyperparameters remain the same as those in the Ima-geNet experiments. Differently, we use only random cropping and horizontal flipping for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-art networks</head><p>We begin by investigating the performance of batch normalization free BNN (BN-Free BNNs). For each network, we apply the Scaled Weight Standardization <ref type="bibr" target="#b6">[7]</ref> to all convolution layers and replace the basic blocks with our BN-Free blocks after removing all batch normalization (BN) layers. We report the accuracy between its three variants: the baseline network with BN, the network without BN, and the BN-Free network.</p><p>Results on ImageNet. We first evaluate our proposed BN-Free (BF) BNN on ImageNet. Specifically, the BN-Free versions of ReActNet-18 and ReActNet-A are constructed to compare with other existing state-of-the-art BNNs (with BN). Top-1 accuracies are collected in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>. <ref type="table">Table 1</ref>. Comparison of the top-1 accuracy with state-of-theart binary methods on ImageNet. The accuracy of other binary networks are collected from the original papers, which include BNN <ref type="bibr" target="#b13">[14]</ref>, PCNN <ref type="bibr" target="#b18">[19]</ref>, XNOR-Net <ref type="bibr" target="#b50">[51]</ref>, Bi-RealNet <ref type="bibr" target="#b38">[39]</ref>, Real-to-Binary Net <ref type="bibr" target="#b41">[42]</ref>, ReActNet-18 (BN) and ReActNet-A (BN) <ref type="bibr" target="#b37">[38]</ref>. "w/o BN" denotes the version without batch normalization; "BN-Free" represents our proposed BN-Free BNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Network</head><p>Top-  As shown in <ref type="table">Table 1</ref>, compared with the binary neural network without BatchNorm layers, our BN-Free binary neural networks achieve substantial performance improvements. Specifically, we obtain 16.5% and 33.9% accuracy gains for the ReActNet-18 and ReActNet-A on ImageNet, respectively. Note that, our BN-Free ReActNet-A achieves a 68.0% top-1 accuracy, which only has marginal gap  (i.e.,1.4%) compared to the state-of-the-art ReActNet <ref type="bibr" target="#b37">[38]</ref> with batch normalization. Detailed training dynamics are presented in <ref type="figure" target="#fig_2">Figure 3</ref>. We observe that the proposed BN-Free BNN not only reaches a superior performance, but also leads to more stable training.</p><p>Results on CIFAR-10 and CIFAR-100. To further evaluate the effectiveness of BN-Free modules in BNN, we implement the three binary networks mentioned in section 4.1, i.e., XNORNet, Bi-RealNet, ReActNet, and compare the performance of their three variants (i.e., BN, w/o BN, BN-Free) on CIFAR-10 and CIFAR-100. With the results in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>, several consistent observations could be drawn as the following:</p><p>• The proposed BN-Free approach serves as a remedy for the accuracy degradation caused by the absence of BN layers across all datasets and networks. Specifically, when compared with their counterparts without BN, BN-Free BNNs achieve accuracy improvements of 1.75% ∼ 8.29%, 5.74% ∼ 15.63% for different binary networks on CIFAR-10 and CIFAR-100.</p><p>• Accuracy achieved by BN-Free ReActNet-A surpasses its BN counterpart surprisingly by 0.96% and 4.70% on CIFAR-10 and CIFAR-100, respectively. And BF-ReActNet-18 also achieves comparable performance with its BN version. However, for XNORNet-18 and Bi-RealNet-18, there remains a moderate performance gap between the BN and BN-Free networks.</p><p>• Training curves of BN-Free ReActNet on CIFAR-10 in <ref type="figure" target="#fig_4">Figure 4</ref>, almost overlaps (BN) ReActNet's curves in both training steps. This indicates our BF networks not only can achieve comparable accuracy but also ensure a stable training process, especially on small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In the previous section, we empirically evaluate the effectiveness of BN-Free modules and verify that our BF-ReActNet-A can reach competitive state-of-the-art perfor-  mance. To further investigate the effects of different clipping thresholds in AGC strategy and different components in the proposed BN-Free framework, we provide an ablation study on CIFAR-10 and CIFAR-100 with ReActNet-18 and ReActNet-A as the backbone BNNs.</p><p>Clipping threshold in AGC. The clipping threshold λ plays an important role in the effectiveness of AGC <ref type="bibr" target="#b6">[7]</ref>. In this paragraph, we empirically analyze how does the threshold values affect the training process and final performance. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the results indicate that with the growth of the clipping threshold, the final test accuracy first increases to the peak then begins to decline. The results also show that we can get an extra accuracy improvement of 1.00% ∼ 6.88% by using an appropriate threshold. In addi-  tion, the performance of BN-Free networks on CIFAR-10 is less affected by the clipping values. A possible explanation is that the performance on the simple CIFAR-10 classification is saturated and less sensitive. Furthermore, <ref type="figure" target="#fig_5">Figure 5</ref> demonstrates that the training process becomes less stable when the threshold in AGC is extremely small. It comes as no surprise that aggressive gradient clipping introduces undesired noise and causes instability.</p><p>Different components in the BN-Free framework. As described in Section 3.1, the BN-Free module is constructed with a specialized block for the variance normalization, and a scaled weight standardization technique that is applied to all convolution layers (WS-Conv). To study the effects of different components in the BN-Free framework, we construct five variants on top of baseline BNNs <ref type="table">(ReActNet-18</ref>   <ref type="table" target="#tab_4">Table 4</ref> and their corresponding training dynamics are presented in <ref type="figure">Figure 6</ref>, from which several observations could be drawn:</p><p>• Either specialized block or WS-Conv can improve the performance of binary networks independently, specifically, the separate improvement achieved by WS-Conv ranges from 1.58% to 13.00% and the separate improvement of the specialized block ranges from 1.03% to 15.07%. In addition, the combination of these two approaches can further benefit the BN-Free binary neural networks.</p><p>• WS-Conv benefits more than the specialized bottleneck by 0.47% ∼ 4.57% performance gains, except the experiment of ReActNet-A on CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>In this section, we provide the visualization of gradient, latent weight, and activation distributions. Three variants of ReActNet-A (i.e., BN, w/o BN, BN-Free) trained on CIFAR-10 are considered.</p><p>Activation distribution. We visualize the activation distribution in <ref type="figure">Figure 7</ref>. Compared with the network without BN, the values of the activation inside the BN-Free network are consistently concentrated in a smaller region, which provides some insights into the training stability. Gradient distribution. In <ref type="figure" target="#fig_8">figure 8</ref>, we show histogram visualizations of the gradient distribution. Our proposed BN-Free BNNs (red bars) tend to have a smaller range for gradients, which potentially prevents the emergence of gra- dient exploration caused by training without batch normalization <ref type="bibr" target="#b54">[55]</ref>.</p><p>Latent weight distribution. <ref type="figure" target="#fig_9">Figure 9</ref> present the latent weight distribution of three variants of ReActNet-A. We observe that BN-Free BNNs have a more zero-centralized weight distribution, which mainly stems from the weight standardization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we for the first time propose a framework for training binary neural networks without batch normalization, i.e., BN-Free BNN, which achieves competitive state-of-the-art performance compared to its BN-based counterpart. Specifically, We introduce the scaled weight standardization to deal with the mean-shift in the hidden activation distributions caused by removing BN and apply a specialized bottleneck block for the purpose of variance preserving. Moreover, adaptive gradient clipping is adopted to mitigate the gradient exploration issue and stabilize training, for the BN-Free BNN. With the contributions jointly achieved by these techniques, our BN-Free ReAct-Net achieves 92.08%, 68.34%, and 68.00% on CIFAR-10, CIFAR-100, and ImageNet, respectively. Note that our BN-Free BNN totally gets rid of batch normalization in both training and inference regimes. In the future, we would be interested to examine the speedup and energy-saving results of the BNN training/inference on a hardware platform.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Top-1 accuracies of different binary neural networks (BNN) evaluated on ImageNet. Blue bars denote previous BNN methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Results of validation accuracy over epochs on ImageNet with ReActNet-18/A. The green background represents the first training step, in which only activations are binarized. And in the orange part, both activations and weights are binary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Results of testing accuracy over epochs on CIFAR-10/100 with ReActNet-18/A. The green background represents the first training step, in which only activations are binarized. And in the orange part, both activations and weights are binary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Results of testing accuracy curves of different clipping values in AGC on CIFAR-10/100 with ReActNet-18/A. The green background represents the first training step, where only activations are binarized. In the orange part, both activations and weights are binary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and ReActNet-A): a) original baseline (with BN); b) baseline (w/o BN); c) baseline (w/o BN) + WS-Conv; d) baseline (w/o BN) + specialized block; e) baseline (w/o BN) + WS-Conv + specialized block which is equivalent to the complete BN-Free setup. AGC with the best clipping threshold is adopted. The results are collected in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>18 Figure 6 .Figure 7 .</head><label>1867</label><figDesc>Results of testing accuracy over epochs on CIFAR-10/100 with ReActNet-18/A. The green background represents the first training step, in which only activations are binarized. And in the orange part, both activations and weights are binary. Histogram of the activation distribution inside three variants of ReActNet-A on CIFAR-10: with BN (top), without BN (middle) and BN-Free (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of gradient distributions of the three variants of ReActNet-A on CIFAR-10: with BN (blue), without BN (green) and BN-Free (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of latent weight distributions of the three variants of ReActNet-A on CIFAR-10: with BN (blue), without BN (green) and BN-Free (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>ReAct Sign 1-bit 3x3 Conv BatchNorm ReAct PReLU ReAct Sign 1-bit 1x1 Conv BatchNorm ReAct PReLU ReAct Sign 1-bit 3x3 Conv BatchNorm ReAct PReLU ReAct Sign 1-bit 1x1 Conv BatchNorm ReAct PReLU 2x2 AvgPool s=2 ReAct Sign 1-bit 1x1 Conv BatchNorm ReAct PReLU Concatenate Duplicate activation ReAct Sign 1-bit 1x1 WS-Conv ReAct PReLU ReAct Sign 1-bit 3x3 WS-Conv ReAct PReLU ReAct Sign 1-bit 3x3 WS-Conv ReAct PReLU ReAct Sign 1-bit 1x1 WS-Conv 2x2 AvgPool s=2 1-bit 1x1 WS-Conv Concatenate ReAct PReLU ReAct Sign Duplicate activation</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Normal</cell><cell>Reduction</cell></row><row><cell></cell><cell>Block</cell><cell>Block</cell></row><row><cell>Normal</cell><cell>Reduction</cell></row><row><cell>Block</cell><cell>Block</cell></row><row><cell>(a) Baseline Network Block of ReActNet</cell><cell>(b) Proposed BN-Free Network Block</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the top-1 accuracy between the three variants (i.e., BN, w/o BN, BN-Free) of binary networks on CIFAR-10 and CIFAR-100. All networks are modified from ResNet-18 except for ReActNet-A, which is constructed from MobileNetv1.</figDesc><table><row><cell>Binary Network</cell><cell></cell><cell cols="2">CIFAR-10 (%)</cell><cell></cell><cell cols="2">CIFAR-100 (%)</cell></row><row><cell></cell><cell>BN</cell><cell cols="2">w/o BN BN-Free</cell><cell>BN</cell><cell cols="2">w/o BN BN-Free</cell></row><row><cell>XNORNet-18</cell><cell>90.21</cell><cell>71.75</cell><cell>79.67</cell><cell>65.35</cell><cell>45.30</cell><cell>53.76</cell></row><row><cell cols="2">Bi-RealNet-18 89.12</cell><cell>71.30</cell><cell>79.59</cell><cell>63.51</cell><cell>47.72</cell><cell>54.34</cell></row><row><cell>ReActNet-18</cell><cell>92.31</cell><cell>90.33</cell><cell>92.08</cell><cell>68.78</cell><cell>62.60</cell><cell>68.34</cell></row><row><cell>ReActNet-A</cell><cell>82.95</cell><cell>77.60</cell><cell>83.91</cell><cell>50.30</cell><cell>39.37</cell><cell>55.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation Study of clipping threshold values in AGC on CIFAR-10/100 with ReActNet-18 and ReActNet-A.</figDesc><table><row><cell>Clipping Value</cell><cell cols="2">ReActNet-18</cell><cell cols="2">ReActNet-A</cell></row><row><cell></cell><cell cols="4">CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100</cell></row><row><cell>w/o AGC</cell><cell>91.08</cell><cell>66.15</cell><cell>82.39</cell><cell>48.12</cell></row><row><cell>1 × 10 −2</cell><cell>91.23</cell><cell>65.38</cell><cell>82.61</cell><cell>49.46</cell></row><row><cell>5 × 10 −3</cell><cell>91.03</cell><cell>66.03</cell><cell>83.10</cell><cell>48.69</cell></row><row><cell>1 × 10 −3</cell><cell>92.08</cell><cell>68.34</cell><cell>83.91</cell><cell>51.32</cell></row><row><cell>8 × 10 −4</cell><cell>91.54</cell><cell>67.95</cell><cell>83.58</cell><cell>52.27</cell></row><row><cell>5 × 10 −4</cell><cell>91.39</cell><cell>67.36</cell><cell>83.31</cell><cell>53.60</cell></row><row><cell>2 × 10 −4</cell><cell>90.43</cell><cell>67.07</cell><cell>83.03</cell><cell>55.00</cell></row><row><cell>1 × 10 −4</cell><cell>89.62</cell><cell>62.45</cell><cell>80.81</cell><cell>52.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Ablation study of the separate effect of scaled weight stan-</cell></row><row><cell cols="3">dardization and normalizer-free block on CIFAR-10 and CIFAR-</cell></row><row><cell cols="3">100 with two binary networks based on ReActNet. Test accuracies</cell></row><row><cell>are reported.</cell><cell></cell><cell></cell></row><row><cell>Settings</cell><cell cols="2">ReActNet-18 (%)</cell></row><row><cell></cell><cell cols="2">CIFAR-10 CIFAR-100</cell></row><row><cell>BN</cell><cell>92.31</cell><cell>68.79</cell></row><row><cell>w/o BN</cell><cell>90.33</cell><cell>62.60</cell></row><row><cell>WS-Conv</cell><cell>91.91</cell><cell>68.20</cell></row><row><cell>Specialized Block (i.e., α, 1 β )</cell><cell>91.44</cell><cell>63.63</cell></row><row><cell>BN-Free</cell><cell>92.08</cell><cell>68.34</cell></row><row><cell>Settings</cell><cell cols="2">ReActNet-A (%)</cell></row><row><cell></cell><cell cols="2">CIFAR-10 CIFAR-100</cell></row><row><cell>BN</cell><cell>82.95</cell><cell>50.30</cell></row><row><cell>w/o BN</cell><cell>77.60</cell><cell>39.37</cell></row><row><cell>WS-Conv</cell><cell>82.34</cell><cell>52.37</cell></row><row><cell>Specialized Block (i.e., α, 1 β )</cell><cell>80.45</cell><cell>54.44</cell></row><row><cell>BN-Free</cell><cell>83.91</cell><cell>55.00</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargava</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venu</forename><surname>Govindaraju</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1168" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanru</forename><forename type="middle">Henry</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennox</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt Wan-Duo</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcwilliams</surname></persName>
		</author>
		<idno>PMLR, 2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11046</idno>
		<title level="m">Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel L</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08692</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13863</idno>
		<title level="m">Xnor-net++: Improved binary neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning with low precision by half-wave gaussian quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5918" to="5926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09282</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00363</idno>
		<title level="m">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fractrain: Fractionally squeezing bit savings both temporally and spatially for efficient dnn training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Comparison of batch normalization and weight normalization algorithms for the large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08145</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Projection convolutional neural networks for 1-bit cnns via discrete back propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training binary neural networks through learning with noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4017" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02626</idno>
		<title level="m">Learning both weights and connections for efficient neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01719</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Latent weights do not exist: Rethinking binarized neural network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Helwegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Widdicombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roeland</forename><surname>Nusselder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02107</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James T</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01600</idno>
		<title level="m">Lossaware binarization of deep networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Freeze and chaos for dnns: an ntk view of batch normalization, checkerboard and boundary effects. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06071</idno>
		<title level="m">Bitwise neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance guaranteed network acceleration via high-order residual quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2584" to="2592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reactnet: Towards precise binary neural network with generalized activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="722" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanglin</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00846</idno>
		<title level="m">Towards understanding regularization in batch normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-tobinary convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debbie</forename><surname>Marr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05852</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel E O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Meta pseudo labels. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05668</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Micro-batch training with batch-channel normalization and weight standardization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Binary neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">107281</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11604</idno>
		<title level="m">How does batch normalization help optimization? arXiv preprint</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fractional skipping: Towards finer-grained dynamic cnn inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianghao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5700" to="5708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Searching for accurate binary neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Four things everyone should know to improve batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael J Dinneen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03548</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Magnus Jahre, and Kees Vissers. Finn: A framework for fast, scalable binarized neural network inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaman</forename><surname>Umuroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">E2-train: Training state-of-the-art cnns with over 80% energy savings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning channel-wise interactions for binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="568" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2082" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">l1-norm batch normalization for efficient training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2043" to="2051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Learning frequency domain approximation for binary neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00841</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Accurate and compact convolutional neural networks with trained binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11366</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08129</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samuel S Schoenholz. A mean field theory of batch normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Shiftaddnet: A hardware-inspired deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09321</idno>
		<title level="m">Fixup initialization: Residual learning without normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Smartexchange: Trading higher-cost memory storage/access for lower-cost computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="954" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
