<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uchicago</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
						</author>
						<title level="a" type="main">The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce three new robustness benchmarks consisting of naturally occurring distribution changes in image style, geographic location, camera operation, and more. Using our benchmarks, we take stock of previously proposed hypotheses for out-of-distribution robustness and put them to the test. We find that using larger models and synthetic data augmentation can improve robustness on real-world distribution shifts, contrary to claims in prior work. Motivated by this, we introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000× more labeled data. We find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Hence no evaluated method consistently improves robustness. We conclude that future research must study multiple distribution shifts simultaneously. * Equal contribution.</p><p>Code is available at https://github.com/hendrycks/imagenet-r . ImageNet: A large-scale hierarchical image database. CVPR, 2009. Samuel Dodge and Lina Karam. A study and comparison of human and deep learning recognition performance under visual distortions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While the research community must create robust models that generalize to new scenarios, the robustness literature <ref type="bibr">(Dodge and Karam, 2017;</ref><ref type="bibr">Geirhos et al., 2020)</ref> lacks consensus on evaluation benchmarks and contains many dissonant hypotheses. <ref type="bibr">Hendrycks et al. (2020a)</ref> find that many recent language models are already robust to many forms of distribution shift, while <ref type="bibr" target="#b9">Yin et al. (2019)</ref> and <ref type="bibr">Geirhos et al. (2019)</ref> find that vision models are largely fragile and argue that data augmentation offers one solution. In contrast, <ref type="bibr" target="#b2">Taori et al. (2020)</ref> provide results suggesting that using pretraining and improving in-distribution test set accuracy improve natural robustness, whereas other methods do not.</p><p>In this paper we articulate and systematically study seven robustness hypotheses. The first four hypotheses concern methods for improving robustness, while the last three hypotheses concern abstract properties about robustness. These hypotheses are as follows.</p><p>• Larger Models: increasing model size improves robustness <ref type="bibr">(Hendrycks and Dietterich, 2019;</ref><ref type="bibr" target="#b7">Xie and Yuille, 2020</ref>). • Self-Attention: adding self-attention layers to models improves robustness <ref type="bibr">(Hendrycks et al., 2019b)</ref>. • Diverse Data Augmentation: robustness can increase through data augmentation <ref type="bibr" target="#b9">(Yin et al., 2019)</ref>. • Pretraining: pretraining on larger and more diverse datasets improves robustness <ref type="bibr">(Orhan, 2019;</ref><ref type="bibr">Hendrycks et al., 2019a)</ref>.</p><p>Figure 1: Images from our three new datasets ImageNet-Renditions (ImageNet-R), DeepFashion Remixed (DFR), and StreetView StoreFronts (SVSF). The SVSF images are recreated from the public Google StreetView, copyright Google 2020. Our datasets test robustness to various naturally occurring distribution shifts including rendition style, camera viewpoint, and geography.</p><p>• Texture Bias: convolutional networks are biased towards texture, which harms robustness <ref type="bibr">(Geirhos et al., 2019)</ref>. • Only IID Accuracy Matters: accuracy on independent and identically distributed test data entirely determines natural robustness. • Synthetic =⇒ Natural: synthetic robustness interventions including diverse data augmentations do not help with robustness on naturally occurring distribution shifts <ref type="bibr" target="#b2">(Taori et al., 2020)</ref>.</p><p>It has been difficult to arbitrate these hypotheses because existing robustness datasets preclude the possibility of controlled experiments by varying multiple aspects simultaneously. For instance, Texture Bias was initially investigated with synthetic distortions <ref type="bibr">(Geirhos et al., 2018)</ref>, which conflicts with the Synthetic =⇒ Natural hypothesis. On the other hand, natural distribution shifts often affect many factors (e.g., time, camera, location, etc.) simultaneously in unknown ways <ref type="bibr">(Recht et al., 2019;</ref><ref type="bibr">Hendrycks et al., 2019b)</ref>. Existing datasets also lack diversity such that it is hard to extrapolate which methods will improve robustness more broadly. To address these issues and test the seven hypotheses outlined above, we introduce three new robustness benchmarks and a new data augmentation method.</p><p>First we introduce ImageNet-Renditions (ImageNet-R), a 30,000 image test set containing various renditions (e.g., paintings, embroidery, etc.) of ImageNet object classes. These renditions are naturally occurring, with textures and local image statistics unlike those of ImageNet images, allowing us to more cleanly separate the Texture Bias and Synthetic =⇒ Natural hypotheses.</p><p>Next, we investigate natural shifts in the image capture process with StreetView StoreFronts (SVSF) and DeepFashion Remixed (DFR). SVSF contains business storefront images taken from Google Streetview, along with metadata allowing us to vary location, year, and even the camera type. DFR leverages the metadata from DeepFashion2 <ref type="bibr" target="#b4">(Ge et al., 2019)</ref> to systematically shift object occlusion, orientation, zoom, and scale at test time. Both SVSF and DFR provide distribution shift controls and do not alter texture, which remove possible confounding variables affecting prior benchmarks.</p><p>Finally, we contribute DeepAugment to increase robustness to some new types of distribution shift. This augmentation technique uses image-to-image neural networks for data augmentation, not data-independent Euclidean augmentations like image shearing or rotating as in previous work. DeepAugment achieves state-of-the-art robustness on our newly introduced ImageNet-R benchmark and a corruption robustness benchmark. DeepAugment can also be combined with other augmentation methods to outperform a model pretrained on 1000× more labeled data.</p><p>After examining our results on these three datasets and others, we can rule out several of the above hypotheses while strengthening support for others. As one example, we find that synthetic data augmentation robustness interventions improve accuracy on ImageNet-R and real-world image blur distribution shifts, providing clear counterexamples to Synthetic =⇒ Natural while lending support to the Diverse Data Augmentation and Texture Bias hypotheses. In the conclusion, we summarize the various strands of evidence for and against each hypothesis. Across our many experiments, we do not find a general method that consistently improves robustness, and some hypotheses require additional qualifications. While robustness is often spoken of and measured as a single scalar property like accuracy, our investigations suggest that robustness is not so simple. In light of our results, we hypothesize in the conclusion that robustness is multivariate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Painting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sculpture Embroidery</head><p>Origami Cartoon Toy For example, we can select a test set with images produced by a camera different from the training set camera. We now describe the structure and collection of each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ImageNet-Renditions (ImageNet-R)</head><p>While current classifiers can learn some aspects of an object's shape <ref type="bibr">(Mordvintsev et al., 2015)</ref>, they nonetheless rely heavily on natural textural cues <ref type="bibr">(Geirhos et al., 2019)</ref>. In contrast, human vision can process abstract visual renditions. For example, humans can recognize visual scenes from line drawings as quickly and accurately as they can from photographs <ref type="bibr">(Biederman and Ju, 1988)</ref>.</p><p>Even some primates species have demonstrated the ability to recognize shape through line drawings <ref type="bibr">(Itakura, 1994;</ref><ref type="bibr">Tanaka, 2006)</ref>.</p><p>To measure generalization to various abstract visual renditions, we create the ImageNet-Rendition (ImageNet-R) dataset. ImageNet-R contains various artistic renditions of object classes from the original ImageNet dataset. Note the original ImageNet dataset discouraged such images since annotators were instructed to collect "photos only, no painting, no drawings, etc." <ref type="bibr">(Deng, 2012)</ref>. We do the opposite.</p><p>Data Collection. ImageNet-R contains 30,000 image renditions for 200 ImageNet classes. We collect images primarily from Flickr and use queries such as "art," "cartoons," "graffiti," "embroidery," "graphics," "origami," "paintings," "patterns," "plastic objects," "plush objects," "sculptures," "line drawings," "tattoos," "toys," "video game," and so on. Examples are depicted in <ref type="figure">Figure 2</ref>. Images are filtered by Amazon MTurk workers using a modified collection interface from ImageNetV2 <ref type="bibr">(Recht et al., 2019)</ref>. The resulting images are then manually filtered by graduate students. ImageNet-R also includes the line drawings from <ref type="bibr" target="#b4">Wang et al. (2019)</ref>, excluding horizontally mirrored duplicate images, pitch black images, and images from the incorrectly collected "pirate ship" class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">StreetView StoreFronts (SVSF)</head><p>Computer vision applications often rely on data from complex pipelines that span different hardware, times, and geographies. Ambient variations in this pipeline may result in unexpected performance degradation, such as degradations experienced by health care providers in Thailand deploying laboratory-tuned diabetic retinopathy classifiers in the field <ref type="bibr" target="#b1">(Beede et al., 2020)</ref>. In order to study the effects of shifts in the image capture process we collect the StreetView StoreFronts (SVSF) dataset, a new image classification dataset sampled from Google StreetView imagery <ref type="bibr" target="#b0">(Anguelov et al., 2010)</ref> focusing on three distribution shift sources: country, year, and camera.</p><p>Data Collection. SVSF consists of cropped images of business store fronts extracted from StreetView images by an object detection model. Each store front image is assigned the class label of the associated Google Maps business listing through a combination of machine learning models and human annotators. We combine several visually similar business types (e.g. drugstores and pharmacies) for a total of 20 classes, listed Appendix C. We are currently unable to release the SVSF data publicly.</p><p>Splitting the data along the three metadata attributes of country, year, and camera, we create one training set and five test sets. We sample a training set and an in-distribution test set (200K and 10K images, respectively) from images taken in US/Mexico/Canada during 2019 using a "new" camera system. We then sample four OOD test sets (10K images each) which alter one attribute at a time while keeping the other two attributes consistent with the training distribution. Our test sets are year: 2017, 2018; country: France; and camera: "old."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DeepFashion Remixed</head><p>Changes in day-to-day camera operation can cause shifts in attributes such as object size, object occlusion, camera viewpoint, and camera zoom.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DeepAugment</head><p>In order to further explore the Diverse Data Augmentation hypothesis, we introduce a new data augmentation technique. DeepAugment works by passing an image through an image-to-image networks (such as an image autoencoder or a superresolution network), but rather than processing the image normally, we distort the internal weights and activations. We distort the image-to-image network's weights by applying randomly sampled operations such as zeroing, negating, convolving, transposing, applying activation functions, and so on. This creates diverse but semantically consistent images as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. We provide the pseudocode in Appendix D. Whereas most previous data augmentations techniques use simple augmentation primitives applied to the raw image itself, we stochastically distort the internal representations of image-to-image networks to augment images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>In this section we briefly describe the evaluated models, pretraining techniques, self-attention mechanisms, data augmentation methods, and note various implementation details.  <ref type="bibr">Hendrycks and Dietterich, 2019)</ref>. We also consider adversarial training as a form of adaptive data augmentation and use the model from <ref type="bibr" target="#b5">Wong et al. (2020)</ref> trained against ∞ perturbations of size ε = 4/255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We now perform experiments on ImageNet-R, StreetView StoreFronts, DeepFashion Remixed. We also evaluate on ImageNet-C and compare and contrast it with real distribution shifts.</p><p>ImageNet-R. Regarding the three more abstract hypotheses, biasing networks away from natural textures through diverse data augmentation improved performance, so we find support for the Texture Bias hypothesis. The IID/OOD generalization gap varies greatly which condtradicts Only IID Accuracy Matters. Finally, since ImageNet-R contains real-world examples, and since synthetic data augmentation helps on ImageNet-R, we now have clear evidence against the Synthetic =⇒ Natural hypothesis.</p><p>StreetView StoreFronts. In <ref type="table">Table 2</ref>, we evaluate data augmentation methods on SVSF and find that all of the tested methods have mostly similar performance and that no method helps much on country shift, where error rates roughly double across the board. Images captured in France contain noticeably different architectural styles and storefront designs than those captured in US/Mexico/Canada; meanwhile, we are unable to find conspicuous and consistent indicators of the camera and year. This may explain the relative insensitivity of evaluated methods to the camera and year shifts. Overall Diverse Data Augmentation shows limited benefit, suggesting either that data augmentation primarily helps combat texture bias as with ImageNet-R, or that existing augmentations are not diverse enough to capture high-level semantic shifts such as building architecture.</p><p>DeepFashion Remixed.   ImageNet-C. We now consider a previous robustness benchmark to reassess all seven hypotheses. We use the ImageNet-C dataset (Hendrycks and Dietterich, 2019) which applies 15 common image corruptions (e.g., Gaussian noise, defocus blur, simulated fog, JPEG compression, etc.) across 5 severities to ImageNet-1K validation images. We find that DeepAugment improves robustness on ImageNet-C. <ref type="figure" target="#fig_1">Figure 4</ref> shows that when models are trained with AugMix and DeepAugment, they attain the state-of-the-art, break the trendline, and exceed the corruption robustness provided by training on 1000× more labeled training data. Note the augmentations from AugMix and DeepAugment are disjoint from ImageNet-C's corruptions. Full results are shown in Appendix B's <ref type="table">Table 8</ref>. This is evidence against the Only IID Accuracy Matters hypothesis and is evidence for the Larger Models, Self-Attention, Diverse Data Augmentation, Pretraining, and Texture Bias hypotheses. <ref type="bibr" target="#b2">Taori et al. (2020)</ref> remind us that ImageNet-C uses various synthetic corruptions and suggest that they are divorced from real-world robustness. Real-world robustness requires generalizing to naturally occurring corruptions such as snow, fog, blur, low-lighting noise, and so on, but it is an open question whether ImageNet-C's simulated corruptions meaningfully approximate real-world corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis</head><p>ImageNet-C Real Blurry Images ImageNet-R DFR SVSF Larger Models Evidence for is denoted "+", and "−" denotes an absence of evidence or evidence against.</p><formula xml:id="formula_0">+ + + − Self-Attention + + − − Diverse Data Augmentation + + + − − Pretraining + + − −</formula><p>We collect a small dataset of 1,000 real-world blurry images and find that ImageNet-C can track robustness to real-world corruptions. We collect the "Real Blurry Images" dataset with Flickr and query ImageNet object class names concatenated with the word "blurry." We then evaluate various models on real-world blurry images and find that all the robustness interventions that help with ImageNet-C also help with real-world blurry images. Hence ImageNet-C can track performance on real-world corruptions. Moreover, DeepAugment+AugMix has the lowest error rate on Real Blurry Images, which again contradicts the Synthetic =⇒ Natural hypothesis. Appendix A has full results. The upshot is that ImageNet-C is a controlled and systematic proxy for real-world robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we introduced three new benchmarks, ImageNet-Renditions, DeepFashion Remixed, and StreetView StoreFronts. With these benchmarks, we thoroughly tested seven robustness hypothesesfour about methods for robustness, and three about the nature of robustness.</p><p>Let us consider the first four hypotheses, using the new information from ImageNet-C and our three new benchmarks. The Larger Models hypothesis was supported with ImageNet-C and ImageNet-R, but not with DFR. While Self-Attention noticeably helped ImageNet-C, it did not help with ImageNet-R and DFR. Diverse Data Augmentation was ineffective for SVSF and DFR, but it greatly improved ImageNet-C and ImageNet-R accuracy. Pretraining greatly helped with ImageNet-C but hardly helped with DFR and ImageNet-R. This is summarized in <ref type="table" target="#tab_6">Table 4</ref>. It was not obvious a priori that synthetic Diverse Data Augmentation could improve ImageNet-R accuracy, nor did previous research suggest that Pretraining would sometimes be ineffective. While no single method consistently helped across all distribution shifts, some helped more than others.</p><p>Our analysis of these four hypotheses have implications for the remaining three hypotheses. Regarding Texture Bias, ImageNet-R shows that networks do not generalize well to renditions (which have different textures), but that diverse data augmentation (which often distorts textures) can recover accuracy. More generally, larger models and diverse data augmentation consistently helped on ImageNet-R, ImageNet-C, and Blurry Images, suggesting that these two interventions reduce texture bias. However, these methods helped little for geographic shifts, showing that there is more to robustness than texture bias alone. Regarding Only IID Accuracy Matters, while IID accuracy is a strong predictor of OOD accuracy, it is not decisive- <ref type="table" target="#tab_6">Table 4</ref> shows that many methods improve robustness across multiple distribution shifts, and recent experiments in NLP provide further counterexamples <ref type="bibr">(Hendrycks et al., 2020a)</ref>. Finally, Synthetic =⇒ Natural has clear counterexamples given that DeepAugment greatly increases accuracy on ImageNet-R and Real Blurry Images. In summary, some previous hypotheses are implausible, and the Texture Bias hypothesis has the most support.</p><p>Our seven hypotheses presented several conflicting accounts of robustness. What led to this conflict? We suspect it is because robustness is not one scalar like accuracy. The research community is reasonable in judging IID accuracy with a univariate metric like ImageNet classification accuracy, as models with higher ImageNet accuracy reliably have better fine-tuned classification accuracy on other tasks <ref type="bibr">(Kornblith et al., 2018)</ref>. In contrast, we argue it is too simplistic to judge OOD accuracy with a univariate metric like, say, ImageNetV2 or ImageNet-C accuracy. Instead we hypothesize that robustness is multivariate. This Multivariate hypothesis means that there is not a single scalar model property that wholly governs natural model robustness.</p><p>If robustness has many faces, future work should evaluate robustness using many distribution shifts; for example, ImageNet models should at least be tested against ImageNet-C and ImageNet-R. Future work could further characterize the space of distribution shifts. However, due to this paper, there are now more out-of-distribution robustness datasets than there are published robustness methods. Hence the research community should prioritize creating new robustness methods. If our Multivariate hypothesis is true, multiple tests are necessary to develop models that are both robust and safe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Real Blurry Images and ImageNet-C</head><p>We collect 1,000 blurry images to see whether improvements on ImageNet-C's simulated blurs correspond to improvements on real-world blurry images. Each image belongs to an ImageNet class. Examples are in <ref type="figure">Figure 5</ref>. Results from <ref type="table">Table 5</ref> show that Larger Models, Self-Attention, Diverse Data Augmentation, Pretraining all help, just like ImageNet-C. Here DeepAugment+AugMix attains state-of-the-art. These results suggest ImageNet-C's simulated corruptions track real-world corruptions. In hindsight, this is expected since various computer vision problems have used synthetic corruptions as proxies for real-world corruptions, for decades. In short, ImageNet-C is a diverse and systematic benchmark that is correlated with improvements on real-world corruptions.  <ref type="table">Table 5</ref>: ImageNet-C vs Real Blurry Images. All values are error rates and percentages. The rank orderings of the models on Real Blurry Images are similar to the rank orderings for "ImageNet-C Blur Mean," so ImageNet-C's simulated blurs track real-world blur performance. Hence synthetic image corruptions and real-world image corruptions are not loose and separate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Results</head><p>ImageNet-R. Expanded ImageNet-R results are in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>WSL pretraining on Instagram images appears to yield dramatic improvements on ImageNet-R, but the authors note the prevalence of artistic renditions of object classes on the Instagram platform. While ImageNet's data collection process actively excluded renditions, we do not have reason to believe the Instagram dataset excluded renditions. On a ResNeXt-101 32×8d model, WSL pretraining improves ImageNet-R performance by a massive 37.5% from 57.5% top-1 error to 24.2%. Ultimately, without examining the training images we are unable to determine whether ImageNet-R represents an actual distribution shift to the Instagram WSL models. However, we also observe that with greater controls, that is with ImageNet-21K pre-training, pretraining hardly helped ImageNet-R performance, so it is not clear that more pretraining data improves ImageNet-R performance.</p><p>Increasing model size appears to automatically improve ImageNet-R performance, as shown in <ref type="figure">Figure 6a</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-C Error Across Severities</head><p>ResNet-50 Style Transfer DeepAugment+AugMix (b) Accuracy as a function of corruption severity. Severity "0" denotes clean data.</p><p>DeepAugment with AugMix shifts the entire Pareto frontier outward.</p><p>error. ResNeXt-50 32×4d (25.0M) attains 62.3% error and ResNeXt-101 32×8d (88M) attains 57.5% error.</p><p>ImageNet-C. Expanded ImageNet-C results are <ref type="table">Table 8</ref>. We also tested whether model size improves performance on ImageNet-C for even larger models. With a different codebase, we trained ResNet-50, ResNet-152, and ResNet-500 models which achieved 80.6, 74.0, and 68.5 mCE respectively.  <ref type="table" target="#tab_11">Table 9</ref>. Notice Res2Net architectures <ref type="bibr">(Gao et al., 2019)</ref> can greatly improve accuracy. Results also show that Larger Models, Self-Attention, and Pretraining help, while Diverse Data Augmentation usually does not help substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications for the Four Method Hypotheses.</head><p>The Larger Models hypothesis has support with ImageNet-C (+), ImageNet-A (+), ImageNet-R (+), yet does not markedly improve DFR (−) performance. The Self-Attention hypothesis has support with ImageNet-C (+), ImageNet-A (+), yet does not help ImageNet-R (−) and DFR (−) performance. The Diverse Data Augmentation hypothesis has support with ImageNet-C (+), ImageNet-R (+), yet does not markedly improve ImageNet-A (−), DFR(−), nor SVSF (−) performance. The Pretraining hypothesis has support with ImageNet-C (+), ImageNet-A (+), yet does not markedly improve DFR (−) nor ImageNet-R (−) performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis</head><p>ImageNet-C ImageNet-A ImageNet-R DFR SVSF Larger Models     </p><formula xml:id="formula_1">+ + + − Self-Attention + + − − Diverse Data Augmentation + − + − − Pretraining + + − −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DeepAugment Details</head><p>Pseudocode. Below is Pythonic pseudocode for DeepAugment. The basic structure of DeepAugment is agnostic to the backbone network used, but specifics such as which layers are chosen for various transforms may vary as the backbone architecture varies. We do not need to train many different image-to-image models to get diverse distortions <ref type="bibr" target="#b12">(Zhang et al., 2018;</ref><ref type="bibr">Lee et al., 2020)</ref>. We only use two existing models, the EDSR super-resolution model <ref type="bibr">(Lim et al., 2017)</ref> and the CAE image compression model <ref type="bibr" target="#b3">(Theis et al., 2017)</ref>. See full code for such details.</p><p>At a high level, we process each image with an image-to-image network. The image-to-image's weights and feedforward signal pass are distorted with each pass. The distortion is made possible by, for example, negating the network's weights and applying dropout to the feedforward signal. The resulting image is distorted and saved. This process generates an augmented dataset.</p><p>1 def main () : </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>DeepAugment examples preserve semantics, are data-dependent, and are far more visually diverse than augmentations such as rotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>ImageNet accuracy and ImageNet-C accuracy. Previous architectural advances slowly translate to ImageNet-C performance improvements, but DeepAugment+AugMix on a ResNet-50 yields a ≈ 19% accuracy increase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-A. ImageNet-A (Hendrycks et al., 2019b) is an adversarially filtered test set. This dataset contains examples that are difficult for a ResNet-50 to classify, so examples solvable by simple spurious cues are are especially infrequent in this dataset. Results are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2##</head><label></label><figDesc>net . apply_weights ( de epA ug men t_g et Net wo rk () ) # EDSR , CAE , ...3 for image in dataset : # May be the ImageNet training set 4 if np . random . uniform () &lt; 0.05: # Arbitrary refresh prob 5 net . apply_weights ( de epA ug men t_g et Net wo rk () ) 6 new_image = net . d ee pA u gm en t _f or wa r dP as s ( image ) 7 8 def d ee pAu gme nt _ge tN etw or k () : Clean forward pass . Compare to de e pA ug me n t_ fo r wa rd Pa s s () Our forward pass . Compare to clean_forwardPass () 51 def de ep Au g me nt _f o rw ar d Pa ss ( X ) : 52 # Returns a list of distortions , each of which 53 # will be applied at a different layer . 54 signal_distortions = s a m p l e _ s ig n a l _ d i s t o r ti o n s () l y _l a y e r _ N _ d i s to r t i o n s (X , signal_distortions ) return X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>To measure this, we repurpose DeepFashion2<ref type="bibr" target="#b4">(Ge et al., 2019)</ref> to create the DeepFashion Remixed (DFR) dataset. We designate a training set with 48K images and create eight out-of-distribution test sets to measure performance under shifts in object size, object occlusion, camera viewpoint, and camera zoom-in. DeepFashion Remixed is a multi-label classification task since images may contain more than one clothing item per image.</figDesc><table /><note>Data Collection. Similar to SVSF, we fix one value for each of the four metadata attributes in the training distribution. Specifically, the DFR training set contains images with medium scale, medium occlusion, side/back viewpoint, and no zoom-in. After sampling an IID test set, we construct eight OOD test distributions by altering one attribute at a time, obtaining test sets with minimal and heavy occlusion; small and large scale; frontal and not-worn viewpoints; and medium and large zoom-in. See Appendix C for details on test set sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Model Architectures and Sizes. Most experiments are evaluated on a standard ResNet-50 model(He et al., 2015). Model size evaluations use ResNets or ResNeXts<ref type="bibr" target="#b8">(Xie et al., 2016)</ref> of varying sizes. Pretraining. For pretraining we use ImageNet-21K which contains approximately 21,000 classes and approximately 14 million labeled training images, or around 10× more labeled training data than ImageNet-1K. We tune Kolesnikov et al. (2019)'s ImageNet-21K model. We also use a large pretrained ResNeXt-101 model from Mahajan et al. (2018). This was pre-trained on on approximately 1 billion Instagram images with hashtag labels and fine-tuned on ImageNet-1K. This Weakly Supervised Learning (WSL) pretraining strategy uses approximately 1000× more labeled data. ImageNet-200 and ImageNet-R top-1 error rates. ImageNet-200 uses the same 200 classes as ImageNet-R. DeepAugment+AugMix improves over the baseline by over 10 percentage points. ImageNet-21K Pretraining tests Pretraining and CBAM tests Self-Attention. Style Transfer, AugMix, and DeepAugment test Diverse Data Augmentation in contrast to simpler noise augmentations such as ∞ Adversarial Noise and Speckle Noise. While there remains much room for improvement, results indicate that progress on ImageNet-R is tractable.</figDesc><table><row><cell></cell><cell cols="3">ImageNet-200 (%) ImageNet-R (%) Gap</cell></row><row><cell>ResNet-50</cell><cell>7.9</cell><cell>63.9</cell><cell>56.0</cell></row><row><cell>+ ImageNet-21K Pretraining (10× labeled data)</cell><cell>7.0</cell><cell>62.8</cell><cell>55.8</cell></row><row><cell>+ CBAM (Self-Attention)</cell><cell>7.0</cell><cell>63.2</cell><cell>56.2</cell></row><row><cell>+ ∞ Adversarial Training</cell><cell>25.1</cell><cell>68.6</cell><cell>43.5</cell></row><row><cell>+ Speckle Noise</cell><cell>8.1</cell><cell>62.1</cell><cell>54.0</cell></row><row><cell>+ Style Transfer Augmentation</cell><cell>8.9</cell><cell>58.5</cell><cell>49.6</cell></row><row><cell>+ AugMix</cell><cell>7.1</cell><cell>58.9</cell><cell>51.8</cell></row><row><cell>+ DeepAugment</cell><cell>7.5</cell><cell>57.8</cell><cell>50.3</cell></row><row><cell>+ DeepAugment + AugMix</cell><cell>8.0</cell><cell>53.2</cell><cell>45.2</cell></row><row><cell>ResNet-152 (Larger Models)</cell><cell>6.8</cell><cell>58.7</cell><cell>51.9</cell></row><row><cell>Table 1:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">, 2019) uses a style transfer</cell></row></table><note>Self-Attention. When studying self-attention, we employ CBAM (Woo et al., 2018) and SE (Hu et al., 2018) modules, two forms of self-attention that help models learn spatially distant dependencies. Data Augmentation. We use Style Transfer, AugMix, and DeepAugment to analyze the Diverse Data Augmentation hypothesis, and we contrast their performance with simpler noise augmentations such as Speckle Noise and adversarial noise. Style transfer (Geirhos et al.network to apply artwork styles to training images. AugMix (Hendrycks et al., 2020b) randomly composes simple augmentation operations (e.g., translate, posterize, solarize). DeepAugment, introduced above, distorts the weights and feedforward passes of image-to-image models to generate image augmentations. Speckle Noise data augmentation muliplies each pixel by (1 + x) with x sampled from a normal distribution (Rusak et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>shows performance on ImageNet-R as well as on ImageNet-200 (the original ImageNet data restricted to ImageNet-R's 200 classes). This has several implications regarding the four method-specific hypotheses. Pretraining with ImageNet-21K (approximately 10× labeled data) hardly helps. Appendix B shows WSL pretraining can help, but Instagram has renditions, while ImageNet excludes them; hence we conclude comparable pretraining was ineffective. Notice Self-Attention increases the IID/OOD gap. Compared to simpler data augmentation techniques such as Speckle Noise, the Diverse Data Augmentation techniques of Style Transfer, AugMix, and DeepAugment improve generalization. Note AugMix and DeepAugment improve in-distribution performance whereas Style transfer hurts it. Also, our new DeepAugment technique is the best standalone method with an error rate of 57.8%. Last, Larger Models reduce the IID/OOD gap.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>SVSF classification error rates. Networks are robust to some natural distribution shifts but are substantially more sensitive the geographic shift. Here Diverse Data Augmentation hardly helps.</figDesc><table><row><cell></cell><cell></cell><cell>Hardware</cell><cell></cell><cell>Year</cell><cell>Location</cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell>IID</cell><cell>Old</cell><cell cols="2">2017 2018</cell><cell>France</cell><cell></cell><cell></cell></row><row><cell>ResNet-50</cell><cell>27.2</cell><cell>28.6</cell><cell cols="2">27.7 28.3</cell><cell>56.7</cell><cell></cell><cell></cell></row><row><cell cols="2">+ Speckle Noise 28.5</cell><cell>29.5</cell><cell cols="2">29.2 29.5</cell><cell>57.4</cell><cell></cell><cell></cell></row><row><cell cols="2">+ Style Transfer 29.9</cell><cell>31.3</cell><cell cols="2">30.2 31.2</cell><cell>59.3</cell><cell></cell><cell></cell></row><row><cell cols="2">+ DeepAugment 30.5</cell><cell>31.2</cell><cell cols="2">30.2 31.3</cell><cell>59.1</cell><cell></cell><cell></cell></row><row><cell>+ AugMix</cell><cell>26.6</cell><cell>28.0</cell><cell cols="2">26.5 27.7</cell><cell>55.4</cell><cell></cell><cell></cell></row><row><cell cols="3">Table 2: Size</cell><cell cols="2">Occlusion</cell><cell cols="2">Viewpoint</cell><cell cols="2">Zoom</cell></row><row><cell>Network</cell><cell cols="8">IID OOD Small Large Slight/None Heavy No Wear Side/Back Medium Large</cell></row><row><cell>ResNet-50</cell><cell cols="2">77.6 55.1 39.4 73.0</cell><cell>51.5</cell><cell>41.2</cell><cell>50.5</cell><cell>63.2</cell><cell>48.7</cell><cell>73.3</cell></row><row><cell cols="3">+ ImageNet-21K Pretraining 80.8 58.3 40.0 73.6</cell><cell>55.2</cell><cell>43.0</cell><cell>63.0</cell><cell>67.3</cell><cell>50.5</cell><cell>73.9</cell></row><row><cell>+ SE (Self-Attention)</cell><cell cols="2">77.4 55.3 38.9 72.7</cell><cell>52.1</cell><cell>40.9</cell><cell>52.9</cell><cell>64.2</cell><cell>47.8</cell><cell>72.8</cell></row><row><cell>+ Random Erasure</cell><cell cols="2">78.9 56.4 39.9 75.0</cell><cell>52.5</cell><cell>42.6</cell><cell>53.4</cell><cell>66.0</cell><cell>48.8</cell><cell>73.4</cell></row><row><cell>+ Speckle Noise</cell><cell cols="2">78.9 55.8 38.4 74.0</cell><cell>52.6</cell><cell>40.8</cell><cell>55.7</cell><cell>63.8</cell><cell>47.8</cell><cell>73.6</cell></row><row><cell>+ Style Transfer</cell><cell cols="2">80.2 57.1 37.6 76.5</cell><cell>54.6</cell><cell>43.2</cell><cell>58.4</cell><cell>65.1</cell><cell>49.2</cell><cell>72.5</cell></row><row><cell>+ DeepAugment</cell><cell cols="2">79.7 56.3 38.3 74.5</cell><cell>52.6</cell><cell>42.8</cell><cell>54.6</cell><cell>65.5</cell><cell>49.5</cell><cell>72.7</cell></row><row><cell>+ AugMix</cell><cell cols="2">80.4 57.3 39.4 74.8</cell><cell>55.3</cell><cell>42.8</cell><cell>57.3</cell><cell>66.6</cell><cell>49.0</cell><cell>73.1</cell></row><row><cell cols="3">ResNet-152 (Larger Models) 80.0 57.1 40.0 75.6</cell><cell>52.3</cell><cell>42.0</cell><cell>57.7</cell><cell>65.6</cell><cell>48.9</cell><cell>74.4</cell></row></table><note>shows our experimental findings on DFR, in which all evaluated methods have an average OOD mAP that is close to the baseline. In fact, most OOD mAP increases</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>68</cell><cell>70</cell><cell>72</cell><cell>74</cell><cell>76</cell><cell>78</cell><cell>80</cell><cell>82</cell></row><row><cell></cell><cell></cell><cell cols="4">ImageNet Accuracy (%)</cell><cell></cell><cell></cell></row></table><note>DeepFashion Remixed results. Unlike the previous tables, higher is better since all values are mAP scores for this multi-label classification benchmark. The "OOD" column is the average of the row's rightmost eight OOD values. All techniques do little to close the IID/OOD generalization gap.track IID mAP increases. In general, DFR's size and occlusion shifts hurt performance the most. We also evaluate with Random Erasure augmentation, which deletes rectangles within the image, to simulate occlusion (Zhong et al., 2017). Random Erasure improved occlusion performance, but Style Transfer helped even more. Nothing substantially improved OOD performance beyond what is explained by IID performance, so here it would appear that Only IID Accuracy Matters. Our results do not provide clear evidence for the Larger Models, Self-Attention, Diverse Data Augmentation, and Pretraining hypotheses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>A highly simplified account of each hypothesis when tested against different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>A highly simplified account of each hypothesis when tested against different datasets. This table includes ImageNet-A results.</figDesc><table><row><cell>ImageNet-200 (%) ImageNet-R (%) Gap</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>ImageNet-200 and ImageNet-Renditions error rates. ImageNet-21K and WSL Pretraining test the Pretraining hypothesis, and here pretraining gives mixed benefits. CBAM and SE test the Self-Attention hypothesis, and these hurt robustness. ResNet-152 and ResNeXt-101 32×8d test the Larger Models hypothesis, and these help. Other methods augment data, and Style Transfer, AugMix, and DeepAugment provide support for the Diverse Data Augmentation hypothesis.</figDesc><table><row><cell>ImageNet-A (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>ImageNet-A top-1 accuracy. Renditions artistic renditions(cartoons, graffiti, embroidery, graphics, origami,  paintings, sculptures, sketches, tattoos, toys, ...)   </figDesc><table><row><cell>C Further Dataset Descriptions</cell></row><row><cell>ImageNet-R Classes. The 200 ImageNet classes and their WordNet IDs in ImageNet-R are as</cell></row><row><cell>follows.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Various distribution shifts represented in our three new benchmarks. ImageNet-Renditions is a new test set for ImageNet trained models measuring robustness to various object renditions. DeepFashion Remixed and StreetView StoreFronts each contain a training set and multiple test sets capturing a variety of distribution shifts.</figDesc><table><row><cell></cell><cell>Training set</cell><cell>Testing images</cell></row><row><cell>ImageNet-R</cell><cell>1281167</cell><cell>30000</cell></row><row><cell>DFR</cell><cell>48000</cell><cell>42640, 7440, 28160, 10360, 480, 11040, 10520, 10640</cell></row><row><cell>SVSF</cell><cell>200000</cell><cell>10000, 10000, 10000, 8195, 9788</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Number of images in each training and test set. ImageNet-R training set refers to the ILSVRC 2012 training set (Deng et al., 2009). DeepFashion Remixed test sets are: in-distribution, occlusion -none/slight, occlusion -heavy, size -small, size -large, viewpoint -frontal, viewpointnot-worn, zoom-in -medium, zoom-in -large. StreetView StoreFronts test sets are: in-distribution, capture year -2018, capture year -2017, camera system -new, country -France.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We should like to thank Collin Burns, Preetum Nakkiran, Aditi Raghunathan, Ludwig Schmidt, and Nicholas Carlini for their discussions or feedback. This material is in part based upon work supported by the National Science Foundation Frontier Award 1804794.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Google street view: Capturing the world at street level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><surname>Dulong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Filip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Frueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Beede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Baylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Hersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Iurchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paisan</forename><surname>Ruamviboonsuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Vardoulakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">When robustness doesn&apos;t promote robustness: Synthetic vs. natural distribution shifts on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxPIyrFvH" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00395</idno>
		<title level="m">Lossy image compression with compressive autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03994</idno>
		<title level="m">Fast is better than free: Revisiting adversarial training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intriguing properties of adversarial training at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Fourier perspective on model robustness in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08988</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<title level="m">Cutmix: Regularization strategy to train strong classifiers with localizable features. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">scottish terrier, west highland white terrier, golden retriever, labrador retriever, cocker spaniels, collie, border collie, rottweiler, german shepherd dog, boxer, french bulldog, saint bernard, husky, dalmatian, pug, pomeranian, chow chow, pembroke welsh corgi, toy poodle, standard poodle, timber wolf, hyena, red fox, tabby cat, leopard, snow leopard, lion, tiger, cheetah, polar bear, meerkat, ladybug, fly, bee, ant, grasshopper, cockroach, mantis, dragonfly, monarch butterfly, starfish, wood rabbit, porcupine, fox squirrel, beaver, guinea pig, zebra, pig, hippopotamus, bison, gazelle, llama, skunk, badger, orangutan, gorilla, chimpanzee, gibbon, baboon, panda, eel, clown fish, puffer fish, accordion, ambulance, assault rifle, backpack, barn, wheelbarrow, basketball, bathtub, lighthouse, beer glass, binoculars, birdhouse, bow tie, broom, bucket, cauldron, candle, cannon, canoe, carousel, castle, mobile phone, cowboy hat, electric guitar, fire engine, flute, gasmask, grand piano, guillotine, hammer, harmonica, harp, hatchet, jeep, joystick, lab coat, lawn mower, lipstick, mailbox, missile, mitten, parachute, pickup truck, pirate ship, revolver, rugby ball, sandal, saxophone, school bus, schooner, shield, soccer ball, space shuttle, spider web, steam locomotive, scarf, submarine, tank, tennis ball, tractor, trombone, vase, violin, military aircraft, wine bottle, ice cream, bagel, pretzel, cheeseburger, hotdog, cabbage, broccoli, cucumber, bell pepper, mushroom, Granny Smith, strawberry, lemon, pineapple, banana, pomegranate, pizza, burrito, espresso, volcano, baseball player, scuba diver, acorn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Goldfish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
	</analytic>
	<monogr>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>n03494278, n03495258, n03498962, n03594945, n03602883, n03630383, n03649909, n03676483, n03710193, n03773504, n03775071, n03888257, n03930630, n03947888, n04086273, n04118538, n04133789, n04141076, n04146614, n04147183, n04192698, n04254680, n04266014, n04275548, n04310018, n04325704, n04347754, n04389033, n04409515, n04465501, n04487394, n04522168, n04536866, n04552348, n04591713, n07614500, n07693725, n07695742, n07697313, n07697537, n07714571, n07714990, n07718472, n07720875, n07734744, n07742313, n07745940, n07749582, n07753275, n07753592, n07768694, n07873807, n07880968, n07920052, n09472597, n09835506, n10565667, n12267677</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Size (small, moderate, or large) defines how much of the image the article of clothing takes up</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Occlusion (slight, medium, or heavy) defines the degree to which the object is occluded from the camera. Viewpoint (front, side/back, or not worn) defines the camera position relative to the article of clothing. Zoom (no zoom, medium, or large) defines how much camera zoom was used to take the picture</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
