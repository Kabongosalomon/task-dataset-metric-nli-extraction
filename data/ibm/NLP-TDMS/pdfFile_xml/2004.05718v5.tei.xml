<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
							<email>dominique@invivoai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
							<email>pietro.lio@cst.cam.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><forename type="middle">Veličković</forename><surname>Deepmind</surname></persName>
							<email>petarv@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features-which occur regularly in real-world input domains and within the hidden layers of GNNs-and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from realworld domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have been an active research field for the last ten years with significant advancements in graph representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, it is difficult to understand the effectiveness of new GNNs due to the lack of standardized benchmarks <ref type="bibr" target="#b4">[5]</ref> and of theoretical frameworks for their expressive power.</p><p>In fact, most work in this domain has focused on improving the GNN architectures on a set of graph benchmarks, without evaluating the capacity of their network to accurately characterize the graphs' structural properties. Only recently there have been significant studies on the expressive power of various GNN models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, these have mainly focused on the isomorphism task in domains with countable features spaces, and little work has been done on understanding their capacity to capture and exploit the underlying properties of the graph structure.</p><p>We hypothesize that the aggregation layers of current GNNs are unable to extract enough information from the nodes' neighbourhoods in a single layer, which limits their expressive power and learning abilities.</p><p>We first mathematically prove the need for multiple aggregators and propose a solution for the uncountable multiset injectivity problem introduced by <ref type="bibr" target="#b5">[6]</ref>. Then, we propose the concept of degreescalers as a generalization to the sum aggregation, which allow the network to amplify or attenuate signals based on the degree of each node. Combining the above, we design the proposed Principal Neighbourhood Aggregation (PNA) model and demonstrate empirically that multiple aggregation strategies improve the performance of the GNN.</p><p>Dehmamy et al. <ref type="bibr" target="#b10">[11]</ref> have also empirically found that using multiple aggregators (mean, sum and normalized mean), which extract similar statistics from the input message, improves the performance of GNNs on the task of graph moments. In contrast, our work extends the theoretical framework by deriving the necessity to use complementary aggregators. Accordingly, we propose the use of different statistical aggregations to allow each node to better understand the distribution of the messages it receives, and we generalize the mean as the first of a set of possible n-moment aggregators. In the setting of graph kernels, Cai et al. <ref type="bibr" target="#b11">[12]</ref> constructed a simple baseline using multiple aggregators. In the field of computer vision, Lee et al. <ref type="bibr" target="#b12">[13]</ref> empirically showed the benefits of combining mean and max pooling. These give us further confidence in the validity of our theoretical analysis.</p><p>We present a consistently well-performing and parameter efficient encode-process-decode architecture <ref type="bibr" target="#b13">[14]</ref> for GNNs. This differs from traditional GNNs by allowing a variable number of convolutions with shared parameters. Using this model, we compare the performances of some of the most diffused models in the literature (GCN <ref type="bibr" target="#b14">[15]</ref>, GAT <ref type="bibr" target="#b15">[16]</ref>, GIN <ref type="bibr" target="#b5">[6]</ref> and MPNN <ref type="bibr" target="#b16">[17]</ref>) with our PNA.</p><p>Previous work on tasks taken from classical graph theory focuses on evaluating the performance of GNN models on a single task such as shortest paths <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, graph moments <ref type="bibr" target="#b10">[11]</ref> or travelling salesman problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Instead, we took a different approach by developing a multi-task benchmark containing problems both on the node level and the graph level. Many of the tasks are based on dynamic programming algorithms and are, therefore, expected to be well suited for GNNs <ref type="bibr" target="#b18">[19]</ref>. We believe this multi-task approach ensures that the GNNs are able to understand multiple properties simultaneously, which is fundamental for solving complex graph problems. Moreover, efficiently sharing parameters between the tasks suggests a deeper understanding of the structural features of the graphs. Furthermore, we explore the generalization ability of the networks by testing on graphs of larger sizes than those present in the training set.</p><p>To further demonstrate the performance of our model, we also run tests on recently proposed realworld GNN benchmark datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> with tasks taken from molecular chemistry and computer vision. Results show the PNA outperforms the other models in the literature in most of the tasks hence further supporting our theoretical findings.</p><p>The code for all the aggregators, scalers, models (in PyTorch, DGL and PyTorch Geometric frameworks), architectures, multi-task dataset generation and real-world benchmarks is available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Principal Neighbourhood Aggregation</head><p>In this section, we first explain the motivation behind using multiple aggregators concurrently. We then present the idea of degree-based scalers, linking to prior related work on GNN expressiveness. Finally, we detail the design of graph convolutional layers which leverage the proposed Principal Neighbourhood Aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proposed aggregators</head><p>Most work in the literature uses only a single aggregation method, with mean, sum and max aggregators being the most used in the state-of-the-art models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. In <ref type="figure" target="#fig_1">Figure 1</ref>, we observe how different aggregators fail to discriminate between different messages when using a single GNN layer.</p><p>We formalize our observations in the theorem below: Theorem 1 (Number of aggregators needed). In order to discriminate between multisets of size n whose underlying set is R, at least n aggregators are needed. Proposition 1 (Moments of the multiset). The moments of a multiset (as defined in Equation 4) exhibit a valid example using n aggregators.</p><p>We prove Theorem 1 in Appendix A and Proposition 1 in Appendix B. Note that unlike Xu et al. <ref type="bibr" target="#b5">[6]</ref>, we consider a continuous input feature space; this better represents many real-world tasks where the observed values have uncertainty, and better models the latent node features within a neural network's representations. Continuous features make the space uncountable, and void the injectivity proof of the sum aggregation presented by Xu et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>Simple aggregators that can differen�ate graph 1 and 2:  Hence, we redefine aggregators as continuous functions of multisets which compute a statistic on the neighbouring nodes, such as mean, max or standard deviation. The continuity is important with continuous input spaces, as small variations in the input should result in small variations of the aggregators' output.</p><p>Theorem 1 proves that the number of independent aggregators used is a limiting factor of the expressiveness of GNNs. To empirically demonstrate this, we leverage four aggregators, namely mean, maximum, minimum and standard deviation. Furthermore, we note that this can be extended to the normalized moment aggregators, which allow advanced distribution information to be extracted whenever the degree of the nodes is high.</p><p>The following paragraphs will describe the aggregators we leveraged in our architectures.</p><p>Mean aggregation µ(X l ) The most common message aggregator in the literature, wherein each node computes a weighted average or sum of its incoming messages. Equation 1 presents, on the left, the general mean equation, and, on the right, the direct neighbour formulation, where X is any multiset, X l are the nodes' features at layer l, N (i) is the neighbourhood of node i and d i = |N (i)|. For clarity we use E[f (X)] where X is a multiset of size d to be defined as</p><formula xml:id="formula_0">E[f (X)] = 1 d x∈X f (x). µ(X) = E[X] , µ i (X l ) = 1 d i j∈N (i) X l j<label>(1)</label></formula><p>Maximum and minimum aggregations max(X l ), min(X l ) Also often used in literature, they are very useful for discrete tasks, for domains where credit assignment is important and when extrapolating to unseen distributions of graphs <ref type="bibr" target="#b17">[18]</ref>. Alternatively, we present the softmax and softmin aggregators in Appendix E, which are differentiable and work for weighted graphs, but don't perform as well on our benchmarks.</p><formula xml:id="formula_1">max i (X l ) = max j∈N (i) X l j , min i (X l ) = min j∈N (i) X l j<label>(2)</label></formula><p>Standard deviation aggregation σ(X l ) The standard deviation (STD or σ) is used to quantify the spread of neighbouring nodes features, such that a node can assess the diversity of the signals it receives. Equation 3 presents, on the left, the standard deviation formulation and, on the right, the STD of a graph-neighbourhood. ReLU is the rectified linear unit used to avoid negative values caused by numerical errors and is a small positive number to ensure σ is differentiable.</p><formula xml:id="formula_2">σ(X) = E[X 2 ] − E[X] 2 , σ i (X l ) = ReLU µ i (X l 2 ) − µ i (X l ) 2 +<label>(3)</label></formula><p>Normalized moments aggregation M n (X l ) The mean and standard deviation are the first and second normalized moments of the multiset (n = 1, n = 2). Additional moments, such as the skewness (n = 3), the kurtosis (n = 4), or higher moments, could be useful to better describe the neighbourhood. These become even more important when the degree of a node is high because four aggregators are insufficient to describe the neighbourhood accurately. As described in Appendix D, we choose the n th root normalization, as presented in <ref type="bibr">Equation 4</ref>, because it gives a statistic that scales linearly with the size of the individual elements (as the other aggregators); this gives the training adequate numerical stability. Once again we add an to the absolute value of the expectation before applying the n th root for numerical stability of the gradient.</p><formula xml:id="formula_3">M n (X) = n E [(X − µ) n ] , n &gt; 1<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Degree-based scalers</head><p>We introduce scalers as functions of the number of messages being aggregated (usually the node degree), which are multiplied with the aggregated value to perform either an amplification or an attenuation of the incoming messages.</p><p>Xu et al. <ref type="bibr" target="#b5">[6]</ref> show that the use of mean and max aggregators by themselves fail to distinguish between neighbourhoods with identical features but with differing cardinalities, and the same applies to all the aggregators described above. They propose the use of the sum aggregator to discriminate between such multisets. We generalise their approach by expressing the sum aggregator as the composition of a mean aggregator and a linear-degree amplifying scaler S amp (d) = d.</p><p>Theorem 2 (Injective functions on countable multisets). The mean aggregation composed with any scaling linear to an injective function on the neighbourhood size can generate injective functions on bounded multisets of countable elements.</p><p>We formalize and prove Theorem 2 in Appendix C; the results proven in <ref type="bibr" target="#b5">[6]</ref> about the sum aggregator become then a particular case of this theorem, and we can use any kind of injective scaler to discriminate between multisets of various sizes.</p><p>Recent work shows that summation aggregation doesn't generalize well to unseen graphs <ref type="bibr" target="#b17">[18]</ref>, especially when larger. One reason is that a small change of the degree will cause the message and gradients to be amplified/attenuated exponentially (a linear amplification at each layer will cause an exponential amplification after multiple layers). Although there are different strategies to deal with this problem, we propose using a logarithmic amplification S ∝ log(d + 1) to reduce this effect. Note that the logarithm is injective for positive values, and d is defined non-negative.</p><p>Further motivation for using logarithmic scalers is to better describe the neighbourhood influence of a given node. Suppose we have a social network where nodes A, B and C have respectively 5 million, 1 million and 100 followers: on a linear scale, nodes B and C are closer than A and B; however, this does not accurately model their relative influence. This scenario exhibits how a logarithmic scale can discriminate better between messages received by influencer and follower nodes.</p><p>We propose the logarithmic scaler S amp presented in <ref type="bibr">Equation 5</ref>, where δ is a normalization parameter computed over the training set, and d is the degree of the node receiving the message.</p><formula xml:id="formula_4">S amp (d) = log(d + 1) δ , δ = 1 |train| i ∈ train log(d i + 1)<label>(5)</label></formula><p>We further generalize this scaler in <ref type="bibr">Equation 6</ref>, where α is a variable parameter that is negative for attenuation, positive for amplification or zero for no scaling. Other definitions of S(d) can be used-such as a linear scaling-as long as the function is injective for d &gt; 0.</p><formula xml:id="formula_5">S(d, α) = log(d + 1) δ α , d &gt; 0, −1 ≤ α ≤ 1<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combined aggregation</head><p>We combine the aggregators and scalers presented in previous sections obtaining the Principal Neighbourhood Aggregation (PNA). This is a general and flexible architecture, which in our tests we used with four neighbour-aggregations with three degree-scalers each, as summarized in <ref type="bibr">Equation 7</ref>.</p><p>The aggregators are defined in Equations 1-3, while the scalers are defined in <ref type="bibr">Equation 6</ref>, with ⊗ being the tensor product.</p><formula xml:id="formula_6">= I S(D, α = 1) S(D, α = −1) scalers ⊗    µ σ max min    aggregators (7)</formula><p>As mentioned earlier, higher degree graphs such as social networks could benefit from further aggregators (e.g. using the moments proposed in <ref type="bibr">Equation 4</ref>). We insert the PNA operator within the framework of a message passing neural network <ref type="bibr" target="#b16">[17]</ref>, obtaining the following GNN layer:</p><formula xml:id="formula_7">X (t+1) i = U   X (t) i , (j,i)∈E M X (t) i , E j→i , X (t) j   (8)</formula><p>where E j→i is the feature (if present) of the edge (j, i), M and U are neural networks (for our benchmarks, a linear layer was enough). U reduces the size of the concatenated message (in space R 13F ) back to R F where F is the dimension of the hidden features in the network. As in the MPNN paper <ref type="bibr" target="#b16">[17]</ref>, we employ multiple towers to improve computational complexity and generalization performance. Using twelve operations per kernel will require the usage of additional weights per input feature in the U function, which could seem to be just quantitatively-not qualitatively-more powerful than an ordinary MPNN with a single aggregator <ref type="bibr" target="#b16">[17]</ref>. However, the overall increase in parameters in the GNN model is modest and, as per our theoretical analysis above, a limiting factor of GNNs is likely their usage of a single aggregation.</p><p>This is comparable to convolutional neural networks (CNN) where a simple 3 × 3 convolutional kernel requires 9 weights per feature (1 weight per neighbour). Using a CNN with a single weight per 3 × 3 kernel will reduce the computational capacity since the feedforward network won't be able to compute derivatives or the Laplacian operator. Hence, it is intuitive that the GNNs should also require multiple weights per node, as previously demonstrated in Theorem 1. In Appendix K, we will demonstrate this observation empirically, by running experiments on baseline models with larger dimensions of the hidden features (and, therefore, more parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>We compare the performance of the PNA layer against some of the most popular models in the literature, namely GCN <ref type="bibr" target="#b14">[15]</ref>, GAT <ref type="bibr" target="#b15">[16]</ref>, GIN <ref type="bibr" target="#b5">[6]</ref> and MPNN <ref type="bibr" target="#b16">[17]</ref> on a common architecture. In Appendix F, we present the details of these graph convolutional layers.</p><p>For the multi-task experiments, we used an architecture, represented in <ref type="figure" target="#fig_4">Figure 3</ref>, with M convolutions followed by three fully-connected layers for node labels and a set2set (S2S) <ref type="bibr" target="#b22">[23]</ref> readout function for graph labels. In particular, we want to highlight:</p><p>Gated Recurrent Units (GRU) <ref type="bibr" target="#b23">[24]</ref> applied after the update function of each layer, as in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. Their ability to retain information from previous layers proved effective when increasing the number of convolutional layers M.</p><p>Weight sharing in all the GNN layers but the first makes the architecture follow an encode-processdecode configuration <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. This is a strong prior which works well on all our experimental tasks, yields a parameter-efficient architecture, and allows the model to have a variable number M of layers.</p><p>Variable depth M, decided at inference time (based on the size of the input graph and/or other heuristics), is important when using models over high variance graph distributions. In our experiments we have only used heuristics dependant on the number of nodes N (M = f (N )) and, for the architectures in the results below, we settled with M = N/2 . It would be interesting to test heuristics based on properties of the graph, such as the diameter, or an adaptive computation time heuristic <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> based on, for example, the convergence of the nodes features <ref type="bibr" target="#b17">[18]</ref>. We leave these analyses to future work.  This architecture layout was chosen for its performance and parameter efficiency. We note that all architectural attempts yielded similar comparative performance of GNN layers and in Appendix I we provide the results for a more standard architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-task benchmark</head><p>The benchmark consists of classical graph theory tasks on artificially generated graphs.</p><p>Random graph generation Following previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, the benchmark contains undirected unweighted randomly generated graphs of a wide variety of types. In Appendix G, we detail these types, and we describe the random toggling used to increase graph diversity. For the presented multi-task results, we used graphs of small sizes (15 to 50 nodes) as they were already sufficient to demonstrate clear differences between the models.</p><p>Multi-task graph properties In the multi-task benchmark, we consider three node labels and three graph labels based on standard graph theory problems. The node properties tasks are the single-source shortest-path lengths, the eccentricity and the Laplacian features (LX where L = (D − A) is the Laplacian matrix and X the node feature vector). The graph properties tasks are whether the graph is connected, the diameter and the spectral radius.</p><p>Input features As input features, the network is provided with two vectors of size N , a one-hot vector (representing the source for the shortest-path task) and a feature vector X where each element is i.i.d. sampled as X i ∼ U[0, 1]. Apart from taking part in the Laplacian features task, this random feature vector also provides a unique identifier for the nodes in other tasks. Similar strengthening via random features was also concurrently discovered by <ref type="bibr" target="#b28">[29]</ref>. This allows for addressing some of the problems highlighted in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>; e.g. the task of whether a graph is connected could be performed by continually aggregating the maximum feature of the neighbourhood and then checking whether they are all equal in the readout.</p><p>Model training While having clear differences, these tasks also share related subroutines (such as graph traversals). While we do not take this sharing of subroutines as prior as in <ref type="bibr" target="#b17">[18]</ref>, we expect models to pick up on these commonalities and efficiently share parameters between the tasks, which reinforce each other during the training.</p><p>We trained the models using the Adam optimizer for a maximum of 10,000 epochs, using early stopping with a patience of 1,000 epochs. Learning rates, weight decay, dropout and other hyperparameters were tuned on the validation set. For each model, we run 10 training runs with different seeds and different hyper-parameters (but close to the tuned values) and report the five with least validation error.</p><p>5 Results and discussion 5.1 Multi-task artificial benchmark</p><p>The multi-task results are presented in <ref type="figure" target="#fig_5">Figure 4a</ref>, where we observe that the proposed PNA model consistently outperforms state-of-the-art models, and in <ref type="figure" target="#fig_5">Figure 4b</ref>, where we note that the PNA performs better on all tasks. The baseline represents the MSE from predicting the average of the training set for all tasks.</p><p>The trend of these multi-task results follows and amplifies the difference in the average performances of the models when trained separately on the individual tasks. This suggests that the PNA model can better capture and exploit the common sub-units of these tasks. Appendix J provides the average results of the models when trained on individual tasks. Moreover, PNA showed to perform the best on all architecture layouts that we attempted (see Appendix I) and on all the various types of graphs (see Appendix H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nodes tasks</head><p>Graph tasks  To demonstrate that the performance improvements of the PNA model are not due to the (relatively small) number of additional parameters it has compared to the other models (about 15%), we ran tests on all the other models with latent size increased from 16 to 20 features. The results, presented in Appendix K, suggest that even when these models are given 30% more parameters than the PNA, they are qualitatively less capable of capturing the graph structure.</p><p>Finally, we explored the extrapolation of the models to larger graphs, in particular, we trained models on graphs of sizes between 15 and 25, validated between 25 and 30 and evaluate between 20 and 50. This task presents many challenges, two of the most significant are: firstly, unlike in <ref type="bibr" target="#b17">[18]</ref> the models are not given any step-wise supervision or trained on easily extendable subroutines; secondly, the models have to cope with their architectures being augmented with further hidden layers than trained on, which can sometimes cause problems with rapidly increasing feature scales.</p><p>Due to the aforementioned challenges, as expected, the performance of the models (as a proportion of the baseline performance) gradually worsens, with some of them having feature explosions. However, the PNA model keeps consistently outperforming all the other models on all graph sizes. Our results also follow the findings in <ref type="bibr" target="#b17">[18]</ref>, i.e. that between single aggregators the max tends to perform best when extrapolating to larger graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-world benchmarks</head><p>The recent works by Dwivedi et al. <ref type="bibr" target="#b4">[5]</ref> and Hu et al. <ref type="bibr" target="#b21">[22]</ref> have shown problems with many benchmarks used for GNNs in recent years and proposed a new range of datasets across different artificial and real-world tasks. To test the capacity of the PNA model in real-world domains, we assessed it on their chemical (ZINC and MolHIV) and computer vision (CIFAR10 and MNIST) datasets.</p><p>To ensure a fair comparison of the different convolutional layers, we followed their method for training procedure (data splits, optimizer, etc.) and GNN structure (layers, normalization and approximate number of parameters). For the MolHIV dataset, we used the same GNN structure as in <ref type="bibr" target="#b30">[31]</ref>.  , GIN <ref type="bibr" target="#b5">[6]</ref>, DiffPool <ref type="bibr" target="#b31">[32]</ref>, GAT <ref type="bibr" target="#b15">[16]</ref>, MoNet <ref type="bibr" target="#b32">[33]</ref> and GatedGCN <ref type="bibr" target="#b33">[34]</ref>). * indicates the training was conducted with additional patience to ensure convergence.</p><p>To better understand the results in the table, we need to take into account how graphs differ among the four datasets. In the chemical benchmarks, graphs are diverse and individual edges (bonds) can significantly impact the properties of the graphs (molecules). This contrasts with computer vision datasets made of graphs with a regular topology (every node has 8 edges) and where the graph structure of the representation is not crucial (the good performance of the MLP is evidence).</p><p>With this and our theoretical analysis in mind, it is understandable why the PNA has a strong performance in the chemical datasets, as it was designed to understand the graph structure and better retain neighbourhood information. At the same time, the version without scalers suffers from the fact it cannot distinguish between neighbourhoods of different size. Instead, in the computer vision datasets the average improvement of the PNA on SOTA was lower due to the smaller importance of the graph structure and the version of the PNA without scalers performs better as the constant degree of these graphs makes scalers redundant (and it is better to 'spend' parameters for larger hidden sizes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have extended the theoretical framework in which GNNs are analyzed to continuous features and proven the need for multiple aggregators in such circumstances. We also have generalized the sum aggregation by presenting degree-scalers and proposed the use of a logarithmic scaling. Taking the above into consideration, we have presented a method, Principal Neighbourhood Aggregation, consisting of the composition of multiple aggregators and degree-scalers. With the goal of understanding the ability of GNNs to capture graph structures, we have proposed a novel multi-task benchmark and an encode-process-decode architecture for approaching it. Empirical results from synthetic and real-world domains support our theoretical evidence. We believe that our findings constitute a step towards establishing a hierarchy of models w.r.t. their expressive power, where the PNA model appears to outperform the prior art in GNN layer design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work focuses mainly on theoretically analyzing the expressive power of Graph Neural Networks and can, therefore, play an indirect role in the (positive or negative) impacts that the field of graph representation learning might have on the domains where it will be applied.</p><p>More directly, our contribution in proving the limitations of existing GNNs on continuous feature spaces should help to provide an insight into their behaviour. We believe this is a significant result which might motivate future research aimed at overcoming such limitations, yielding more reliable models. However, we also recognize that, in the short-term, proofs of such weaknesses might spark mistrust against applications of these systems or steer adversarial attacks towards existing GNN architectures.</p><p>In an effort to overcome some of these short-term negative impacts and contribute to the search for more reliable models, we propose the Principal Neighbourhood Aggregation, a method that overcomes some of these theoretical limitations. Our tests demonstrate the higher capacity of the PNA compared to the prior art on both synthetic and real-world tasks; however, we recognize that our tests are not exhaustive and that our proofs do not allow for generating "optimal" aggregators for any task. As such, we do not rule out sub-optimal performance when applying the exact architecture proposed here to novel domains.</p><p>We propose the usage of aggregation functions, such as standard deviation and higher-order moments, and logarithmic scalers. To the best of our knowledge, these have not been used before in GNN literature. To further test their behaviour, we conducted out-of-distribution experiments, testing our models on graphs much larger than those in the training set. While the PNA model consistently outperformed other models and baselines, there was still a noticeable drop in performance. We therefore strongly encourage future work on analyzing the stability and efficacy of these novel aggregation methods on new domains and, in general, on finding GNN architectures that better generalize to graphs from unseen distributions, as this will be essential for the transition to industrial applications.</p><p>[41] Duncan J. Watts. Networks, dynamics, and the small-world phenomenon. American Journal of Sociology, 105(2):493-527, 1999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof for Theorem 1 (Number of aggregators needed)</head><p>In order to discriminate between multisets of size n whose underlying set is R, at least n aggregators are needed.</p><p>Proof. Let S be the n-dimensional subspace of R n formed by all tuples (x 1 , x 2 , . . . , x n ) such that x 1 ≤ x 2 ≤ . . . ≤ x n , and notice how S is the collection of the aforementioned multisets. We defined an aggregator as a continuous function from multisets to reals, which corresponds to a continuous function g : S → R.</p><p>Assume by contradiction that it is possible to discriminate between all the multisets of size n using only n − 1 aggregators, viz. g 1 , g 2 , . . . , g n−1 .</p><p>Define f : S → R n−1 to be the function mapping each multiset X to its output vector (g 1 (X), g 2 (X), . . . , g n−1 (X)). Since g 1 , g 2 , . . . , g n−1 are continuous, so is f , and, since we assumed these aggregators are able to discriminate between all the multisets, f is injective.</p><p>As S is a n-dimensional Euclidean subspace, it is possible to define a (n − 1)-sphere C n−1 entirely contained within it, i.e. C n−1 ⊆ S. According to Borsuk-Ulam theorem <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, there are two distinct (in particular, non-zero and antipodal) points</p><formula xml:id="formula_8">x 1 , x 2 ∈ C n−1 satisfying f ( x 1 ) = f ( x 2 ),</formula><p>showing f not to be injective; hence the required contradiction.</p><p>Note: n aggregators are actually sufficient. A simple example is to use g 1 , g 2 , . . . , g n where g k (X) = the k-th smallest item in X. It's clear to see that the multiset whose elements are g 1 (X), g 2 (X), . . . , g n (X) is X, which can hence be uniquely determined by the aggregators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof for Proposition 1 (Moments of the multiset)</head><p>The moments of a multiset (as defined in Equation 4) exhibit a valid example using n aggregators.</p><p>Proof. Since n ≥ 1, and the first aggregator is mean, we know µ. Let X = {x 1 , x 2 , . . . , x n } be the multiset to be found, and define R = {r 1 = x 1 − µ, r 2 = x 2 − µ, . . . , r n = x n − µ}.</p><p>Notice how r i 1 = 0, and for 1 &lt; k ≤ n we have r i k = n M k (X) k , i.e. all the symmetric power sums p k = r i k (k ≤ n) are uniquely determined by the moments.</p><p>Additionally, e k , the elementary symmetric sums of R, i.e. the sum of the products of all the sub-multisets of size k (1 ≤ k ≤ n), are determined as follow: e 1 , the sum of all elements, is equal to p 1 ; e 2 , the sum of the products of all pairs in R, is (e 1 p 1 − p 2 ) /2; e 3 , the sum of the products of all triplets, is (e 2 p 1 − e 1 p 2 + p 3 ) /3, and so on.</p><p>Notice how e 1 , e 2 , . . . , e n can be computed using the following recursive formula <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_9">1≤i1&lt;i2&lt;···&lt;i k ≤n   k j=1 r ij   = e k = 1 k k j=1 (−1) j−1 e k−j p j , e 0 = 1</formula><p>Consider polynomial P (x) = Π(x − r i ), i.e. the unique polynomial of degree n with leading coefficient 1 whose roots are R. This defines A, the coefficients of P , i.e. the real numbers a 0 , a 1 , . . . , a n−1 for which P (x) = x n + a n−1 x n−1 + . . . + a 1 x + a 0 . Using Vieta's formulas <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_10">1≤i1&lt;i2&lt;···&lt;i k ≤n   k j=1</formula><p>r ij   = (−1) k a n−k a n we obtain e k = (−1) k a n−k a n = (−1) k a n−k recall a n = 1 ∴ a i = (−1) n+i e n+i letting k = n + i and rearranging Hence A is uniquely determined, and so is P , being its coefficients a valid definition of it. By the fundamental theorem of algebra, P has n (possibly repeated) roots, which are the elements of R, hence uniquely determining the latter.</p><p>Finally, X can be easily determined adding µ to each element of R.</p><p>Note: the proof above assumes the knowledge of n. In the case that n is variable (as in GNNs), and so we have multisets of up to n elements, an extra aggregator will be needed. An example of such aggregator is the mean multiplied by any injective scaler which would allow the degree of the node to be inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof for Theorem 2 (Injective functions on countable multisets)</head><p>The mean aggregation composed with any scaling linear to an injective function on the neighbourhood size can generate injective functions on bounded multisets of countable elements.</p><p>Proof. Let χ be the countable input feature space from which the elements of the multisets are taken and X an arbitrary multiset. Since χ is countable and the cardinality of multisets is bounded, let Z : χ → N + be an injection from χ to natural numbers, and N ∈ N such that |X| + 1 &lt; N for all X. </p><formula xml:id="formula_11">(x) = N −Z(x) + K. ∀x ∈ χ, Z(x) ∈ [1, N ] ⇒ N −Z(x) ∈ [0, 1] ⇒ f (x) ∈ [K, K +1] , so E x∈X [f (x)] ∈ [K, K +1].</formula><p>We proceed to show that the cardinality of X can be uniquely determined, and X itself can be determined as well, by showing that exist an injection h over the multisets.</p><p>Let us h as a function that scales the mean of f by an injective function of the cardinality:</p><formula xml:id="formula_12">h(X) = s(|X|) E x∈X [f (x)]</formula><p>We want show that the value of |X| can be uniquely inferred from the value of h(X). Assume by contradiction ∃ X , X multisets of size at most N such that |X | = |X | but h(X ) = h(X ); since s is injective s(|X |) = s(|X |), without loss of generality let s(|X |) &gt; s(|X |), then:</p><formula xml:id="formula_13">s(|X |)(K +1) ≥ s(|X |) E x∈X [f (x)] = h(X ) = h(X ) = s(|X |) E x∈X [f (x)] ≥ s(|X |) K =⇒ K ≤ 1 s(|X |) s(|X |) − 1 ≤ 1 γ − 1</formula><p>which is a contradiction. So it is impossible for the size of a multiset X to be ambiguous from the value of h(X).</p><p>Let us define d as the function mapping h(X) to |X|.</p><formula xml:id="formula_14">h (X) = x∈X N −Z(x) = h(X)|X| s(|X|) − K|X| = h(X)d(h(X)) s(d(h(X))) − Kd(h(X))</formula><p>Considering the Z(j)-th digit i after the decimal point in the base N representation of h (X), it can be inferred that X contains i elements j, and, so, all the elements in X can be determined; hence h is injective over the multisets in X.</p><p>Note: this proof is a generalization of the one by Xu et al. <ref type="bibr" target="#b5">[6]</ref> on the sum aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Normalized moments aggregation</head><p>The main motivation for choosing the n th root normalization for the moments is numerical stability. In fact, one property of our version is that it scales linearly with L, for uniformly distributed random variables U [0, L], as do other aggregators such as mean, max and min (std is a particular case). Other common formulations of the moments such as those in Equation 9 scale respectively as the n th power and constantly with L. This difference causes numerical instability when combined in the same layer.</p><formula xml:id="formula_15">M n (X) = E [(X − µ) n ] M n (X) = E [(X − µ) n ] σ n<label>(9)</label></formula><p>To demonstrate the usefulness of higher moments aggregation and further motivate the need for multiple aggregation functions, we ran an ablation study showing how different moments affect the performance of the model. We conduct this by testing five different models, each taking a different number of moments, on our multi-task benchmark. <ref type="figure">Figure 7</ref>: Multi-task log 10 MSE on different versions of the PNA model with increasing number of moments aggregators (specified in the legend), using mean as first moment. All the models use the identity, amplification and attenuation scalers. The model on the right is the complete PNA as described before (mean, max, min and std aggregators).</p><p>The results in <ref type="figure">Figure 7</ref> demonstrate that with the increase of the number of aggregators the models reach a higher expressive power, but at a certain point (dependent on the graphs and tasks, in this case around 3) the increase in expressiveness given by higher moments reduces the performance since the model becomes harder to optimize and prone to overfitting. We expect that higher moments will be more beneficial on graphs with a higher average degree since they will better characterize the neighbourhood distributions.</p><p>Finally, we note how the addition of the max and min aggregators in the PNA (rightmost column) gives a better and more consistent performance in these tasks than higher moments. We believe this is task-dependent, and, for algorithmic tasks, discrete aggregators can be valuable. As a side note, we point out how the max and min aggregators of positive values can be considered as the n th -root of the n th (non-centralized) moment as n tends to, respectively, +∞ and −∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Alternative aggregators</head><p>Besides those described above, we have experimented with additional aggregators. We detail some examples below. Domain-specific metrics can also be an effective choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax and softmin aggregations</head><p>As an alternative to max and min, softmax and softmin are differentiable and can be weighted in the case of edge features or attention networks. They also allow an asymmetric message passing in the direction of the strongest signal. Equation 10 presents their direct neighbour formulations, where X l are the nodes features at layer l with respect to node i and N (i) is the neighbourhood of node i:</p><formula xml:id="formula_16">softmax i (X l ) = j∈N (i) X l j exp(X l j ) k∈N (i) exp(X l k )</formula><p>,</p><formula xml:id="formula_17">softmin i (X l ) = −softmax i (−X l )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Alternative graph convolutions</head><p>In this section, we present the details of the four graph convolutional layers from existing models that we used to compare the performance of the PNA in the multi-task benchmark.</p><p>Graph Convolutional Networks (GCN) <ref type="bibr" target="#b14">[15]</ref> use a normalized mean aggregator followed by a linear transformation and an activation function. We define it in <ref type="bibr">Equation 11</ref>, whereÃ = A + I N is the adjacency matrix with self-connections, W is a trainable weight matrix and b a learnable bias.</p><formula xml:id="formula_18">X (t+1) = σ D − 1 2ÃD − 1 2 X (t) W + b<label>(11)</label></formula><p>Graph Attention Networks (GAT) <ref type="bibr" target="#b15">[16]</ref> perform a linear transformation of the input features followed by an aggregation of the neighbourhood as a weighted sum of the transformed features, where the weights are set by an attention mechanism a. We define it in <ref type="bibr">Equation 12</ref>, where W is a trainable projection matrix. As in the original paper, we employ the use of multi-head attention.</p><formula xml:id="formula_19">X (t+1) i = σ   (j,i)∈E a X (t) i , X (t) j W X (t) j  <label>(12)</label></formula><p>Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b5">[6]</ref> perform a sum aggregation over the neighbourhood, followed by an update function U consisting of a multi-layer perceptron. We define it in <ref type="bibr">Equation 13</ref>, where is a learnable parameter. As in the original paper, we use a 2-layer MLP for U .</p><formula xml:id="formula_20">X (t+1) i = U 1 + X (t) i + j∈N (i) X (t) j<label>(13)</label></formula><p>Message Passing Neural Networks (MPNN) <ref type="bibr" target="#b16">[17]</ref> perform a transformation before and after an arbitrary aggregator. We define it in <ref type="bibr">Equation 14</ref>, where M and U are neural networks and is a single aggregator. In particular, we test models with sum and max aggregators, as they are the most used in literature. As with PNA layers, we found that linear transformations are sufficient for M and U and, as in the original paper <ref type="bibr" target="#b16">[17]</ref>, we employ multiple towers.</p><formula xml:id="formula_21">X (t+1) i = U X (t) i , (j,i)∈E M X (t) i , X (t) j<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Random graph generation</head><p>In this section, we present the details of the random generation of the graphs we used in the multi-task benchmark. Following previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, we opted for undirected unweighted graphs from a wide variety of types (we provide, in parentheses, the approximate proportion of such graphs in the benchmark). Letting N be the total number of nodes per graph: Additional randomness was introduced to the generated graphs by randomly toggling arcs, without strongly impacting the average degree and main structure. If e is the number of edges and m the number of 'missing edges' (2e + 2m = N (N − 1)), the probabilities of toggling an existing and missing edge, respectively P e and P m , are:</p><formula xml:id="formula_22">P e = 0.1 e ≤ m 0.1 m e e &gt; m P m = 0.1 e m e ≤ m 0.1 e &gt; m<label>(15)</label></formula><p>After performing the random toggling, we discarded graphs containing singleton nodes, as they are in no way affected by the choice of aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Graph type experiments</head><p>In order to better interpret the improvements in performance that the PNA brings, we tested the models against the various types of graphs in the multi-task benchmark. In particular, in these experiments, we trained the models on the whole dataset with the proportions described above and then tested them against datasets composed by just one category of graphs.  The results, presented in <ref type="figure" target="#fig_10">Figure 8</ref>, show that the PNA improves across all types. However, it performs the worst on the graphs with a higher diameter (especially graphs close to lines), suggesting that the number of layers is not enough to reach the complete graph. Therefore, the main limitation to the PNA performance seems to be the message passing framework; this could motivate future research to try to improve the framework itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Erdos-Rényi</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Standard architecture</head><p>In this section we will provide more intuition on the motivation behind our choice of architecture, presented in Section 3, which we will refer to as recurrent, 2 and present the results on a more standard architecture.</p><p>The main motivations behind the choice of the architecture were: (1) provide a fairer comparison between the models (2) showcase a parameter-efficient recurrent architecture with a prior 3 that works very well with the tasks at hand. In particular:</p><p>1. The GRU helps to avoid over-smoothing, and the models that do not have a skip connection across the aggregation (GAT, GIN and GCN) are those benefiting the most from it; therefore, to still provide a fair comparison in the results below, we added skip connections from every convolutional layer to the readout, in all the models.</p><p>2. The S2S (as opposed to a mean readout used below) helps the most architectures without scalers as it can provide an alternative counting mechanism.</p><p>3. The repeated convolutions are a parameter-saving prior which works well in these tasks but does not change the rank between the various models.</p><p>For completeness, we present in <ref type="figure" target="#fig_11">Figure 9</ref> the comparison of the average results of the recurrent architecture and standard one which uses no GRU but skip connections, mean readout rather than S2S and a fixed number of convolutions <ref type="bibr" target="#b7">(8)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Single task experiments</head><p>Apart from a good method to evaluate the performance on a variety of different problems, the multitask approach offers a regularization opportunity that some models capture more than others. In particular, we found that models without scalers (or sum aggregator) are those benefiting the most from the approach; we hypothesise that the reason for this lies in some supervision that specific tasks give to recognise the size of a model neighbourhood. Moreover, more complex models are more prone to overfitting when trained on a single task. <ref type="figure" target="#fig_1">Figure 10</ref> shows the average performance on the individual tasks of the various models.  <ref type="figure" target="#fig_1">Figure 10</ref>: Average log 10 MSE error across the various tasks of a particular model either trained concurrently on all the tasks (multi-task) or trained separately on the individual tasks (single task).</p><p>With the exception of the output layer, the two settings use the same architecture. <ref type="figure" target="#fig_1">Figure 11</ref> shows the results of testing all the other models on the multi-task benchmark with increased latent size.  <ref type="figure" target="#fig_1">Figure 11</ref>: Average score of different models using feature sizes of 16 and 20, compared to the PNA with 16 on the multi-task benchmark. "# params" is the total number of parameters in each architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Parameters comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We observe that, even with fewer parameters, PNA performs consistently better and an increased number of parameters does not boost the performance of the other models. This suggests that the multiple aggregators in the PNA produce a qualitative improvement to the capacity of the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Examples where, for a single GNN layer and continuous input feature spaces, some aggregators fail to differentiate between neighbourhood messages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Diagram for the Principal Neighbourhood Aggregation or PNA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Layout of the architecture used. When comparing different models, the difference lies only in the type of graph convolution used in place of GC 1 and GC m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Multi-task benchmarks for different GNN models using the same architecture and various near-optimal hyper-parameters. (a) Distribution of the log 10 MSE errors for the top 5 performances of each model. (b) Mean log 10 MSE error for each task and their combined average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Multi-task log 10 of the ratio of the MSE for different GNN models and the MSE of the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Results of the PNA and MPNN models in comparison with those analysed by Dwivedi et al. and Xu et al. (GCN[15]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Let's define an injective function s, and without loss of generality, assume s(0), s(1), . . . , s(N ) &gt; 0 (otherwise for the rest of the proof consider s as s (i) = s(i) − min j∈[0,N ] s(j) + which is positive for all i ∈ [0, N ]). s(|X|) can only take value in {s(0), s(1), . . . , s(N )}, therefore let us define γ = min s(i) s(j) | i, j ∈ [0, N ], s(i) ≥ s(j) . Since s is injective, s(i) = s(j) for i = j, which implies γ &gt; 1. Let K &gt; 1 γ−1 be a positive real number and consider f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>• 3 •</head><label>3</label><figDesc>Erdős-Rényi<ref type="bibr" target="#b38">[39]</ref> (20%): with probability of presence for each edge equal to p, where p is independently generated for each graph from U[0, 1]• Barabási-Albert<ref type="bibr" target="#b39">[40]</ref> (20%): the number of edges for a new node is k, which is taken randomly from {1, 2, ..., N − 1} for each graph• Grid (5%): m × k2d grid graph with N = mk and m and k as close as possible • Caveman [41] (5%): with m cliques of size k, with m and k as close as possible • Tree (15%): generated with a power-law degree distribution with exponent Ladder graphs (5%) • Line graphs (5%) • Star graphs (5%) • Caterpillar graphs (10%): with a backbone of size b (drawn from U[1, N ) ), and N − b pendent vertices uniformly connected to the backbone • Lobster graphs (10%): with a backbone of size b (drawn from U[1, N ) ), p (drawn from U[1, N −b ] ) pendent vertices uniformly connected to the backbone, and additional N −b−p pendent vertices uniformly connected to the previous pendent vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Average log 10 MSE error across the various tasks of a particular model against a particular type of graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Average log 10 MSE error across the various tasks of a particular model when inserted in the recurrent or the standard model. The mean, max &amp; min model represents a baseline MPNN which employs mean, max and min aggregators and no scaler.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that this was only used in the synthetic benchmarks, while in the real-world benchmarks, we kept the same architecture from Dwivedi et al.<ref type="bibr" target="#b2">3</ref> This prior corresponds to the knowledge that these tasks can be solved by the convergence of an aggregation function in the message passing context, potentially with an additional readout/function.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Saro Passaro for the valuable insights and discussion for the mathematical proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head><p>Funding Disclosure Dominique Beaini is currently a Machine Learning Researcher at InVivo AI. Pietro Liò is a Full Professor at the Department of Computer Science and Technology of the University of Cambridge. Petar Veličković is a Research Scientist at DeepMind.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06157</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational pooling for graph representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15387" to="15397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for non-attributed graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jessica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01203</idno>
		<title level="m">Relational inductive bias for physical construction in humans and machines</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10593</idno>
		<title level="m">Neural execution of graph algorithms</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13211</idno>
		<title level="m">What can neural networks reason about? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><forename type="middle">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01227</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adaptive propagation graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10306</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04817</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<title level="m">Random features strengthen graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02863</idno>
		<title level="m">Directional graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An Introduction to Algebraic Topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Rotman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graduate Texts in Mathematics</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Drei sätze über die n-dimensionale euklidische sphäre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Borsuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta Mathematicae</title>
		<imprint>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Enumerative Combinatorics</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Encyclopaedia of mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hazewinkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STO-ZYG. Encyclopaedia of mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1988" />
			<publisher>STO-ZYG. Kluwer Academic</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rényi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="page" from="17" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="97" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
