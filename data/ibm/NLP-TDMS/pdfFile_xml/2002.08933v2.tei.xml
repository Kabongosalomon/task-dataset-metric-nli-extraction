<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wavesplit: End-to-End Speech Separation by Speaker Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
							<email>neilz@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
							<email>grangier@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wavesplit: End-to-End Speech Separation by Speaker Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Wavesplit, an end-to-end source separation system. From a single mixture, the model infers a representation for each source and then estimates each source signal given the inferred representations. The model is trained to jointly perform both tasks from the raw waveform. Wavesplit infers a set of source representations via clustering, which addresses the fundamental permutation problem of separation. For speech separation, our sequence-wide speaker representations provide a more robust separation of long, challenging recordings compared to prior work. Wavesplit redefines the state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2/3mix), as well as in noisy and reverberated settings (WHAM/WHAMR). We also set a new benchmark on the recent LibriMix dataset. Finally, we show that Wavesplit is also applicable to other domains, by separating fetal and maternal heart rates from a single abdominal electrocardiogram.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Source separation is a fundamental problem in machine learning and signal processing, in particular in the ill-posed setting of separating multiple sources from a single mixture. An additional difficulty arises when the sources to separate belong to the same class of signals. For example, tasks such as separating overlapped speech, isolating appliance electric consumption from meter reading <ref type="bibr" target="#b0">[1]</ref>, separating overlapped fingerprints <ref type="bibr" target="#b1">[2]</ref>, identifying exoplanets in multi-planetary systems from light curves <ref type="bibr" target="#b2">[3]</ref> or retrieving individual compounds in chemical mixtures from spectroscopy <ref type="bibr" target="#b3">[4]</ref> are particularly difficult as the sources are similar in nature, and as any permutation of them is a correct prediction. This leads to the fundamental permutation problem where predicted channels are well separated but inconsistent along time <ref type="bibr" target="#b4">[5]</ref>. This situation does not occur when separating different musical instruments from predefined categories <ref type="bibr" target="#b5">[6]</ref>, or separating speech from non-speech noise <ref type="bibr" target="#b6">[7]</ref>. Thus, designing a model that maintains a consistent assignment between the ground-truth sources and the predicted channels is crucial for the tasks with similar sources.</p><p>This work precisely aims at separating sources of the same nature from a single mixture. In particular, speech separation aims at isolating individual speaker voices from a recording with overlapping speech <ref type="bibr" target="#b7">[8]</ref>. This task is particularly important for public events, conversations and meeting recordings. Research on speech separation spans several decades <ref type="bibr" target="#b7">[8]</ref> and it is the most active and competitive field of research in separation <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref>. We therefore introduce our model in the context of this application. Still, to show its generality, we also apply our model to the separation of fetal and maternal heart rate from a single abdominal electrode.</p><p>Our approach, Wavesplit, aims at separating novel sources at test time, called open speaker separation in speech, but leverages source identities during training. Specifically, our joint training procedure for speaker identification and speech separation differs from prior research <ref type="bibr" target="#b12">[13]</ref>. The training objective encourages identifying instantaneous speaker representations such that (i) these representations can be grouped into individual speaker clusters and (ii) the cluster centroids provide a long-term speaker representation for the reconstruction of individual speaker signals. The extraction of an explicit, long-term representation per source is novel and is beneficial for both speech and non-speech separation. This representation limits inconsistent channel-source assignments (channel swap), a common type of error for permutation-invariant training (PIT), the dominant approach in neural source separation.</p><p>Our contributions are six-fold, (i) we leverage training speaker labels but do not need any information about the test speakers beside the mixture recording, (ii) we aggregate information about sources over the whole input mixture which limits channel-swap, (iii) we use clustering to infer source representations which naturally outputs sets, i.e. order-agnostic predictions, (iv) we report state-of-the-art results on the most common speech separation benchmarks, both for clean (WSJ0-2/3mix, Libri2/3mix clean) and noisy settings (WHAM and WHAMR, Libri2/3mix noisy), (v) we analyze the empirical advantages and drawbacks of our method, (vi) we show that our approach is generic and can be applied to non-speech tasks, by separating maternal and fetal heart rate from a single abdominal electrode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Single-channel separation takes a single recording with overlapping source signals and predicts the isolated sources. This task is classical in speech processing <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and has witnessed fast progress recently with supervised neural  networks <ref type="bibr" target="#b15">[16]</ref>. These models have historically relied on learning time-frequency masks. They divide the input mixture in time-frequency bins (TFB) using a short-term Fourier Transform <ref type="bibr" target="#b16">[17]</ref>, and identify the source with the maximal energy for each TFB. Source spectrograms can then be produced by masking the TFBs of the other sources, and source signals are later estimated by phase reconstruction <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Soft masking variants assign each TFB to multiple speakers with different weights <ref type="bibr" target="#b19">[20]</ref>.</p><p>Deep clustering approaches devise a clustering model for masking: the model learns a latent representation for each TFB such that the distance between TFBs from the same source is lower than the distance between TFBs from different sources. The inference procedure clusters these representations to group TFBs by source <ref type="bibr" target="#b8">[9]</ref>. Wavesplit also relies on clustering to infer source representations but these representations are not tied to frequency bins and no masking is performed. Instead its representations are learned to (i) predict training speaker identity and (ii) provide conditioning variables to our separation convolutional network.</p><p>Permutation-Invariant Training (PIT) avoids clustering and predicts multiple masks directly <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. The predictions are compared to the ground-truth masks by searching over the permutations of the source orderings. The minimum error over all permutations is used to train the model. PIT acknowledges that the order of predictions and labels in speech separation is irrelevant, i.e. separation is a set prediction problem. Among PIT systems, time domain approaches avoid phase reconstruction and its degradation. They predict audio directly with convolutional <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref> or recurrent networks <ref type="bibr" target="#b10">[11]</ref>. In that case, PIT compares audio predictions to all permutations of the ground-truth signals. Wavesplit is also a time domain approach but it solves the permutation problem prior to signal estimation: at train time, the latent source representations are ordered to best match the labels prior to conditioning the separation network.</p><p>Discriminative speaker representations can be extracted from short speech segments <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> to help separation. Wang et al. <ref type="bibr" target="#b12">[13]</ref> extract a representation of the targeted speaker from a clean enrollment sequence and then isolate that speaker in a mixture. This method however does not apply to open-set speaker separation, where enrollment data is not available for the test speakers. Nachmani et al. <ref type="bibr" target="#b11">[12]</ref> use a separately trained speaker identification network to prevent channel swap. Conversely, Wavesplit jointly learns to identify and separate sources.</p><p>Separation of fetal and maternal heart rates from abdominal electrocardiograms (ECGs) allows for affordable, noninvasive monitoring of fetal health during pregnancy and labour <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, as an alternative to fetal scalp ECG. Source separation is an intermediate step for detecting fetal heart rate peaks <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. This work evaluates Wavesplit for separating maternal and fetal heart rate signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. WAVESPLIT</head><p>Wavesplit combines two convolutional subnetworks: the speaker stack and the separation stack ( <ref type="figure" target="#fig_1">Figure 1</ref>). The speaker stack maps the mixture to a set of vectors representing the recorded speakers. The separation stack consumes both the mixture and the set of speaker representations from the speaker stack. It produces a multi-channel audio output with separated speech from each speaker.</p><p>The separation stack is classical and resembles previous architectures conditioned on pre-trained speaker vectors <ref type="bibr" target="#b12">[13]</ref>, or trained with PIT <ref type="bibr" target="#b9">[10]</ref>. The speaker stack is novel and constitutes the heart of our contribution. This stack is trained jointly with the separation stack. At training time, speaker labels are used to learn a vector representation per speaker such that the inter-speaker distances are large, while the intra-speaker distances are small. At the same time, this representation is also learned to allow the separation stack to reconstruct the clean signals. At test time, the speaker stack relies on clustering to identify a centroid representation per speaker.</p><p>Our strategy contrasts with prior work. Unlike Wang et al. <ref type="bibr" target="#b12">[13]</ref>, we do not need an enrollment sequence for test speakers since the representation of all speakers is directly inferred from the mixture. With joint training, the speaker representation is not solely optimized for identification but also for the reconstruction of separated speech. In contrast with PIT <ref type="bibr" target="#b20">[21]</ref>, we condition decoding with a speaker representation valid for the whole sequence. This long-term representation yields excellent performance on long sequences, especially when the relative energy between speakers is varying, see Section IV-B. Our model is less prone to channel swap since clustering assigns a persistent source representation to each channel. Still in contrast with PIT, we resolve the permutation ambiguity during training at the level of the speaker representation, i.e. the separation stack is conditioned with speaker vectors ordered consistently with the labels. This does not force the separation stack to choose a latent ordering and allows training this stack with different permutations of the same labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Setting &amp; Notations</head><p>We consider a mixture of N sources. Each single-channel source waveform i ∈ [1, N ] is represented by a continuous vector y i ∈ X 1,T , with T the length of the sequence. Given a mixture x = N i=1 y i , the source separation task is to reconstruct each y i .</p><p>A separation model f predicts an estimate for each channel. Its quality is assessed by comparing its predictions to the reference channels {y i } N i=1 up to a permutation since the channel order is arbitrary,</p><formula xml:id="formula_0">Q(ŷ, y) = max σ∈S N 1 N N i=1 q(ŷ σ(i) , y i ) where ∀i,ŷ i = f i (x).<label>(1)</label></formula><p>q(·, ·) denotes a single-channel reconstruction quality metric and S N denotes the space of permutations over <ref type="bibr">[1, N ]</ref>. The speaker separation literature typically relies on Signal-to-Distortion Ratio (SDR) to assess reconstruction quality. SDR is the opposite of the log squared error normalized by the energy of the reference signal, SDR(ŷ, y) = −10 log 10 y −ŷ 2 + 10 log 10 y 2 . Scale-invariant SDR (SI-SDR) considers prediction scale irrelevant and searches over gains <ref type="bibr" target="#b30">[31]</ref>. Variants searching over richer signal transforms have also been proposed <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Architecture</head><p>Wavesplit is a residual convolutional network with two sub-networks or stacks. The first stack transforms the input mixture into a representation of each speaker, while the second stack transforms the input mixture into multiple isolated recordings conditioned on the speaker representation.</p><p>The speaker stack produces speaker representations at each time step and then performs an aggregation over the whole sequence. Precisely, the speaker stack first maps the input</p><formula xml:id="formula_1">x = x T t=1 into N same-length sequences of latent vectors of dimension d, i.e. h(x) = {h i } N i=1 where ∀i, h i ∈ R T ×d .</formula><p>N represents the maximum number of simultaneous speakers targeted by the system, while d is a hyper-parameter selected by cross-validation. Intuitively, h produces a latent representation of each speaker at every time step. It is important to note that h is not required to order speakers consistently across a sequence. E.g. a given speaker Bob could be represented by the first vector h 1 t at time t and by the second vector h 2 t at a different time t . At the end of the sequence, the aggregation step groups all vectors by speaker and outputs N summary vectors for the whole sequence. K-means clustering performs this aggregation at inference <ref type="bibr" target="#b32">[33]</ref> and returns the centroids of the N identified clusters,</p><formula xml:id="formula_2">c = {c i } N i=1 = kmeans({h i t } i,t ; N )</formula><p>. In the following, we refer to the local vectors h i t as the speaker vectors, and to the vectors c i as the speaker centroids. During training, clustering is not used. Speaker centroids are derived by grouping speaker vectors by speaker identity, relying on the speaker training objective described in Section III-C.</p><p>The separation stack maps the mixture x and the speaker centroids c into an N -channel signalŷ,</p><formula xml:id="formula_3">y = f (x, c) = f (x, kmeans(h(x); N )).</formula><p>Inspired by Luo and Mesgarani <ref type="bibr" target="#b9">[10]</ref>, we rely on a residual convolutional architecture for both stacks. Each residual block in the speaker stack composes a dilated convolution dconv <ref type="bibr" target="#b33">[34]</ref>, a non-linearity nl and layer normalization lnorm <ref type="bibr" target="#b34">[35]</ref>,</p><formula xml:id="formula_4">x l+1 = x l + lnorm(nl(dconv(x l ))).</formula><p>We use parametric rectified linear units <ref type="bibr" target="#b35">[36]</ref> for nl after experimenting with multiple alternatives. The last layer of the speaker stack applies Euclidean normalization to the speaker vectors.</p><p>The residual blocks of the separation stack are conditioned by the speaker centroids relying on FiLM, Feature-wise Linear Modulation <ref type="bibr" target="#b36">[37]</ref>,</p><formula xml:id="formula_5">x l+1 = x l + lnorm(nl(a * dconv(x l ) + b))</formula><p>where a = lin(c) and b = lin (c) are different linear projections of c, the concatenation of the speaker centroids. Section IV-E shows the advantage of FiLM conditioning over classical bias only conditioning, a = 1 <ref type="bibr" target="#b37">[38]</ref>. We learn distinct parameters for each layer for all parametric functions, i.e. dconv, nl, lnorm, lin and lin .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Training Objective</head><p>Model training addresses two objectives: (i) it learns speaker vectors which can be clustered by speaker identity into well separated clusters; (ii) it optimizes the reconstruction of the separated signals from aggregated speaker vectors.</p><p>Wavesplit assumes the training data is annotated with speaker identities from a finite set of M training speakers but does not require any speaker annotation at test time. Speaker identities are labeled in most separation datasets, including Wall Street Journal variants <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, meeting recordings <ref type="bibr" target="#b40">[41]</ref> or cocktail party recordings <ref type="bibr" target="#b41">[42]</ref>, but mostly unused in the source separation literature. Wavesplit exploits this information to build an internal model of each source and improve long-term separation.</p><p>The speaker vector objective encourages the speaker stack outputs to have small intra-speaker and large inter-speaker distances. From an input x with target signals {y i } N i=1 and corresponding speakers {s i } N i=1 ∈ [1, M ] N , the speaker loss favors correct speaker identification at each time step t, i.e.</p><formula xml:id="formula_6">L speaker (x, {s i } N i=1 ) = T t=1 min σ∈S N N i=1 speaker (h σ(i) t , s i )</formula><p>where speaker defines a loss function between a vector of R d and a speaker identity of <ref type="bibr">[1, M ]</ref>. The minimum over permutations expresses that each identity should be identified at each time step, in any arbitrary order. The best permutation (argmin) at each time-step is used to re-order the speaker vectors in an order consistent with the training labels. This allows averaging the speaker vectors originating from the same speaker at training time. This makes optimization simpler compared to work relying on k-means during training <ref type="bibr" target="#b42">[43]</ref>. This permutation per time-step differs from PIT [23]: we do not require the model to pick a single ordering over the output channels, as eventual channel swaps at the level of speaker vectors will be corrected by k-means. Moreover, the separation stack is trained from different permutations of the same labels. Three alternative are explored for speaker . All three maintain an embedding table over training speakers E ∈ R M ×d . First, dist speaker is a distance objective. This loss favors small distances between a speaker vector and the corresponding embedding while enforcing the distance between different speaker vectors at the same time-step to be larger than a margin of 1,</p><formula xml:id="formula_7">dist speaker (h j t , s i ) = h j t − E si 2 + k =j max(0, 1 − h j t − h k t 2 )<label>(2)</label></formula><p>Second, local speaker is a local classifier objective which discriminates among the speakers present in the sequence. It relies on the log softmax over the distances between speaker vectors and embeddings,</p><formula xml:id="formula_8">local speaker (h j t , s i ) = d(h j t , E si ) + log N k=1 exp −d(h j t , E s k )<label>(3)</label></formula><p>where d(h j t , E si ) = α h j t − E si 2 + β is the squared Euclidean distance rescaled with learned scalar parameters α &gt; 0, β. Finally, the global classifier objective global speaker is similar, except that the partition function is computed over all speakers in the training set, i.e. </p><formula xml:id="formula_9">) = d(h j t , E si ) + log M k=1 exp −d(h j t , E k ) .<label>(4)</label></formula><p>We use L speaker to update the speaker stack, as well as the speaker embedding table. The reconstruction objective aims at optimizing the separation quality, as defined in Eq. <ref type="formula" target="#formula_0">(1)</ref>.</p><formula xml:id="formula_10">L reconstr (f (x, c), y) = 1 N N i=1 reconstr (f (x, c) i , y i ).<label>(5)</label></formula><p>Contrasting with PIT <ref type="bibr" target="#b20">[21]</ref>, this expression does not require searching over the space of permutations since the centroids c = {c i } N i=1 are consistent with the order of the labels {y i } N i=1 as explained above. For reconstr , we use negative SDR with a clipping τ to limit the influence of the best training predictions, reconstr (ŷ, y) = − min(τ, SDR(ŷ, y)). Inspired by <ref type="bibr" target="#b11">[12]</ref>, we compute L reconstr at each layer of the separation stack, and use the average over all layers as our reconstruction loss.</p><p>We consider different forms of regularization to improve generalization to new speakers. At training time, we add Gaussian noise to speaker centroids, we replace full centroids with zeros (speaker dropout) and we replace some centroids with a linear combination with other centroids from the same training batch (speaker mixup). Speaker dropout and mixup are inspired by dropout <ref type="bibr" target="#b43">[44]</ref> and mixup <ref type="bibr" target="#b44">[45]</ref>. Note that regardless of the number of speakers in a sequence, speaker dropout removes at most one centroid, such that the separation task is not ambiguous (N − 1 speaker-channel assignments are sufficient to reconstruct the N th). Finally, we favor well separated embeddings for the training speakers with entropy regularization <ref type="bibr" target="#b45">[46]</ref>, reg = − i min j =i log E i − E j . Section IV-E analyses the benefits of regularization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Algorithm</head><p>Model training optimizes the weighted sum of L speaker and L reconstr with Adam <ref type="bibr" target="#b46">[47]</ref>. We train on mini batches of fixed-size windows. Wavesplit performs well for a wide-range of window sizes starting at 750ms, unlike most PIT approaches <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> that require longer segments (∼ 4s). The training set is shuffled at each epoch and a window starting point is uniformly sampled each time a sequence is visited. This sampling gives the same importance to each sequence regardless of its length. This strategy is consistent with the averaging of per-sequence SDR used for evaluation. We replicate each training sequence for all permutations of the target signals to avoid over-fitting a specific ordering. Replication, windowing and shuffling are not applied to validation or test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Data Augmentation with Dynamic Mixing</head><p>Separation benchmarks like WSJ0-2mix <ref type="bibr" target="#b8">[9]</ref> create a standard split between train, valid and test sequences and then generate a finite set of input mixtures by summing specific clean signals with specific weights (gains). As an orthogonal contribution to our model, we consider creating training mixtures dynamically. Our training augmentation creates new examples indefinitely by sampling random windows of training recordings to be summed after applying random gains. A similar method has been used in music source separation <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. This simple method brings systematic improvements, which advocate for the use of this training scheme when generating mixtures on the fly is not costly. We also experiment without augmentation to isolate the impact of Wavesplit alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS &amp; RESULTS</head><p>Most experiments are performed on the speaker separation dataset [58] built from the LDC WSJ-0 dataset <ref type="bibr" target="#b57">[59]</ref> as introduced in <ref type="bibr" target="#b8">[9]</ref>. We rely on the 8kHz version of the data, with 2 or 3 concurrent speakers. This setting is a de-facto benchmark for open-speaker source separation and we compare our results to alternative methods. Appendix I reports the dataset statistics. Additionally, we perform experiments in noisy settings. We rely on WHAM! with urban noise <ref type="bibr" target="#b38">[39]</ref> and WHAMR! with noise and reverberation <ref type="bibr" target="#b39">[40]</ref>. These datasets are derived from WSJ0-2mix and have identical statistics. We further evaluate variants of Wavesplit with different loss functions and architectural alternatives. We conduct an error analysis examining a small fraction of sequences with a strong negative impact on overall performance. We also perform experiments on the recently released LibriMix dataset <ref type="bibr" target="#b58">[60]</ref>. We also rely on the 8 kHz version of the data. Like for WSJ-0, we evaluate our model in clean and noisy settings with 2 or 3 concurrent speakers. The statistics of this larger dataset are also in Appendix. Finally, we show results on a fetus/mother heart rate separation task.</p><p>Our evaluation uses signal-to-distortion ratio (SDR) and scale-invariant SDR (SI-SDR) <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, see Section III-A. SDR is measured using the standard MIR-eval library <ref type="bibr">[61]</ref>. Like prior work, we report results as improvements, i.e. the metric obtained using the system output minus the metric obtained by using the input mixture as the prediction. We provide recordings processed by our system on a public webpage 1 as well as in supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameter Selection</head><p>Preliminary experiments on WSJ0-2mix <ref type="bibr" target="#b8">[9]</ref> drove our architecture choices for subsequent experiments. Both stacks have a latent dimension of 512 and the dilated convolutions have a kernel size of 3 without striding, therefore all activations preserve the temporal resolution of the input signal and no upsampling is necessary at the output layers. The dilation factor varies with depth. The speaker stack is 14-layer deep and dilation grows exponentially from 2 0 to 2 13 . The separation stack has 40 layers with the dilation pattern from Oord et al. <ref type="bibr" target="#b37">[38]</ref>, i.e. δ l = 2 l mod 10 . Every 10 layers, the dilation is reset to 1 allowing multiple fine-to-coarse-to-fine interactions across the time axis.</p><p>For training, we validated a learning rate of 1e−3 in [1e−3, 2e−3, 3e−3] and a speaker loss weight of 2 in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>. For regularization, we validated a distance regularization weight at 0.3 in [0, 0.2, 0.3, 0.5] and a Gaussian noise with standard deviation at 0.2 in [0, 0.1, 0.2, 0.3]. We use a speaker dropout rate of 0.4 (picked in [0, 0.2, 0.4, 0.6]) and a speaker mixup rate of 0.5 (picked in [0, 0.5, 1]). The clipping threshold on the negative SDR loss was validated at 30 for clean data and 27 for noisy data within <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. For L speaker , we found the global classifier to be the most effective, see Section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Clean Settings</head><p>WSJ0-2mix/3-mix is the de facto benchmark for separation. <ref type="table" target="#tab_0">Table I</ref> reports the results for 2 and 3 simultaneous speakers. In both cases, Wavesplit outperforms alternatives and dynamic mixing further increases this advantage. For instance, on WSJ0-2mix we report 21.0 ∆SI-SDR compared to 20.1 for the recent gated dual path RNN <ref type="bibr" target="#b11">[12]</ref>. This number improves to 22.2 with dynamic augmentation.</p><p>On WSJ0-2mix, <ref type="table" target="#tab_0">Table II</ref> analyses the error distribution. The test set has more sentences with poor ∆SDR (&lt; 10) compared to validation, 5.6% versus 0.9%. Unlike test data, validation data contains the same speakers as the training set and we observe that test examples with low ∆SDR are sequences where both speakers are close to the same training speaker identity, according to the learned embeddings (confusing speaker). Our oracle permutes the predicted samples across channels, and reports the best permutation, showing that most of the errors are channel assignment errors.</p><p>WSJ0-2mix recordings have a single dominant speaker, i.e. the same speaker stays the loudest throughout the whole recording. PIT might implicitly rely on this bias to address the channel assignment ambiguity and we evaluate robustness to change in dominant speaker on long sequences. We concatenate test sequences with the same pair of speakers, for length up to 10 times the original length. Unlike the training data, the loudest speaker changes between each concatenated sequence. For PIT models, we retrain Conv-TasNet [62] and take a pre-trained Dual-Path RNN model <ref type="bibr">[63]</ref>. <ref type="table" target="#tab_0">Table III</ref> shows the advantage of Wavesplit, which explicitly models sources in a time-independent fashion rather than relying on implicit rules that exploit biases in the training data. This is remarkable since Wavesplit is trained on 1s long windows compared to longer 4s windows for both PIT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Noisy and Reverberated Settings</head><p>WSJ0-2mix was recorded in clean conditions and noisy variants have been introduced to devise more challenging use cases. WHAM! <ref type="bibr" target="#b38">[39]</ref> adds noise recorded in public areas to the mixtures. As the model should only predict clean signals, it cannot exploit the fact that predicted channels should sum to the input signal. WHAMR! <ref type="bibr" target="#b39">[40]</ref> adds the same noise, but also reverberates the clean signals. The task is even harder as the model should predict clean signals without reverberation, i.e. jointly addressing denoising, dereverberation and source separation. <ref type="table" target="#tab_0">Table IV</ref> shows that our model outperforms previous work by a substantial margin. We also adapted dynamic mixing for these datasets. For WHAM!, we also sampled a gain for the noise, and combined it to reweighted clean signals to generate noisy mixtures on the fly. We similarly remixed WHAMR!, except that we reweighted reverberated signals with noise. On both datasets, this leads to an even larger improvement over previous work: e.g. our accuracy on WHAMR! is comparable to results on clean inputs (WSJ0-2mix) prior to <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Large scale experiments on LibriMix</head><p>We train Wavesplit on the newly released dataset LibriMix <ref type="bibr" target="#b58">[60]</ref>, which contains artificial mixtures of utterances from Librispeech <ref type="bibr" target="#b60">[65]</ref>, in four conditions (2/3 speakers, clean or noisy). We train on the train-360 subset of LibriMix to allow a fair comparison with the baselines from <ref type="bibr" target="#b58">[60]</ref>, which are trained in the same conditions. Like for WSJ-0, we rely on the 8 kHz version of the data. LibriMix is a good testbed for Wavesplit, as its training set contains significantly more speakers than WSJ0-2/3mix (921 in train-360 against 101 in the training set of WSJ0-2/3mix) which improves the robustness of the speaker stack. <ref type="table" target="#tab_3">Table V</ref> reports the results on all four conditions. The baselines are a Conv-TasNet model, as well as frequency masking oracles, Ideal Ratio Mask (IRM) and Ideal Binary Mask (IBM). Wavesplit significantly outperforms Conv-TasNet in all conditions, and even consistently outperforms oracle ideal masks. Moreover, the results are in the same range as the equivalent condition in WSJ0-2/3mix and WHAM!, which again confirms the robustness of the method across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>Table VI compares the base result (no dynamic mixing) obtained with the global classifier loss, Eq. (4), with the distance loss, Eq. <ref type="bibr" target="#b1">(2)</ref>. Although this type of loss is common in distance learning for clustering <ref type="bibr" target="#b53">[54]</ref>, the global classifier reports better results. We also experimented with the local classifier loss, Eq. (3), which yielded slower training and worse genereralization. Table VI also reports the advantage of multiplicative FiLM conditioning compared to standard additive conditioning <ref type="bibr" target="#b37">[38]</ref>. Not only reported SDRs are better but FiLM allows using a higher learning rate and yields faster training. Table VI also shows the benefit of regularizing the speaker representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Separation of Maternal and Fetal Electrocardiograms</head><p>Wavesplit separation method can be applied beyond speech. Electrocardiogram (ECG) reports voltage time series of the electrical activity of the heart from electrodes placed on the skin. During pregnancy, ECG informs about the function of the fetal heart but maternal and fetal ECG are mixed. We aim at separating these signals from a single noisy electrode recording. We use the FECGSYNDB data <ref type="bibr" target="#b61">[66]</ref> which simulates noisy abdominal ECG measurements of pregnancies in varying conditions (e.g. fetal movement, uterine contraction) over 34 electrode locations, each recording being 5 minutes at 250Hz. The database contains 10 pregnancies, that differ by the intrinsic characteristics of the mother and the fetus. We use 6 pregnancies for training, 2 for validation, and 2 for testing. Each sample of an electrode provides the noisy ECG mixture and ground-truth for maternal and fetal ECG. All 34 electrodes are given independently to the model, without location information. We train a single model independently of the electrode's location and rely on the same architecture as in our speech experiments, only validating regularization parameters. In this context, the source/speaker stack learns a representation of a mother and its fetus to condition the separation stack. Wavesplit, Dual-Path RNN, and Conv-TasNet are trained on the same data. <ref type="table" target="#tab_0">Table VII</ref> illustrates the advantage of Wavesplit on this task. A visualization of separated heart rates is shown in Appendix II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We introduce Wavesplit, a neural network for source separation. From the input mixed signal, our model extracts a representation for each source and estimates the separated signals conditioned on the inferred representations. Contrary to prior work, we learn both tasks jointly in an end-to-end manner, optimizing the reconstruction of the separated signals. For each mixed signal, the model learns to predict local representations which can be aggregated into a consistent representation for each source via clustering. Clustering is well suited for separation as it naturally represents a set of sources without arbitrarily ordering them. Separation with Waveplit relies on a single consistent representation of each source regardless of the input signal length. This is advantageous on long recordings, as shown by our experiments on speech separation. For this competitive application, our model redefines the state-of-the-art on standard benchmarks (WSJ-0-mix and LibriMix), both in clean and noisy conditions. We also report the benefits of Wavesplit on fetal/maternal heart rate separation from electrocardiograms. These results open perspectives in other separation domains, e.g. light curves in astronomy, electrical energy consumption, or spectroscopy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I WALL STREET JOURNAL MIX (WSJ0-MIX) AND LIBRIMIX DATASET</head><p>The Wall Street Journal mix dataset (WSJ0-mix) was introduced in <ref type="bibr" target="#b8">[9]</ref>, while LibriMix was introduced in <ref type="bibr" target="#b58">[60]</ref>. For LibriMix, we rely on the train-360 version of the training set. <ref type="table" target="#tab_0">Table VIII</ref> reports split statistics for the datasets we used.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Wavesplit for 2-speaker separation. The speaker stack extracts speaker vectors at each timestep. The vectors are clustered and aggregated into speaker centroids. The separation stack ingests the centroids and the input mixture to output two clean channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>global speaker (h j t , s i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 Fig. 2 .</head><label>22</label><figDesc>provides an example signal of Wavesplit applied to maternal/fetal heart rate separation. The input mixture (left) shows that the signal is almost indistinguishable from noise outside of peaks. However, our model extracts both the mother (center) and fetal (right) signal with high accuracy. Example of separation of maternal and foetal heart rate from a simulated abdominal electrode on the FECGSYNDB test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SI</head><label>I</label><figDesc>-SDR AND SDR IMPROVEMENTS (DB) ON WSJ0-2MIX AND WSJ0-3MIX.</figDesc><table><row><cell>Model</cell><cell cols="2">2 speakers</cell><cell cols="2">3 speakers</cell></row><row><cell></cell><cell cols="4">∆SI-SDR ∆SDR ∆SI-SDR ∆SDR</cell></row><row><cell>Deep Clustering [50]</cell><cell>10.8</cell><cell>-</cell><cell>7.1</cell><cell>-</cell></row><row><cell>uPIT-blstm-st [22]</cell><cell>-</cell><cell>10.0</cell><cell>-</cell><cell>7.7</cell></row><row><cell>Deep Attractor Net. [51]</cell><cell>10.5</cell><cell>-</cell><cell>8.6</cell><cell>8.9</cell></row><row><cell>Anchored Deep Attr. [52]</cell><cell>10.4</cell><cell>10.8</cell><cell>9.1</cell><cell>9.4</cell></row><row><cell>Grid LSTM PIT [23]</cell><cell>-</cell><cell>10.2</cell><cell>-</cell><cell>-</cell></row><row><cell>ConvLSTM-GAT [53]</cell><cell>-</cell><cell>11.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Chimera++ [54]</cell><cell>11.5</cell><cell>12.0</cell><cell>-</cell><cell>-</cell></row><row><cell>WA-MISI-5 [19]</cell><cell>12.6</cell><cell>13.1</cell><cell>-</cell><cell>-</cell></row><row><cell>blstm-TasNet [55]</cell><cell>13.2</cell><cell>13.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Conv-TasNet [10]</cell><cell>15.3</cell><cell>15.6</cell><cell>12.7</cell><cell>13.1</cell></row><row><cell>Conv-TasNet+MBT [56]</cell><cell>15.5</cell><cell>15.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepCASA [57]</cell><cell>17.7</cell><cell>18.0</cell><cell>-</cell><cell>-</cell></row><row><cell>FurcaNeXt [24]</cell><cell>-</cell><cell>18.4</cell><cell>-</cell><cell>-</cell></row><row><cell>DualPathRNN [11]</cell><cell>18.8</cell><cell>19.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Gated DualPathRNN [12]</cell><cell>20.1</cell><cell>-</cell><cell>16.9</cell><cell>-</cell></row><row><cell>Wavesplit</cell><cell>21.0</cell><cell>21.2</cell><cell>17.3</cell><cell>17.6</cell></row><row><cell>Wavesplit + Dynamic mixing</cell><cell>22.2</cell><cell>22.3</cell><cell>17.8</cell><cell>18.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ERROR</head><label>II</label><figDesc>ANALYSIS ON WSJ0-2MIX. valid test ∆SDR Split &lt; 10 ≥ 10 &lt; 10 ≥ 10</figDesc><table><row><cell>Examples %</cell><cell>0.9</cell><cell>99.1</cell><cell>5.6</cell><cell>94.4</cell></row><row><cell>Confusing spkr %</cell><cell>60.7</cell><cell>5.5</cell><cell>77.2</cell><cell>12.4</cell></row><row><cell>Mean ∆SDR</cell><cell>4.3</cell><cell>22.4</cell><cell>3.9</cell><cell>22.2</cell></row><row><cell>Oracle ∆SDR</cell><cell>17.4</cell><cell>22.9</cell><cell>18.8</cell><cell>22.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ∆SDR</head><label>III</label><figDesc>(DB) ON LONG SEQUENCES. SDR AND SDR IMPROVEMENTS (DB) ON WHAM! AND WHAMR!.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Model</cell><cell>Sequence Length</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>×1</cell><cell>×4</cell><cell>×10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Conv-TasNet</cell><cell>15.6 13.6 14.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">DualPathRNN 19.1 17.3 16.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Wavesplit</cell><cell></cell><cell>21.2 20.2 20.0</cell></row><row><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SI-Model</cell><cell>WHAM!</cell><cell></cell><cell cols="2">WHAMR!</cell></row><row><cell></cell><cell cols="4">∆SI-SDR ∆SDR ∆SI-SDR ∆SDR</cell></row><row><cell>Conv-TasNet [40], [64]</cell><cell>12.7</cell><cell>-</cell><cell>8.3</cell><cell>-</cell></row><row><cell>Learnable fbank [64]</cell><cell>12.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BLSTM-TasNet [40]</cell><cell>12.0</cell><cell>-</cell><cell>9.2</cell><cell>-</cell></row><row><cell>Wavesplit</cell><cell>15.4</cell><cell>15.8</cell><cell>12.0</cell><cell>11.1</cell></row><row><cell>Wavesplit + Dynamic mixing</cell><cell>16.0</cell><cell>16.5</cell><cell>13.2</cell><cell>12.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V SI</head><label>V</label><figDesc>-SDR AND SDR IMPROVEMENTS (DB) ON LIBRIMIX.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Libri2mix</cell><cell></cell><cell></cell><cell cols="2">Libri3mix</cell><cell></cell></row><row><cell>Condition</cell><cell>clean</cell><cell></cell><cell>noisy</cell><cell></cell><cell>clean</cell><cell></cell><cell>noisy</cell><cell></cell></row><row><cell></cell><cell cols="8">∆SI-SDR ∆SDR ∆SI-SDR ∆SDR ∆SI-SDR ∆SDR ∆SI-SDR ∆SDR</cell></row><row><cell>Conv-TasNet [60]</cell><cell>14.7</cell><cell>-</cell><cell>12.0</cell><cell>-</cell><cell>12.1</cell><cell>-</cell><cell>10.4</cell><cell>-</cell></row><row><cell>IRM (oracle) [60]</cell><cell>12.9</cell><cell>-</cell><cell>12.0</cell><cell>-</cell><cell>13.1</cell><cell>-</cell><cell>12.6</cell><cell>-</cell></row><row><cell>IBM (oracle) [60]</cell><cell>13.7</cell><cell>-</cell><cell>12.6</cell><cell>-</cell><cell>13.9</cell><cell>-</cell><cell>13.3</cell><cell>-</cell></row><row><cell>Wavesplit</cell><cell>19.5</cell><cell>20.0</cell><cell>15.1</cell><cell>15.8</cell><cell>15.8</cell><cell>16.3</cell><cell>13.1</cell><cell>13.8</cell></row><row><cell>Wavesplit + Dynamic mixing</cell><cell>20.5</cell><cell>20.9</cell><cell>15.2</cell><cell>15.9</cell><cell>17.5</cell><cell>18.0</cell><cell>13.4</cell><cell>14.1</cell></row><row><cell>TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell></row><row><cell cols="2">ABLATION ON WSJ0-2MIX.</cell><cell></cell><cell></cell><cell cols="5">SI-SDR AND SDR IMPROVEMENTS (DB) ON FECGSYNDB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(FETAL/MATERNAL ECG).</cell><cell></cell></row><row><cell>Model</cell><cell>∆SDR (dB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Base model w/ distance loss w/o FiLM w/o distance reg. w/o speaker dropout w/o speaker mixup</cell><cell>21.2 19.3 20.6 20.6 20.6 19.6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Model Conv-TasNet DualPathRNN Wavesplit</cell><cell>∆SI-∆SDR SDR 11.4 11.9 11.4 11.4 12.3 14.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VIII WSJ0</head><label>VIII</label><figDesc>-MIX AND LIBRIMIX STATISTICS.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>train</cell><cell>valid</cell><cell>test</cell></row><row><cell>WSJ0-2mix</cell><cell># examples</cell><cell>20k</cell><cell>5k</cell><cell>3k</cell></row><row><cell></cell><cell># speakers</cell><cell>101</cell><cell></cell><cell>18</cell></row><row><cell></cell><cell>mean length</cell><cell>5.4 sec</cell><cell>5.5 sec</cell><cell>5.7 sec</cell></row><row><cell>WSJ0-3mix</cell><cell># sequences</cell><cell>20k</cell><cell>5k</cell><cell>3k</cell></row><row><cell></cell><cell># speakers</cell><cell>101</cell><cell></cell><cell>18</cell></row><row><cell></cell><cell>mean length</cell><cell>4.9 sec</cell><cell>4.9 sec</cell><cell>5.2 sec</cell></row><row><cell>Libri2mix</cell><cell># examples</cell><cell>51k</cell><cell>3k</cell><cell>3k</cell></row><row><cell></cell><cell># speakers</cell><cell>921</cell><cell>40</cell><cell>40</cell></row><row><cell></cell><cell>mean length</cell><cell>15.0 sec</cell><cell>13.2 sec</cell><cell>13.2 sec</cell></row><row><cell>Libri3mix</cell><cell># sequences</cell><cell>34k</cell><cell>3k</cell><cell>3k</cell></row><row><cell></cell><cell># speakers</cell><cell>921</cell><cell>40</cell><cell>40</cell></row><row><cell></cell><cell>mean length</cell><cell>15.5 sec</cell><cell>13.2 sec</cell><cell>13.2 sec</cell></row><row><cell></cell><cell cols="2">APPENDIX II</cell><cell></cell><cell></cell></row><row><cell cols="5">MATERNAL AND FETAL HEART RATE SEPARATION</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://soundcloud.com/wavesplitdemo/sets</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors are grateful to Adam Roberts, Chenjie Gu, Raphael Marinier and Olivier Teboul for their advice on model implementation. They are also grateful to Jonathan Le Roux, John Hershey, Richard F. Lyon, Norman Casagrande and Olivier Pietquin for their help navigating speech separation prior work. The authors also thank Fernando Andreotti and Joachim Behar for the FECGSYNDB dataset.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An electrical load measurements dataset of united kingdom households from a two-year longitudinal study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stankovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sokoto coventry fingerprint dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Shehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruiz-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Palade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10609</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying exoplanets with deep learning: A five-planet resonant chain around kepler-80 and an eighth planet around kepler-90</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vanderburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astronomical Journal</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review of blind source separation in nmr spectroscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Toumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caldarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torrésani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in nuclear magnetic resonance spectroscopy</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="37" to="64" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for single-channel multi-talker speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1670" to="1679" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1117372</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1117372" />
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: theory and practice</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio Source Separation and Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Ltd</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="31" to="35" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<idno>abs/1810.04826</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="793" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind separation of speech mixtures via time-frequency masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1830" to="1847" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discrete-time Signal Processing: An Algebraic Approach, ser. Advanced Textbooks in Control and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williamson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2708" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underdetermined blind speech separation with directivity pattern based continuous mask and ica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mukai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 12th European Signal Processing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1991" to="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single channel speech separation with constrained utterance level permutation invariant training using grid LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Furcanext: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalized end-to-end loss for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4879" to="4883" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint learning of speaker and phonetic similarities with siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1295" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The persistent challenge of foetal heart rate monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ayres-De Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Obstetrics and Gynecology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="104" to="109" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A non-invasive methodology for fetal monitoring during pregnancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Karvounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsipouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papaloukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsalikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Naka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fotiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of information in medicine</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="238" to="253" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Noninvasive fetal ecg: the physionet/computing in cardiology challenge 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sameni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in Cardiology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fetal electrocardiogram extraction and analysis using adaptive noise cancellation and wavelet transformation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sutha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayanthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">SDR -half-baked or well done?&quot; in ICASSP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW. ISCA</publisher>
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wham!: Extending speech separation to noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Whamr!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The ami meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reidsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In: Proceedings Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral</title>
		<editor>Research. L.P.J.J. Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmerman</editor>
		<meeting><address><addrLine>Wageningen</addrLine></address></meeting>
		<imprint>
			<publisher>Noldus Information Technology</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>in</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The fifth &apos;chime&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1561" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Novel deep architectures in speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-64680-0_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-64680-06" />
	</analytic>
	<monogr>
		<title level="m">New Era for Robust Speech Recognition, Exploiting Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spreading vectors for similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkGuG2R5tm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Demucs: Deep extractor for music sources with extra unlabeled data remixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="246" to="250" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Speaker-independent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cbldnn-based speaker-independent speech separation via generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="711" to="715" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. IEEE</title>
		<imprint>
			<biblScope unit="page" from="686" to="690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tasnet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. IEEE</title>
		<imprint>
			<biblScope unit="page" from="696" to="700" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mixup-breakdown: a consistency training method for improving generalization of speech separation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W Y</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>CSR-I (WSJ0) complete</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Filterbank design for end-to-end speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An open-source framework for stress-testing non-invasive foetal ecg extraction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zaunseder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological measurement</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">627</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
