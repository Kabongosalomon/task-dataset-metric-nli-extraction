<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Activity Driven Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Activity Driven Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object detection aims at reducing the amount of supervision required to train detection models. Such models are traditionally learned from images/videos labelled only with the object class and not the object bounding box. In our work, we try to leverage not only the object class labels but also the action labels associated with the data. We show that the action depicted in the image/video can provide strong cues about the location of the associated object. We learn a spatial prior for the object dependent on the action (e.g. "ball" is closer to "leg of the person" in "kicking ball"), and incorporate this prior to simultaneously train a joint object detection and action classification model. We conducted experiments on both video datasets and image datasets to evaluate the performance of our weakly supervised object detection model. Our approach outperformed the current state-of-the-art (SOTA) method by more than 6% in mAP on the Charades video dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning techniques and development of large datasets have been vital to the success of image and video classification models. One of the main challenges in extending this success to object detection is the difficulty in collecting fully labelled object detection datasets. Unlike classification labels, detection labels (object bounding boxes) are more tedious to annotate. This is even more challenging in the video domain due to the added complexity of annotating along the temporal dimension.</p><p>On the other hand, there are a large number of video and image datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> labelled with human actions which are centered around objects. Action labels provide strong cues about the location of the corresponding objects in a scene ( <ref type="figure">Fig. 1</ref>) and could act as weak supervision for object detection. In light of this, we investigate the idea of learning object detectors from data labelled only with action classes as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>All images/videos associated with an action contain the object mentioned in the action (e.g."cup" in the action "drink from cup"). Yuan et al. <ref type="bibr" target="#b49">[50]</ref> leveraged this prop-Spatial correlation between subject and object Object appearance consistency hold vacuum fix vacuum</p><p>Object is strong cue for action <ref type="figure">Figure 1</ref>: Our framework is built upon three observations we draw: <ref type="bibr" target="#b0">(1)</ref> there is spatial dependence between the subject and the interacted object; (2) the object appearance is consistent across different training samples and across different actions involving the same object; (3) the most informative object about the action is the one mentioned in the action. erty to learn object detection from videos of corresponding actions. However, the actions ("drink from" in above example) themselves are not utilized in this work. On the other hand, the spatial location, appearance and movement of objects in a scene are dependent on the action performed with the object. The key contribution of our work is to leverage this intuition to build better object detection models. Specifically, we have three observations (see <ref type="figure">Fig. 1</ref>): <ref type="bibr" target="#b0">(1)</ref> There is spatial dependence between the position of a person and the object mentioned in the action, e.g.in action "hold cup", the location of cup is tightly correlated with the location of the hand. This could provide a strong prior for the object; <ref type="bibr" target="#b1">(2)</ref> The object appearance is consistent across images and videos of action classes which involve the object; <ref type="bibr" target="#b2">(3)</ref> Detecting the object should help in predicting the action and vice-versa.</p><p>The above observations can be used to address one of the main challenges of weakly supervised detection: the presence of a large search space for object bounding boxes during training. Each training image/video has many candidate object bounding boxes (object proposals). In our weakly su-  pervised setting, the only label we have is that, one of these candidates should correspond to the object mentioned in the action. The training algorithm is required to automatically identify the correct object bounding box from this large set of candidates. In our approach, we narrow down this search by incorporating the three observations in our model. In particular, we (1) explicitly learn the spatial prior of objects w.r.t. the human in different actions; (2) train a generic object classifier for modeling object appearances across different actions; (3) jointly learn the action classifier and associated object classifier.</p><p>We conducted comprehensive experiments over two video datasets: Charades <ref type="bibr" target="#b36">[37]</ref>, EPIC KITCHENS <ref type="bibr" target="#b6">[7]</ref> and an image dataset: HICO-DET <ref type="bibr" target="#b4">[5]</ref>. Our method outperforms the previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b40">41]</ref> by a large margin on all datasets. Specifically, we have achieved a 6% mAP boost on Charades compared to current state-of-the-art weakly supervised models for videos. Visualization results and ablation experiments show the effectiveness of each module in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly overview some related research topics and how we are motivated by these works.</p><p>Supervised Object detection. Object detection is a very active research topic in the computer vision field. There has been significant progress in the recent years with the advances of deep learning. R-CNN <ref type="bibr" target="#b14">[15]</ref> is the first work that introduces CNN features to object detection. A sequence of later works are developed based on R-CNN. Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> accelerates R-CNN by introducing an ROI pooling layer and improve the performance by applying proposal classification and bounding box regression jointly. Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> further improves the speed and accuracy by replacing the proposal generation stage with a learnable network: region proposal network and the whole framework is trained in an end-to-end fashion. Mask R-CNN <ref type="bibr" target="#b20">[21]</ref> proposed to add a segmentation branch and achieved the stateof-the-art (SoTA) performance. All the methods require full object bounding box annotations and mask R-CNN requires dense segmentation labels.</p><p>Weakly supervised object detection. The fully supervised object detection methods rely heavily on large scale bounding box annotations, which is inefficient and labor consuming. To alleviate this issue, there have been various weakly-supervised works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref> that leverage the more efficient image-level object class annotations. Weakly supervised deep detection networks (WSDDN) <ref type="bibr" target="#b2">[3]</ref> proposed an end-to-end architecture to perform region selection and classification simultaneously. It is achieved by separately performing classification and detection headers and the supervision comes from a combination classification score. ContextLocNet <ref type="bibr" target="#b25">[26]</ref> further improves WSDDN by taking contextual region into consideration. Beyond the image domain, another line of research works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45]</ref> try to leverage the temporal information in videos to facilitate the weakly supervised object detection. Kwak et al. <ref type="bibr" target="#b27">[28]</ref> proposed to discover the object appearance presentation across videos and then track the object in temporal space for supervision. Wang et al. <ref type="bibr" target="#b44">[45]</ref> perform unsupervised tracking on videos and then cluster similar deep features to form visual representation.</p><p>Yuan et al. <ref type="bibr" target="#b49">[50]</ref> proposed a much more efficient actiondriven weakly supervised object detection setting which aims to learn the object appearance representation given only videos with clip-level action class labels. They proposed to first extract spatial features from object proposals. The features are then updated using long short-term memory (LSTM) <ref type="bibr" target="#b21">[22]</ref> applied on neighboring frames. The frame-level object classification loss is computed on the updated features. We implemented the same setting as in <ref type="bibr" target="#b49">[50]</ref>: pipeline trained on videos/images with only action labels and test on images. Unlike TD-LSTM <ref type="bibr" target="#b49">[50]</ref> that only leverages object class information, we propose to jointly exploit both action and object class labels. Considering all actions are interactions between person and objects, we incorporate human pose estimation into the framework.</p><p>Activity recognition There are a variety of works in the field of action recognition <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. Maji et al. <ref type="bibr" target="#b28">[29]</ref> train action specific poselets that are then classified using SVMs. The contextual cues are captured by explicitly detecting objects and exploiting action labels of other people in the image. R*CNN <ref type="bibr" target="#b16">[17]</ref> proposed to implicitly model the main objects. The features from both person region and object proposal regions are extracted and a fusion of classification scores from these two types of features is used for action classification loss. R*CNN showed that the most informative object in the scene is the object mentioned in action class. We are inspired by the similar idea to jointly consider the action and object lables.</p><p>Human-object interaction There are mainly two tasks in the human-object interaction (HOI) topic: HOI recogi- There are three streams in the proposed framework: object spatial prior module (colored in blue), object classification stream (colored in yellow) and action classification module (colored in green). We incorporate human keypoint detection into the framework and jointly leverage action and object labels. nition and HOI detection. HOI recognition aims at recognizing the interaction between subject and object. There have been a surge of works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref> on HOI recognition since 2009. HOI detection task aims at localizing subject and object and also recognizing HOI class. Chao et al. <ref type="bibr" target="#b4">[5]</ref> proposed a three-stream network for this task, one stream each for person detection, object detection and person-object pair wise classification respectively. Gkioxari et al. <ref type="bibr" target="#b15">[16]</ref> model the interaction with shared weights between human centric branch and interaction branch. Kalogeiton et al. <ref type="bibr" target="#b24">[25]</ref> proposed to jointly learn the object and action (e.g. dog running). All works have shown that jointly learning the object, person localization and HOI/action classification benefits the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The main challenge of weakly supervised detection is the lack of bounding box information during training and the availability of only image/video level labels. This problem is typically handled in a Multiple Instance Learning (MIL) setting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, where the training method implicitly chooses the best bounding box from a set of candidate proposals in the image/video to explain the overall image/video label.</p><p>However, in practice the number of candidate object proposals can be quite large, making the problem challenging. In our work, we address this issue by imposing additional constraints on the choice of the best object bounding box based on the location prior of the object w.r.t. the human and the importance of the chosen object proposal for action classification. In practice, we model each of these as three different streams in our model which finally contribute to a single action classification loss and an object classification loss. Note that in our work we assume that a pre-trained person detection model and human keypoint detection model are available to extract the signals needed for capturing human-object dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>Formally, for a training sample (video clip or image), the action label a is provided. The action a belongs to a predefined set of actions a ∈ A, which is of size n a : ||A|| = n a . We assume that all human actions are interactive and there is one object involved in each action. For example, the object cup appears in the action holding a cup. The object class associated with action a is denoted by o a and there are n o object classes in total: o a ∈ O, ||O|| = n o .</p><p>A pre-trained human detector <ref type="bibr" target="#b20">[21]</ref> and pose estimation network <ref type="bibr" target="#b45">[46]</ref> are used to extract human bounding box h and keypoint locations k(p), p ∈ P where P represents the set of human keypoints. For training samples with multiple people, we pick the detection result with the highest detection confidence. The object proposals R are extracted. We remove proposals with high overlap (IoU &gt; θ h ) with human region h and we keep the top n r with highest confidence.</p><p>Our model has three streams which are explained in detail in the next sections. An overview of our models is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The first stream models the spatial prior of the object w.r.t. to human keypoints in each action. The prior is used to construct an object classification stream which weights the object classification losses of different proposals in an image/video. The weights and features from the object proposals, along with features of human bounding box, are used to construct an action classification loss. The combined loss from action classification and object classification is minimized during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object spatial prior</head><p>The object spatial prior is modeled in two stages: <ref type="bibr" target="#b0">(1)</ref> given an action class a and keypoint detection results P, we estimate an anchor location based on a weighted combination of the keypoint locations; (2) given the action class and the anchor position, the position of the object is modeled as a normal distribution w.r.t. the anchor point. This is based on our observation that for a given action, certain human keypoints provide strong location priors for the object locations("hand" for drinking from a cup, "foot" for kicking a ball etc.).</p><p>The anchor location k anchor is calculated as a weighted sum of all keypoint locations. The keypoint weight is modeled with a probability vector w a key (p), p ∈ P for the action class a.</p><formula xml:id="formula_0">k anchor = p∈P w a key (p)k(p)<label>(1)</label></formula><p>where k(p) is the detected position of the keypoint p in the training image/video. Given the action class a, the weight of object location w.r.t. the anchor location is modeled with a learned normal distribution:</p><formula xml:id="formula_1">N (µa,σa) µ a ∈ R 2 , σ a ∈ R (2×2)</formula><p>. µ a represents the mean location of the object w.r.t.</p><p>the anchor and σ a represents the variance. This distribution is used to calculate the object location probabilities of different locations. Specifically, the probability of an object being at the location of a proposal r ∈ R for an action class a is</p><formula xml:id="formula_2">w a r = N (µa,σa) (kprop(r) − k anchor )<label>(2)</label></formula><p>where k prop (r) is the center of the proposal r. Note that the distributions w key , N (µa,σa) are learned automatically during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object classification</head><p>For each proposal r ∈ R in a training sample, we compute an object classification score for each object o: s O (r; o). Here s O corresponds to an ROI-pooling layer followed by a Multi Layer Perceptron (MLP) which classifies the input region into n o object classes. Apart from only leveraging image-level object labels for classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>, the spatial location weights from previous section are also used to guide the selection of the object proposal. Formally, the binary cross-entropy (BCE) loss is calculated on each proposal region, against the image-level object class ground truth. The BCE losses are weighted by the location probabilities of different proposals and the weighted sum is used to compute object classification loss:</p><formula xml:id="formula_3">L obj = − 1 nr r∈R w a r · Lo(r),</formula><p>Lo(r) = 1 no o∈O yolog(P (o|r))+(1−yo)log(1−P (o|r)),</p><formula xml:id="formula_4">P (o|r) = exp(sO(r; o)) o∈O exp(sO(r; o)) ,<label>(3)</label></formula><p>where y o is the binary object classification label for the object o. Note that y o is non-zero only for the object mentioned in the action corresponding to the image/video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Action classification</head><p>For the task of action recognition, especially for interactive actions as in our task, both the person and the object appearances are vital cues. As indicated in <ref type="bibr" target="#b16">[17]</ref>, the spatial location of the most informative object can be mined from action recognition task. We incorporate a similar idea into the action classification stream by fusing features from the proposal regions and person region. Formally, for a training instance with action label a, the appearance features of both person region h and proposal regions R are extracted, and then classified to n a -dimension action classification scores: , <ref type="bibr" target="#b3">(4)</ref> where y a is the binary action classification label for the action a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Temporal pooling for videos</head><p>Our experiments are conducted on both video and image datasets, thus the training samples can be video sequences or static images with action labels. For models trained with video clips, we adopt a few pre-processing steps and also pool scores across the temporal dimension to improve person detection and object proposal quality. Formally, n frames are uniformly sampled from the training clip, followed by person detection and object proposal generation for the sampled frames. The object proposals as well as person bounding boxes across the frames are then connected by an optimization based linking method <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48]</ref> to form object proposal tubelets and person tubelets respectively. We observed that temporal linking of proposals avoids spurious proposals and leads to more robust features from the proposals. These are fed as inputs into the object classification and action classification streams. Temporal pooling is used to aggregate classification scores across the person and object tubelets. The pooled scores are finally used for loss computation as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss terms</head><p>The combined loss is a weighted sum of both classification loss terms.</p><formula xml:id="formula_5">L = α o L obj + α a L act<label>(5)</label></formula><p>The hyper-parameters α o and α a are weights to trade off the relative importance of object classification and action classification in the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Inference</head><p>During testing, object proposals are firstly extracted on the test sample. The trained object classifier (s O ) is applied on each proposal region to obtain the object classification scores (P (o|r)). Then the non-maximal suppression (NMS) is applied and the object proposals with higher classification scores than the threshold are preserved as detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our method is applicable to both video and image domains. We require only human action label annotations for training. Object bounding box annotations are used only during evaluation. Code will be released in the Github repository <ref type="bibr" target="#b0">1</ref> .</p><p>Video datasets: The Charades dataset <ref type="bibr" target="#b36">[37]</ref> includes 9,848 videos of 157 action classes, among which, 66 are interactive actions with objects. There are on average 6.8 action labels for a video. The official Charades dataset doesn't provide object bounding box annotations and we use the annotations released by <ref type="bibr" target="#b49">[50]</ref>. In the released annotations, 1,812 test videos are down-sampled to 1 frame per second (fps) and 17 object classes are labeled with bounding boxes on these frames. There are 3.4 bounding box annotations per frame on average. We follow the same practice as in <ref type="bibr" target="#b49">[50]</ref>: train on 7,986 videos (54,000 clips) and evaluate on 5,000 randomly selected test frames from 200 test videos.</p><p>The EPIC-KITCHENS <ref type="bibr" target="#b6">[7]</ref> is an ego-centric video dataset which is captured by head-mounted camera in different kitchen scenes. In the training data, the action class is annotated for 28,473 trimmed video clips and the object bounding boxes are labeled for 331 object classes. As the object bounding box annotations are not provided for the test splits, we divide the training data into training, validation and test parts. The top 15 frequent object classes (which are present in 85 action classes) are selected for experiments, resulting in 8,520 training, 1,000 validation and 200 test video clips. We randomly sample three times from each training clip and generate 28,560 training samples. We also randomly sample 1,200 test frames from the test clips.</p><p>Image dataset The HICO-DET dataset <ref type="bibr" target="#b4">[5]</ref> is designed for human-object interaction (HOI) detection task. This dataset includes 38,118 training images and 9,658 test images. The human bounding box, object bounding box and an HOI class label are annotated for both training and test images. In total, there are 80 object classes (e.g. cup, dog, etc.) and 600 HOI classes (e.g. hold cup, feed dog, etc.). We filter out all samples with "no interaction" HOI labels, interaction class with less than 20 training samples and all "person" as object class samples. This results in 32,100 training samples of 510 interaction classes and 79 object classes. We use the HOI labels as action class labels during training and the object bounding box annotations are used only for evaluation. Unlike Charades where the interactions mostly happen between one subject and one object, there are cases where multiple people interact with one object (e.g. "boarding the airplane") and one person interacts with multiple objects (e.g. "herding cows"), which makes it more challenging to learn the object appearance.</p><p>We report per-class average precision (AP) at intersection-over-union (IoU) of 0.5 between detection and ground truth boxes, and also mean AP (mAP) as a combined metric, following the tradition of <ref type="bibr" target="#b49">[50]</ref>. We also report CorLoc <ref type="bibr" target="#b8">[9]</ref>, a commonly-used weakly supervised detection metric. CorLoc represents the percentage of images where at least one instance of the target object class is correctly detected (IoU&gt;0.5) over all images that contain at least one instance of the class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We use VGG-16 and ResNet-101 pre-trained on Ima-geNet dataset as our backbone feature extraction networks. All conv layers in the network are followed with ReLU activation except for the top classification layer. Batch normalization <ref type="bibr" target="#b22">[23]</ref> is applied after all convolutional layers. In order to compute the classification scores (s O , s H A , s O A ), three branches are built on top of the last convolutional block. Each branch consists of ROI-pooling layer and 2-layer multiple layer perception (MLP) of intermediate dimension of 4096. The threshold for removing person proposal regions is set as θ h = 0.5. Selective search <ref type="bibr" target="#b41">[42]</ref> is used to extract object proposals for all our experiments.</p><p>The Adam optimizer <ref type="bibr" target="#b26">[27]</ref> is applied with learning rate of 2 × 10 −5 and batch size of 4. The loss weights are set as α a = 1.0, α o = 2.0. The number of sampled frames in a clip is set as n = 8 and the number of proposals is set as n r = 700. The whole framework is implemented with PyTorch <ref type="bibr" target="#b30">[31]</ref>. We train on a single Nvidia Tesla M40 GPU. The whole training converges in 20 hrs. More details of implementation are presented in supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Influence of modeling spatial location of object</head><p>Unlike many existing methods for weakly supervised object detection, our framework explicityly models the spatial locaition of the object w.r.t. to the detected person and encodes it into two different loss functions in Eq. 3, 4. We explore the effect of modeling this spatial prior through different distributions and its contribution to each of the loss terms. The different distributions include: (a) normal distribution, (b) a fixed grid of probability values, where we make a discrete version of spatial prior module by pre-defining a 3 × 3 grid around the keypoint, and (c) a simple center prior where we penalize object detections farther away from the center of the object. Note that, we totally removed person detection bounding box and pose estimation in the center prior baseline. For this baseline, we use the frame center as the anchor location L and learn the µ a and σ a .</p><p>We also experimented with learning distribution mean only (µ a ), learning variance only (σ a ) and joint learning of mean and variance (µ a + σ) for the normal distribution. We alos experimented with using only object classification or action classification loss.</p><p>The quantitative results of VGG-16 as the backbone network are presented in Tab. 1 for different ablation settings. First, we observe that a learnable grid-based or normal distribution for the anchor location outperforms a simple heuristic choice of the image center as the anchor. We also see that the normal distribution, where both mean and variance are learned for each action-object pair leads to better results compared to the other settings. This shows that good modeling of the object spatial prior w.r.t. human in an action provides strong cues for detection. We also notice that jointly modelling both action and object classification achieves the best result.</p><p>We also visualize the learned distribution of object location probabilities from the prior module for a few sample videos/images in <ref type="figure" target="#fig_5">Fig. 4</ref>. The learned distribution often has large probability weights around the object mentioned in the action. For example, in the first two columns of the visualization, it is much easier to localize the object with the cues from the heatmap. However, we also note that this distribution is less useful for actions where there is no consistent physical interaction between the human and the object. This is shown in the last column of the figure, for actions like "watching television" and "flying kite". Our approach reports relatively low mAP performance on such object classes (Tab. 2 and Tab. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with existing methods</head><p>We compare our method with other weakly supervised methods and their variants: (1) WSDDN <ref type="bibr" target="#b2">[3]</ref>; (2) Context-LocNet <ref type="bibr" target="#b25">[26]</ref>; (3) PCL <ref type="bibr" target="#b40">[41]</ref>; action-driven weakly supervised object detection method: (4) TD-LSTM <ref type="bibr" target="#b49">[50]</ref> and (5) R*CNN <ref type="bibr" target="#b16">[17]</ref> which is designed for action recognition with  awareness of the main object. We used the main object bounding box as the object detection result. R*CNN is pretrained on Pascal-action dataset and then finetuned on Charades or HICO-DET dataset. Note that existing methods (1), (2), (3), (4) do not use person bounding box or keypoint detection results in their model unlike our method. While <ref type="bibr" target="#b4">(5)</ref> uses person bounding box, it doesn't use person keypoints. The person detection and pose models used in our model were trained only once and kept fixed during training. The annotations required to train the person models are very inexpensive in comparison to fully supervised models which need bounding box annotation for every object class. The resource demands of annotating person bounding boxes and pose is amortized across all object classes. However, since these models are not used in traditional weakly supervised methods, we enable fair comparison by constructing variants of PCL and R*CNN: (6) R*CNN with spatial prior and (7) PCL with spatial prior, where we replace the max pooling in R*CNN and mean pooling in PCL with a weighted sum where the weights are computed from spatial prior distribution as in our implementation (more details are presented in supplemental material).</p><p>Results from TD-LSTM <ref type="bibr" target="#b49">[50]</ref> are shown only for Charades, since it is a video-specific model and code is not available. Also, we report results from weakly-supervised models whose code is available or whose results on Charades, HICO-DET or EPIC KITCHENS datasets is readily available. Also, note that many methods such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b1">2]</ref> are built on top of the vanilla WSDDN method by adding signals such as segmentation, contextual information, in-   <ref type="bibr" target="#b40">[41]</ref> stance refinement, etc. and these ideas are complementary to the ones presented in this work, and can be added to our model to achiever better results. The per-class AP and combined mAP performances on the two datasets are presented in Tab. 2 and Tab. 3 respectively. 10 object classes on HICO-DET are randomly selected and presented. On Charades dataset, our method achieves 6% mAP boost compared to PCL <ref type="bibr" target="#b40">[41]</ref>. Our method performs better on object classes like broom, refrigerator, vacuum, etc.. The spatial prior patterns of the interactions involving these object classes are more predictable and thus the prior modeling benefits our approach more than on other object classes. For object like tv, the spatial prior pattern of the interaction (e.g.watch tv) is more diverse and thus difficult to model, resulting only a small boost in mAP. The same performance pattern also applies to HICO-DET dataset. On the object class kite, our method slightly performs inferior to the baseline method.</p><p>We observe that the spatial prior from our model is effective in localizing the object during training even when combined with other models such as R*CNN <ref type="bibr" target="#b16">[17]</ref> and PCL <ref type="bibr" target="#b40">[41]</ref>. R*CNN with spatial prior modeling outperforms TD-LSTM, which is specifically designed for the action driven weakly supervised object detection task.</p><p>We also report our model's performance without the spatial prior module (ours (w/o prior)). This variant of the model doesn't require any person bounding box and keypoint information, and is directly comparable to existing weakly supervised methods. We note that even without these signals, our model can outperform existing methods. This can be attributed to the fact that our model jointly uses both action and object labels during training. It identifies the object bounding box which can both help action classification and object classification during training.</p><p>The qualitative comparison between our method and PCL is presented in <ref type="figure" target="#fig_6">Fig. 5</ref>. Our approach localizes the object more accurately. Multiple object classes and multiple instances can be detected through our trained object classification stream. The last column shows our failure cases. On Charades, both PCL <ref type="bibr" target="#b40">[41]</ref> and our method fails to detect the windows and on HICO-DET, our method fails to localize the kite. One possible reason is that actions like "watch out of the window" do not have direct human-object interaction.</p><p>Our approach is also extended to ego-centric EPIC KITCHENS datset. Since human keypoints are not visible in this dataset, we applied "center" spatial prior modeling used in Sec. 4.2. As the camera is fixed with respect to the human, the anchor location is already implicitly modeled by this center prior. We compare with R*CNN <ref type="bibr" target="#b16">[17]</ref> and PCL <ref type="bibr" target="#b40">[41]</ref> on the 1,200 test frames. Egocentric videos have a strong prior for object spatial locations and hence our method is able to outperform other methods in Tab. 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effect of supervision in training</head><p>Weakly supervised object detection aims to train object detection models without any bounding box labels. However, in practice it is easy and efficient to annotate at least a few bounding boxes in training images/videos. This is similar to low-shot and semi-sueprvised settings. We believe that it is important to test weakly supervised approaches in such a practical setting as well.</p><p>To this end, we explore the effect of adding varying amounts of ground truth object bounding box annotations into our training data. We achieve this by augmenting the losses described in Sec. 3, with an additional supervised object detection loss for videos/images where bounding box annotations are available. This loss is the same as the traditional object detection loss used in Fast-RCNN.</p><p>In practice, the IoU between object proposals and ground truth object bounding boxes is calculated and proposals having higher IoU than the threshold are considered positive samples and rest as negative. The threshold IoU is set as 0.45 to guarantee a reasonable positive samples per image. The negative and positive sample ratio is set as 5.</p><p>We compare with two baselines: (1) model without weak supervision: model trained only with supervised detection loss on images/videos with bounding box annotations and without any weakly supervised data (Ours (w/ only strong supervision)), and (2) R*CNN <ref type="bibr" target="#b16">[17]</ref> with additional object bounding box supervision as above (R*CNN (w/ strong+weak supervision)).</p><p>We evaluate this setup on both Charades and HICO-DET datasets. The quantitative results are presented in <ref type="figure" target="#fig_7">Fig. 6</ref>. The x-axis (log scale) represents the percentage of training samples with object bounding box annotations. For example, the point x% represents that for a random x% of training data samples, the bounding box annotations are present. The remaining training samples, only have action class label. Note that 0% is the weakly-supervised setting considered earlier, while 100% represents fully-supervised setting. We observe that the mAP increases log-linearly as more su- pervision is added to the training. For Charades, when small amount of supervision is added, we observe that our model which uses additional weakly-supervised data outperforms the model without any weak supervision. This clearly shows the potential of our weakly-supervised approach to provide complementary value in a low-shot detection setting. With as low as 70% supervision, our approach already matches the performance of fully-supervised method at 100% supervision. This means that we could cut down the amount of supervision needed to train the model without sacrificing performance. As expected, the gap between the two approaches decrease with increase in supervision. Even with "100%" bounding box annotations, our model still outperforms the fully supervised method by 2 mAP points due to joint training with action and object classification loses.</p><p>We also observe that performance gap is smaller for images (HICO-DET). We believe weak supervision is more effective in videos compared to images, where temporal linking of proposals helps in avoiding spurious detections during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We observe that object spatial location, appearance and movement are tightly related to the action performed with the object in images and videos. We propose a model that leverages these observations to train object detection models from samples annotated only with action labels. Comprehensive experiments are conducted on both video and image datasets. The comparison with SoTA methods shows that out approach outperforms existing weakly supervised approaches. Further our approach can also help reduce the amount of supervision required for object detection models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Setting of the action-driven weakly supervised object detection task. Training samples include videos or images with action class labels (left). The inference is conducted on single frame/image for object localization and classification (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The diagram of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>s O A (r; a), r ∈ R and s H A (h; a). Here s H A , s O A correspond to an ROI-pooling layer followed by a Multi Layer Perceptron (MLP). The weights and biases of the MLP are learned during training. The final proposal score is computed as an average of action classficiation scores weighted by the spatial prior probabilities as in the previous section. This ensures that only scores from the most relevant proposals are given a higher weight. The sum of action classification scores from object proposals and person regions is used to compute the final BCE action classification loss. The loss is computed as follows: Lact=− 1 na a∈A yalog(P (a))+(1−ya)log(1−P (a)), P (a) = exp s H A (h; a) + r∈R w a r s O A (r; a) a∈A exp s H A (h; a) + r∈R w a r s O A (r; a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Heatmap on three Charades actions (b) Heatmap on three HICO-DET actions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of learned object location probability w.r.t. selected person keypoint. The heatmap represents the probability of object location (brighter color represents larger probability value) and the white circle represents the selected keypoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative detection results on (a) Charades test frames and (b) HICO-DET test images. Red bounding boxes denote our results and green bounding boxes denote results of PCL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Performance comparison between our method trained with different supervision settings and R*CNN trained with both strong and weak supervision on (a) Charades and (b) HICO-DET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detection performance of different variants on Charades</figDesc><table><row><cell>Spatial prior</cell><cell>Loss term</cell><cell>mAP CorLoc</cell></row><row><cell>Center</cell><cell cols="2">action+object 3.43 34.27</cell></row><row><cell>Grid</cell><cell cols="2">action+object 4.32 36.94</cell></row><row><cell>Normal (µ)</cell><cell cols="2">action+object 6.27 42.36</cell></row><row><cell>Normal (σ)</cell><cell cols="2">action+object 4.86 38.05</cell></row><row><cell>Normal (µ + σ)</cell><cell>action</cell><cell>2.61 31.60</cell></row><row><cell>Normal (µ + σ)</cell><cell>object</cell><cell>5.86 39.24</cell></row><row><cell cols="3">Normal (µ + σ) action+object 8.76 47.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AP performance (%) on each object class and mAP (%) comparison with different weakly supervised methods on Charades. Methods bed broom chair cup dish door laptop mirror pillow refri shelf sofa table tv towel vacuum window mAP(%) WSDDN [3] 2.38 0.04 1.17 0.03 0.13 0.31 2.81 0.28 0.02 0.12 0.03 0.41 1.74 1.18 0.07</figDesc><table><row><cell>0.08</cell><cell>0.22</cell><cell>0.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>AP performance (%) on selected object classes and mAP (%) comparison with other weakly supervised methods on HICO-DET.</figDesc><table><row><cell>Methods</cell><cell cols="8">apple bicycle bottle chair cellphone frisbee kite surfboard train umbrella mAP(%)</cell></row><row><cell>R*CNN [17]</cell><cell>1.13 3.26</cell><cell>1.57 2.35</cell><cell>1.47</cell><cell>1.02 0.32</cell><cell>2.70</cell><cell>2.86</cell><cell>3.04</cell><cell>2.15</cell></row><row><cell>WSDDN [3]</cell><cell>1.46 5.19</cell><cell>1.52 3.87</cell><cell>2.02</cell><cell>2.44 1.15</cell><cell>2.86</cell><cell>6.76</cell><cell>3.35</cell><cell>3.27</cell></row><row><cell>PCL [41]</cell><cell>1.27 5.82</cell><cell>2.31 2.84</cell><cell>3.06</cell><cell>3.11 1.16</cell><cell>2.60</cell><cell>7.93</cell><cell>3.47</cell><cell>3.62</cell></row><row><cell>PCL + prior</cell><cell>2.06 6.49</cell><cell>2.54 3.69</cell><cell>5.14</cell><cell>2.96 1.37</cell><cell>4.06</cell><cell>8.13</cell><cell>4.87</cell><cell>4.19</cell></row><row><cell cols="2">Ours-vgg-16 (w/o prior) 1.23 5.15</cell><cell>1.19 3.47</cell><cell>3.82</cell><cell>2.24 0.73</cell><cell>3.65</cell><cell>6.22</cell><cell>3.14</cell><cell>3.16</cell></row><row><cell>Ours-vgg-16</cell><cell>2.47 8.64</cell><cell>3.59 5.74</cell><cell>7.36</cell><cell>2.85 0.87</cell><cell>7.29</cell><cell>8.47</cell><cell>6.63</cell><cell>5.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: mAP (%) comparison with other weakly supervised meth-</cell></row><row><cell>ods on EPIC KITCHENS</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">mAP CorLoc</cell></row><row><cell>R*CNN [17]</cell><cell>2.54</cell><cell>32.68</cell></row><row><cell>PCL [41]</cell><cell>4.68</cell><cell>40.64</cell></row><row><cell>PCL + prior</cell><cell>6.82</cell><cell>46.69</cell></row><row><cell>Ours-vgg-16</cell><cell>9.75</cell><cell>52.53</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/zhenheny/ Activity-Driven-Weakly-Supervised-Object-Detection</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02738</idno>
		<title level="m">Joint discovery of object states and manipulation actions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<editor>Peng Tang Xinggang Wang Xiang Bai and Wenyu Liu</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multifold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and part-based representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2010-21st British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Mohammad</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="849" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07333</idno>
		<title level="m">Piotr Dollár, and Kaiming He. Detecting and recognizing human-object interactions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r* cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08421</idno>
		<title level="m">A video dataset of spatio-temporally localized atomic visual actions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint learning of object and action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2017-IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and tracking in video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3173" to="3181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2017-International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection via objectspecific pixel gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using things and stuff transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis.(ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis.(ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1024</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Weak supervision for detecting object classes from activities. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilash</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="138" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pcl: Proposal cluster learning for weakly supervised object detection. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1297" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ts2c: tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatiotemporal action detection with cascade proposal and location anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Temporal dynamic graph lstm for action-driven video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit</forename><forename type="middle">Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09466</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">W2f: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Weaklysupervised video object grounding from text by loss weighting and object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02834</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis.(ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis.(ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
