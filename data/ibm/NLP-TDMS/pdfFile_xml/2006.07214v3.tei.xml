<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse and Continuous Attention Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">António</forename><surname>Farinhas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Treviso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Aguiar</surname></persName>
							<email>aguiar@isr.ist.utl.pt</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mário</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
							<email>mario.figueiredo@tecnico.ulisboa.pt</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Sistemas e Robótica</orgName>
								<orgName type="institution" key="instit1">Instituto de Telecomunicações</orgName>
								<orgName type="institution" key="instit2">Instituto Superior Técnico</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="laboratory">LUMLIS (Lisbon ELLIS Unit)</orgName>
								<orgName type="institution">Instituto Superior Técnico</orgName>
								<address>
									<settlement>Lisbon, Lisbon</settlement>
									<country>Portugal, Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>The Netherlands Unbabel</addrLine>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse and Continuous Attention Mechanisms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and αentmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend α-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for α ∈ {1, 2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Exponential families are ubiquitous in statistics and machine learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. They enjoy many useful properties, such as the existence of conjugate priors (crucial in Bayesian inference) and the classical Pitman-Koopman-Darmois theorem <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, which states that, among families with fixed support (independent of the parameters), exponential families are the only having sufficient statistics of fixed dimension for any number of i.i.d. samples.</p><p>Departing from exponential families, there has been recent work on discrete, finite-domain distributions with varying and sparse support, via the sparsemax and the entmax transformations <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Those approaches drop the link to exponential families of categorical distributions provided by the softmax transformation, which always yields dense probability mass functions. In contrast, sparsemax and entmax can lead to sparse distributions, whose support is not constant throughout the family. This property has been used to design sparse attention mechanisms with improved interpretability <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>However, sparsemax and entmax are so far limited to discrete domains. Can a similar approach be extended to continuous domains? This paper provides that extension and pinpoints a connection with "deformed exponential families" <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> and Tsallis statistics <ref type="bibr" target="#b12">[13]</ref>, leading to α-sparse families ( §2).  We use this construction to obtain new density families with varying support, including the truncated parabola and paraboloid distributions (2-sparse counterpart of the Gaussian, §2.4 and <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>Softmax and its variants are widely used in attention mechanisms, an important component of neural networks <ref type="bibr" target="#b13">[14]</ref>. Attention-based neural networks can "attend" to finite sets of objects and identify relevant features. We use our extension above to devise new continuous attention mechanisms ( §3), which can attend to continuous data streams and to domains that are inherently continuous, such as images. Unlike traditional attention mechanisms, ours are suitable for selecting compact regions, such as 1D-segments or 2D-ellipses. We show that the Jacobian of these transformations are generalized covariances, and we use this fact to obtain efficient backpropagation algorithms ( §3.2).</p><p>As a proof of concept, we apply our models with continuous attention to text classification, machine translation, and visual question answering tasks, with encouraging results ( §4).</p><p>Notation. Let (S, A, ν) be a measure space, where S is a set, A is a σ-algebra, and ν is a measure. We denote by M 1 + (S) the set of ν-absolutely continuous probability measures. From the Radon-Nikodym theorem <ref type="bibr">[15, §31]</ref>, each element of M 1 + (S) is identified (up to equivalence within measure zero) with a probability density function p : S → R + , with S p(t) dν(t) = 1. For convenience, we often drop dν(t) from the integral. We denote the measure of A ∈ A as |A| = ν(A) = A 1, and the support of a density p ∈ M 1 + (S) as supp(p) = {t ∈ S | p(t) &gt; 0}. Given φ : S → R m , we write expectations as E p [φ(t)] := S p(t) φ(t). Finally, we define [a] + := max{a, 0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sparse Families</head><p>In this section, we provide background on exponential families and its generalization through Tsallis statistics. We link these concepts, studied in statistical physics, to sparse alternatives to softmax recently proposed in the machine learning literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, extending the latter to continuous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Regularized prediction maps (Ω-RPM)</head><p>Our starting point is the notion of Ω-regularized prediction maps, introduced by Blondel et al. <ref type="bibr" target="#b6">[7]</ref> for finite domains S. This is a general framework for mapping vectors in R |S| (e.g., label scores computed by a neural network) into probability vectors in |S| (the simplex), with a regularizer Ω encouraging uniform distributions. Particular choices of Ω recover argmax, softmax <ref type="bibr" target="#b15">[16]</ref>, and sparsemax <ref type="bibr" target="#b5">[6]</ref>. Our definition below extends this framework to arbitrary measure spaces M 1 + (S), where we assume Ω : M 1 + (S) → R is a lower semi-continuous, proper, and strictly convex function.</p><formula xml:id="formula_0">Definition 1.</formula><p>The Ω-regularized prediction map (Ω-RPM)p Ω : F → M 1 + (S) is defined aŝ p Ω [f ] = arg max</p><formula xml:id="formula_1">p∈M 1 + (S) E p [f (t)] − Ω(p),<label>(1)</label></formula><p>where F is the set of functions for which the maximizer above exists and is unique.</p><p>It is often convenient to consider a "temperature parameter" τ &gt; 0, absorbed into Ω via Ω := τΩ. If f has a unique global maximizer t , the low-temperature limit yields lim τ →0p τΩ [f ] = δ t , a Dirac delta distribution at the maximizer of f . For finite S, this is the argmax transformation shown in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Other interesting examples of regularization functionals are shown in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shannon's negentropy and exponential families</head><p>A natural choice of regularizer is the Shannon's negentropy, Ω(p) = S p(t) log p(t). In this case, if we interpret −f (t) as an energy function, the Ω-RPM corresponds to the well-known free energy variational principle, leading to Boltzmann-Gibbs distributions ( <ref type="bibr" target="#b16">[17]</ref>; see App. A):</p><p>p</p><formula xml:id="formula_2">Ω [f ](t) = exp(f (t)) S exp(f (t ))dν(t ) = exp f (t) − A(f ) ,<label>(2)</label></formula><p>where A(f ) := log S exp(f (t)) is the log-partition function. If S is finite and ν is the counting measure, the integral in <ref type="formula" target="#formula_2">(2)</ref> is a summation and we can write f as a vector [f 1 , . . . , f |S| ] ∈ R |S| . In this case, the Ω-RPM is the softmax transformation,</p><formula xml:id="formula_3">p Ω [f ] = softmax(f ) = exp(f ) |S| k=1 exp(f k ) ∈ |S| .<label>(3)</label></formula><formula xml:id="formula_4">If S = R N , ν is the Lebesgue measure, and f (t) = − 1 /2(t − µ) Σ −1 (t − µ) for µ ∈ R N and Σ 0 (i.e., Σ is a positive definite matrix), we obtain a multivariate Gaussian,p Ω [f ](t) = N (t; µ, Σ). This becomes a univariate Gaussian N (t; µ, σ 2 ) if N = 1. For S = R and defining f (t) = −|t − µ|/b, with µ ∈ R and b &gt; 0, we get a Laplace density,p Ω [f ](t) = 1 2b exp (−|t − µ|/b). Exponential families. Let f θ (t) = θ φ(t), where φ(t) ∈ R M is a vector of statistics and θ ∈ Θ ⊆ R M is a vector of canonical parameters. A family of the form (2) parametrized by θ ∈ Θ ⊆ R M</formula><p>is called an exponential family <ref type="bibr" target="#b1">[2]</ref>. Exponential families have many appealing properties, such as the existence of conjugate priors and sufficient statistics, and a dually flat geometric structure <ref type="bibr" target="#b17">[18]</ref>. Many well-known distributions are exponential families, including the categorical and Gaussian distributions above, and Laplace distributions with a fixed µ. A key property of exponential families is that the support is constant within the same family and dictated by the base measure ν: this follows immediately from the positiveness of the exp function in <ref type="bibr" target="#b1">(2)</ref>. We abandon this property in the sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tsallis' entropies and α-sparse families</head><p>Motivated by applications in statistical physics, Tsallis <ref type="bibr" target="#b12">[13]</ref> proposed a generalization of Shannon's negentropy. This generalization is rooted on the notions of β-logarithm, log β : R ≥0 → R (not to be confused with base-β logarithm), and β-exponential, exp β : R → R:</p><formula xml:id="formula_5">log β (u) := u 1−β −1 1−β , β = 1 log u, β = 1; exp β (u) := [1 + (1 − β)u] 1/(1−β) + , β = 1 exp u, β = 1.<label>(4)</label></formula><p>Note that lim β→1 log β (u) = log u, lim β→1 exp β (u) = exp u, and log β (exp β (u)) = u for any β.</p><p>Another important concept is that of "β-escort distribution" <ref type="bibr" target="#b12">[13]</ref>: this is the distributionp β given bỹ</p><formula xml:id="formula_6">p β (t) := p(t) β p β β , where p β β = S p(t ) β dν(t ).<label>(5)</label></formula><p>Note that we havep 1 (t) = p(t).</p><p>The α-Tsallis negentropy <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> is defined as: 1</p><formula xml:id="formula_7">Ω α (p) := 1 α E p [log 2−α (p(t))] = 1 α(α−1) S p(t) α − 1 , α = 1, S p(t) log p(t), α = 1.<label>(6)</label></formula><p>Note that lim α→1 Ω α (p) = Ω 1 (p), for any p ∈ M 1 + (S), with Ω 1 (p) recovering Shannon's negentropy (proof in App. B). Another notable case is Ω 2 (p) = 1 /2 S p(t) 2 − 1 /2, the negative of which is called the Gini-Simpson index <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. We come back to the α = 2 case in §2.4.</p><p>For α &gt; 0, Ω α is strictly convex, hence it can be plugged in as the regularizer in Def. 1. The next proposition ( <ref type="bibr" target="#b9">[10]</ref>; proof in App. B) provides an expression for Ω α -RPM using the β-exponential (4):</p><formula xml:id="formula_8">Proposition 1. For α &gt; 0 and f ∈ F, p Ωα [f ](t) = exp 2−α (f (t) − A α (f )),<label>(7)</label></formula><p>where A α : F → R is a normalizing function:</p><formula xml:id="formula_9">A α (f ) = 1 1−α + S p θ (t) 2−α f (t) S p θ (t) 2−α − 1 1−α .</formula><p>Let us contrast <ref type="bibr" target="#b6">(7)</ref> with Boltzmann-Gibbs distributions (2), recovered with α = 1. One key thing to note is that the (2 − α)-exponential, for α &gt; 1, can return zero values. Therefore, the distribution p Ωα [f ] in <ref type="formula" target="#formula_8">(7)</ref>  Relation to sparsemax and entmax. Blondel et al. <ref type="bibr" target="#b6">[7]</ref> showed that, for finite S,</p><formula xml:id="formula_10">Ω 2 -RPM is the sparsemax transformation,p Ω [f ] = sparsemax(f ) = arg min p∈ |S| p − f 2 .</formula><p>Other values of α were studied by Peters et al. <ref type="bibr" target="#b7">[8]</ref>, under the name α-entmax transformation. For α &gt; 1, these transformations have a propensity for returning sparse distributions, where several entries have zero probability. Proposition 1 shows that similar properties can be obtained when S is continuous.</p><p>Deformed exponential families. With a linear parametrization f θ (t) = θ φ(t), distributions with the form <ref type="formula" target="#formula_8">(7)</ref> are called deformed exponential or q-exponential families <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b23">24]</ref>. The geometry of these families induced by the Tsallis q-entropy was studied by Amari <ref type="bibr">[18, §4.3]</ref>. <ref type="bibr" target="#b2">3</ref> Unlike those prior works, we are interested in the sparse, light tail scenario (α &gt; 1), not in heavy tails. For α &gt; 1, we call these α-sparse families. When α → 1, α-sparse families become exponential families and they cease to be "sparse", in the sense that all distributions in the same family have the same support.</p><p>A relevant problem is that of characterizing A α (θ). When α = 1,</p><formula xml:id="formula_11">A 1 (θ) = lim α→1 A α (θ) = log S exp(θ φ(t))</formula><p>is the log-partition function (see <ref type="bibr" target="#b1">(2)</ref>), and its first and higher order derivatives are equal to the moments of the sufficient statistics. The following proposition (stated as Amari and Ohara <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">Theorem 5]</ref>, and proved in our App. D) characterizes A α (θ) for α = 1 in terms of an expectation under the β-escort distribution for β = 2 − α (see <ref type="bibr" target="#b4">(5)</ref>). This proposition will be used later to derive the Jacobian of entmax attention mechanisms.</p><p>Proposition 2. A α (θ) is a convex function and its gradient is given by</p><formula xml:id="formula_12">∇ θ A α (θ) = Ep2−α θ [φ(t)] = S p θ (t) 2−α φ(t) S p θ (t) 2−α .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The 2-Tsallis entropy: sparsemax</head><p>In this paper, we focus on the case α = 2. For finite S, this corresponds to the sparsemax transfomation proposed by Martins and Astudillo <ref type="bibr" target="#b5">[6]</ref>, which has appealing theoretical and computational properties. In the general case, plugging α = 2 in (7) leads to the Ω 2 -RPM,</p><formula xml:id="formula_13">p Ω2 [f ](t) = [f (t) − λ] + , where λ = A 2 (f ) − 1,<label>(9)</label></formula><p>i.e.,p Ω2 [f ] is obtained from f by subtracting a constant (which may be negative) and truncating, where that constant λ must be such that S [f (t) − λ] + = 1.</p><p>If S is continuous and ν the Lebesgue measure, we call Ω 2 -RPM the continuous sparsemax transformation. Examples follow, some of which correspond to novel distributions.</p><p>Truncated parabola. If f (t) = − (t−µ) 2 2σ 2 , we obtain the continuous sparsemax counterpart of a Gaussian, which we dub a "truncated parabola":</p><formula xml:id="formula_14">p Ω2 [f ](t) = − (t−µ) 2 2σ 2 − λ + =: TP(t; µ, σ 2 ),<label>(10)</label></formula><p>where λ = − 1 2 3/(2σ) 2/3 (see App. E.1). This function, depicted in <ref type="figure" target="#fig_0">Fig. 1</ref> (top left), is widely used in density estimation. For µ = 0 and σ = 2/3, it is known as the Epanechnikov kernel <ref type="bibr" target="#b25">[26]</ref>.</p><p>Truncated paraboloid. The previous example can be generalized to</p><formula xml:id="formula_15">S = R N , with f (t) = − 1 2 (t−µ) Σ −1 (t−µ)</formula><p>, where Σ 0, leading to a "multivariate truncated paraboloid," the sparsemax counterpart of the multivariate Gaussian (see middle and rightmost plots in <ref type="figure" target="#fig_0">Fig. 1)</ref>:</p><formula xml:id="formula_16">p Ω2 [f ](t) = −λ− 1 2 (t−µ)Σ −1 (t−µ) + , where λ = − Γ N 2 + 2 / det(2πΣ) 2 2+N . (11)</formula><p>The expression above, derived in App. E.2, reduces to <ref type="bibr" target="#b9">(10)</ref> for N = 1. Notice that (unlike in the Gaussian case) a diagonal Σ does not lead to a product of independent truncated parabolas.</p><p>Triangular. Setting f (t) = −|t − µ|/b, with b &gt; 0, yields the triangular distribution Location-scale families. More generally, let f µ,σ (t) := − 1 σ g (|t − µ|/σ) for a location µ ∈ R and a scale σ &gt; 0, where g : R + → R is convex and continuously differentiable. Then, we havê</p><formula xml:id="formula_17">p Ω2 [f ](t) = −λ − |t−µ| b + =: Tri(t; µ, b),<label>(12)</label></formula><formula xml:id="formula_18">where λ = −1/ √ b (see App. E.3).</formula><formula xml:id="formula_19">p Ω2 [f ](t) = −λ − 1 σ g (|t − µ|/σ) + ,<label>(13)</label></formula><p>where λ = −g (a)/σ and a is the solution of the equation ag (a) − g(a) + g(0) = 1 2 (a sufficient condition for such solution to exist is g being strongly convex; see App. E.4 for a proof). This example subsumes the truncated parabola (g(t) = t 3 /6) and the triangular distribution (g(t) = t 2 /2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Continuous Attention</head><p>Attention mechanisms have become a key component of neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. They dynamically detect and extract relevant input features (such as words in a text or regions of an image). So far, attention has only been applied to discrete domains; we generalize it to continuous spaces.</p><p>Discrete attention. Assume an input object split in L = |S| pieces, e.g., a document with L words or an image with L regions. A vanilla attention mechanism works as follows: each piece has a D-dimensional representation (e.g., coming from an RNN or a CNN), yielding a matrix V ∈ R D×L . These representations are compared against a query vector (e.g., using an additive model <ref type="bibr" target="#b13">[14]</ref>), leading to a score vector f = [f 1 , . . . , f L ] ∈ R L . Intuitively, the relevant pieces that need attention should be assigned high scores. Then, a transformation ρ : R L → L (e.g., softmax or sparsemax) is applied to the score vector to produce a probability vector p = ρ(f ). We may see this as an Ω-RPM.</p><p>The probability vector p is then used to compute a weighted average of the input representations, via c = V p ∈ R D . This context vector c is finally used to produce the network's decision.</p><p>To learn via the backpropagation algorithm, the Jacobian of the transformation ρ, J ρ ∈ R L×L , is needed. Martins and Astudillo <ref type="bibr" target="#b5">[6]</ref> gave expressions for softmax and sparsemax,</p><formula xml:id="formula_20">J softmax (f ) = Diag(p) − pp , J sparsemax (f ) = Diag(s) − ss /(1 s),<label>(14)</label></formula><p>where p = softmax(f ), and s is a binary vector whose th entry is 1 iff ∈ supp(sparsemax(f )).</p><p>Algorithm 1: Continuous softmax attention with S = R D , Ω = Ω 1 , and Gaussian RBFs.</p><formula xml:id="formula_21">Parameters: Gaussian RBFs ψ(t) = [N (t; µj, Σj)] N j=1 , basis functions φ(t) = [t, vec(tt )], value function VB(t) = Bψ(t) with B ∈ R D×N , score function f θ (t) = θ φ(t) with θ ∈ R M Function Forward(θ := [Σ −1 µ, − 1 2 Σ −1 ]): rj ← Ep Ω [f θ ] [ψj(t)] = N (µ, µj, Σ + Σj), ∀j ∈ [N ] // Eqs. 15, 46 return c ← Br (context vector) Function Backward( ∂L ∂c , θ := [Σ −1 µ, − 1 2 Σ −1 ]): for j ← 1 to N dõ s ← N (µ, µj, Σ + Σj),Σ ← (Σ −1 + Σ −1 j ) −1 ,μ ←Σ(Σ −1 µ + Σ −1 j µj) ∂r j ∂θ ← covp Ω [f θ ] (φ(t), ψj(t)) = [s(μ − µ);s(Σ +μμ − Σ − µµ )] // Eqs. 18, 47-48 return ∂L ∂θ ← ∂r ∂θ B ∂L ∂c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The continuous case: score and value functions</head><p>Our extension of Ω-RPMs to arbitrary domains (Def. 1) opens the door for constructing continuous attention mechanisms. The idea is simple: instead of splitting the input object into a finite set of pieces, we assume an underlying continuous domain: e.g., text may be represented as a function V : S → R D that maps points in the real line (S ⊆ R, continuous time) onto a D-dimensional vector representation, representing the "semantics" of the text evolving over time; images may be regarded as a smooth function in 2D (S ⊆ R 2 ), instead of being split into regions in a grid.</p><p>Instead of scores [f 1 , . . . , f L ], we now have a score function f : S → R, which we map to a probability density p ∈ M 1 + (S). This density is used in tandem with the value mapping V : </p><formula xml:id="formula_22">S → R D to obtain a context vector c = E p [V (t)] ∈ R D . Since M 1 + (S) may be infinite dimensional,</formula><formula xml:id="formula_23">ρ(θ) = E p [ψ(t)],<label>(15)</label></formula><formula xml:id="formula_24">with p =p Ω [f θ ] and f θ (t) = θ φ(t).</formula><p>If Ω = Ω α , we call this entmax attention, denoted as ρ α . The values α = 1 and α = 2 lead to softmax and sparsemax attention, respectively.</p><p>Note that, if S = {1, ..., L} and φ(k) = ψ(k) = e k (Euclidean canonical basis), we recover the discrete attention of Bahdanau et al. <ref type="bibr" target="#b13">[14]</ref>. Still in the finite case, if φ(k) and ψ(k) are key and value vectors and θ is a query vector, this recovers the key-value attention of Vaswani et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>On the other hand, for S = R D and φ(t) = [t, vec(tt )], we obtain new attention mechanisms (assessed experimentally for the 1D and 2D cases in §4): for α = 1, the underlying density p is Gaussian, and for α = 2, it is a truncated paraboloid (see §2.4). In both cases, we show (App. G) that the expectation (15) is tractable (1D) or simple to approximate numerically (2D) if ψ are Gaussian RBFs, and we use this fact in §4 (see Alg. 1 for pseudo-code for the case α = 1).</p><p>Defining the value function V B (t). In many problems, the input is a discrete sequence of observations (e.g., text) or it was discretized (e.g., images), at locations {t } L =1 . To turn it into a continuous signal, we need to smooth and interpolate these observations. If we start with a discrete encoder representing the input as a matrix H ∈ R D×L , one way of obtaining a value mapping V B : S → R D is by "approximating" H with multivariate ridge regression. With V B (t) = Bψ(t), and packing the basis vectors ψ(t ) as columns of matrix F ∈ R N ×L , we obtain:</p><formula xml:id="formula_25">B = arg min B BF − H 2 F + λ B 2 F = HF (F F + λI N ) −1 = HG,<label>(16)</label></formula><p>where . F is the Frobenius norm, and the L × N matrix G = F (F F + λI N ) −1 depends only on the values of the basis functions at discrete time steps and can be obtained off-line for different input lenghts L. The result is an expression for V B with N D coefficients, cheaper than H if N L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient backpropagation with continuous attention</head><p>The next proposition, based on Proposition 2 and proved in App. F, allows backpropagating over continuous entmax attention mechanisms. We define, for β ≥ 0, a generalized β-covariance,</p><formula xml:id="formula_26">cov p,β [φ(t), ψ(t)] = p β β × Ep β φ(t)ψ(t) − Ep β [φ(t)] Ep β [ψ(t)] ,<label>(17)</label></formula><p>wherep β is the β-escort distribution in <ref type="bibr" target="#b4">(5)</ref>. For β = 1, we have the usual covariance; for β = 0, we get a covariance taken w.r.t. a uniform density on the support of p, scaled by |supp(p)|.</p><formula xml:id="formula_27">Proposition 3. Let p =p Ωα [f θ ] with f θ (t) = θ φ(t).</formula><p>The Jacobian of the α-entmax is:</p><formula xml:id="formula_28">J ρα (θ) = ∂ρ α (θ) ∂θ = cov p,2−α (φ(t), ψ(t)).<label>(18)</label></formula><p>Note that in the finite case, <ref type="bibr" target="#b17">(18)</ref> reduces to the expressions in <ref type="bibr" target="#b13">(14)</ref> for softmax and sparsemax.</p><p>Example: Gaussian RBFs. As before, let S = R D , φ(t) = [t, vec(tt )], and ψ j (t) = N (t; µ j , Σ j ). For α = 1, we obtain closed-form expressions for the expectation (15) and the Jacobian <ref type="formula" target="#formula_1">(18)</ref>, for any D ∈ N:p Ω [f θ ] is a Gaussian, the expectation <ref type="formula" target="#formula_1">(15)</ref> is the integral of a product of Gaussians, and the covariance <ref type="formula" target="#formula_1">(18)</ref> involves first-and second-order Gaussian moments. Pseudocode for the case α = 1 is shown as Alg. 1. For α = 2,p Ω [f θ ] is a truncated paraboloid. In the 1D case, both <ref type="formula" target="#formula_1">(15)</ref> and <ref type="formula" target="#formula_1">(18)</ref> can be expressed in closed form in terms of the erf function. In the 2D case, we can reduce the problem to 1D integration using the change of variable formula and working with polar coordinates. See App. G for details.</p><p>We use the facts above in the experimental section ( §4), where we experiment with continuous variants of softmax and sparsemax attentions in natural language processing and vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>As proof of concept, we test our continuous attention mechanisms on three tasks: document classification, machine translation, and visual question answering (more experimental details in App. H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document classification.</head><p>Although textual data is fundamentally discrete, modeling long documents as a continuous signal may be advantageous, due to smoothness and independence of length.</p><p>To test this hypothesis, we use the IMDB movie review dataset <ref type="bibr" target="#b28">[29]</ref>, whose inputs are documents (280 words on average) and outputs are sentiment labels (positive/negative). Our baseline is a biLSTM with discrete attention. For our continuous attention models, we normalize the document length L into the unit interval [0, 1], and use f (t) = − (t − µ) 2 /2σ 2 as the score function, leading to a 1D Gaussian (α = 1) or truncated parabola (α = 2) as the attention density. We compare three attention variants: discrete attention with softmax <ref type="bibr" target="#b13">[14]</ref> and sparsemax <ref type="bibr" target="#b5">[6]</ref>; continuous attention, where a CNN and max-pooling yield a document representation v from which we compute µ = sigmoid(w 1 v) and σ 2 = softplus(w 2 v); and combined attention, which obtains p ∈ L from discrete attention,</p><formula xml:id="formula_29">computes µ = E p [ /L] and σ 2 = E p [( /L) 2 ] − µ 2</formula><p>, applies the continuous attention, and sums the two context vectors (this model has the same number of parameters as the discrete attention baseline). <ref type="table" target="#tab_2">Table 1</ref> shows accuracies for different numbers N of Gaussian RBFs. The accuracies of the individual models are similar, suggesting that continuous attention is as effective as its discrete counterpart, despite having fewer basis functions than words, i.e., N L. Among the continuous variants, the sparsemax outperforms the softmax, except for N = 64. We also see that a large N is not necessary to obtain good results, which is encouraging for tasks with long sequences. Finally, combining discrete and continuous sparsemax produced the best results, without increasing the number of parameters.</p><p>Machine translation. We use the De→En IWSLT 2017 dataset <ref type="bibr" target="#b29">[30]</ref>, and a biLSTM model with discrete softmax attention as a baseline. For the continuous attention models, we use the combined   <ref type="figure" target="#fig_2">Figure 2</ref>: Attention maps in machine translation: discrete (left), continuous softmax (middle), and continuous sparsemax (right), for a sentence in the De-En IWSLT17 validation set. In the rightmost plot, the selected words are the ones with positive density. In the test set, these models attained BLEU scores of 23.92 (discrete), 24.00 (continuous softmax), and 24.25 (continuous sparsemax). attention setting described above, with 30 Gaussian RBFs andμ linearly spaced in [0, 1] andσ ∈ {.03, .1, .3}. The results (caption of <ref type="figure" target="#fig_2">Fig. 2)</ref> show a slight benefit in the combined attention over discrete attention only, without any additional parameters. <ref type="figure" target="#fig_2">Fig. 2</ref> shows heatmaps for the different attention mechanisms on a De→En sentence. The continuous mechanism tends to have attention means close to the diagonal, adjusting the variances based on alignment confidence or when a larger context is needed (e.g., a peaked density for the target word "sea", and a flat one for "of").</p><p>Visual QA. Finally, we report experiments with 2D continuous attention on visual question answering, using the VQA-v2 dataset <ref type="bibr" target="#b30">[31]</ref> and a modular co-attention network as a baseline <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr" target="#b3">4</ref> The discrete attention model attends over a 14×14 grid. <ref type="bibr" target="#b4">5</ref> For continuous attention, we normalize the image size into the unit square [0, 1] 2 . We fit a 2D Gaussian (α = 1) or truncated paraboloid (α = 2) as the attention density; both correspond to f (t) = − 1 2 (t − µ) Σ −1 (t − µ), with Σ 0. We use the mean and variance according to the discrete attention probabilities and obtain µ and Σ with moment matching. We use N = 100 14 2 Gaussian RBFs, withμ linearly spaced in [0, 1] 2 and Σ = 0.001 · I. Overall, the number of neural network parameters is the same as in discrete attention.</p><p>The results in <ref type="table" target="#tab_3">Table 2</ref> show similar accuracies for all attention models, with a slight advantage for continuous softmax. <ref type="figure" target="#fig_5">Figure 3</ref> shows an example (see App. H for more examples and some failure cases): in the baseline model, the discrete attention is too scattered, possibly mistaking the lamp with a TV screen. The continuous attention models focus on the right region and answer the question correctly, with continuous sparsemax enclosing all the relevant information in its supporting ellipse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Relation to the Tsallis maxent principle. Our paper unifies two lines of work: deformed exponential families from statistical physics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>, and sparse alternatives to softmax recently proposed in the machine learning literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>, herein extended to continuous domains. This link  Continuity in other architectures and dimensions. In our paper, we consider attention networks exhibiting temporal/spatial continuity in the input data, be it text (1D) or images (2D). Recent work propose continuous-domain CNNs for 3D structures like point clouds and molecules <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. The dynamics of continuous-time RNNs have been studied in <ref type="bibr" target="#b35">[36]</ref>, and similar ideas have been applied to irregularly sampled time series <ref type="bibr" target="#b36">[37]</ref>. Other recently proposed frameworks produce continuous variants in other dimensions, such as network depth <ref type="bibr" target="#b37">[38]</ref>, or in the target domain for machine translation tasks <ref type="bibr" target="#b38">[39]</ref>. Our continuous attention networks can be used in tandem with these frameworks.</p><p>Gaussian attention probabilities. Cordonnier et al. <ref type="bibr" target="#b39">[40]</ref> analyze the relationship between (discrete) attention and convolutional layers, and consider spherical Gaussian attention probabilities as relative positional encodings. By contrast, our approach removes the need for positional encodings: by converting the input to a function on a predefined continuous space, positions are encoded implicitly, not requiring explicit positional encoding. Gaussian attention has also been hard-coded as input-agnostic self-attention layers in transformers for machine translation tasks by You et al. <ref type="bibr" target="#b40">[41]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We proposed extensions to regularized prediction maps, originally defined on finite domains, to arbitrary measure spaces ( §2). With Tsallis α-entropies for α &gt; 1, we obtain sparse families, whose members can have zero tails, such as triangular or truncated parabola distributions. We then used these distributions to construct continuous attention mechanisms ( §3). We derived their Jacobians in terms of generalized covariances (Proposition 3), allowing for efficient forward and backward propagation. Experiments for 1D and 2D cases were shown on attention-based text classification, machine translation, and visual question answering ( §4), with encouraging results.</p><p>There are many avenues for future work. As a first step, we considered unimodal distributions only (Gaussian, truncated paraboloid), for which we show that the forward and backpropagation steps have closed form or can be reduced to 1D integration. However, there are applications in which multiple attention modes are desirable. This can be done by considering mixtures of distributions, multiple attention heads, or sequential attention steps. Another direction concerns combining our continuous attention models with other spatial/temporal continuous architectures for CNNs and RNNs <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> or with continuity in other dimensions, such as depth <ref type="bibr" target="#b37">[38]</ref> or output space <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We discuss the broader impact of our work, including ethical aspects and future societal consequences. Given the early stage of our work and its predominantly theoretical nature, the discussion is mostly speculative.</p><p>The continuous attention models developed in our work can be used in a very wide range of applications, including natural language processing, computer vision, and others. For many of these applications, current state-of-the-art models use discrete softmax attention, whose interpretation capabilities have been questioned in prior work <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>. Our models can potentially lead to more interpretable decisions, since they lead to less scattered attention maps (as shown in our <ref type="figure" target="#fig_2">Figures 2-3</ref>) and are able to select contiguous text segments or image regions. As such, they may provide better inductive bias for interpretation.</p><p>In addition, our attention densities using Gaussian and truncated paraboloids include a variance term, being potentially useful as a measure of confidence-for example, a large ellipse in an image may indicate that the model had little confidence about where it should attend to answer a question, while a small ellipse may denote high confidence on a particular object.</p><p>We also see opportunities for research connecting our work with other continuous models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> leading to end-to-end continuous models which, by avoiding discretization, have the potential to be less susceptible to adversarial attacks via input perturbations. Outside the machine learning field, the links drawn in §2 between sparse alternatives to softmax and models used in non-extensive (Tsallis) statistical physics suggest a potential benefit in that field too.</p><p>Note, however, that our work is a first step into all these directions, and as such further investigation will be needed to better understand the potential benefits. We strongly recommend carrying out user studies before deploying any such system, to better understand the benefits and risks. Some of the examples in App. H may help understand potential failure modes.</p><p>We should also take into account that, for any computer vision model, there are important societal risks related to privacy-violating surveillance applications. Continuous attention holds the promise to scale to larger and multi-resolution images, which may, in the longer term, be deployed in such undesirable domains. Ethical concerns hold for natural language applications such as machine translation, where biases present in data can be arbitrarily augmented or hidden by machine learning systems. For example, our natural language processing experiments mostly use English datasets (as a target language in machine translation, and in document classification). Further work is needed to understand if our findings generalize to other languages. Likewise, in the vision experiments, the VQA-v2 dataset uses COCO images, which have documented biases <ref type="bibr" target="#b45">[46]</ref>. In line with the fundamental scope and early stage of this line of research, we deliberately choose applications on standard benchmark datasets, in an attempt to put as much distance as possible from malevolent applications. Finally, although we chose the most widely used evaluation metrics for each task (accuracy for document classification and visual question answering, BLEU for machine translation), these metrics do not always capture performance quality-for example, BLEU in machine translation is far from being a perfect metric.</p><p>The data, memory, and computation requirements for training systems with continuous attention do not seem considerably higher than the ones which use discrete attention. On the other hand, for NLP applications, our approach has the potential to better compress sequential data, by using fewer basis functions than the sequence length (as suggested by our document classification experiments). While there is nothing specific about our research that poses environmental concerns or that promises to alleviate such concerns, our models share the same problematic property as other neural network models in terms of their energy consumption to train models and tune hyperparameters <ref type="bibr" target="#b46">[47]</ref>.</p><p>contract UIDB/50008/2020. We would like to thank Pedro Martins, Zita Marinho, and the anonymous reviewers for their helpful feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material A Differential Negentropy and Boltzmann-Gibbs distributions</head><p>We adapt a proof from Cover and Thomas <ref type="bibr" target="#b16">[17]</ref>. Let Ω be the Shannon negentropy, which is proper, lower semi-continuous, and strictly convex <ref type="bibr">[48, example 9.41]</ref>, and let</p><formula xml:id="formula_30">KL(p q) := S p(t) log p(t) q(t)</formula><p>be the Kullback-Leibler divergence between distributions p and q (which is always non-negative and equals 0 iff p = q). Take q(t) = exp(f (t))</p><p>S exp(f (t ))dν(t ) = exp(f (t) − A(f )) as in <ref type="formula" target="#formula_2">(2)</ref>, where A(f ) is the log-partition function.</p><p>We have, for any p ∈ M 1 + (S):</p><formula xml:id="formula_31">0 ≤ KL(p q) = S p(t) log p(t) q(t) = Ω(p) − S p(t) log q(t) = Ω(p) − S p(t)(f (t) − A(f )) = Ω(p) − E p [f (t)] + A(f ).<label>(19)</label></formula><p>Therefore, we have, for any p ∈ M 1 + (S), that</p><formula xml:id="formula_32">E p [f (t)] − Ω(p) ≤ A(f ),<label>(20)</label></formula><p>with equality if and only if p = q. Since the right hand side is constant with respect to p, we have that the posited q must be the maximizer of (1). = g(1) h(1) = 0 0 , so we are in an indeterminate case. We take the derivatives of g and h:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Tsallis Negentropy and Sparse Distributions</head><formula xml:id="formula_33">g (β) = exp(log u 1−β ) = exp(log u 1−β ) · ((1 − β) log u) = −u 1−β log u,<label>(21)</label></formula><p>and h (β) = −1. From l'Hôpital's rule,</p><formula xml:id="formula_34">lim β→1 g(β) h(β) = lim β→1 g (β) h (β) = log u.<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 1</head><p>The proof of Proposition 1 is similar to the one in §A, replacing the KL divergence by the Bregman divergence induced by Ω α , and using an additional bound. Let</p><formula xml:id="formula_35">B Ωα (p, q) := Ω α (p) − Ω α (q) − ∇Ω α (q), p − q</formula><p>be the (functional) Bregman divergence between distributions p and q induced by Ω α , and let</p><formula xml:id="formula_36">q(t) = exp 2−α (f (t) − A α (f )) = [1 + (α − 1)(f (t) − A α (f ))] 1 α−1 + .</formula><p>Note that, from <ref type="formula" target="#formula_7">(6)</ref>,</p><formula xml:id="formula_37">(∇ q Ω α (q)) (t) = q(t) α−1 α − 1 .</formula><p>From the non-negativity of the Bregman divergence <ref type="bibr" target="#b48">[49]</ref>, we have, for any p ∈ M 1</p><formula xml:id="formula_38">+ (S): 0 ≤ (a) B Ωα (p, q) = Ω α (p) − Ω α (q) − ∇Ω α (q), p − q = Ω α (p) − Ω α (q) − S q(t) α−1 α − 1 (p(t) − q(t)) = Ω α (p) − Ω α (q) − E p [[f (t) − A α (f ) + (α − 1) −1 ] + ] ≥Ep[f (t)−Aα(f )+(α−1) −1 ] + 1 α − 1 S q(t) α ≤ (b) Ω α (p) − Ω α (q) − E p [f (t) − A α (f ) + (α − 1) −1 ] + 1 α − 1 S q(t) α = Ω α (p) − E p [f (t)] − Ω α (q) + 1 α − 1 S q(t) α − 1 =αΩα(q) +A α (f ) = Ω α (p) − E p [f (t)] + (α − 1)Ω α (q) + A α (f ).<label>(23)</label></formula><p>Therefore, we have, for any p ∈ M 1</p><formula xml:id="formula_39">+ (S), E p [f (t)] − Ω α (p) ≤ (α − 1)Ω α (q) + A α (f ),<label>(24)</label></formula><p>with equality iff p = q, which leads to zero Bregman divergence (i.e., a tight inequality (a)) and to</p><formula xml:id="formula_40">E p [[f (t) − A α (f ) + (α − 1) −1 ] + ] = E p [f (t) − A α (f ) + (α − 1) −1 ] (i.e.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a tight inequality (b)).</head><p>We can use the equality above to obtain an expression for the Fenchel conjugate Ω *</p><formula xml:id="formula_41">α (f ) = E q [f (t)]</formula><p>− Ω α (q) (i.e., the value of the maximum in (1) and the right hand side in <ref type="formula" target="#formula_2">(24))</ref>:</p><formula xml:id="formula_42">Ω * α (f ) = (α − 1)Ω α (q) + A α (f ).<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Normalizing function A α (f )</head><p>Let p =p Ωα [f ]. The expression for A α in Prop. 1 is obtained by inverting <ref type="formula" target="#formula_8">(7)</ref>, yielding A α (f ) = f (t) − log 2−α (p(t)), and integrating with respect to p(t) 2−α dν(t), leading to:</p><formula xml:id="formula_43">S p θ (t) 2−α A α (f ) = S p(t) 2−α f (t) − S p(t) 2−α log 2−α (p(t)) = S p θ (t) 2−α f (t) − S (p(t) − p(t) 2−α ) α − 1 = S p(t) 2−α f (t) − 1 α − 1 + S p(t) 2−α α − 1 ,<label>(26)</label></formula><p>from which the desired expression follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Relation to the Tsallis Maxent Principle</head><p>Note first that the Ω-RPM in our Def. 1 is a generalization of the free energy variational principle, if we see −f θ (t) = −θ φ(t) as an energy function and Ω the entropy scaled by a temperature. Let Ω = Ω α be the Tsallis α-entropy. An equivalent constrained version of this problem is the maximum entropy (maxent) principle <ref type="bibr" target="#b50">[51]</ref>: max</p><formula xml:id="formula_44">p∈M 1 + (S) −Ω α (p), s.t. E p [φ(t)] = b.<label>(27)</label></formula><p>The solution of this problem corresponds to a distribution in the (2 − α)-exponential family <ref type="formula" target="#formula_8">(7)</ref>:</p><formula xml:id="formula_45">p (t) = exp 2−α (θ φ(t) − A α (θ)),<label>(28)</label></formula><p>for some Lagrange multiplier θ.</p><p>However, this construction differs from the one by Tsallis <ref type="bibr" target="#b12">[13]</ref> and others, who use escort distributions (Eq. 5) in the expectation constraints. Namely, instead of (27), they consider the problem: max</p><formula xml:id="formula_46">p∈M 1 + (S) −Ω α (p), s.t. Epα [φ(t)] = b.<label>(29)</label></formula><p>The solution of (29) is of the form</p><formula xml:id="formula_47">p (t) = B α (θ) exp α (θ (φ(t) − b)),<label>(30)</label></formula><p>where θ is again a Lagrange multiplier. This is derived, for example, in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">Eq. 15]</ref>. There are two main differences between <ref type="formula" target="#formula_2">(28)</ref> and <ref type="formula" target="#formula_3">(30)</ref>:</p><p>• While <ref type="formula" target="#formula_2">(28)</ref> involves the (2 − α)-exponential, (30) involves the α-exponential.</p><p>• In <ref type="formula" target="#formula_2">(28)</ref>, the normalizing term A α (θ) is inside the (2 − α)-exponential. In <ref type="bibr" target="#b29">(30)</ref>, there is an normalizing factor B α (θ) outside the α-exponential.</p><p>Naturally, when α = 1, these two problems become equivalent, since an additive term inside the exponential is equivalent to a multiplicative term outside. However, this does not happen with β-exponentials (exp β (u + v) = exp β (u) exp β (v) in general, for β = 1), and therefore these two alternative paths lead to two different definitions of q-exponential families. Unfortunately, both have been considered in the physics literature, under the same name, and this has been subject of debate. Quoting Naudts <ref type="bibr">[10, §1]</ref>:</p><p>"An important question is then whether in the modification the normalization should stand in front of the deformed exponential function, or whether it should be included as ln Z(β) inside. From the general formalism mentioned above it follows that the latter is the right way to go."</p><p>Throughout our paper, we use the definition of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>, equivalent to the maxent problem <ref type="bibr" target="#b26">(27)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Proposition 2</head><p>We adapt the proof from Amari and Ohara <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">Theorem 5]</ref>. Note first that, for t ∈ supp(p θ ),</p><formula xml:id="formula_48">∇ θ p θ (t) = ∇ θ [(α − 1)(θ φ(t) − A α (θ)) + 1] 1/(α−1) = [(α − 1)(θ φ(t) − A α (θ)) + 1] (2−α)/(α−1) (φ(t) − ∇ θ A α (θ)) = p θ (t) 2−α (φ(t) − ∇ θ A α (θ)),<label>(31)</label></formula><p>and</p><formula xml:id="formula_49">∇ 2 θ p θ (t) = ∇ θ p 2−α θ (t)(φ(t) − ∇ θ A α (θ)) − p 2−α θ (t)∇ 2 θ A α (θ) = (2 − α)p 1−α θ (t)∇ θ p θ (t)(φ(t) − ∇ θ A α (θ)) − p 2−α θ (t)∇ 2 θ A α (θ) = (2 − α)p θ (t) 3−2α φ(t) − ∇ θ A α (θ) φ(t) − ∇ θ A α (θ) −p θ (t) 2−α ∇ 2 θ A α (θ).<label>(32)</label></formula><p>Therefore we have:</p><formula xml:id="formula_50">0 = ∇ θ S p θ (t) =1 = S ∇ θ p θ (t) = S p θ (t) 2−α (φ(t) − ∇ θ A α (θ)),<label>(33)</label></formula><p>from which we obtain</p><formula xml:id="formula_51">∇ θ A α (θ) = S p θ (t) 2−α φ(t) S p θ (t) 2−α .<label>(34)</label></formula><p>To prove that A α (θ) is convex, we will show that its Hessian is positive semidefinite. Note that</p><formula xml:id="formula_52">0 = ∇ 2 θ S p θ (t) =1 = S ∇ 2 θ p θ (t) = S (2 − α)p θ (t) 3−2α φ(t) − ∇ θ A α (θ) φ(t) − ∇ θ A α (θ) − p θ (t) 2−α ∇ 2 θ A α (θ) = (2 − α) S p θ (t) 3−2α φ(t) − ∇ θ A α (θ) φ(t) − ∇ θ A α (θ) −∇ 2 θ A α (θ) S p θ (t) 2−α ,<label>(35)</label></formula><p>hence, for α ≤ 2,</p><formula xml:id="formula_53">∇ 2 θ A α (θ) = (2 − α) S p θ (t) 3−2α 0 φ(t) − ∇ θ A α (θ) φ(t) − ∇ θ A α (θ) S p θ (t) 2−α 0,<label>(36)</label></formula><p>where we used the fact that p θ (t) ≥ 0 for t ∈ S and that integrals of positive semidefinite functions and positive semidefinite. as in <ref type="bibr" target="#b9">(10)</ref>. Let us determine the constant λ that ensures this distribution normalizes to 1. Note that λ does not depend on the location parameter µ, hence we can assume µ = 0 without loss of generality. We must have λ = − a 2 2σ 2 and 1 = a −a −λ − x 2 2σ 2 = −2λa− a 3 3σ 2 = 2a 3 3σ 2 , hence a = 3 2 σ 2 1/3 , which finally gives:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Normalization Constants for Continuous Sparsemax Distributions</head><formula xml:id="formula_54">λ = − 1 2 3 2σ 2/3 .<label>(37)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Multivariate truncated paraboloid</head><p>Let <ref type="bibr" target="#b10">(11)</ref>. Let us determine the constant λ that ensures this distribution normalizes to 1, where we assume again µ = 0 without loss of generality. To obtain λ, we start by invoking the formula for computing the volume of an ellipsoid defined by the equation</p><formula xml:id="formula_55">p(t) = −λ − 1 2 (t − µ)Σ −1 (t − µ) + as in</formula><formula xml:id="formula_56">x Σ −1 x ≤ 1: V ell (Σ) = π n/2 Γ(n/2 + 1) det(Σ) 1/2 ,<label>(38)</label></formula><p>where Γ(t) is the Gamma function. Since each slice of a paraboloid is an ellipsoid, we can apply Cavalieri's principle to obtain the volume of a paraboloid y = 1 2 x Σ −1 x of height h = −λ as follows: </p><formula xml:id="formula_57">V par (h) =</formula><p>Equating the volume to 1, we obtain λ = −h as:</p><formula xml:id="formula_59">λ = − Γ( n 2 + 2) (2π) n det(Σ) 2 2+n</formula><p>.</p><p>(40)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Triangular</head><p>Let p(t) = −λ − |t−µ| b + as in <ref type="bibr" target="#b11">(12)</ref>. Let us determine the constant λ that ensures this distribution normalizes to 1. Assuming again µ = 0 without loss of generality, we must have λ = − a b and</p><formula xml:id="formula_60">1 = a −a −λ − |x| b = −2λa − a 2 b = a 2 b , hence a = √ b, which finally gives λ = −b −1/2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Location-scale families</head><p>We first show that a is the solution of the equation ag (a) − g(a) + g(0) = 1 2 . From symmetry around µ, we must have</p><formula xml:id="formula_61">1 2 = µ+aσ µ 1 σ g (a) − 1 σ g t − µ σ dt = a 0 (g (a) − g (s)) ds = ag (a) − g(a) + g(0),<label>(41)</label></formula><p>where we made a variable substitution s = (t − µ)/σ, which proves the desired result. Now we show that a solution always exists if g is strongly convex, i.e., if there is some γ &gt; 0 such that g(0) ≥ g(s) − sg (s) + γ 2 s 2 for any s ≥ 0. Let F (s) := sg (s) − g(s) + g(0). We want to show that the equation F (a) = 1 2 has a solution. Since g is continuously differentiable, F is continuous. From the strong convexity of g, we have that F (s) ≥ γ 2 s 2 for any s ≥ 0, which implies that lim s→+∞ F (s) = +∞. Therefore, since F (0) = 0, we have by the intermediate value theorem that there must be some a such that F (a) = 1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proof of Proposition 3</head><p>We have</p><formula xml:id="formula_62">∇ θ E p [ψ i (t)] = ∇ θ S p θ (t)ψ i (t) = S ∇ θ p θ (t)ψ i (t) = S p 2−α θ (t)∇ θ log 2−α (p θ (t))ψ i (t) = S p 2−α θ (t)∇ θ (θ φ(t) − A α (θ))ψ i (t) = S p 2−α θ (t)(φ(t) − ∇ θ A α (θ))ψ i (t).<label>(42)</label></formula><p>Using the expression for ∇ θ A α (θ) from Proposition 2 yields the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Continuous Attention with Gaussian RBFs</head><p>We derive expressions for the evaluation and gradient computation of continuous attention mechanisms where ψ(t) are Gaussian radial basis functions, both for the softmax (α = 1) and sparsemax (α = 2) cases. For softmax, we show closed-form expressions for any number of dimensions (including the 1D and 2D cases). For sparsemax, we derive closed-form expressions for the 1D case, and we reduce the 2D case to a univariate integral on an interval, easy to compute numerically.</p><p>This makes it possible to plug both continuous attention mechanisms in neural networks and learn them end-to-end with the gradient backpropagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Continuous softmax (α = 1)</head><p>We derive expressions for continuous softmax for multivariate Gaussians in R D . This includes the 1D and 2D cases, where D ∈ {1, 2}.</p><formula xml:id="formula_63">If S = R D , for φ(t) = [t, tt ], the distribution p =p Ω1 [f θ ], with f θ (t) = θ φ(t)</formula><p>, is a multivariate Gaussian where the mean µ and the covariance matrix Σ are related to the canonical parameters as θ = [Σ −1 µ, − 1 2 Σ −1 ]. We derive closed form expressions for the attention mechanism output ρ 1 (θ) = E p [ψ(t)] in <ref type="bibr" target="#b14">(15)</ref> and for its Jacobian J ρ1 (θ) = cov p,1 (φ(t), ψ(t)) in <ref type="bibr" target="#b17">(18)</ref>, when ψ(t) are Gaussian RBFs, i.e., each ψ j is of the form ψ j (t) = N (t; µ j , Σ j ).</p><p>Forward pass. Each coordinate of the attention mechanism output becomes the integral of a product of Gaussians,</p><formula xml:id="formula_64">E p [ψ j (t)] = R D N (t; µ, Σ)N (t; µ j , Σ j ).<label>(43)</label></formula><p>We use the fact that the product of two Gaussians is a scaled Gaussian:</p><formula xml:id="formula_65">N (t; µ, Σ)N (t; µ j , Σ j ) =sN (t;μ,Σ),<label>(44)</label></formula><formula xml:id="formula_66">wheres = N (µ; µ j , Σ + Σ j ),Σ = (Σ −1 + Σ −1 j ) −1 ,μ =Σ(Σ −1 µ + Σ −1 j µ j ).<label>(45)</label></formula><p>Therefore, the forward pass can be computed as:</p><formula xml:id="formula_67">E p [ψ j (t)] =s R D N (t;μ,Σ) =s = N (µ; µ j , Σ + Σ j ).<label>(46)</label></formula><p>Backward pass. To compute the backward pass, we have that each row of the Jacobian J ρ1 (θ) becomes a first or second moment under the resulting Gaussian:</p><formula xml:id="formula_68">cov p,1 (t, ψ j (t)) = E p [tψ j (t)] − E p [t]E p [ψ j (t)] = R D tN (t; µ, Σ)N (t; µ j , Σ j ) −sµ =s R D tN (t;μ,Σ) −sµ =s(μ − µ),<label>(47)</label></formula><p>and, noting that</p><formula xml:id="formula_69">Σ = E[(t − µ)(t − µ) ] = E[tt ] − µµ , cov p,1 (tt , ψ j (t)) = E p [tt ψ j (t)] − E p [tt ]E p [ψ j (t)] = R D tt N (t; µ, Σ)N (t; µ j , Σ j ) −s(Σ + µµ ) =s R D tt N (t;μ,Σ) −s(Σ + µµ ) =s(Σ +μμ ) −s(Σ + µµ ) =s(Σ +μμ − Σ − µµ ).<label>(48)</label></formula><p>G.2 Continuous sparsemax in 1D (α = 2, D = 1)</p><formula xml:id="formula_70">With φ(t) = [t, t 2 ], the distribution p =p Ω2 [f θ ], with f θ (t) = θ φ(t)</formula><p>, becomes a truncated parabola where µ and σ 2 are related to the canonical parameters as above, i.e., θ = [ µ σ 2 , − 1 2σ 2 ]. We derive closed form expressions for the attention mechanism output ρ 2 (θ) = E p [ψ(t)] in <ref type="bibr" target="#b14">(15)</ref> and its Jacobian J ρ2 (θ) = ∂ρ2(θ) ∂θ = cov p,2 (φ(t), ψ(t)) in (18) when ψ(t) and Gaussian RBFs, i.e., each ψ j is of the form ψ j (t) = N (t; µ j , σ 2 j ).</p><p>Forward pass. Each coordinate of the attention mechanism output becomes:</p><formula xml:id="formula_71">E p [ψ j (t)] = µ+a µ−a −λ − (t − µ) 2 2σ 2 N (t; µ j , σ 2 j ) = µ−µ j +a σ j µ−µ j −a σ j 1 σ j −λ − (σ j t + µ j − µ) 2 2σ 2 N (s; 0, 1)ds,<label>(49)</label></formula><p>where a = ( 3 2 σ 2 ) 1/3 and λ = − a 2 2σ 2 = − 1 2 ( 3 2σ ) 2/3 , as stated in <ref type="bibr" target="#b36">(37)</ref>, and we made the substitution s = t−µj σj . We use the fact that, for any</p><formula xml:id="formula_72">u, v ∈ R such that u ≤ v: v u N (t; 0, 1) = 1 2 erf v √ 2 − erf u √ 2 , v u tN (t; 0, 1) = −N (v; 0, 1) + N (u; 0, 1), v u t 2 N (t; 0, 1) = 1 2 erf v √ 2 − erf u √ 2 − vN (v; 0, 1) + uN (u; 0, 1),<label>(50)</label></formula><p>from which the expectation (49) can be computed directly.</p><p>Backward pass. Since |supp(p)| = 2a, we have from <ref type="formula" target="#formula_1">(17)</ref> and <ref type="formula" target="#formula_6">(50)</ref> that each row of the Jacobian J ρ2 (θ) becomes: </p><formula xml:id="formula_73">cov p,2 (t, ψ j (t)) = µ+a µ−a tN (t; µ j , σ 2 j ) − 1 2a µ+a µ−a t µ+a µ−a N (t; µ j , σ 2 j ) = µ−µ j +a σ j µ−µ j −a σ j (µ j + σ j s)N (s; 0, 1) − 1 2a (µ + a) 2 2 − (µ − a) 2 2 =µ   µ−µ j +a σ j µ−µ j −</formula><formula xml:id="formula_74">= µ j − µ 2 erf µ − µ j + a √ 2σ j − erf µ − µ j − a √ 2σ j −σ j N µ − µ j + a σ j ; 0, 1 − N µ − µ j − a σ j ; 0, 1 ,<label>(51)</label></formula><p>and </p><formula xml:id="formula_75">cov p,2 (t 2 , ψ j (t)) = µ+a µ−a t 2 N (t; µ j , σ 2 j ) − 1 2a µ+a µ−a t 2 µ+a µ−a N (t; µ j , σ 2 j ) = µ−µ j +a σ j µ−µ j −a σ j (µ j + σ j s) 2 N (s; 0, 1) − 1 2a (µ + a) 3 3 − (µ − a) 3 3 = a 2 3 +µ 2   µ−µ j +a σ j µ−µ j −a σ j N (s; 0, 1)   = µ 2 j − µ 2 −</formula><formula xml:id="formula_76">= µ 2 j − µ 2 + σ 2 j − a 2 3 erf µ − µ j + a √ 2σ j − erf µ − µ j − a √ 2σ j −σ j (µ + µ j + a)N µ − µ j + a σ j ; 0, 1 + σ j (µ + µ j − a)N µ − µ j − a σ j ; 0, 1 .<label>(52)</label></formula><p>G.3 Continuous sparsemax in 2D (α = 2, D = 2)</p><p>Let us now consider the case where D = 2. For φ(t) = [t, tt ], the distribution p =p Ω2 [f θ ], with f θ (t) = θ φ(t), becomes a bivariate truncated paraboloid where µ and Σ are related to the canonical parameters as before, θ = [Σ −1 µ, − 1 2 Σ −1 ]. We obtain expressions for the attention mechanism output ρ 2 (θ) = E p [ψ(t)] and its Jacobian J ρ2 (θ) = cov p,2 (φ(t), ψ(t)) that include 1D integrals (simple to integrate numerically), when ψ(t) are Gaussian RBFs, i.e., when each ψ j is of the form ψ j (t) = N (t; µ j , Σ j ).</p><p>We start with the following lemma: <ref type="figure">N (t, µ, Σ)</ref> be a D-dimensional multivariate Gaussian, Let A ∈ R D×R be a full column rank matrix (with R ≤ D), and b ∈ R D . Then we have N (Au + b; µ, Σ) =sN (u;μ,Σ) with:</p><formula xml:id="formula_77">Lemma 1. Let</formula><formula xml:id="formula_78">Σ = (A Σ −1 A) −1 µ =ΣA Σ −1 (µ − b) s = (2π) R−D 2 |Σ| 1/2 |Σ| 1/2 exp − 1 2 (µ − b) P (µ − b) , P = Σ −1 − Σ −1 AΣA Σ −1 .</formula><p>If R = D, then A is invertible and the expressions above can be simplified to:</p><formula xml:id="formula_79">Σ = A −1 ΣA − μ = A −1 (µ − b) s = |A| −1 .</formula><p>Proof. The result can be derived by writing <ref type="figure">− µ)</ref>) and splitting the exponential of the sum as a product of exponentials.</p><formula xml:id="formula_80">N (Au + b; µ, Σ) = (2π) −R/2 |Σ| −1/2 exp(− 1 2 (Au + b − µ) Σ −1 (Au + b</formula><p>Forward pass. For the forward pass, we need to compute</p><formula xml:id="formula_81">E p [ψ j (t)] = R 2 −λ − 1 2 (t − µ) Σ −1 (t − µ) + N (t; µ j , Σ j )dt,<label>(53)</label></formula><p>with</p><formula xml:id="formula_82">N (t; µ j , Σ j ) = 1 2π |Σ j | 1 2 exp − 1 2 (t − µ j ) Σ −1 j (t − µ j ) ,<label>(54)</label></formula><p>and (from <ref type="formula" target="#formula_1">(11)</ref>)</p><formula xml:id="formula_83">λ = − 1 π det(Σ) 1 2 .<label>(55)</label></formula><p>Using Lemma 1 and the change of variable formula (which makes the determinants cancel), we can reparametrize u = (−2λ) − 1 2 Σ − 1 2 (t − µ) and write this as an integral over the unit circle:</p><formula xml:id="formula_84">E p [ψ j (t)] = u ≤1 −λ(1 − u 2 )N (u;μ,Σ)du,<label>(56)</label></formula><formula xml:id="formula_85">withμ = (−2λ) − 1 2 Σ − 1 2 (µ j − µ),Σ = (−2λ) −1 Σ − 1 2 Σ j Σ − 1 2</formula><p>. We now do a change to polar coordinates, u = (r cos θ, r sin θ) = ar, where a = [cos θ, sin θ] ∈ R 2×1 . The integral becomes: </p><formula xml:id="formula_86">E p [ψ j (t)] =</formula><p>where in the second line we applied again Lemma 1, resulting in</p><formula xml:id="formula_88">σ 2 (θ) ≡ σ 2 = (a Σ −1 a) −1 r 0 (θ) ≡ r 0 = σ 2 a Σ −1μ s(θ) ≡s = 1 √ 2π σ |Σ| 1/2 exp − 1 2μ Pμ , P =Σ −1 − σ 2Σ−1 aa Σ −1 .</formula><p>Applying Fubini's theorem, we fix θ and integrate with respect to r. We use the formulas (50) and the fact that, for any u, v ∈ R such that u ≤ v: v u t 3 N (t; 0, 1) = −N (v; 0, 1)(2 + v 2 ) + N (u; 0, 1)(2 + u 2 ).</p><p>We obtain a closed from expression for the inner integral:</p><formula xml:id="formula_90">F (θ) = 1 0 r(1 − r 2 )N (r; r 0 , σ 2 ) dr = (2σ 3 + r 2 0 σ + r 0 σ)N 1 − r 0 σ ; 0, 1 − (2σ 3 + r 2 0 σ − σ)N − r 0 σ ; 0, 1 − r 3 0 + (3σ 2 − 1)r 0 2 erf 1 − r 0 √ 2σ − erf − r 0 √ 2σ .<label>(59)</label></formula><p>The desired integral can then be expressed in a single dimension as</p><formula xml:id="formula_91">E p [ψ j (t)] = −λ 2π 0s (θ)F (θ),<label>(60)</label></formula><p>in polar coordinates, </p><p>which can be then expressed in a single dimension as</p><formula xml:id="formula_93">E tN (t; µ j , Σ j ) = 2π 0s (θ)G(θ)dθ,<label>(68)</label></formula><p>with</p><formula xml:id="formula_94">G(θ) = 1 0 r (−2λ) 1 2 Σ 1 2 ar + µ N (r; r 0 , σ 2 ) dr = 1−r 0 σ − r 0 σ (sσ + r 0 ) (−2λ) 1 2 Σ 1 2 a(sσ + r 0 ) + µ N (r; r 0 , σ 2 ) ds = (−2λ) 1 2 Σ 1 2 aσ(r 0 ) + µσ N − r 0 σ ; 0, 1 − (−2λ) 1 2 Σ 1 2 aσ(1 + r 0 ) + µσ N 1 − r 0 σ ; 0, 1 + 1 2 (−2λ) 1 2 Σ 1 2 a(σ 2 + r 2 0 ) + µr 0 erf 1 − r 0 √ 2σ − erf − r 0 √ 2σ .<label>(69)</label></formula><p>We do the same for</p><formula xml:id="formula_95">E N (t; µ j , Σ j ) = u ≤1 N (u;μ,Σ)du = 2π 0 1 0 rs N (r; r 0 , σ 2 )dr dθ,<label>(70)</label></formula><p>which can then be expressed in a single dimension as</p><formula xml:id="formula_96">E N (t; µ j , Σ j ) = 2π 0s (θ)H(θ)dθ,<label>(71)</label></formula><p>with</p><formula xml:id="formula_97">H(θ) = 1 0 rN (r; r 0 , σ 2 ) dr = 1−r 0 σ − r 0 σ (sσ + r 0 )N (r; r 0 , σ 2 ) ds = σ N − r 0 σ ; 0, 1 − N 1 − r 0 σ ; 0, 1 + r 0 2 erf 1 − r 0 √ 2σ − erf − r 0 √ 2σ .</formula><p>Finally, to solve (62) we simplify the integral</p><formula xml:id="formula_98">E tt N (t; µ j , Σ j ) = u ≤1 (−2λ) 1 2 Σ 1 2 u + µ (−2λ) 1 2 Σ 1 2 u + µ N (u;μ,Σ)du = 2π 0 1 0 r(r 2 A + rB + C)s N (r; r 0 , σ 2 )dr dθ (72) with A = (−2λ)Σ 1 2 aa (Σ 1 2 ) (73) B = (−2λ) 1 2 Σ 1 2 aµ + µa (Σ 1 2 ) (74) C = µµ .<label>(75)</label></formula><p>The integral can then be expressed in a single dimension as </p><formula xml:id="formula_99">E tt N (t; µ j , Σ j ) = 2π 0s (θ)M (θ)dθ,<label>(76)</label></formula><formula xml:id="formula_100">= 2 + − r 0 σ 2 Ã − r 0 σB +C N − r 0 σ ; 0, 1 − 2 + 1 − r 0 σ 2 Ã + 1 − r 0 σB +C N 1 − r 0 σ ; 0, 1 + 1 2 B +D erf 1 − r 0 √ 2σ − erf − r 0 √ 2σ (77) whereÃ = σ 3 A (78) B = σ 2 (3r 0 A + B) (79) C = σ(3r 2 0 A + 2r 0 B + C) (80) D = r 3 0 A + r 2 0 B + r 0 C.<label>(81)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Experimental Details and Model Hyperparameters H.1 Document classification</head><p>We used the IMDB movie review dataset <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b5">6</ref> which consist of user-written text reviews with binary labels (positive/negative). Following <ref type="bibr" target="#b42">[43]</ref>, we used 25K training documents, 10% of which for validation, and 25K for testing. The training and test sets are perfectly balanced: 12.5K negative and 12.5K positive examples. The documents have 280 words on average.</p><p>Our architecture is the same as <ref type="bibr" target="#b28">[29]</ref>, a BiLSTM with attention. We used pretrained GloVe embeddings from the 840B release, 7 kept frozen. We tuned three hyperparameters using the discrete softmax attention baseline: learning rate within {0.003, 0.001, 0.0001}; 2 within {0.01, 0.001, 0.0001, 0}; number of epochs within {5, 10, 20}. We picked the best configuration by doing a grid search and by taking into consideration the accuracy on the validation set (selected values in bold). <ref type="table" target="#tab_4">Table 3</ref> shows the hyperparameters and model configurations used for all document classification experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Machine translation</head><p>We used the De→En dataset from the IWSLT 2017 evaluation campaign <ref type="bibr" target="#b29">[30]</ref>, with the standard splits (206K, 9K, and 2K sentence pairs for train/dev/test). <ref type="bibr" target="#b7">8</ref> We used BPE <ref type="bibr" target="#b51">[52]</ref> with 32K merges to reduce the vocabulary size. Our implementation is based on Joey-NMT <ref type="bibr" target="#b52">[53]</ref> and we used the provided configuration script for the baseline, a BiLSTM model with discrete softmax attention 9 with the hyperpameters in <ref type="table" target="#tab_5">Table 4</ref>.  (MCAN). Our architecture is the same as <ref type="bibr" target="#b31">[32]</ref> except that we represent the image input with grid features generated by a ResNet <ref type="bibr" target="#b53">[54]</ref> pretrained on ImageNet <ref type="bibr" target="#b54">[55]</ref>, instead of bounding-box features <ref type="bibr" target="#b56">[56]</ref>. The images are resized to 448 × 448 before going through the ResNet that outputs a feature map of size 14 × 14 × 2048. To represent the input question words we use 300-dimensional GloVe word embeddings <ref type="bibr" target="#b57">[57]</ref>, yielding a question feature matrix representation. <ref type="table" target="#tab_6">Table 5</ref> shows the hyperparameters used for all the VQA experiments presented.</p><p>All the models we experimented with use the same features and were trained only on the train set without data augmentation.</p><p>Examples. <ref type="figure">Figure 4</ref> illustrates the difficulties that continuous attention models may face when trying to focus on objects that are too far from each other or that seem to have different relative importance to answer the question. Intuitively, in VQA, this becomes a problem when counting objects in those conditions. On the other side, in counting questions that require the understanding of a contiguous region of the image only, continuous attention may perform better (see <ref type="figure">Figure 5</ref>). <ref type="figure">Figures 6 and 7</ref> show other examples where continuous attention focus on the right region of the image and answers the question correctly. For these cases, discrete attention is more diffuse than its How many men are seen in this picture? </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>1D and 2D distributions generated by the Ω α -RPM for α ∈ {1, 2}. Left: Univariate location-scale families, including Gaussian and truncated parabola (top) and Laplace and triangular (bottom). Middle and right: Bivariate Gaussian N (t; 0, I) and truncated paraboloid TP(t; 0, I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 (</head><label>1</label><figDesc>bottom left) depicts this distribution alongside Laplace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 -</head><label>2</label><figDesc>sparse families. Truncated parabola and paraboloid distributions form a 2-sparse family, with statistics φ(t) = [t, vec(tt )] and canonical parameters θ = [Σ −1 µ, vec(− 1 2 Σ −1 )]. Gaussian distributions form an exponential family with the same sufficient statistics and canonical parameters. In 1D, truncated parabola and Gaussians are both particular cases of the so-called "q-Gaussian" [10, §4.1], for q = 2 − α. Triangular distributions with a fixed location µ and varying scale b also form a 2-sparse family (similarly to Laplace distributions with fixed location being exponential families).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>we need to parametrize f , p, and V to be able to compute in a finite-dimensional parametric space.Building attention mechanisms. We represent f and V using basis functions, φ : S → R M and ψ : S → R N , defining f θ (t) = θ φ(t) and V B (t) = Bψ(t), where θ ∈ R M and B ∈ R D×N . The score function f θ is mapped into a probability density p :=p Ω [f θ ], from which we compute the context vector as c = E p [V B (t)] = Br, with r = E p [ψ(t)]. Summing up yields the following: Definition 2. Let S, Ω, φ, ψ be a tuple with Ω : M 1 + (S) → R, φ : S → R M , and ψ : S → R N . An attention mechanism is a mapping ρ : Θ ⊆ R M → R N , defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Attention maps for an example in VQA-v2: original image, discrete attention, continuous softmax, and continuous sparsemax. The latter encloses all probability mass within the outer ellipse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Finally, in their DRAW architecture for image generation, Gregor et al. [42, §3.1] propose a selective attention component which is parametrized by a spherical Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>B. 1</head><label>1</label><figDesc>Shannon as a limit case of Tsallis when α → 1 We show that lim α→1 Ω α (p) = Ω 1 (p) for any p ∈ M 1 + (S). From (6), it suffices to show that lim β→1 log β (u) = log(u) for any u ≥ 0. Let g(β) := u 1−β − 1, and h(β) := 1 − β. Observe that lim β→1 log β (u) = lim β→1 g(β) h(β)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E. 1</head><label>1</label><figDesc>Truncated parabola Let p(t) = −λ − (t−µ) 2 2σ 2 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>n/2 det(Σ) 1/2 ( n 2 + 1)Γ( n 2 + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>1 0</head><label>1</label><figDesc>1 − r 2 )N (ar;μ,Σ)r dr dθ = 2π 0 −λr(1 − r 2 )sN (r; r 0 , σ 2 ) dr dθ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>1 2</head><label>1</label><figDesc>ar + µ s N (r; r 0 , σ 2 )dr dθ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>with M (θ) = 1 0(r 3 A</head><label>13</label><figDesc>+ r 2 B + rC) N (r; r 0 , σ 2 + s 2B + sC +D)N (s; 0, 1) ds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 :Figure 5 :1EFigure 6 :Figure 7 :</head><label>4567</label><figDesc>Attention maps for an example in VQA-v2: original image, discrete attention, continuous softmax, and continuous sparsemax. continuous counterpart: in both examples, it attends to two different regions in the image, leading to incorrect answers.How many birds in the water? Attention maps for an example in VQA-v2: original image, discrete attention, continuous softmax, and continuous sparsemax.Is the man wearing a hat?yes Attention maps for an example in VQA-v2: original image, discrete attention, continuous softmax, and continuous sparsemax. Attention maps for an example in VQA-v2: original image, discrete attention, continuous softmax, and continuous sparsemax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on IMDB in terms of accuracy (%). For the continuous attentions, we used N ∈ {32, 64, 128} Gaussian RBFs N (t,μ,σ 2 ), withμ linearly spaced in [0, 1] andσ ∈ {.1, .5}.</figDesc><table><row><cell></cell><cell></cell><cell>ATTENTION</cell><cell>N = 32</cell><cell>N = 64</cell><cell>N = 128</cell></row><row><cell cols="2">ATTENTIONL ≈ 280</cell><cell>Continuous softmax</cell><cell>90.20</cell><cell>90.68</cell><cell>90.52</cell></row><row><cell>Discrete softmax</cell><cell>90.78</cell><cell>Continuous sparsemax</cell><cell>90.52</cell><cell>89.63</cell><cell>90.90</cell></row><row><cell>Discrete sparsemax</cell><cell>90.58</cell><cell>Disc. + Cont. softmax</cell><cell>90.98</cell><cell>90.69</cell><cell>89.62</cell></row><row><cell></cell><cell></cell><cell>Disc. + Cont. sparsemax</cell><cell>91.10</cell><cell>91.18</cell><cell>90.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracies of different models on the test-dev and test-standard splits of VQA-v2.</figDesc><table><row><cell>ATTENTION</cell><cell></cell><cell cols="2">Test-Dev</cell><cell></cell><cell cols="2">Test-Standard</cell></row><row><cell></cell><cell cols="6">Yes/No Number Other Overall Yes/No Number Other Overall</cell></row><row><cell>Discrete softmax</cell><cell>83.40</cell><cell>43.59</cell><cell>55.91 65.83</cell><cell>83.47</cell><cell>42.99</cell><cell>56.33 66.13</cell></row><row><cell>2D continuous softmax</cell><cell>83.40</cell><cell>44.80</cell><cell>55.88 65.96</cell><cell>83.79</cell><cell>44.33</cell><cell>56.04 66.27</cell></row><row><cell cols="2">2D continuous sparsemax 83.10</cell><cell>44.12</cell><cell>55.95 65.79</cell><cell>83.38</cell><cell>43.91</cell><cell>56.14 66.10</cell></row></table><note>may be fruitful for future research in both fields. While most prior work is focused on heavy-tailed distributions (α &lt; 1), we focus instead on light-tailed, sparse distributions, the other side of the spectrum (α &gt; 1). See App. C for the relation to the Tsallis maxent principle.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Hyperparmeters for document classification.</figDesc><table><row><cell>HYPERPARAMETER</cell><cell>VALUE</cell></row><row><cell>Batch size</cell><cell>16</cell></row><row><cell>Word embeddings size</cell><cell>300</cell></row><row><cell>BiLSTM hidden size</cell><cell>128</cell></row><row><cell>Merge BiLSTM states</cell><cell>Concat</cell></row><row><cell>Attention scorer</cell><cell>[14]</cell></row><row><cell>Conv filters</cell><cell>128</cell></row><row><cell>Conv kernel size</cell><cell>3</cell></row><row><cell cols="2">Early stopping patience 5</cell></row><row><cell>Number of epochs</cell><cell>10</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>2 regularization</cell><cell>0.0001</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyperparmeters for neural machine translation.</figDesc><table><row><cell>HYPERPARAMETER</cell><cell>VALUE</cell></row><row><cell>Batch size</cell><cell>80</cell></row><row><cell>Word embeddings size</cell><cell>620</cell></row><row><cell>BiLSTM hidden size</cell><cell>1000</cell></row><row><cell>Attention scorer</cell><cell>[14]</cell></row><row><cell>Early stopping patience</cell><cell>8</cell></row><row><cell>Number of epochs</cell><cell>100</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>2 regularization</cell><cell>0</cell></row><row><cell>Dropout</cell><cell>0.0</cell></row><row><cell>Hidden dropout</cell><cell>0.2</cell></row><row><cell>Learning rate</cell><cell>0.0002</cell></row><row><cell>Scheduling</cell><cell>Plateau</cell></row><row><cell>Decrease factor</cell><cell>0.7</cell></row><row><cell>Lower case</cell><cell>True</cell></row><row><cell>Normalization</cell><cell>Tokens</cell></row><row><cell cols="2">Maximum output length 80</cell></row><row><cell>Beam size</cell><cell>5</cell></row><row><cell>RNN type</cell><cell>GRU</cell></row><row><cell>RNN layers</cell><cell>1</cell></row><row><cell>Input feeding</cell><cell>True</cell></row><row><cell>Init. hidden</cell><cell>Bridge</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Hyperparmeters for VQA. at epoch t starting from 1 min(2.5t · 10 −5 , 1 · 10 −4 ) Learning rate decay ratio at epoch t ∈ {10, 12} 0.2 Number of epochs 13</figDesc><table><row><cell>HYPERPARAMETER</cell><cell>VALUE</cell></row><row><cell>Batch size</cell><cell>64</cell></row><row><cell>Word embeddings size</cell><cell>300</cell></row><row><cell>Input image features size</cell><cell>2048</cell></row><row><cell>Input question features size</cell><cell>512</cell></row><row><cell>Fused multimodal features size</cell><cell>1024</cell></row><row><cell>Multi-head attention hidden size</cell><cell>512</cell></row><row><cell>Number of MCA layers</cell><cell>6</cell></row><row><cell>Number of attention heads</cell><cell>8</cell></row><row><cell>Dropout rate</cell><cell>0.1</cell></row><row><cell>MLP size in flatten layers</cell><cell>512</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Base learning rate</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This entropy is normally defined up to a constant, often presented without the 1 α factor. We use the same definition as Blondel et al.[7,  §4.3]  for convenience.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This should not be confused with sparsity-inducing distributions<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.<ref type="bibr" target="#b2">3</ref> Unfortunately, the literature is inconsistent in defining these coefficients. Our α matches that of Blondel et al.<ref type="bibr" target="#b6">[7]</ref>; Tsallis' q equals 2 − α; this family is also related to Amari's α-divergences, but their α = 2q − 1. Inconsistent definitions have also been proposed for q-exponential families regarding how they are normalized; for example, the Tsallis maxent principle leads to a different definition. See App. C for a detailed discussion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Software code is available at https://github.com/deep-spin/mcan-vqa-continuous-attention.<ref type="bibr" target="#b4">5</ref> An alternative would be bounding box features from an external object detector<ref type="bibr" target="#b32">[33]</ref>. We opted for grid regions to check if continuous attention has the ability to detect relevant objects on its own. However, our method can handle bounding boxes too, if the {t } L =1 coordinates in the regression<ref type="bibr" target="#b15">(16)</ref> are placed on those regions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported by the European Research Council (ERC StG DeepSPIN 758969), by the P2020 program MAIA (contract 045909), and by the Fundação para a Ciência e Tecnologia through</p><p>We discuss here the relation between the (2 − α)-exponential family of distributions as presented in Prop. 1 and the distributions arising from the Tsallis maxent principle <ref type="bibr" target="#b12">[13]</ref>. We put in perspective the related work in statistical physics <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b9">10]</ref>, information geometry <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref>, and the discrete case presented in the machine learning literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>We start by noting that our α parameter matches the α used in prior machine learning literature related to the "α-entmax transformation" <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In the definition of Tsallis entropies (6), our α corresponds to the entropic index q defined by Tsallis <ref type="bibr" target="#b12">[13]</ref>. However, our (2 − α)-exponential families correspond to the q-exponential families as defined by Naudts <ref type="bibr" target="#b9">[10]</ref>, and to the t-exponential families described by Ding and Vishwanathan <ref type="bibr" target="#b11">[12]</ref> (which include the t-Student distribution). The family of Amari's α-divergences relates to this q as α = 2q − 1 [18, §4.3].</p><p>These differences in notation have historical reasons, and they are explained by the different ways in which Tsallis entropies relate to q-exponential families. In fact, the physics literature has defined q-exponential distributions in two distinct ways, as we next describe.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Information and Exponential Families in Statistical Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Barndorff-Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sufficient statistics and intrinsic accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin James George</forename><surname>Pitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1936" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="567" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sur les lois de probabilitéa estimation exhaustive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Darmois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CR Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On distributions admitting a sufficient statistic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Osgood Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="409" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning with fenchel-young losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niculae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptively sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gonçalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Ft</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The q-exponential family in statistical physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Naudts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Central European Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="413" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalized Maximum Entropy, Convexity and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Sears</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>The Australian National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">t-logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Possible generalization of Boltzmann-Gibbs statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantino</forename><surname>Tsallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="479" to="487" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul R Halmos</surname></persName>
		</author>
		<title level="m">Measure Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Françoise Fogelman-Soulié and Jeanny Hérault</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information geometry and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">194</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantification method of classification processes. concept of structural a-entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Havrda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">František</forename><surname>Charvát</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetika</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="35" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy and diversity. Oikos</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="363" to="375" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gini-Simpson index of diversity: a characterization, generalization, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Utilitas Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive sparseness using Jeffreys prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning and the relevance vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometry for q-exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Matsuzoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsumi</forename><surname>Ohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Progress in Differential Geometry and its Related Fields</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="55" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometry of q-exponential family of probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsumi</forename><surname>Shun-Ichi Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1170" to="1185" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-parametric estimation of a multivariate probability density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vassiliy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Epanechnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2017 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niehues</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stüker</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudoh</forename><surname>Katsuitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshino</forename><surname>Koichiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federmann</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="398" to="414" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6274" to="6283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Ken-Ichi Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Von mises-fisher loss for training sequence to sequence models with continuous outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hard-coded Gaussian attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.687</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.687" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="7689" to="7700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Is attention interpretable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5310" to="5319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Convex Analysis and Monotone Operator Theory in Hilbert Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Combettes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lev M Bregman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="217" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Geometry of escort distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumiyoshi</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31101</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Information theory and statistical mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">620</biblScope>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joey nmt: A minimalist nmt toolkit for novices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP: System Demonstrations</title>
		<meeting>of EMNLP-IJCNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">We used the VQA-v2 dataset [31] with the standard splits (443K, 214K, and 453K question-image pairs for train/dev/test, the latter subdivided into test-dev, test-standard, test-challenge and testreserve)</title>
		<ptr target="https://ai.stanford.edu/~amaas/data/sentiment7http://nlp.stanford.edu/data/glove.840B.300d.zip8https://wit3.fbk.eu/mt.php?release=2017-01-trnted9https://github.com/joeynmt/joeynmt/blob/master/configs/iwslt14_deen_bpe.yaml10https://github.com/MILVLG/mcan-vqa" />
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>We adapted the implementation of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
