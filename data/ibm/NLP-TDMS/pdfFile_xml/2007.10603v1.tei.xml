<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature-metric Loss for Self-supervised Learning of Depth and Egomotion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
							<email>shuchang02@meituan.com</email>
							<affiliation key="aff0">
								<orgName type="department">Meituan Dianping Group 2 DeepMotion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
							<email>kunyu@deepmotion.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Meituan Dianping Group 2 DeepMotion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
							<email>zhixiangduan@deepmotion.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Meituan Dianping Group 2 DeepMotion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<email>kuiyuanyang@deepmotion.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Meituan Dianping Group 2 DeepMotion</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature-metric Loss for Self-supervised Learning of Depth and Egomotion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by δ1 for depth estimation, and significantly outperforms previous method for visual odometry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating depth and egomotion from monocular camera is a fundamental and valuable task in computer vision, which has wide applications in augmented reality <ref type="bibr" target="#b34">[35]</ref>, robotics navigation <ref type="bibr" target="#b7">[8]</ref> and autonomous driving <ref type="bibr" target="#b30">[31]</ref>. Though monocular camera is cheap and lightweight, the task is hard for conventional SfM/SLAM algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref> and continues challenging deep learning based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Deep learning for depth and egomotion estimation can be broadly categorized into supervised and self-supervised learning. For depth estimation, supervised learning takes images paired with depth maps as input <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>, where depth maps are sparsely collected from expensive LiDAR sensors <ref type="bibr" target="#b13">[14]</ref> or densely rendered from simulation engines <ref type="bibr" target="#b28">[29]</ref>, while supervision from LiDAR limits the generalization to new cameras and supervision from rendering limits the generalization to real scenes. For egomotion estimation, supervised signals come from trajectories computed by classical methods with high precision sensors like IMU and GPS, which are also costly and cannot guarantee absolute accuracy. Self-supervised learning unifies these two tasks into one framework, and only uses monocular videos as inputs, and supervision is from view synthesis <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref>. The setup is simpler, and easy to generalize among cameras.</p><p>This work is done when Chang Shu is an intern at DeepMotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2007.10603v1 [cs.CV] 21 Jul 2020</head><p>However, self-supervised approaches are still inferior to supervised ones by large margins when compared on standard benchmarks. The main problem lies in the weak supervision added as photometric loss, which is defined as the photometric difference between a pixel warped from source view by estimated depth and pose and the pixel captured in the target view. Nevertheless, small photometric loss does not necessarily guarantee accurate depth and pose, especially for pixels in textureless regions. The problem can be partially solved by adding smoothness loss on depth map, which encourages first-order smoothness <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref> or second-order smoothness <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b25">26]</ref>, and forces depth propagation from discriminative regions to textureless regions. However, such propagation is with limited range and tends to cause over-smooth results around boundaries.</p><p>Considering the basic limitation is from representation, feature-metric loss is proposed to use learned feature representation for each pixel, which is explicitly constrained to be discriminative even in textureless regions. For learning feature representation, a single view reconstruction pathway is added as an auto-encoder network. To ensure loss landscapes defined on the learned feature representation having desired shapes, two additional regularizing losses are added to the auto-encoder loss, i.e., discriminative loss and convergent loss. The discriminative loss encourages feature differences across pixels modeled by first-order gradients, while the convergent loss ensures a wide convergence basin by penalizing feature gradients' variances across pixels.</p><p>In total, our network architecture contains three sub-networks, i.e., DepthNet and PoseNet for cross-view reconstruction, and FeatureNet for single-view reconstruction, where features generated by FeatureNet are used to define feature-metric loss for Depth-Net and PoseNet.</p><p>In experiment, feature-metric loss outperforms widely used first-order and secondorder smoothness losses, and improves state-of-the-art depth estimation from 0.885 to 0.925 measured by δ 1 on KITTI dataset. In addition, our method generates better egomotion estimation and results in more accurate visual odometry.</p><p>In general, our contributions are summarized as three-fold:</p><p>-Feature-metric loss is proposed for self-supervised depth and egomotion estimation. -FeatureNet is proposed for feature representation learning for depth and egomotion estimation. -State-of-the-art performances on depth and egomotion estimation are achieved on KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review related works of self-supervised learning for two tasks, i.e., monocular depth and egomotion estimation, as well as visual representation learning. Monocular depth and egomotion estimation: SfMLearner is a pioneering work <ref type="bibr" target="#b55">[56]</ref> for this task, where geometry estimation from DepthNet and PoseNet is supervised by photometric loss. To tackle moving objects that break the assumption of static scenes, optical flow is estimated to compensate these moving pixels <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">57]</ref>, segmentation masks provided by pre-trained segmentation models are also to handle potential moving objects separately <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17]</ref>. More geometric priors are also used to strengthen the self-supervised learning. Depth-normal consistency loss is proposed as as extra constraint <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. 3D consistency between point clouds backprojected from adjacent views is considered in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>. In addition, binocular videos are used for training to solve both scale ambiguity and scene dynamics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b25">26]</ref>, where only inference can be carried on monocular video.</p><p>In contrast to all above methods where focuses are on the geometry parts of the task, deep feature reconstruction <ref type="bibr" target="#b52">[53]</ref> proposed to use deep features from pre-trained models to define reconstruction loss. Our method shares the same spirit, but takes a step further to explicitly learn deep features from the geometry problem under the same self-supervised learning framework. Visual representation learning: It is of great interest of self-supervised visual representation learning for downstream tasks. Without explicitly provided labels, the losses are defined by manipulating the data itself in different ways, which could be reconstructing input data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref>, predicting spatial transformations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, coloring grayscale input images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b53">54]</ref> etc. Our work belongs to reconstruct the input through an auto-encoder network. Different from previous works mainly aiming for learning better features for recognition tasks, our method is designed to learn better features for the geometry task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we firstly introduce geometry models with required notations, then define two reconstruction losses, one for depth and ego-motion learning, the other for feature representation learning. Finally, we present our overall pipeline and implementation details about loss settings and network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Geometry models</head><p>Camera model and depth. The camera operator π : R 3 → R 2 projects a 3D point P = (X, Y, Z) to a 2D pixel p = (u, v) by:</p><formula xml:id="formula_0">π(P ) = (f x X Z + c x , f y Y Z + c y )<label>(1)</label></formula><p>where (f x , f y , c x , c y ) are the camera intrinsic parameters. Similarly, a pixel p is projected to a 3D point P given its depth D(p), i.e., backprojection π −1 :</p><formula xml:id="formula_1">R 2 × R → R 3 : π −1 p, D(p) = D(p) x − c x f x , y − c y f y ,<label>1 (2)</label></formula><p>Ego-motion. Ego-motion is modeled by transformation G ∈ SE(3), together with π and π −1 , we can define a projective warping function ω :</p><formula xml:id="formula_2">R 2 × R × SE(3) → R 2 ,</formula><p>which maps a pixel p in one frame to the other frame transformed by G:</p><formula xml:id="formula_3">p = ω p, D(p), G = π G · π −1 p, D(p)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-view reconstruction</head><p>With the above geometry models, target frame I t can be reconstructed from source frame I s via,</p><formula xml:id="formula_4">I s→t (p) = I s ( p)<label>(4)</label></formula><p>where p is defined in Eq. 3 and depends on both depth and ego-motion. I t (p) and I s ( p) should be similar given a set of assumptions, including both depth and ego-motion are correct; the corresponding 3D point is static with Lambertian reflectance and not occluded in both views. Then, a multi-view reconstruction loss can be defined for learning depth and motion, i.e.,</p><formula xml:id="formula_5">L s→t = p I s ( p), I t (p) ,<label>(5)</label></formula><p>where (, ) is the per-pixel loss which measures the photometric difference, i.e, photometric loss. Though the loss works, it is fundamentally problematic since correct depth and pose is sufficient but not necessary for small photometric error, e.g., pixels in a textureless with the same photometric values can have small photometric losses even the depth and pose are wrongly estimated. The problem can be formally analysed from the optimization perspective by deriving the gradients with respect to both depth D(p) and egomotion G,</p><formula xml:id="formula_6">∂L s→t ∂D(p) = ∂ I s ( p), I t (p) ∂I s ( p) · ∂I s ( p) ∂ p · ∂ p ∂D(p) ,<label>(6)</label></formula><formula xml:id="formula_7">∂L s→t ∂G = p ∂ I s ( p), I t (p) ∂I s ( p) · ∂I s ( p) ∂ p · ∂ p ∂G ,<label>(7)</label></formula><p>where both gradients depend on the image gradient ∂Is( p) ∂ p . For textureless region, the image gradients are close to zero which further causes zero gradients for Eq. 6 and contributes zero to Eq. 7 for egomotion estimation. In addition, locally non-smooth gradient directions are also challenging convergence due to inconsistent update directions towards minima.</p><p>Therefore, we propose to learn feature representation φ s (p) with better gradient ∂φs( p) ∂ p to overcome the above problems, and generalizes photometric loss to featuremetric loss accordingly,</p><formula xml:id="formula_8">L s→t = p φ s ( p), φ t (p) .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Single-view reconstruction</head><p>The feature representation φ(p) is also learned in self-supervised manner with singleview reconstruction through an auto-encoder network. The auto-encoder network contains an encoder for deep feature extractions from an image and an decoder to reconstruct the input image based on the deep features. The deep features are learned to encode large patterns in an image where redundancies and noises are removed. To ensure the learned representation with good properties for optimizing Eq. 8, we add two extra regularizers L dis and L cvt to the image reconstruction loss L rec , i.e.,</p><formula xml:id="formula_9">L s = L rec + αL dis + βL cvt<label>(9)</label></formula><p>where α and β are set to 1e-3 via cross validation. These three loss terms are described in detail below. For simplicity, we denote first-order derivative and second-order derivative with respect to image coordinates by ∇ 1 and ∇ 2 , which equals ∂ x +∂ y and ∂ xx +2∂ xy +∂ yy respectively.</p><p>Image reconstruction loss Image reconstruction loss L rec is the standard loss function for an auto-encoder network, which requires the encoded features can be used to reconstruct its input, i.e.,</p><formula xml:id="formula_10">L rec = p |I(p) − I rec (p)| 1<label>(10)</label></formula><p>where I(p) is the input image, and I rec (p) is the image reconstructed from the autoencoder network.</p><p>Discriminative loss L dis is defined to ensure the learned features have gradients ∂φ( p) ∂ p by explicitly encouraging large gradient, i.e.,</p><formula xml:id="formula_11">L dis = − p |∇ 1 φ(p)| 1<label>(11)</label></formula><p>Furthermore, image gradients are used to emphasize low-texture regions,</p><formula xml:id="formula_12">L dis = − p e −|∇ 1 I(p)|1 |∇ 1 φ(p)| 1<label>(12)</label></formula><p>where low-texture regions receive large weights. Convergent loss L cvt is defined to encourage smoothness of feature gradients, which ensures consistent gradients during optimization and large convergence radii accordingly. The loss is defined to penalize the second-order gradients, i.e.,</p><formula xml:id="formula_13">L cvt = p |∇ 2 φ(p)| 1<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall pipeline</head><p>Single-view reconstruction and cross-view reconstruction are unified to form the final framework as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. DepthNet is a monodepth estimator which takes the target frame as input and outputs a depth map. PoseNet is an egomotion estimator, which takes two frames from both source and target view and outputs the relative pose between them. DepthNet and PoseNet provide the geometry information to establish point-to-point correspondences for cross-view reconstruction. FeatureNet is for feature representation learning, which follows the auto-encoder architecture and supervised by single-view reconstruction loss. Features from FeatureNet are used to define the crossview reconstruction loss. Therefore, the total loss for the whole architecture contains two parts, where L s constrains the quality of learned features through single-view reconstruction, whilst L s→t penalizes the discrepancy from cross-view reconstruction, i.e.,</p><formula xml:id="formula_14">L total = L s + L s→t<label>(14)</label></formula><p>Toward better performance, the proposed feature-metric loss is combined with used photometric loss, i.e.,</p><formula xml:id="formula_15">L s→t = p L f m φ s ( p), φ t (p) + p L ph (I s ( p), I t (p))<label>(15)</label></formula><p>where L f m and L ph are the feature-metric loss and photometric loss respectively. Specifically, feature-metric loss is defined by</p><formula xml:id="formula_16">L f m = |φ s ( p) − φ t (p)| 1 ,<label>(16)</label></formula><p>and photometric loss is defined following <ref type="bibr" target="#b15">[16]</ref> using a combination of L 1 and SSIM losses, i.e.,</p><formula xml:id="formula_17">L ph = 0.15 p |I s ( p) − I t (p)| 1 + 0.85 1 − SSIM(I s ( p), I t (p)) 2<label>(17)</label></formula><p>Furthermore, we resolve the occlusion problem following the practices in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b52">53]</ref>, where two source views are used to define the cross-view reconstruction loss, Where V is a set composed of source frames. When trained on the monocular videos, V contains the previous and posterior source frames of current target frame; when trained on the calibrated binocular videos, an extra frame of opposite stereo pair is added.</p><formula xml:id="formula_18">L s→t = p min s∈V L s→t φ s ( p), φ t (p)<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation details</head><p>For FeatureNet, ResNet-50 <ref type="bibr" target="#b17">[18]</ref> with fully-connected layer removed is used as the encoder, where deepest feature map goes through 5 downsampling stages and reduces to 1/32 resolution of input image, the decoder contains five 3 × 3 convolutional layers and each followed by a bilinear upsampling layer. Multi-scale feature maps from convolutional layers of the decoder are used to generate multi-scale reconstructed images, where feature map of each scale further goes through a 3 × 3 convolution with sigmoid function for image reconstruction. The largest feature map with 64 channels from encoder is regularized by L dis and L cvt and will be used for feature-metric loss. DepthNet also adopts an encoder-decoder structure, where ResNet-50 without fullyconnected layer is used as encoder and multi-scale feature maps are outputted. The decoder for depth is implemented in a cascaded refinement manner, which decodes depth maps in a top-down pathway. Specifically, multiple-scale features from encoder are used to predict maps of corresponding sizes via a 3 × 3 convolution followed by sigmoid, and these maps are refined in a coarse-to-fine manner towards the final depth map. Both FeatureNet and DepthNet take image size of 320 × 1024 as inputs.</p><p>The PoseNet is a pose estimator with a structure of ResNet-18 <ref type="bibr" target="#b17">[18]</ref>, which is modified to receive a concatenated image pair and predicts a relative pose therein. Here axis angle is chosen to represent the 3D rotation. The input resolution is 192×640. Comparing with both FeatureNet and DepthNet, PoseNet uses lower image resolution and more light-weight backbone, which observes this has no obvious influence to pose accuracy, but significantly save both memory and computation.</p><p>We adopt the setting in <ref type="bibr" target="#b14">[15]</ref> for data preprocessing. Our models are implemented on PyTorch <ref type="bibr" target="#b38">[39]</ref> with distributed computing, and trained for 40 epochs using Adam <ref type="bibr" target="#b19">[20]</ref> optimizer, with a batch size of 2, on the 8 GTX 1080Ti GPUs. The learning rate is gradually warmed up to 1e −4 in 3 steps, where each step increases learning rate by 1e −4 /3 in 500 iterations. After warmping, learning rate 1e −4 is used for the first 20 epochs and halved twices at 20th and 30th epoch. As for online refinement technique <ref type="table">Table 1</ref>: Performance metrics for depth evaluation. d and d * respectively denotes predicted and ground truth depth, D presents a set of all the predicted depth values of an image, |.| returns the number of the elements in the input set.</p><formula xml:id="formula_19">Abs Rel : 1 |D| d∈D |d * − d|/d * RMSE : 1 |D| d∈D ||d * − d|| 2 Sq Rel : 1 |D| d∈D ||d * − d|| 2 /d * RMSE log : 1 |D| d∈D ||logd * − logd|| 2 δ t : 1 |D| |{d ∈ D| max( d * d , d d * ) &lt; 1.25 t }| × 100%</formula><p>we used during testing, we follow the practice proposed by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. We keep the model training while performing inference. The batch size is set to 1. Each batch consists of the test image and its two adjacent frames. Online refinement is performed for 20 iterations on one test sample with the same setting introduced before. No data augmentation is used in the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we show extensive experiments for evaluating the performance of our approach. We make a fair comparison on KITTI 2015 dataset <ref type="bibr" target="#b13">[14]</ref> with prior art on both single view depth and visual odometry estimation tasks. And detailed ablation studies of our approach are done to show the effectiveness of the feature-metric loss.</p><p>KITTI 2015 dataset contains videos in 200 street scenes captured by RGB cameras, with sparse depth ground truths captured by Velodyne laser scanner. We follow <ref type="bibr" target="#b55">[56]</ref> to remove static frames as pre-processing step. We use the Eigen split of <ref type="bibr" target="#b10">[11]</ref> to divide KITTI raw data, and resulting in 39,810 monocular triplets for training, 4,424 for validation and 697 for testing. For depth evaluation, we test our depth model on divided 697 KITTI testing data. For odometry evaluation, we test our system to the official KITTI odometry split which containing 11 driving sequences with ground truth odometry obtained through the IMU and GPS readings. Following previous works <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b55">56]</ref>, we train our model on the sequence 00-08 and use the sequence 09-10 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Depth evaluation.</head><p>Performance metrics. Standard metrics are used for depth evaluation, as shown in Tab. 1. During evaluation, depth is capped to 80m. For the methods trained on monocular videos, the depth is defined up to scale factor <ref type="bibr" target="#b55">[56]</ref>, which is computed by   <ref type="bibr" target="#b3">[4]</ref>, which advocated keeping the model training while performing inference. †: using post processing steps.</p><formula xml:id="formula_20">scale = median(D gt )/median(D pred )<label>(19)</label></formula><p>of data -monocular videos (M), stereo pairs (S), binocular videos (MS) and labelled single images (Sup), while all of them are tested with single image as input.</p><p>We achieve the best performance compared to all self-supervised methods, no matter which training data is used. Our method achieves more significant improvement in the performance metric Sq Rel. According to Tab. 1, this metric penalizes more on large</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Seq. 09 Seq. 10 t err r err t err r err ORB-SLAM <ref type="bibr" target="#b32">[33]</ref>   <ref type="table">Table 3</ref>: Comparison of performances are reported on the KITTI odometry dataset <ref type="bibr" target="#b13">[14]</ref>. Best results are in bold. errors in short range, where more textureless regions exist due near objects are large in images and our method handles well. The closest results in self-supervised methods are from DepthHint <ref type="bibr" target="#b46">[47]</ref>, which uses the same input size but adds an extra post processing step. It utilizes a traditional stereo matching method -SGM <ref type="bibr" target="#b18">[19]</ref> to provide extra supervisory signals for training, since SGM is less likely to be trapped by local minimums. However, in its settings, the object function of SGM is still photometric loss, the drawbacks of photometric loss are still inevitable. In contrast, proposed feature-metric loss will largely avoid the interference of local minimums.</p><p>Moreover, compared with state-of-the-art supervised methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, which achieve top performances on the KITTI depth prediction competition, our model with online refinement technique even exceeds in many metrics. Our advantage over supervised methods is that the gap between the distributions of training and testing data does exist, we can make full use of online refinement technique. What is more, as shown in Sec. 4.3, the introduction of feature-metric loss can obtain more performance gain from online refinement technique. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the qualitative results. Compared with state-of-the-art method Mon-oDepth2 <ref type="bibr" target="#b14">[15]</ref>, we achieve better performance on low-texture regions and finer details, e.g., walls, billboards, silhouette of humans and poles.</p><p>However, MonoDepth2 is built on the photometric loss, which is easily trapped by local minimums especially on low-texture regions like walls and billboards. In contrast, the introduction of feature-metric loss leads the network into jumping out of local minimums, since our features are designed to form a desirable loss for easier optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Odometry evaluation</head><p>Performance metric. Average translational root mean square error drift (t err ) and average rotational root mean square error drift (r err ) on length of 100m -800m are adopted for evaluation. For the methods who suffer from scale ambiguity, one global scale that best align the whole sequence is used.</p><p>Comparison with state-of-the-art. As shown in Tab. 3, we report the performance of ORB-SLAM <ref type="bibr" target="#b32">[33]</ref> as a reference and compare with recent deep methods. our method gets top performances in two metrics and comparable performance in the rest metrics  compared to other deep learning methods. When compared to traditional SLAM method <ref type="bibr" target="#b32">[33]</ref>, our translation performance is comparable, while in the rotation estimation we still fall short like other deep learning methods. We believe that it is because the bundle adjustment of the traditional SLAM method can optimize subtler rotation errors along a long sequence which can't be observed in a small sequence used by current deep learning based methods. Moreover current reconstruction process may be not sensible to variation of rotation <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>To get a better understanding of the contribution of proposed losses-feature-metric loss, discriminative loss and convergent loss-to the overall performance, we perform an ablation study in Tab. 4. The losses for cross-view reconstruction. In Tab. 4a, different components of L s→t have been tried. The smoothness losses which are widely used are used as baselines:</p><formula xml:id="formula_21">L i ds = p e −|∇ i I(p)|1 |∇ i D(p)| 1<label>(20)</label></formula><p>where D(p) = D(p)/D, this operation is the mean normalization technique advocated by <ref type="bibr" target="#b45">[46]</ref>. i denotes the order of the derivatives. These smoothness losses are used as baselines to verify the effectiveness of the feature-metric loss. Compared with smoothness losses, feature-metric loss leads to much better effect. We can see that a biggest performance boost is gained by introducing the feature-metric loss. As we discussed before, the propagation range of smoothness losses is limited, in contrast, the feature-metric loss enable a long-range propagation, since it has a large <ref type="figure">Fig. 3</ref>: A visualization of a learned visual representation, which is achieved by selecting one principle channel through PCA decomposition, then showing the feature map as a heat map, hotter color indicates a higher feature value. First row shows a typical image which is full of textureless regions like walls and shadows. The visualization of corresponding feature maps is shown in second to fourth rows. The feature maps are respectively learned with different loss combinations, which sequentially correspond with the settings in the first three rows in Tab. 4b. In order to get a better understanding, we crop three typical textureless regions as shown in (a-c), cropped feature maps are visualized according to the dynamic range after cropping. convergence radius. We also observe that when feature-metric loss can benefit more from the performance gain provided by online refinement than other loss combination. Higher performance gain is attributed to better supervised signal provided by featuremetric loss during online refinement phase, where incorrect depth values can be appropriately penalized with larger losses based on more discriminative features.</p><p>The losses for single-view reconstruction. Tab.4b shows that the model without any of our contributions performs the worst. When combined together, all our components lead to a significant improvement.</p><p>And as shown in right part of Tab. 4b, although small deviations are less obvious in some metrics of the depth evaluation, small errors will be magnified via accumulation and propagation during trajectory prediction, big differences are shown in the odometry evaluation. Note that different from previous odometry evaluation, we directly applied the model trained on the kitti raw data to sequence 09-10 to get t err and r err .</p><p>Merely using L rec gets similar performance as merely using photometric loss (the third row in Tab. 4a), since it plays a similar role as the photometric loss at textureless regions. Results get better when equipped with L dis , since discrimination at low-texture regions is improved. Best performance is achieved when added L cvt , which means discrimination is not enough, a correct optimization direction is also important.</p><p>Visualization analysis. In order to see whether learned visual representations have promised properties, we visualize it in <ref type="figure">Fig. 3</ref>. The feature maps learned with different loss combinations: L rec , L rec +L dis and L rec +L dis +L cvt are sequentially shown from the second to the fourth row. Although we require our feature to be discriminative, this effect is not sufficient to be shown in a large view, since the gap between the features of different sorts are much larger than that of spatially adjacent features. Therefore, we cropped three typical textureless regions, and visualize them again according to the dynamic range after cropping.</p><p>We can see that merely using L rec get small variations at textureless regions. The close-ups of original images are similar to feature maps only trained with L rec , which verifies the proposed losses in improving feature representations. The feature map learned with L rec + L dis is not smooth and disordered, since L dis overemphasizes the discrepancy between adjacent features, the network degenerates to form a landscape of a zigzag shape. This phenomenon can be approved by the results in the second row of Tab. 4b, which is only slightly higher than merely using L rec .</p><p>A desired landscape for feature maps is a smooth slope, in this way, feature-metric loss will be able to form a basin-like landscape. The feature map learned with all the proposed losses approximates this ideal landscape, from zoom-in views we can see a clear and smooth transition along a certain direction. On this landscape, gradient descent approaches can move smoothly toward optimal solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, feature-metric loss is proposed for self-supervised learning of depth and egomotion, where feature representation is additionally learned with two extra regularizers to ensure convergence towards correct depth and pose. The whole framework is end-to-end trainable in self-supervised setting, and achieves state-of-the-art depth estimation which is even comparable to supervised learning methods. Furthermore, visual odometry based on estimated egomotion also significantly outperforms previous stateof-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An illustration of the overall framework, which contains DepthNet, PoseNet and FeatureNet for depth map prediction, egomotion prediction and feature learning respectively. FeatureNet uses L s to learn require visual representation, the encoder from FeatureNet is used to extract features for cross-view reconstruction loss L s→t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Qualitative comparison between Monodepth2<ref type="bibr" target="#b14">[15]</ref> (second row) and our method (last row). It can be seen that we achieve better performance on the low-texture regions like walls and billboards, and finer details are present like silhouette of humans and poles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For evaluation, those predicted depth maps are multiplied by computed scale to match the median with the ground truth, this step is called median scaling.Comparison with state-of-the-art. Tab. 2 shows performances of current state-ofthe-art approaches for monocular depth estimation. They are trained on different kinds</figDesc><table><row><cell>Method</cell><cell>Train</cell><cell cols="6">The lower the better Abs Rel Sq Rel RMSE RMSE log δ 1 The higher the better δ 2 δ 3</cell></row><row><cell>SfMLearner [56]</cell><cell>M</cell><cell cols="2">0.208 1.768 6.958</cell><cell cols="4">0.283 0.678 0.885 0.957</cell></row><row><cell>DNC [51]</cell><cell>M</cell><cell cols="2">0.182 1.481 6.501</cell><cell cols="4">0.267 0.725 0.906 0.963</cell></row><row><cell>Vid2Depth [27]</cell><cell>M</cell><cell cols="2">0.163 1.240 6.220</cell><cell cols="4">0.250 0.762 0.916 0.968</cell></row><row><cell>LEGO [50]</cell><cell>M</cell><cell cols="2">0.162 1.352 6.276</cell><cell cols="4">0.252 0.783 0.921 0.969</cell></row><row><cell>GeoNet [52]</cell><cell>M</cell><cell cols="2">0.155 1.296 5.857</cell><cell cols="4">0.233 0.793 0.931 0.973</cell></row><row><cell>DF-Net [57]</cell><cell>M</cell><cell cols="2">0.150 1.124 5.507</cell><cell cols="4">0.223 0.806 0.933 0.973</cell></row><row><cell>DDVO [46]</cell><cell>M</cell><cell cols="2">0.151 1.257 5.583</cell><cell cols="4">0.228 0.810 0.936 0.974</cell></row><row><cell>EPC++ [26]</cell><cell>M</cell><cell cols="2">0.141 1.029 5.350</cell><cell cols="4">0.216 0.816 0.941 0.976</cell></row><row><cell>Struct2Depth [4]</cell><cell>M</cell><cell cols="2">0.141 1.036 5.291</cell><cell cols="4">0.215 0.816 0.945 0.979</cell></row><row><cell>SIGNet [30]</cell><cell>M</cell><cell cols="2">0.133 0.905 5.181</cell><cell cols="4">0.208 0.825 0.947 0.981</cell></row><row><cell>CC [43]</cell><cell>M</cell><cell cols="2">0.140 1.070 5.326</cell><cell cols="4">0.217 0.826 0.941 0.975</cell></row><row><cell>LearnK [17]</cell><cell>M</cell><cell cols="2">0.128 0.959 5.230</cell><cell cols="4">0.212 0.845 0.947 0.976</cell></row><row><cell>DualNet [55]</cell><cell>M</cell><cell cols="2">0.121 0.837 4.945</cell><cell cols="4">0.197 0.853 0.955 0.982</cell></row><row><cell>SuperDepth [40]</cell><cell>M</cell><cell>0.116 1.055</cell><cell>-</cell><cell cols="4">0.209 0.853 0.948 0.977</cell></row><row><cell>Monodepth2 [15]</cell><cell>M</cell><cell cols="2">0.115 0.882 4.701</cell><cell cols="4">0.190 0.879 0.961 0.982</cell></row><row><cell>Ours</cell><cell>M</cell><cell cols="2">0.104 0.729 4.481</cell><cell cols="4">0.179 0.893 0.965 0.984</cell></row><row><cell>Struct2Depth [4]</cell><cell>M  *</cell><cell cols="2">0.109 0.825 4.750</cell><cell cols="4">0.187 0.874 0.958 0.983</cell></row><row><cell>GLNet [5]</cell><cell>M  *</cell><cell cols="2">0.099 0.796 4.743</cell><cell cols="4">0.186 0.884 0.955 0.979</cell></row><row><cell>Ours</cell><cell>M  *</cell><cell cols="2">0.088 0.712 4.137</cell><cell cols="4">0.169 0.915 0.965 0.982</cell></row><row><cell>Dorn [13]</cell><cell cols="3">Sup 0.099 0.593 3.714</cell><cell cols="4">0.161 0.897 0.966 0.986</cell></row><row><cell>BTS [23]</cell><cell cols="3">Sup 0.091 0.555 4.033</cell><cell cols="4">0.174 0.904 0.967 0.984</cell></row><row><cell>MonoDepth [16]</cell><cell>S</cell><cell cols="2">0.133 1.142 5.533</cell><cell cols="4">0.230 0.830 0.936 0.970</cell></row><row><cell cols="2">MonoDispNet [48] S</cell><cell cols="2">0.126 0.832 4.172</cell><cell cols="4">0.217 0.840 0.941 0.973</cell></row><row><cell cols="2">MonoResMatch [44] S</cell><cell cols="2">0.111 0.867 4.714</cell><cell cols="4">0.199 0.864 0.954 0.979</cell></row><row><cell cols="2">MonoDepth2 [15] S</cell><cell cols="2">0.107 0.849 4.764</cell><cell cols="4">0.201 0.874 0.953 0.977</cell></row><row><cell>RefineDistill [41]</cell><cell>S</cell><cell cols="2">0.098 0.831 4.656</cell><cell cols="4">0.202 0.882 0.948 0.973</cell></row><row><cell>UnDeepVO [24]</cell><cell>MS</cell><cell cols="2">0.183 1.730 6.570</cell><cell>0.268</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DFR [53]</cell><cell>MS</cell><cell cols="2">0.135 1.132 5.585</cell><cell cols="4">0.229 0.820 0.933 0.971</cell></row><row><cell>EPC++ [26]</cell><cell>MS</cell><cell cols="2">0.128 0.935 5.011</cell><cell cols="4">0.209 0.831 0.945 0.979</cell></row><row><cell cols="2">MonoDepth2 [15] MS</cell><cell cols="2">0.106 0.818 4.750</cell><cell cols="4">0.196 0.874 0.957 0.979</cell></row><row><cell>DepthHint [47]</cell><cell cols="3">MS  † 0.100 0.728 4.469</cell><cell cols="4">0.185 0.885 0.962 0.982</cell></row><row><cell>Ours</cell><cell>MS</cell><cell cols="2">0.099 0.697 4.427</cell><cell cols="4">0.184 0.889 0.963 0.982</cell></row><row><cell>Ours</cell><cell cols="3">MS  *  0.079 0.666 3.922</cell><cell cols="4">0.163 0.925 0.970 0.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of performances are reported on the KITTI dataset. Best results are in bold, second best are underlined. M: trained on monocular videos. S: trained on stereo pairs. MS: trained on calibrated binocular videos. Sup: trained on labelled single images.</figDesc><table /><note>* : using the online refinement technique</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The ablation study of different loss settings of our work.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enhancing self-supervised monocular depth estimation with traditional visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andraghetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myriokefalitakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Dovesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03127</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised scaleconsistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised depth learning in challenging indoor video: Weak rectification to rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02708</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Noise-aware unsupervised deep lidar-stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning large-scale automatic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vision for mobile robot navigation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multiscale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pose graph optimization for unsupervised monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06125</idno>
		<title level="m">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICANN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Signet: Semantic instance aided unsupervised 3d geometry perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sunarjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dtam: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<editor>NeurIPS-W</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Refine and distill: Exploiting cycleinconsistency and knowledge distillation for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">S-ptam: Stereo parallel tracking and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Cristóforis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Berlles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="27" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Every pixel counts: Unsupervised geometry learning with holistic 3d motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<editor>ECCV-w</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and egomotion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
