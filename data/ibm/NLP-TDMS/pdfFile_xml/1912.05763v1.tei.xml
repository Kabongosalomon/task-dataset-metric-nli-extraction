<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks (a) (b) (c) (d) (e)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
							<email>li@ids.osaka-u.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
							<email>mverma@ids.osaka-u.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Kawasaki</surname></persName>
							<email>ryo.kawasaki@ophthal.med.osaka-u.ac.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nakashima Osaka University</orgName>
								<address>
									<settlement>Yuta</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hajime Nagahara Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks (a) (b) (c) (d) (e)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose Iter-Net, a new model based on UNet [1], with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. Iter-Net consists of multiple iterations of a mini-UNet, which can be 4× deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10∼20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Retinal examination serve as an important diagnostic modality in finding retinal diseases as well as systemic diseases, such as high blood pressure, arteriolosclerosis, and diabetic retinopathy, a microvascular complications of diabetes. In fact, it is the only feasible way for the doctors to inspect the blood vessel system in the human body in vivo. It has been used as a routine examination not only by ophthalmologists but also many other specialists <ref type="bibr">[2]</ref>. Retinal examination is non-invasive and economical to perform, and it has been widely conducted all over the world. However, at the same time, there will be a huge gap between the needs and the capacity of handling the ever-increasing retinal images by ophthalmologists. Computer-aided diagnosis will be an obvious solution in this scenario, and vessel segmentation is the essential basis of following analysis.</p><p>One major difficulty in the vessel segmentation task is that vessels have no significant differences in appearance from the background, especially for the micro vessels in noisy images. It is challenging to find every vessel while not introducing too many false positives. Actually, things will be more complicated if we consider the problem of photo imaging quality. Much essential information may be lost due to improper illumination, sensor noises, etc. In this case, it is indeed impossible for segmentation models to find a complete yet accurate vessel network. For example, in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, because of the optic disk (brighter spot) in the image, segmentation results suffer from severe deterioration around its boundary: Some pixels have been "lost" in the large gap in luminance. <ref type="figure" target="#fig_0">Figure 1</ref>(b) is the gold standard marked by human experts, who can do this because they know vessels should be lines/curves, which should be connected to each other to form a network. In other words, this structural redundancy enables the experts to interpolate vessels in obscured regions in retinal images. Deep learning models may also be capable of learning this kind of knowledge if they are exposed to a large amount of perfectly labeled data, which are extremely limited in the retinal image segmentation field. In fact, there are no more than 20 images for training in publicly available datasets, i.e., DRIVE <ref type="bibr">[3]</ref>, CHASE-DB1 <ref type="bibr" target="#b3">[4]</ref>, and STARE <ref type="bibr" target="#b4">[5]</ref>.</p><p>Existing approaches struggle with this scarceness of data. As shown in Figs. 1(c) and (d), the two state-of-the-art models with top performance, namely UNet <ref type="bibr">[1]</ref> and Deform UNet <ref type="bibr" target="#b5">[6]</ref>, face noticeable errors in their prediction. They either mix up the vessel and the boundary of optic disk or fail to detect vessels around their intersection, leading to e.g., segmentation in which a single vessel is split into two unconnected parts. This is a common phenomenon in medical image segmentation and can lead to a defective vessel map consisting of a set of disconnected or broken up segments. This issue makes it very difficult to analyze the blood vessel condition by doctors or standard imaging methodologies using the segmented images <ref type="bibr" target="#b6">[7]</ref>. Therefore, connectivity is also an important problem for retina segmentation.</p><p>One interesting observation in Figs. 1(c) and (d) is that humans may still be able to infer where the actual vessels are from these resulting vessel maps. This is because, like the experts, we can also make use of structural redundancy; we can guess that two parts in predicted vessels are connected if their edges are close and pointing to each other. This may also apply to deep learning models. Although it is hard for deep learning models to directly overcome the problem of missing or extra predictions, it may be possible to let them know which segmented vessel is false and which is not. Consequently, they may be able to learn how to fix errors in segmentation results. Based on this observation, we design a new UNet-based model, coined IterNet, which can well utilize the structural redundancy in the vessel system. The resulting vessel map by IterNet is shown in <ref type="figure" target="#fig_0">Fig. 1(e)</ref>, which gives precise segmentation of the vessels and almost avoid the interference around the optical disk.</p><p>The key idea is to shift the focus of the deep learning model from dealing with every pixel in raw input images to the whole vessel network system. More specifically, we build a model that refines imprecise vessel segmentation results to more precise ones, but not directly maps raw input images to precise segmentation results. In order to let the model learn sufficient knowledge of what real vessel networks and ones with failure in segmentation results look like, it is essential to provide them with enough training samples. However, again, there are no datasets available for this sake as mentioned above.</p><p>One feasible way is to use the outputs of a certain segmentation model, which actually is vessel maps, like the ones in Figs. 1(c) and (d), as inputs to the model dedicated for refinement. We implement this by adding some refinery modules (mini-UNets) after a base module (UNet) for initial segmentation, as shown in <ref type="figure">Fig. 2</ref>. The input of each refinery module is the output of the second last layer of its preceding module. Each module has an output of vessel segmentation, which has a respective loss function. In training, the base module will consistently adjust its parameters to improve its own output. Therefore, the first refinery module will get virtually different inputs, even with a fixed number of training samples. This applies to other refinery modules as well. In this process, the refinery modules can be exposed to a large number of false vessel patterns and thus can learn how to fix them because they are all bound to the correct labels. The number of refinery modules is a hyperparameter to be tuned according to the number of training samples, GPU capacities, and training time. The output from the last module, "Out N " in <ref type="figure">Fig. 2</ref>, will be the actual output in prediction and all other outputs are only used for training. In addition, to avoid the overfitting problem and to improve the training efficiency, we design IterNet with the weight-sharing feature and a skip-connection structure.</p><p>The main contributions of our work are as follows.</p><p>• A vessel segmentation model with top performance over all mainstream datasets. • An iterative design of neural network architecture to learn the nature of vessels, with avoiding overfitting by weight-sharing. • Drastically improved connectivity of segmentation results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out 3</head><p>Out N · · · · · · Concat Dimension Reduction <ref type="figure">Figure 2</ref>. The structure of IterNet, which consists of one UNet and iteration of N − 1 mini-UNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image segmentation: Currently, most state-of-the-art models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> for semantic segmentation stem from a fully-convolutional design, which is first introduced by the fully convolutional network (FCN) <ref type="bibr" target="#b10">[11]</ref>. The main idea is to encode the raw images into a feature space and convert feature vectors into segmented images in an end-to-end manner. FCN has innovated many iterative segmentation approaches, which have similar ideas with our IterNet, but with totally different implementations. For example, the iterated interactive model <ref type="bibr" target="#b11">[12]</ref> runs the FCN model several times and takes the users' feedback to add more accurate training labels during each iteration. Drozdzal's model uses FCN to preprocess input images into a normalized version and then applies a fully convolutional ResNet for iteratively refining the segmented images <ref type="bibr" target="#b12">[13]</ref>. UNet <ref type="bibr">[1]</ref> is another well-known fully-convolutional model. Unlike FCN, UNet has multiple decoding layers to upsample the features. It also adds some skip-connections to allow decoding layers to use the features from the encoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retinal image segmentation:</head><p>The traditional way to conduct blood vessel segmentation is to utilize the local information, such as image intensity, or some hand-crafted features to perform classification. One earliest attempt is to use thresholding and masking. Roychowdhury et al. <ref type="bibr" target="#b13">[14]</ref> introduced an iterative segmentation method. Several processes in the segmentation algorithm run multiple times, which is very similar to our IterNet. Their method literately looks for the possible vessel pixels by adaptive thresholding on a retinal image, which is masked with the segmentation result obtained from the last iteration.</p><p>The emergence of UNet <ref type="bibr">[1]</ref> leads to a new era of im-age segmentation in the medical domain, and has revolutionized most image segmentation tasks in relevant domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Kim et al. <ref type="bibr" target="#b19">[20]</ref> adopted the concept of iterative learning in an UNet-like model. Being similar to IterNet, their model also uses the last output as the next input. The main difference from ours is that they simply run one same model for multiple times. The encoding and decoding modules still need to deal with both raw retinal images and vessel segmentation results. In contrast, Iter-Net is one single model with iterated mini-UNets, which completely separates raw image input and segmentation result input. This is the key design concept of IterNet, boosting the state-of-the-art performance. Another recent model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, named DenseBlock-UNet transforms the convolutional modules in the common UNet model into the dense block introduced in <ref type="bibr" target="#b22">[23]</ref>. The dense block can improve UNet in some aspects, like alleviating the gradient vanishing, strong feature propagation, enabling feature reuse, and decreasing the whole parameter size. Deform-UNet <ref type="bibr" target="#b5">[6]</ref> is another encouraging model. The authors modified the UNet model for better performance. They applied two key modules from the deform convolutional networks <ref type="bibr" target="#b23">[24]</ref>, namely deformable convolution and deformable RoI pooling, which replace the original modules in standard convolutional neural network (CNN) models and empower them with the ability to dynamically adjust their receptive fields according to the actual objects in input images.</p><p>One of the main differences between IterNet and other UNet-based models is that our focus is not on modifying the structure of UNet; we think the feature extraction ability of UNet is enough for the vessel segmentation task. We are instead trying to make a better use of well-extracted features from the UNet model to infer missing pieces in them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IterNet</head><p>Based on the observation mentioned in Section 1, we design our model to learn what the human blood vessel system in retinal images looks like to exploit its structural redundancy. The network is designed by keeping human annotators in mind. That is, an annotator may segment a raw retinal image in several stages: The first stage is to make a rough segmentation map. In the following stages, they keep improving the map with the help of raw retinal image and previous vessel map until the annotator is satisfied with the resulting vessel map. This leads to the idea of using resulting vessel maps (as in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>) from a base module as an input to a refinery module that learns to correct it. With this architecture, the refinery module can infer missing/extra predictions based on the structure of the vessel system. In order to complete correction, we can apply the refinery module iteratively as shown in <ref type="figure">Fig. 2</ref>.</p><p>More specifically, our network consists of two slightly different architectures: One is UNet, and the other is a simplified version of UNet, referred to as mini-UNet. We use UNet as our base module because of its superior performance in various segmentation tasks, especially in the medical applications. The output of UNet is the one-channel map of the probabilities of pixels being on a vessels. The refinery modules' architecture is mini-UNet, and they use the output of the second last layer of its precedent module, which is a 32-channel feature map and thus can have more information, compared with the one-channel vessel probability map. The mini-UNet actually is a light-weight version of the UNet architecture with fewer parameters because the input to the refinery modules is a feature map that we consider is simpler than the raw retinal images with all the background and noises. In addition, we conduct an experiment to test the performance when replacing mini-UNets with full-size UNets, and the results get worse on all three datasets (Refer to the supplementary material for detailed results).</p><p>As we can see in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, the mapping from original retinal images to vessel maps is mostly learnt by the base module, and the refinery modules are responsible only for small parts of the vessels (e.g., thin vessels). Hence, Iter-Net achieves good segmentation results if we have enough samples to train the refinery modules. In our architecture, all refinery modules (the modules marked in blue in <ref type="figure">Fig. 2</ref>) share the same weights and biases. The input of first refinery module is the feature map from the second last layer of the base module, and the rest refinery modules follow the similar procedure. Essentially, this can be interpreted as the same module running for multiple times in a single forward path. The most obvious benefit is that they can have varying inputs. The intermediate results of the vessel map always changes after each refinery module as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. As a result, the refinery modules are consistently exposed to new patterns of failure in the vessel segmentation. This architecture makes it possible to train the refinery modules with only 20 training samples. Another reason for this architecture is to use iterative prediction, which can improve the segmentation performance. We observe that one single model prefers to modify the results only by small differences, and the concept of iterated prediction has been used in many existing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. Even with the same segmentation model, iterative applications can still get better results. Therefore, we can say that iterative application of the same model allows to refine the missing parts of the vessel network without explicitly modeling its structural redundancy. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we show the output from the base module and three outputs from following three refinery modules. The fourth output is shown in <ref type="figure" target="#fig_0">Fig. 1(e)</ref>. We can see that, with the iterative predictions by the refinery modules, IterNet gradually connects split micro vessels together.</p><formula xml:id="formula_0">(a) (b) (c)</formula><p>One important issue is that our IterNet is a many-layered feed-forward network. In general, upper-layers of a manylayered network hardly have an access to the input (or the features from layers close to the input layers), whereas it can serve as an important reference for the mini-UNets to see what the original vessels look like and make decisions based on it. Even for human annotators, it is necessary to check the specific area in the raw vessel images when refining some extremely-fuzzy parts of the vessel network. Therefore, we should enable the higher-layers to utilize the features from the lower layers. In addition, deep learning models may suffer from the vanishing gradient problem when they are deep. Hence IterNet demands paths from the upper layers to lower layers for efficient back-propagation.</p><p>We therefore add some skip-connections to IterNet, similar to common UNet. There are three kinds of skipconnections in IterNet. The first one is the intra-module connection to connect the encoding layers of each to the decoding layers. The second one is from the base UNet to all refinery mini-UNets. This connection provides an access to the feature from the first layer of the base UNet, which is very close to the input retinal image. The feature is concatenated with the feature from the first layer of every mini-UNet. The third one is the connections among the mini-UNets, inspired by the dense connection of the dense network <ref type="bibr" target="#b22">[23]</ref>. The features from lower modules are concatenated with those from the upper modules. To keep the same structure and for weights-sharing among the mini-UNets, we add a 1 × 1 convolutional layer, which is marked in yellow in <ref type="figure">Fig. 2</ref>, for dimensionality reduction. This is the only component in the mini-UNets that has private parameters.</p><p>For training IterNet, we employ losses for each output Out i. We use the sigmoid cross entropy, defined as:</p><formula xml:id="formula_1">L i = −y i log(p i ) − (1 − y i ) log(1 − p i ),<label>(1)</label></formula><p>where y i represents the binary indicator (0 or 1) whether the label is correct for the pixel i, and p i is the predicted probability that the pixel i is a vessel pixel. Then they are summed up with certain weights as:</p><formula xml:id="formula_2">L = i w i L i<label>(2)</label></formula><p>where w i 's are set to 1 as we put no particular importance to any output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Augmentation</head><p>As the number of training images is no more than 20 in publicly-available common datasets, some augmentation techniques are necessary to avoid overfitting. We attempt to feed the IterNet model with all possible variations, including color, shape, brightness, and position, to make the model adapt to various imaging sensors, environments, color ranges, etc. We use a training sample generator to consistently produce randomly modified samples during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image patches in Training and Prediction</head><p>It is common to divide an input image into image patches in the same size on a regular grid, which increase the number of available training samples. As IterNet has no requirement on the consistency of the input image sizes. There are three different ways for training and prediction:</p><p>• Use image patches for training and testing, conquering the resulting image patches together as the final result. This strategy may make the best use of the training material and gave the most refined prediction results. However, it will cost much longer time than the other two methods because inference process has to be conducted for many times (see the supplementary material for detailed time cost).  • Use image patches only in training, and use the whole image in prediction to directly get the final result. This strategy can also use augmented training data. However, it may not perform as well as the first strategy in prediction because the model is trained with image patches. • Use the original image in both training or prediction, which is seldom adopted because the available retina data are very limited; therefore, data augmentation usually helps. We employ image patch size of 128 pixels for training Iter-Net to avoid overfitting. For prediction, we test both whole image prediction and image patch prediction. The second row of <ref type="figure" target="#fig_3">Fig. 4</ref> is the field of view (FoV) masks of the retinal images. Although the DRIVE dataset provides official masks for the test images, the other two datasets do not have such masks. In order to assure fair comparison, we also generated the FoV masks for CHASE-DB1 and STARE. This can be easily done by simple color thresholding on the raw images because the pixels out of FoV are usually close to black. In DRIVE dataset, we use 20 images for training and 20 images for testing. In CHASE-DB1 and STARE, there is no official description about training and test splits, so we made 20 and 16 images, respectively, as the training sets, and the remaining 8 and 4 images as the test sets. We do not use any validation images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compared our models with some state-of-the-art ones, including UNet <ref type="bibr">[1]</ref>, DenseBlock-UNet <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, and Deform-UNet <ref type="bibr" target="#b5">[6]</ref>. We trained and evaluated these models using their public code by ourselves on three datasets, because training and test splits are unknown for CHASE-DB1 and STARE and we want to produce the receiver operating characteristics (ROC) curves, which are presented in <ref type="figure" target="#fig_4">Fig. 5</ref>. As can be seen in the figure, in most cases, IterNet shows better performance than the other three models. The performance boost is small as all state-of-the-art models already have good performance (AUC &gt; 0.97). Yet, the results proved that our model gave a stable performance over all three datasets. Among them, IterNet worked well on the STARE dataset, which has fewer training and test samples. This result implies that the IterNet can find the proper features and patterns in the vessel network even with limited training images. In contrast, all other models suffer from a big deterioration in the STARE dataset. Among the other three state-of-the-art models, Deform-UNet usually showed significantly better performance due to its dynamic receptive field. However the STARE dataset decreased its advan-tages over the DenseBlock-UNet because the dense-block module makes the model less prone to overfitting.</p><p>We also compared the results with some existing models, including the aforementioned three UNet-based models, Residual UNet <ref type="bibr" target="#b24">[25]</ref>, Recurrent UNet <ref type="bibr" target="#b24">[25]</ref>, R2UNet <ref type="bibr" target="#b24">[25]</ref>, and one iterate prediction methods, i.e. Iter-Seg <ref type="bibr" target="#b13">[14]</ref>, which have been introduced in Section 2. Only the results of UNet, DenseBlock-UNet, and Deform-UNet were from our reproduced tests, while all other results were adopted from the corresponding papers. The results on the DRIVE dataset are shown in <ref type="table" target="#tab_1">Table 1</ref>. We show results of two variants of IterNet. Both of them use image patches with the size of 128 for training. In prediction, one takes a whole image as input and outputs the final results, while the other (denoted by "patched") uses image patches for both training and prediction, and the resulting vessel maps are concatenated. As we can see, image patch-based prediction brought some improvement while it costs longer running time (see the supplementary material for the detailed time cost). These two variants have shown the superior AUCs to all other models. Actually, they are the only models in our test that have AUCs higher than 0.98.</p><p>We also conducted the comparison experiments on CHASE-DB1 and STARE, which do not have officiallyspecified training and test sets as well as the FoV masks. Therefore, we only compare the results with the results of our reproduced models with the same settings. The results are shown in <ref type="table" target="#tab_2">Tables 2 and 3</ref>. To ensure a fair comparison, we list the performance both with or without FoV masks. It can be seen that the proposed IterNet has the best performance in the most metrics on both datasets. However, all the metrics above are pixel-level and do not reflect the segmentation performance on the vessel network level. Therefore, we adopt a new metric, i.e., connectivity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, which is an important requirement for clinicians to conduct analysis on retinal images using some vesselrelated patterns, such as crossing or branching <ref type="bibr" target="#b27">[28]</ref>. connectivity C is defined as follows.</p><formula xml:id="formula_3">C(θ) =    1 − |S P (θ) − S G | S Max for |S P (θ) − S G | ≤ S Max 0 otherwise (3) where S P (θ)</formula><p>is the number of segments in the predicted segmentation binarized with threshold θ, and S G the number of segments in the gold standard segmentation, respectively. S Max is the maximum number of segments allowed for one vessel map. Since the maximum number of segments involves the total vessel length L, it should be defined according to L, which can be calculated by skeletonizing the gold standard and counting the number of skeleton pixels. We set S Max = αL and we make α = 0.05 in this experiment.</p><p>With this definition, we drew a curve of θ versus C(θ) (refer to the supplementary material for some examples). We adopt the area under this curve as connectivity metric (abbreviated to Conn.). As shown in Tables 1, 2, and 3, IterNet achieved the highest connectivity in all three datasets.</p><p>We present some example results in <ref type="figure" target="#fig_5">Fig. 6</ref>. As we can see, over all three datasets, our IterNet model worked the best. We consider that this is due to deep understanding of vessel networks by IterNet's iterative architecture: It knows how to connect vessel segments together even they look visually disconnected on the raw retinal images.</p><p>As introduced in Section 3, weight-sharing among mini-UNets helps to avoid overfitting in the training process. We conduct an experimental test to see the actual performance  of IterNet without weight-sharing. When N = 1, there is no mini-UNets, the IterNet can be trained as common UNet; when N = 2, the mini-UNet only runs for one time and we get an AUC of 0.9795 on the DRIVE dataset, which is very similar with the performance of Out1 from the Iter-Net with N = 3; while when N ≥ 2, IterNet encounters serious overfitting problems that the loss can reach a low level on the training set while keeps high on the test set. We also conduct an experiment to test the performance of Iter-Net without skip connection, the AUCs respectively drop to 0.9799, 0.9770, 0.9808 on three datasets (refer to the supplementary material for more results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a segmentation model named IterNet to address some existing problems in retinal image segmentation. We use a standard UNet to analyze the raw input images and map them into an initial prediction of the vessel network. In order to remove errors, such as inconsistent vessels, missing pixels, etc., which are very common in existing vessel segmentation models, we add an iteration of mini-UNets after UNet, and use the output of UNet as the input of the following mini-UNets. By introducing weightsharing in mini-UNets and skip-connections, we successfully empower IterNet with the ability to find possible defections in the intermediate results and fix them in a reasonable way. The experimental results prove that the proposed IterNet has achieved state-of-the-art performance over three commonly-used datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks</head><p>(Supplemental Materials) <ref type="figure" target="#fig_0">Figure 1</ref> shows the connectivity value under different threshold for several methods on three popular datasets, i.e., DRIVE <ref type="bibr">[1]</ref>, CHASE-DB1 <ref type="bibr">[2]</ref>, and STARE <ref type="bibr">[3]</ref>. The area under the curve is used as the measurement in the paper. We can see that IterNet almost always outperforms the other three methods.</p><p>Tables 1, 2, and 3 give the results on various criteria for two variants of IterNet. The first one is the IterNet model without skip connections among the first layer of the base UNet and the first layers of the mini-UNets, while the second one is to replace mini-UNets in IterNet with full-size UNets. Results show that they both suffer from a performance drop on all three datasets. <ref type="table">Table 4</ref> shows the detailed time cost in the inference process. We used 128 × 128 image patches and tested different strides (the image patches are extracted every 3 or 8 pixels in both horizontal and vertical directions). We can see that a smaller stride may lead to a better refinement, while it also brings much bigger time cost. <ref type="figure" target="#fig_1">Figures 2, 3</ref>, and 4 present the visualization results of the segments in the prediction results. We can see that IterNet almost consistently produces a smaller number of segments.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>IterNet analyzes the vessel network in a retinal image for fine segmentation. The first row is the whole image and the second row is an enlarged image of an area near the bright spot. Red color means a high possibility for a pixel to be part of the vessel while blue color represents a low possibility. We can see that IterNet well handles incomplete details in the retinal image and infers the possible location of the vessels. (a) An example image from the DRIVE dataset, (b) The gold standard, (c) UNet (AUC: 0.9752), (d) Deform UNet (AUC: 0.9778) and (e) IterNet (AUC: 0.9816).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The result of Out 1, 2, and 3 from IterNet. The corresponding AUCs are 0.9793, 0.9812, and 0.9815, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Raw images and masks from the dataset. (a) DRIVE. (b) CHASE-DB1. (c) STARE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>ROC Curves on Three Datasets (With Masks). (a) DRIVE. (b) CHASE-DB1. (c) STARE.diction, overlapping image patches are extracted with the stride of 3 (we compared the stride of 3 and of 8 in the supplementary material), and we used the average of all overlapping image patches as the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the segmentation results on DRIVE, CHASE-DB1, and STARE datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 .</head><label>1</label><figDesc>Connectivity versus threshold on the three datasets: (a) DRIVE. (b) CHASE-DB1. (c) STARE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Vessel segments visualization of a retina image from DRIVE (when threshold = 110 and the connectivity values are provided for each method in the parentheses). (a) Raw image. (b) Extracted center-line from the ground-truth. (c) UNet (0.7905). (d) DenseNet (0.8282). (e) DUNet (0.8290). (f) IterNet (0.9049). Different colors means different segments. IterNet produces the fewest segments among all these methods. Vessel segments visualization of a retina image from CHASE-DB1 (when threshold = 110 and the connectivity values are provided for each method in the parentheses). (a) Raw image. (b) Extracted center-line from the ground-truth. (c) UNet (0.8085). (d) DenseNet (0.8019). (e) DUNet (0.8423). (f) IterNet (0.9034). IterNet also gives the smallest number of segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Vessel segments visualization of a retina image from STARE (when threshold = 110 and the connectivity values are provided for each method in the parentheses). (a) Raw image. (b) Extracted center-line from the ground-truth. (c) UNet (0.7128). (d) DenseNet (0.7260). (e) DUNet (0.7095). (f) IterNet (0.9035). Different colors mean different segments. Again, IterNet is the best in connectivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the DRIVE dataset (with mask).</figDesc><table><row><cell>Method</cell><cell cols="2">Year Conn.</cell><cell>F1 Score</cell><cell cols="2">Sensitivity Specificity</cell><cell>Accuracy</cell><cell>AUC</cell></row><row><cell cols="5">Iter-Seg [14] UNet(reported [6]) Residual UNet [25] Recurrent UNet [25] 2018 2016 2018 0.7948 0.8174(0.8021) 0.7822(-) --0.739 2018 -0.8149 0.7726 -0.8155 0.7751 R2UNet [25] 2018 -0.8171 0.7792 DenseBlock-UNet 2018 0.8332 0.8146 0.7928 DUNet(reported [6]) 2019 0.8314 0.8190(0.8203) 0.7863(-) IterNet 2019 0.9001 0.8218 0.7791 IterNet(Patched) 2019 0.9193 0.8205 0.7735</cell><cell cols="3">0.978 0.9808(-) 0.9555(0.9681) 0.9752(0.9830) 0.949 0.967 0.9820 0.9553 0.9779 0.9816 0.9556 0.9782 0.9813 0.9556 0.9784 0.9776 0.9541 0.9756 0.9805(-) 0.9558(0.9697) 0.9778(0.9856) 0.9831 0.9574 0.9813 0.9838 0.9573 0.9816</cell></row><row><cell>FoV</cell><cell>Method</cell><cell cols="6">Table 2. Performance comparison on the CHASE-DB1 dataset. Year Conn. F1 Score Sensitivity Specificity Accuracy</cell><cell>AUC</cell></row><row><cell>Without Masks With Masks</cell><cell cols="3">UNet DenseBlock-UNet 2018 0.8269 2018 0.8198 DUNet 2019 0.8402 IterNet 2019 0.9091 UNet 2018 0.8198 DenseBlock-UNet 2018 0.8269 DUNet 2019 0.8402 IterNet 2019 0.9091</cell><cell>0.7993 0.8005 0.8000 0.8072 0.7993 0.8006 0.8001 0.8073</cell><cell>0.7840 0.8177 0.7858 0.7969 0.7841 0.8178 0.7859 0.7970</cell><cell>0.9880 0.9848 0.9880 0.9881 0.9823 0.9775 0.9822 0.9823</cell><cell>0.9752 0.9743 0.9752 0.9760 0.9643 0.9631 0.9644 0.9655</cell><cell>0.9870 0.9880 0.9887 0.9899 0.9812 0.9826 0.9834 0.9851</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on the STARE dataset.</figDesc><table><row><cell>FoV</cell><cell>Method</cell><cell cols="5">Year Conn. F1 Score Sensitivity Specificity Accuracy</cell><cell>AUC</cell></row><row><cell>Without Masks With Masks</cell><cell cols="2">UNet DenseBlock-UNet 2018 0.7229 2018 0.7148 DUNet 2019 0.7479 IterNet 2019 0.8977 UNet 2018 0.7148 DenseBlock-UNet 2018 0.7229 DUNet 2019 0.7479 IterNet 2019 0.8977</cell><cell>0.7594 0.7691 0.7629 0.8146 0.7595 0.7691 0.7629 0.8146</cell><cell>0.6681 0.6807 0.6810 0.7715 0.6681 0.6807 0.6810 0.7715</cell><cell>0.9939 0.9940 0.9931 0.9919 0.9915 0.9916 0.9903 0.9886</cell><cell>0.9736 0.9745 0.9736 0.9782 0.9639 0.9651 0.9639 0.9701</cell><cell>0.9779 0.9801 0.9823 0.9915 0.9710 0.9755 0.9758 0.9881</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .Table 2 .Table 3 .Table 4 .</head><label>1234</label><figDesc>Performance comparison on the DRIVE dataset (with mask). Performance comparison on the CHASEDB1 dataset (with mask). Performance comparison on the STARE dataset (with mask). Time costs for prediction of one image using IterNet with and without cropping.</figDesc><table><row><cell>Method IterNet w/o Skip Connection 0.9106 Conn. F1 Score Sensitivity Specificity Accuracy 0.9193 0.8205 0.7735 0.9838 0.9573 0.8160 0.7659 0.9839 0.9565 Iterated UNets 0.8893 0.8123 0.7575 0.9845 0.9559</cell><cell>AUC 0.9816 0.9799 0.9794</cell></row><row><cell>Method IterNet w/o Skip Connection 0.8920 Conn. F1 Score Sensitivity Specificity Accuracy 0.9091 0.8073 0.7970 0.9823 0.9655 0.7647 0.7001 0.9870 0.9610 Iterated UNets 0.8773 0.7997 0.7670 0.9849 0.9652</cell><cell>AUC 0.9851 0.9770 0.9845</cell></row><row><cell>Method IterNet w/o Skip Connection 0.8967 Conn. F1 Score Sensitivity Specificity Accuracy 0.8977 0.8146 0.7715 0.9886 0.9701 0.7482 0.6494 0.9920 0.9628 Iterated UNets 0.8977 0.7641 0.6764 0.9913 0.9645</cell><cell>AUC 0.9881 0.9808 0.9830</cell></row><row><cell cols="2">Method w. Image Patch (Stride 3) 8.55s Read Load Model Crop Pred (Patches) Combine Write SUM 2.51s 2.94s 58.45s (22801) 1.03s 0.01s 73.49s 0.9816 AUC w. Image Patch (Stride 8) 8.56s 2.50s 0.43s 10.49s (3249) 0.16s 0.01s 22.15s 0.9815 w. Whole Image. Crop 8.56s 2.50s -0.01 (1) -0.01s 11.08s 0.9813</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by Council for Science, Technology and Innovation (CSTI), cross-ministerial Strategic Innovation Promotion Program (SIP), "Innovative AI Hospital System" (Funding Agency: National Institute of Biomedical Innovation, Health and Nutrition (NIBIOHN)).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The value of fundoscopy in general practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Chatziralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Kanonidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Keryttopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Papazisis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Open Ophthalmology Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4" to="5" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ridge based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the computer-assisted image analysis of the retina (CAIAR) program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Whincup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative Ophthalmology &amp; Visual Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DUNet: A deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new tool to connect blood vessels in fundus retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Calivãą</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Al-Diri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4343" to="4346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="456" to="470" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UI-Net: Interactive artificial neural networks for iterative image segmentation based on a user model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amrehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gaube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schebesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strumia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowarschik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Workshop on Visual Computing for Biology and Medicine</title>
		<meeting>the Eurographics Workshop on Visual Computing for Biology and Medicine</meeting>
		<imprint>
			<publisher>VCBM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="143" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning normalized inputs for iterative estimation in medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative vessel segmentation of fundus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Koozekanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1738" to="1749" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint segment-level and pixel-wise losses for deep learning based retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1912" to="1923" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NAS-Unet: Neural architecture search for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="44" to="247" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weighted Res-UNet for high-quality retina vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probability map guided bi-directional recurrent UNet for pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno>abs/1903.00923</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient skin lesion segmentation using separable-Unet with stochastic weight averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coppola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Iterative deep convolutional encoder-decoder network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="685" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully dense UNet for 2D sparse photoacoustic tomography artifact removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sikdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chitnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">H-DenseUNet: Hybrid densely connected UNet for liver and tumor segmentation from CT volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on U-Net (R2U-Net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation algorithms âȂŤ review of methods, datasets and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Momi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Hadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Mattos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="71" to="91" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A function for quality evaluation of retinal vessel segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegundez-Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal changes in retinal vascular parameters associated with successful panretinal photocoagulation in proliferative diabetic retinopathy: A prospective clinical interventional study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Torp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grauslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Ophthalmologica</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="410" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ridge based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the computer-assisted image analysis of the retina (CA-IAR) program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Whincup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative Ophthalmology &amp; Visual Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
